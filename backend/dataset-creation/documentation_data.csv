id,file_path,function_signature,body,parameters,examples
0,..\pandas\reference\api\pandas.Series.cat.reorder_categories.html,pandas.Series.cat.reorder_categories,"Series.cat.reorder_categories(*args, **kwargs)[source]# Reorder categories as specified in new_categories. new_categories need to include all old categories and no new category items.","Parameters: new_categoriesIndex-likeThe categories in new order. orderedbool, optionalWhether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information. Returns: CategoricalCategorical with reordered categories. Raises: ValueErrorIf the new categories do not contain all old category items or any new ones","["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser = ser.cat.reorder_categories(['c', 'b', 'a'], ordered=True)\n>>> ser\n0   a\n1   b\n2   c\n3   a\ndtype: category\nCategories (3, object): ['c' < 'b' < 'a']"", "">>> ser.sort_values()\n2   c\n1   b\n0   a\n3   a\ndtype: category\nCategories (3, object): ['c' < 'b' < 'a']"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a'])\n>>> ci\nCategoricalIndex(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c'],\n                 ordered=False, dtype='category')\n>>> ci.reorder_categories(['c', 'b', 'a'], ordered=True)\nCategoricalIndex(['a', 'b', 'c', 'a'], categories=['c', 'b', 'a'],\n                 ordered=True, dtype='category')""]"
1,..\pandas\reference\api\pandas.Index.union.html,pandas.Index.union,"final Index.union(other, sort=None)[source]# Form the union of two Index objects. If the Index objects are incompatible, both Index objects will be cast to dtype(‘object’) first.","Parameters: otherIndex or array-like sortbool or None, default NoneWhether to sort the resulting Index. None : Sort the result, except when self and other are equal. self or other has length 0. Some values in self or other cannot be compared. A RuntimeWarning is issued in this case. False : do not sort the result. True : Sort the result (which may raise TypeError). Returns: Index","["">>> idx1 = pd.Index([1, 2, 3, 4])\n>>> idx2 = pd.Index([3, 4, 5, 6])\n>>> idx1.union(idx2)\nIndex([1, 2, 3, 4, 5, 6], dtype='int64')"", "">>> idx1 = pd.Index(['a', 'b', 'c', 'd'])\n>>> idx2 = pd.Index([1, 2, 3, 4])\n>>> idx1.union(idx2)\nIndex(['a', 'b', 'c', 'd', 1, 2, 3, 4], dtype='object')"", '>>> idx1 = pd.MultiIndex.from_arrays(\n...     [[1, 1, 2, 2], [""Red"", ""Blue"", ""Red"", ""Blue""]]\n... )\n>>> idx1\nMultiIndex([(1,  \'Red\'),\n    (1, \'Blue\'),\n    (2,  \'Red\'),\n    (2, \'Blue\')],\n   )\n>>> idx2 = pd.MultiIndex.from_arrays(\n...     [[3, 3, 2, 2], [""Red"", ""Green"", ""Red"", ""Green""]]\n... )\n>>> idx2\nMultiIndex([(3,   \'Red\'),\n    (3, \'Green\'),\n    (2,   \'Red\'),\n    (2, \'Green\')],\n   )\n>>> idx1.union(idx2)\nMultiIndex([(1,  \'Blue\'),\n    (1,   \'Red\'),\n    (2,  \'Blue\'),\n    (2, \'Green\'),\n    (2,   \'Red\'),\n    (3, \'Green\'),\n    (3,   \'Red\')],\n   )\n>>> idx1.union(idx2, sort=False)\nMultiIndex([(1,   \'Red\'),\n    (1,  \'Blue\'),\n    (2,   \'Red\'),\n    (2,  \'Blue\'),\n    (3,   \'Red\'),\n    (3, \'Green\'),\n    (2, \'Green\')],\n   )']"
2,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.argsort.html,pandas.api.extensions.ExtensionArray.argsort,"ExtensionArray.argsort(*, ascending=True, kind='quicksort', na_position='last', **kwargs)[source]# Return the indices that would sort this array.","Parameters: ascendingbool, default TrueWhether the indices should result in an ascending or descending sort. kind{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, optionalSorting algorithm. na_position{‘first’, ‘last’}, default ‘last’If 'first', put NaN values at the beginning. If 'last', put NaN values at the end. *args, **kwargs:Passed through to numpy.argsort(). Returns: np.ndarray[np.intp]Array of indices that sort self. If NaN values are contained, NaN values are placed at the end.","['>>> arr = pd.array([3, 1, 2, 5, 4])\n>>> arr.argsort()\narray([1, 2, 0, 4, 3])']"
3,..\pandas\reference\api\pandas.DataFrame.reset_index.html,pandas.DataFrame.reset_index,"DataFrame.reset_index(level=None, *, drop=False, inplace=False, col_level=0, col_fill='', allow_duplicates=<no_default>, names=None)[source]# Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels.","Parameters: levelint, str, tuple, or list, default NoneOnly remove the given levels from the index. Removes all levels by default. dropbool, default FalseDo not try to insert index into dataframe columns. This resets the index to the default integer index. inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one. col_levelint or str, default 0If the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level. col_fillobject, default ‘’If the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated. allow_duplicatesbool, optional, default lib.no_defaultAllow duplicate column labels to be created. Added in version 1.5.0. namesint, str or 1-dimensional list, default NoneUsing the given string, rename the DataFrame column which contains the index data. If the DataFrame has a MultiIndex, this has to be a list or tuple with length equal to the number of levels. Added in version 1.5.0. Returns: DataFrame or NoneDataFrame with the new index or None if inplace=True.","["">>> df = pd.DataFrame([('bird', 389.0),\n...                    ('bird', 24.0),\n...                    ('mammal', 80.5),\n...                    ('mammal', np.nan)],\n...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n...                   columns=('class', 'max_speed'))\n>>> df\n         class  max_speed\nfalcon    bird      389.0\nparrot    bird       24.0\nlion    mammal       80.5\nmonkey  mammal        NaN"", '>>> df.reset_index()\n    index   class  max_speed\n0  falcon    bird      389.0\n1  parrot    bird       24.0\n2    lion  mammal       80.5\n3  monkey  mammal        NaN', '>>> df.reset_index(drop=True)\n    class  max_speed\n0    bird      389.0\n1    bird       24.0\n2  mammal       80.5\n3  mammal        NaN', "">>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n...                                    ('bird', 'parrot'),\n...                                    ('mammal', 'lion'),\n...                                    ('mammal', 'monkey')],\n...                                   names=['class', 'name'])\n>>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n...                                      ('species', 'type')])\n>>> df = pd.DataFrame([(389.0, 'fly'),\n...                    (24.0, 'fly'),\n...                    (80.5, 'run'),\n...                    (np.nan, 'jump')],\n...                   index=index,\n...                   columns=columns)\n>>> df\n               speed species\n                 max    type\nclass  name\nbird   falcon  389.0     fly\n       parrot   24.0     fly\nmammal lion     80.5     run\n       monkey    NaN    jump"", "">>> df.reset_index(names=['classes', 'names'])\n  classes   names  speed species\n                     max    type\n0    bird  falcon  389.0     fly\n1    bird  parrot   24.0     fly\n2  mammal    lion   80.5     run\n3  mammal  monkey    NaN    jump"", "">>> df.reset_index(level='class')\n         class  speed species\n                  max    type\nname\nfalcon    bird  389.0     fly\nparrot    bird   24.0     fly\nlion    mammal   80.5     run\nmonkey  mammal    NaN    jump"", "">>> df.reset_index(level='class', col_level=1)\n                speed species\n         class    max    type\nname\nfalcon    bird  389.0     fly\nparrot    bird   24.0     fly\nlion    mammal   80.5     run\nmonkey  mammal    NaN    jump"", "">>> df.reset_index(level='class', col_level=1, col_fill='species')\n              species  speed species\n                class    max    type\nname\nfalcon           bird  389.0     fly\nparrot           bird   24.0     fly\nlion           mammal   80.5     run\nmonkey         mammal    NaN    jump"", "">>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                genus  speed species\n                class    max    type\nname\nfalcon           bird  389.0     fly\nparrot           bird   24.0     fly\nlion           mammal   80.5     run\nmonkey         mammal    NaN    jump""]"
4,..\pandas\reference\api\pandas.Series.cat.set_categories.html,pandas.Series.cat.set_categories,"Series.cat.set_categories(*args, **kwargs)[source]# Set the categories to the specified new categories. new_categories can include new categories (which will result in unused categories) or remove old categories (which results in values set to NaN). If rename=True, the categories will simply be renamed (less or more items than in old categories will result in values set to NaN or in unused categories respectively). This method can be used to perform more than one action of adding, removing, and reordering simultaneously and is therefore faster than performing the individual steps via the more specialised methods. On the other hand this methods does not do checks (e.g., whether the old categories are included in the new categories on a reorder), which can result in surprising changes, for example when using special string dtypes, which does not considers a S1 string equal to a single char python string.","Parameters: new_categoriesIndex-likeThe categories in new order. orderedbool, default FalseWhether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information. renamebool, default FalseWhether or not the new_categories should be considered as a rename of the old categories or as reordered categories. Returns: Categorical with reordered categories. Raises: ValueErrorIf new_categories does not validate as categories","["">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'A'],\n...                           categories=['a', 'b', 'c'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser\n0   a\n1   b\n2   c\n3   NaN\ndtype: category\nCategories (3, object): ['a' < 'b' < 'c']"", "">>> ser.cat.set_categories(['A', 'B', 'C'], rename=True)\n0   A\n1   B\n2   C\n3   NaN\ndtype: category\nCategories (3, object): ['A' < 'B' < 'C']"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'A'],\n...                          categories=['a', 'b', 'c'], ordered=True)\n>>> ci\nCategoricalIndex(['a', 'b', 'c', nan], categories=['a', 'b', 'c'],\n                 ordered=True, dtype='category')"", "">>> ci.set_categories(['A', 'b', 'c'])\nCategoricalIndex([nan, 'b', 'c', nan], categories=['A', 'b', 'c'],\n                 ordered=True, dtype='category')\n>>> ci.set_categories(['A', 'b', 'c'], rename=True)\nCategoricalIndex(['A', 'b', 'c', nan], categories=['A', 'b', 'c'],\n                 ordered=True, dtype='category')""]"
5,..\pandas\reference\api\pandas.Series.to_csv.html,pandas.Series.to_csv,"Series.to_csv(path_or_buf=None, *, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='""', lineterminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)[source]# Write object to a comma-separated values (csv) file.","Parameters: path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=’’, disabling universal newlines. If a binary file object is passed, mode might need to contain a ‘b’. sepstr, default ‘,’String of length 1. Field delimiter for the output file. na_repstr, default ‘’Missing data representation. float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes precedence over other numeric formatting parameters, like decimal. columnssequence, optionalColumns to write. headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode{‘w’, ‘x’, ‘a’}, default ‘w’Forwarded to either open(mode=) or fsspec.open(mode=) to control the file opening. Typical values include: ‘w’, truncate the file first. ‘x’, exclusive creation, failing if the file already exists. ‘a’, append to the end of file if it exists. encodingstr, optionalA string representing the encoding to use in the output file, defaults to ‘utf-8’. encoding is not supported if path_or_buf is a non-binary file object. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. May be a dict with key ‘method’ as compression mode and other entries as additional compression options if compression mode is ‘zip’. Passing compression options as keys in dict is supported for compression modes ‘gzip’, ‘bz2’, ‘zstd’, and ‘zip’. quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotecharstr, default ‘""’String of length 1. Character used to quote fields. lineterminatorstr, optionalThe newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (’\n’ for linux, ‘\r\n’ for Windows, i.e.). Changed in version 1.5.0: Previously was line_terminator, changed for consistency with read_csv and the standard library ‘csv’ module. chunksizeint or NoneRows to write at a time. date_formatstr, default NoneFormat string for datetime objects. doublequotebool, default TrueControl quoting of quotechar inside a field. escapecharstr, default NoneString of length 1. Character used to escape sep and quotechar when appropriate. decimalstr, default ‘.’Character recognized as decimal separator. E.g. use ‘,’ for European data. errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Returns: None or strIf path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.","["">>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv('out.csv', index=False)"", "">>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  \n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)"", "">>> from pathlib import Path  \n>>> filepath = Path('folder/subfolder/out.csv')  \n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  \n>>> df.to_csv(filepath)"", "">>> import os  \n>>> os.makedirs('folder/subfolder', exist_ok=True)  \n>>> df.to_csv('folder/subfolder/out.csv')""]"
6,..\pandas\reference\api\pandas.tseries.offsets.Milli.delta.html,pandas.tseries.offsets.Milli.delta,Milli.delta#,No parameters found,[]
7,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.ngroup.html,pandas.core.groupby.SeriesGroupBy.ngroup,"SeriesGroupBy.ngroup(ascending=True)[source]# Number each group from 0 to the number of groups - 1. This is the enumerative complement of cumcount.  Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed. Groups with missing keys (where pd.isna() is True) will be labeled with NaN and will be skipped from the count.","Parameters: ascendingbool, default TrueIf False, number in reverse, from number of group - 1 to 0. Returns: SeriesUnique numbers for each group.","['>>> df = pd.DataFrame({""color"": [""red"", None, ""red"", ""blue"", ""blue"", ""red""]})\n>>> df\n   color\n0    red\n1   None\n2    red\n3   blue\n4   blue\n5    red\n>>> df.groupby(""color"").ngroup()\n0    1.0\n1    NaN\n2    1.0\n3    0.0\n4    0.0\n5    1.0\ndtype: float64\n>>> df.groupby(""color"", dropna=False).ngroup()\n0    1\n1    2\n2    1\n3    0\n4    0\n5    1\ndtype: int64\n>>> df.groupby(""color"", dropna=False).ngroup(ascending=False)\n0    1\n1    0\n2    1\n3    2\n4    2\n5    1\ndtype: int64']"
8,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.freqstr.html,pandas.tseries.offsets.BYearBegin.freqstr,BYearBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
9,..\pandas\reference\api\pandas.Index.unique.html,pandas.Index.unique,"Index.unique(level=None)[source]# Return unique values in the index. Unique values are returned in order of appearance, this does NOT sort.","Parameters: levelint or hashable, optionalOnly return values from specified level (for MultiIndex). If int, gets the level by integer position, else by level name. Returns: Index","["">>> idx = pd.Index([1, 1, 2, 3, 3])\n>>> idx.unique()\nIndex([1, 2, 3], dtype='int64')""]"
10,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.astype.html,pandas.api.extensions.ExtensionArray.astype,"ExtensionArray.astype(dtype, copy=True)[source]# Cast to a NumPy array or ExtensionArray with ‘dtype’.","Parameters: dtypestr or dtypeTypecode or data-type to which the array is cast. copybool, default TrueWhether to copy the data, even if not necessary. If False, a copy is made only if the old dtype does not match the new dtype. Returns: np.ndarray or pandas.api.extensions.ExtensionArrayAn ExtensionArray if dtype is ExtensionDtype, otherwise a Numpy ndarray with dtype for its dtype.","['>>> arr = pd.array([1, 2, 3])\n>>> arr\n<IntegerArray>\n[1, 2, 3]\nLength: 3, dtype: Int64', "">>> arr1 = arr.astype('Float64')\n>>> arr1\n<FloatingArray>\n[1.0, 2.0, 3.0]\nLength: 3, dtype: Float64\n>>> arr1.dtype\nFloat64Dtype()"", "">>> arr2 = arr.astype('float64')\n>>> arr2\narray([1., 2., 3.])\n>>> arr2.dtype\ndtype('float64')""]"
11,..\pandas\reference\api\pandas.DataFrame.rfloordiv.html,pandas.DataFrame.rfloordiv,"DataFrame.rfloordiv(other, axis='columns', level=None, fill_value=None)[source]# Get Integer division of dataframe and other, element-wise (binary operator rfloordiv). Equivalent to other // dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, floordiv. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
12,..\pandas\reference\api\pandas.Series.clip.html,pandas.Series.clip,"Series.clip(lower=None, upper=None, *, axis=None, inplace=False, **kwargs)[source]# Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.","Parameters: lowerfloat or array-like, default NoneMinimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value. upperfloat or array-like, default NoneMaximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value. axis{{0 or ‘index’, 1 or ‘columns’, None}}, default NoneAlign object with lower and upper along the given axis. For Series this parameter is unused and defaults to None. inplacebool, default FalseWhether to perform the operation in place on the data. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with numpy. Returns: Series or DataFrame or NoneSame type as calling object with the values outside the clip boundaries replaced or None if inplace=True.","["">>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n>>> df = pd.DataFrame(data)\n>>> df\n   col_0  col_1\n0      9     -2\n1     -3     -7\n2      0      6\n3     -1      8\n4      5     -5"", '>>> df.clip(-4, 6)\n   col_0  col_1\n0      6     -2\n1     -3     -4\n2      0      6\n3     -1      6\n4      5     -4', '>>> df.clip([-2, -1], [4, 5])\n    col_0  col_1\n0      4     -1\n1     -2     -1\n2      0      5\n3     -1      5\n4      4     -1', '>>> t = pd.Series([2, -4, -1, 6, 3])\n>>> t\n0    2\n1   -4\n2   -1\n3    6\n4    3\ndtype: int64', '>>> df.clip(t, t + 4, axis=0)\n   col_0  col_1\n0      6      2\n1     -3     -4\n2      0      3\n3      6      8\n4      5      3', '>>> t = pd.Series([2, -4, np.nan, 6, 3])\n>>> t\n0    2.0\n1   -4.0\n2    NaN\n3    6.0\n4    3.0\ndtype: float64', '>>> df.clip(t, axis=0)\ncol_0  col_1\n0      9      2\n1     -3     -4\n2      0      6\n3      6      8\n4      5      3']"
13,..\pandas\reference\api\pandas.Series.to_dict.html,pandas.Series.to_dict,"Series.to_dict(*, into=<class 'dict'>)[source]# Convert Series to {label -> value} dict or dict-like object.","Parameters: intoclass, default dictThe collections.abc.MutableMapping subclass to use as the return object. Can be the actual class or an empty instance of the mapping type you want.  If you want a collections.defaultdict, you must pass it initialized. Returns: collections.abc.MutableMappingKey-value representation of Series.","["">>> s = pd.Series([1, 2, 3, 4])\n>>> s.to_dict()\n{0: 1, 1: 2, 2: 3, 3: 4}\n>>> from collections import OrderedDict, defaultdict\n>>> s.to_dict(into=OrderedDict)\nOrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n>>> dd = defaultdict(list)\n>>> s.to_dict(into=dd)\ndefaultdict(<class 'list'>, {0: 1, 1: 2, 2: 3, 3: 4})""]"
14,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.copy.html,pandas.api.extensions.ExtensionArray.copy,ExtensionArray.copy()[source]# Return a copy of the array.,Returns: ExtensionArray,"['>>> arr = pd.array([1, 2, 3])\n>>> arr2 = arr.copy()\n>>> arr[0] = 2\n>>> arr2\n<IntegerArray>\n[1, 2, 3]\nLength: 3, dtype: Int64']"
15,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.html,pandas.tseries.offsets.BYearBegin,class pandas.tseries.offsets.BYearBegin# DateOffset increments between the first business day of the year. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. month n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of years represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. monthint, default 1A specific integer for the month of the year.","["">>> from pandas.tseries.offsets import BYearBegin\n>>> ts = pd.Timestamp('2020-05-24 05:01:15')\n>>> ts + BYearBegin()\nTimestamp('2021-01-01 05:01:15')\n>>> ts - BYearBegin()\nTimestamp('2020-01-01 05:01:15')\n>>> ts + BYearBegin(-1)\nTimestamp('2020-01-01 05:01:15')\n>>> ts + BYearBegin(2)\nTimestamp('2022-01-03 05:01:15')\n>>> ts + BYearBegin(month=11)\nTimestamp('2020-11-02 05:01:15')""]"
16,..\pandas\reference\api\pandas.Index.values.html,pandas.Index.values,"property Index.values[source]# Return an array representing the data in the Index. Warning We recommend using Index.array or Index.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array.",Returns: array: numpy.ndarray or ExtensionArray,"["">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.values\narray([1, 2, 3])"", '>>> idx = pd.interval_range(start=0, end=5)\n>>> idx.values\n<IntervalArray>\n[(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]]\nLength: 5, dtype: interval[int64, right]']"
17,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.nlargest.html,pandas.core.groupby.SeriesGroupBy.nlargest,"SeriesGroupBy.nlargest(n=5, keep='first')[source]# Return the largest n elements. Notes Faster than .sort_values(ascending=False).head(n) for small n relative to the size of the Series object.","Parameters: nint, default 5Return this many descending sorted values. keep{‘first’, ‘last’, ‘all’}, default ‘first’When there are duplicate values that cannot all fit in a Series of n elements: first : return the first n occurrences in order of appearance. last : return the last n occurrences in reverse order of appearance. all : keep all occurrences. This can result in a Series of size larger than n. Returns: SeriesThe n largest values in the Series, sorted in decreasing order.","['>>> countries_population = {""Italy"": 59000000, ""France"": 65000000,\n...                         ""Malta"": 434000, ""Maldives"": 434000,\n...                         ""Brunei"": 434000, ""Iceland"": 337000,\n...                         ""Nauru"": 11300, ""Tuvalu"": 11300,\n...                         ""Anguilla"": 11300, ""Montserrat"": 5200}\n>>> s = pd.Series(countries_population)\n>>> s\nItaly       59000000\nFrance      65000000\nMalta         434000\nMaldives      434000\nBrunei        434000\nIceland       337000\nNauru          11300\nTuvalu         11300\nAnguilla       11300\nMontserrat      5200\ndtype: int64', '>>> s.nlargest()\nFrance      65000000\nItaly       59000000\nMalta         434000\nMaldives      434000\nBrunei        434000\ndtype: int64', '>>> s.nlargest(3)\nFrance    65000000\nItaly     59000000\nMalta       434000\ndtype: int64', "">>> s.nlargest(3, keep='last')\nFrance      65000000\nItaly       59000000\nBrunei        434000\ndtype: int64"", "">>> s.nlargest(3, keep='all')\nFrance      65000000\nItaly       59000000\nMalta         434000\nMaldives      434000\nBrunei        434000\ndtype: int64""]"
18,..\pandas\reference\api\pandas.DataFrame.rmod.html,pandas.DataFrame.rmod,"DataFrame.rmod(other, axis='columns', level=None, fill_value=None)[source]# Get Modulo of dataframe and other, element-wise (binary operator rmod). Equivalent to other % dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mod. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
19,..\pandas\reference\api\pandas.tseries.offsets.Milli.freqstr.html,pandas.tseries.offsets.Milli.freqstr,Milli.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
20,..\pandas\reference\api\pandas.Series.combine.html,pandas.Series.combine,"Series.combine(other, func, fill_value=None)[source]# Combine the Series with a Series or scalar according to func. Combine the Series and other using func to perform elementwise selection for combined Series. fill_value is assumed when value is missing at some index from one of the two objects being combined.","Parameters: otherSeries or scalarThe value(s) to be combined with the Series. funcfunctionFunction that takes two scalars as inputs and returns an element. fill_valuescalar, optionalThe value to assume when an index is missing from one Series or the other. The default specifies to use the appropriate NaN value for the underlying dtype of the Series. Returns: SeriesThe result of combining the Series with the other object.","["">>> s1 = pd.Series({'falcon': 330.0, 'eagle': 160.0})\n>>> s1\nfalcon    330.0\neagle     160.0\ndtype: float64\n>>> s2 = pd.Series({'falcon': 345.0, 'eagle': 200.0, 'duck': 30.0})\n>>> s2\nfalcon    345.0\neagle     200.0\nduck       30.0\ndtype: float64"", '>>> s1.combine(s2, max)\nduck        NaN\neagle     200.0\nfalcon    345.0\ndtype: float64', '>>> s1.combine(s2, max, fill_value=0)\nduck       30.0\neagle     200.0\nfalcon    345.0\ndtype: float64']"
21,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.dropna.html,pandas.api.extensions.ExtensionArray.dropna,ExtensionArray.dropna()[source]# Return ExtensionArray without NA values.,Returns:,"['>>> pd.array([1, 2, np.nan]).dropna()\n<IntegerArray>\n[1, 2]\nLength: 2, dtype: Int64']"
22,..\pandas\reference\api\pandas.Series.to_excel.html,pandas.Series.to_excel,"Series.to_excel(excel_writer, *, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, inf_rep='inf', freeze_panes=None, storage_options=None, engine_kwargs=None)[source]# Write object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased. Notes For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook.","Parameters: excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter. sheet_namestr, default ‘Sheet1’Name of sheet which will contain DataFrame. na_repstr, default ‘’Missing data representation. float_formatstr, optionalFormat string for floating point numbers. For example float_format=""%.2f"" will format 0.1234 to 0.12. columnssequence or list of str, optionalColumns to write. headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrowint, default 0Upper left cell row to dump data frame. startcolint, default 0Upper left cell column to dump data frame. enginestr, optionalWrite engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this via the options io.excel.xlsx.writer or io.excel.xlsm.writer. merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells. inf_repstr, default ‘inf’Representation for infinity (there is no native representation for infinity in Excel). freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that is to be frozen. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Added in version 1.2.0. engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.","['>>> df1 = pd.DataFrame([[\'a\', \'b\'], [\'c\', \'d\']],\n...                    index=[\'row 1\', \'row 2\'],\n...                    columns=[\'col 1\', \'col 2\'])\n>>> df1.to_excel(""output.xlsx"")', '>>> df1.to_excel(""output.xlsx"",\n...              sheet_name=\'Sheet_name_1\')', "">>> df2 = df1.copy()\n>>> with pd.ExcelWriter('output.xlsx') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n...     df2.to_excel(writer, sheet_name='Sheet_name_2')"", "">>> with pd.ExcelWriter('output.xlsx',\n...                     mode='a') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_3')"", "">>> df1.to_excel('output1.xlsx', engine='xlsxwriter')""]"
23,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_anchored.html,pandas.tseries.offsets.BYearBegin.is_anchored,BYearBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
24,..\pandas\reference\api\pandas.Index.value_counts.html,pandas.Index.value_counts,"Index.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]# Return a Series containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.","Parameters: normalizebool, default FalseIf True then the object returned will contain the relative frequencies of the unique values. sortbool, default TrueSort by frequencies when True. Preserve the order of the data when False. ascendingbool, default FalseSort in ascending order. binsint, optionalRather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data. dropnabool, default TrueDon’t include counts of NaN. Returns: Series","['>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n>>> index.value_counts()\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nName: count, dtype: int64', '>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n>>> s.value_counts(normalize=True)\n3.0    0.4\n1.0    0.2\n2.0    0.2\n4.0    0.2\nName: proportion, dtype: float64', '>>> s.value_counts(bins=3)\n(0.996, 2.0]    2\n(2.0, 3.0]      2\n(3.0, 4.0]      1\nName: count, dtype: int64', '>>> s.value_counts(dropna=False)\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nNaN    1\nName: count, dtype: int64']"
25,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.nsmallest.html,pandas.core.groupby.SeriesGroupBy.nsmallest,"SeriesGroupBy.nsmallest(n=5, keep='first')[source]# Return the smallest n elements. Notes Faster than .sort_values().head(n) for small n relative to the size of the Series object.","Parameters: nint, default 5Return this many ascending sorted values. keep{‘first’, ‘last’, ‘all’}, default ‘first’When there are duplicate values that cannot all fit in a Series of n elements: first : return the first n occurrences in order of appearance. last : return the last n occurrences in reverse order of appearance. all : keep all occurrences. This can result in a Series of size larger than n. Returns: SeriesThe n smallest values in the Series, sorted in increasing order.","['>>> countries_population = {""Italy"": 59000000, ""France"": 65000000,\n...                         ""Brunei"": 434000, ""Malta"": 434000,\n...                         ""Maldives"": 434000, ""Iceland"": 337000,\n...                         ""Nauru"": 11300, ""Tuvalu"": 11300,\n...                         ""Anguilla"": 11300, ""Montserrat"": 5200}\n>>> s = pd.Series(countries_population)\n>>> s\nItaly       59000000\nFrance      65000000\nBrunei        434000\nMalta         434000\nMaldives      434000\nIceland       337000\nNauru          11300\nTuvalu         11300\nAnguilla       11300\nMontserrat      5200\ndtype: int64', '>>> s.nsmallest()\nMontserrat    5200\nNauru        11300\nTuvalu       11300\nAnguilla     11300\nIceland     337000\ndtype: int64', '>>> s.nsmallest(3)\nMontserrat   5200\nNauru       11300\nTuvalu      11300\ndtype: int64', "">>> s.nsmallest(3, keep='last')\nMontserrat   5200\nAnguilla    11300\nTuvalu      11300\ndtype: int64"", "">>> s.nsmallest(3, keep='all')\nMontserrat   5200\nNauru       11300\nTuvalu      11300\nAnguilla    11300\ndtype: int64""]"
26,..\pandas\reference\api\pandas.DataFrame.rmul.html,pandas.DataFrame.rmul,"DataFrame.rmul(other, axis='columns', level=None, fill_value=None)[source]# Get Multiplication of dataframe and other, element-wise (binary operator rmul). Equivalent to other * dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mul. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
27,..\pandas\reference\api\pandas.Series.combine_first.html,pandas.Series.combine_first,Series.combine_first(other)[source]# Update null elements with value in the same location in ‘other’. Combine two Series objects by filling null values in one Series with non-null values from the other Series. Result index will be the union of the two indexes.,Parameters: otherSeriesThe value(s) to be used for filling null values. Returns: SeriesThe result of combining the provided Series with the other object.,"['>>> s1 = pd.Series([1, np.nan])\n>>> s2 = pd.Series([3, 4, 5])\n>>> s1.combine_first(s2)\n0    1.0\n1    4.0\n2    5.0\ndtype: float64', "">>> s1 = pd.Series({'falcon': np.nan, 'eagle': 160.0})\n>>> s2 = pd.Series({'eagle': 200.0, 'duck': 30.0})\n>>> s1.combine_first(s2)\nduck       30.0\neagle     160.0\nfalcon      NaN\ndtype: float64""]"
28,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_month_end.html,pandas.tseries.offsets.BYearBegin.is_month_end,BYearBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
29,..\pandas\reference\api\pandas.Series.to_frame.html,pandas.Series.to_frame,Series.to_frame(name=<no_default>)[source]# Convert Series to DataFrame.,"Parameters: nameobject, optionalThe passed name should substitute for the series name (if it has one). Returns: DataFrameDataFrame representation of Series.","['>>> s = pd.Series([""a"", ""b"", ""c""],\n...               name=""vals"")\n>>> s.to_frame()\n  vals\n0    a\n1    b\n2    c']"
30,..\pandas\reference\api\pandas.Index.view.html,pandas.Index.view,Index.view(cls=None)[source]#,No parameters found,[]
31,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.dtype.html,pandas.api.extensions.ExtensionArray.dtype,property ExtensionArray.dtype[source]# An instance of ExtensionDtype.,No parameters found,"['>>> pd.array([1, 2, 3]).dtype\nInt64Dtype()']"
32,..\pandas\reference\api\pandas.tseries.offsets.Milli.html,pandas.tseries.offsets.Milli,class pandas.tseries.offsets.Milli# Offset n milliseconds. Examples You can use the parameter n to represent a shift of n milliseconds. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of milliseconds represented.","["">>> from pandas.tseries.offsets import Milli\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Milli(n=10)\nTimestamp('2022-12-09 15:00:00.010000')"", "">>> ts - Milli(n=10)\nTimestamp('2022-12-09 14:59:59.990000')"", "">>> ts + Milli(n=-10)\nTimestamp('2022-12-09 14:59:59.990000')""]"
33,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.nth.html,pandas.core.groupby.SeriesGroupBy.nth,"property SeriesGroupBy.nth[source]# Take the nth row from each group if n is an int, otherwise a subset of rows. Can be either a call or an index. dropna is not available with index notation. Index notation accepts a comma separated list of integers and slices. If dropna, will take the nth non-null row, dropna is either ‘all’ or ‘any’; this is equivalent to calling dropna(how=dropna) before the groupby.","Parameters: nint, slice or list of ints and slicesA single nth value for the row or a list of nth values or slices. Changed in version 1.4.0: Added slice and lists containing slices. Added index notation. dropna{‘any’, ‘all’, None}, default NoneApply the specified dropna operation before counting which row is the nth row. Only supported if n is an int. Returns: Series or DataFrameN-th value within each group.","["">>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n>>> g = df.groupby('A')\n>>> g.nth(0)\n   A   B\n0  1 NaN\n2  2 3.0\n>>> g.nth(1)\n   A   B\n1  1 2.0\n4  2 5.0\n>>> g.nth(-1)\n   A   B\n3  1 4.0\n4  2 5.0\n>>> g.nth([0, 1])\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0\n4  2 5.0\n>>> g.nth(slice(None, -1))\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0"", '>>> g.nth[0, 1]\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0\n4  2 5.0\n>>> g.nth[:-1]\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0', "">>> g.nth(0, dropna='any')\n   A   B\n1  1 2.0\n2  2 3.0"", "">>> g.nth(3, dropna='any')\nEmpty DataFrame\nColumns: [A, B]\nIndex: []""]"
34,..\pandas\reference\api\pandas.DataFrame.rolling.html,pandas.DataFrame.rolling,"DataFrame.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=<no_default>, closed=None, step=None, method='single')[source]# Provide rolling window calculations. Notes See Windowing Operations for further usage details and examples.","Parameters: windowint, timedelta, str, offset, or BaseIndexer subclassSize of the moving window. If an integer, the fixed number of observations used for each window. If a timedelta, str, or offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link. If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods, center, closed and step will be passed to get_window_bounds. min_periodsint, default NoneMinimum number of observations in window required to have a value; otherwise, result is np.nan. For a window that is specified by an offset, min_periods will default to 1. For a window that is specified by an integer, min_periods will default to the size of the window. centerbool, default FalseIf False, set the window labels as the right edge of the window index. If True, set the window labels as the center of the window index. win_typestr, default NoneIf None, all points are evenly weighted. If a string, it must be a valid scipy.signal window function. Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature. onstr, optionalFor a DataFrame, a column label or Index level on which to calculate the rolling window, rather than the DataFrame’s index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axisint or str, default 0If 0 or 'index', roll across the rows. If 1 or 'columns', roll across the columns. For Series this parameter is unused and defaults to 0. Deprecated since version 2.1.0: The axis keyword is deprecated. For axis=1, transpose the DataFrame first instead. closedstr, default NoneIf 'right', the first point in the window is excluded from calculations. If 'left', the last point in the window is excluded from calculations. If 'both', the no points in the window are excluded from calculations. If 'neither', the first and last points in the window are excluded from calculations. Default None ('right'). stepint, default None Added in version 1.5.0. Evaluate the window at every step result, equivalent to slicing as [::step]. window must be an integer. Using a step argument other than None or 1 will produce a result with a different shape than the input. methodstr {‘single’, ‘table’}, default ‘single’ Added in version 1.3.0. Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Returns: pandas.api.typing.Window or pandas.api.typing.RollingAn instance of Window is returned if win_type is passed. Otherwise, an instance of Rolling is returned.","["">>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0"", '>>> df.rolling(2).sum()\n     B\n0  NaN\n1  1.0\n2  3.0\n3  NaN\n4  NaN', "">>> df_time = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n...                        index=[pd.Timestamp('20130101 09:00:00'),\n...                               pd.Timestamp('20130101 09:00:02'),\n...                               pd.Timestamp('20130101 09:00:03'),\n...                               pd.Timestamp('20130101 09:00:05'),\n...                               pd.Timestamp('20130101 09:00:06')])"", '>>> df_time\n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0', "">>> df_time.rolling('2s').sum()\n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0"", '>>> indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2)\n>>> df.rolling(window=indexer, min_periods=1).sum()\n     B\n0  1.0\n1  3.0\n2  2.0\n3  4.0\n4  4.0', '>>> df.rolling(2, min_periods=1).sum()\n     B\n0  0.0\n1  1.0\n2  3.0\n3  2.0\n4  4.0', '>>> df.rolling(3, min_periods=1, center=True).sum()\n     B\n0  1.0\n1  3.0\n2  3.0\n3  6.0\n4  4.0', '>>> df.rolling(3, min_periods=1, center=False).sum()\n     B\n0  0.0\n1  1.0\n2  3.0\n3  3.0\n4  6.0', '>>> df.rolling(2, min_periods=1, step=2).sum()\n     B\n0  0.0\n2  3.0\n4  4.0', "">>> df.rolling(2, win_type='gaussian').sum(std=3)\n          B\n0       NaN\n1  0.986207\n2  2.958621\n3       NaN\n4       NaN"", "">>> df = pd.DataFrame({\n...     'A': [pd.to_datetime('2020-01-01'),\n...           pd.to_datetime('2020-01-01'),\n...           pd.to_datetime('2020-01-02'),],\n...     'B': [1, 2, 3], },\n...     index=pd.date_range('2020', periods=3))"", '>>> df\n                    A  B\n2020-01-01 2020-01-01  1\n2020-01-02 2020-01-01  2\n2020-01-03 2020-01-02  3', "">>> df.rolling('2D', on='A').sum()\n                    A    B\n2020-01-01 2020-01-01  1.0\n2020-01-02 2020-01-01  3.0\n2020-01-03 2020-01-02  6.0""]"
35,..\pandas\reference\api\pandas.Series.compare.html,pandas.Series.compare,"Series.compare(other, align_axis=1, keep_shape=False, keep_equal=False, result_names=('self', 'other'))[source]# Compare to another Series and show the differences. Notes Matching NaNs will not appear as a difference.","Parameters: otherSeriesObject to compare with. align_axis{0 or ‘index’, 1 or ‘columns’}, default 1Determine which axis to align the comparison on. 0, or ‘index’Resulting differences are stacked verticallywith rows drawn alternately from self and other. 1, or ‘columns’Resulting differences are aligned horizontallywith columns drawn alternately from self and other. keep_shapebool, default FalseIf true, all rows and columns are kept. Otherwise, only the ones with different values are kept. keep_equalbool, default FalseIf true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs. result_namestuple, default (‘self’, ‘other’)Set the dataframes names in the comparison. Added in version 1.5.0. Returns: Series or DataFrameIf axis is 0 or ‘index’ the result will be a Series. The resulting index will be a MultiIndex with ‘self’ and ‘other’ stacked alternately at the inner level. If axis is 1 or ‘columns’ the result will be a DataFrame. It will have two columns namely ‘self’ and ‘other’.","['>>> s1 = pd.Series([""a"", ""b"", ""c"", ""d"", ""e""])\n>>> s2 = pd.Series([""a"", ""a"", ""c"", ""b"", ""e""])', '>>> s1.compare(s2)\n  self other\n1    b     a\n3    d     b', '>>> s1.compare(s2, align_axis=0)\n1  self     b\n   other    a\n3  self     d\n   other    b\ndtype: object', '>>> s1.compare(s2, keep_shape=True)\n  self other\n0  NaN   NaN\n1    b     a\n2  NaN   NaN\n3    d     b\n4  NaN   NaN', '>>> s1.compare(s2, keep_shape=True, keep_equal=True)\n  self other\n0    a     a\n1    b     a\n2    c     c\n3    d     b\n4    e     e']"
36,..\pandas\reference\api\pandas.Series.to_hdf.html,pandas.Series.to_hdf,"Series.to_hdf(path_or_buf, *, key, mode='a', complevel=None, complib=None, append=False, format=None, index=True, min_itemsize=None, nan_rep=None, dropna=None, data_columns=None, errors='strict', encoding='UTF-8')[source]# Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. Warning One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing. For more information see the user guide.","Parameters: path_or_bufstr or pandas.HDFStoreFile path or HDFStore object. keystrIdentifier for the group in the store. mode{‘a’, ‘w’, ‘r+’}, default ‘a’Mode to open file: ‘w’: write, a new file is created (an existing file with the same name would be deleted). ‘a’: append, an existing file is opened for reading and writing, and if the file does not exist it is created. ‘r+’: similar to ‘a’, but the file must already exist. complevel{0-9}, default NoneSpecifies a compression level for data. A value of 0 or None disables compression. complib{‘zlib’, ‘lzo’, ‘bzip2’, ‘blosc’}, default ‘zlib’Specifies the compression library to be used. These additional compressors for Blosc are supported (default if no compressor specified: ‘blosc:blosclz’): {‘blosc:blosclz’, ‘blosc:lz4’, ‘blosc:lz4hc’, ‘blosc:snappy’, ‘blosc:zlib’, ‘blosc:zstd’}. Specifying a compression library which is not available issues a ValueError. appendbool, default FalseFor Table formats, append the input data to the existing. format{‘fixed’, ‘table’, None}, default ‘fixed’Possible values: ‘fixed’: Fixed format. Fast writing/reading. Not-appendable, nor searchable. ‘table’: Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. If None, pd.get_option(‘io.hdf.default_format’) is checked, followed by fallback to “fixed”. indexbool, default TrueWrite DataFrame index as a column. min_itemsizedict or int, optionalMap column names to minimum string sizes for columns. nan_repAny, optionalHow to represent null values as str. Not allowed with append=True. dropnabool, default False, optionalRemove missing values. data_columnslist of columns or True, optionalList of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See Query via data columns. for more information. Applicable only to format=’table’. errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options. encodingstr, default “UTF-8”","["">>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n...                   index=['a', 'b', 'c'])  \n>>> df.to_hdf('data.h5', key='df', mode='w')"", "">>> s = pd.Series([1, 2, 3, 4])  \n>>> s.to_hdf('data.h5', key='s')"", "">>> pd.read_hdf('data.h5', 'df')  \nA  B\na  1  4\nb  2  5\nc  3  6\n>>> pd.read_hdf('data.h5', 's')  \n0    1\n1    2\n2    3\n3    4\ndtype: int64""]"
37,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.nunique.html,pandas.core.groupby.SeriesGroupBy.nunique,SeriesGroupBy.nunique(dropna=True)[source]# Return number of unique elements in the group.,Returns: SeriesNumber of unique values within each group.,"["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    3\ndtype: int64\n>>> ser.groupby(level=0).nunique()\na    2\nb    1\ndtype: int64"", "">>> ser = pd.Series([1, 2, 3, 3], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    3\ndtype: int64\n>>> ser.resample('MS').nunique()\n2023-01-01    2\n2023-02-01    1\nFreq: MS, dtype: int64""]"
38,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.duplicated.html,pandas.api.extensions.ExtensionArray.duplicated,ExtensionArray.duplicated(keep='first')[source]# Return boolean ndarray denoting duplicate values.,"Parameters: keep{‘first’, ‘last’, False}, default ‘first’ first : Mark duplicates as True except for the first occurrence. last : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True. Returns: ndarray[bool]","['>>> pd.array([1, 1, 2, 3, 3], dtype=""Int64"").duplicated()\narray([False,  True, False, False,  True])']"
39,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_month_start.html,pandas.tseries.offsets.BYearBegin.is_month_start,BYearBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
40,..\pandas\reference\api\pandas.DataFrame.round.html,pandas.DataFrame.round,"DataFrame.round(decimals=0, *args, **kwargs)[source]# Round a DataFrame to a variable number of decimal places.","Parameters: decimalsint, dict, SeriesNumber of decimal places to round each column to. If an int is given, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if decimals is a dict-like, or in the index if decimals is a Series. Any columns not included in decimals will be left as is. Elements of decimals which are not columns of the input will be ignored. *argsAdditional keywords have no effect but might be accepted for compatibility with numpy. **kwargsAdditional keywords have no effect but might be accepted for compatibility with numpy. Returns: DataFrameA DataFrame with the affected columns rounded to the specified number of decimal places.","["">>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\n...                   columns=['dogs', 'cats'])\n>>> df\n    dogs  cats\n0  0.21  0.32\n1  0.01  0.67\n2  0.66  0.03\n3  0.21  0.18"", '>>> df.round(1)\n    dogs  cats\n0   0.2   0.3\n1   0.0   0.7\n2   0.7   0.0\n3   0.2   0.2', "">>> df.round({'dogs': 1, 'cats': 0})\n    dogs  cats\n0   0.2   0.0\n1   0.0   1.0\n2   0.7   0.0\n3   0.2   0.0"", "">>> decimals = pd.Series([0, 1], index=['cats', 'dogs'])\n>>> df.round(decimals)\n    dogs  cats\n0   0.2   0.0\n1   0.0   1.0\n2   0.7   0.0\n3   0.2   0.0""]"
41,..\pandas\reference\api\pandas.Index.where.html,pandas.Index.where,"final Index.where(cond, other=None)[source]# Replace values where the condition is False. The replacement is taken from other.","Parameters: condbool array-like with the same length as selfCondition to select the values on. otherscalar, or array-like, default NoneReplacement if the condition is False. Returns: pandas.IndexA copy of self with values replaced from other where the condition is False.","["">>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n>>> idx\nIndex(['car', 'bike', 'train', 'tractor'], dtype='object')\n>>> idx.where(idx.isin(['car', 'train']), 'other')\nIndex(['car', 'other', 'train', 'other'], dtype='object')""]"
42,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_anchored.html,pandas.tseries.offsets.Milli.is_anchored,Milli.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
43,..\pandas\reference\api\pandas.Series.convert_dtypes.html,pandas.Series.convert_dtypes,"Series.convert_dtypes(infer_objects=True, convert_string=True, convert_integer=True, convert_boolean=True, convert_floating=True, dtype_backend='numpy_nullable')[source]# Convert columns to the best possible dtypes using dtypes supporting pd.NA. Notes By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA. By using the options convert_string, convert_integer, convert_boolean and convert_floating, it is possible to turn off individual conversions to StringDtype, the integer extension types, BooleanDtype or floating extension types, respectively. For object-dtyped columns, if infer_objects is True, use the inference rules as during normal Series/DataFrame construction.  Then, if possible, convert to StringDtype, BooleanDtype or an appropriate integer or floating extension type, otherwise leave as object. If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type. In the future, as new dtypes are added that support pd.NA, the results of this method will change to support those new dtypes.","Parameters: infer_objectsbool, default TrueWhether object dtypes should be converted to the best possible types. convert_stringbool, default TrueWhether object dtypes should be converted to StringDtype(). convert_integerbool, default TrueWhether, if possible, conversion can be done to integer extension types. convert_booleanbool, defaults TrueWhether object dtypes should be converted to BooleanDtypes(). convert_floatingbool, defaults TrueWhether, if possible, conversion can be done to floating extension types. If convert_integer is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: Series or DataFrameCopy of input object with new dtype.","['>>> df = pd.DataFrame(\n...     {\n...         ""a"": pd.Series([1, 2, 3], dtype=np.dtype(""int32"")),\n...         ""b"": pd.Series([""x"", ""y"", ""z""], dtype=np.dtype(""O"")),\n...         ""c"": pd.Series([True, False, np.nan], dtype=np.dtype(""O"")),\n...         ""d"": pd.Series([""h"", ""i"", np.nan], dtype=np.dtype(""O"")),\n...         ""e"": pd.Series([10, np.nan, 20], dtype=np.dtype(""float"")),\n...         ""f"": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(""float"")),\n...     }\n... )', '>>> df\n   a  b      c    d     e      f\n0  1  x   True    h  10.0    NaN\n1  2  y  False    i   NaN  100.5\n2  3  z    NaN  NaN  20.0  200.0', '>>> df.dtypes\na      int32\nb     object\nc     object\nd     object\ne    float64\nf    float64\ndtype: object', '>>> dfn = df.convert_dtypes()\n>>> dfn\n   a  b      c     d     e      f\n0  1  x   True     h    10   <NA>\n1  2  y  False     i  <NA>  100.5\n2  3  z   <NA>  <NA>    20  200.0', '>>> dfn.dtypes\na             Int32\nb    string[python]\nc           boolean\nd    string[python]\ne             Int64\nf           Float64\ndtype: object', '>>> s = pd.Series([""a"", ""b"", np.nan])\n>>> s\n0      a\n1      b\n2    NaN\ndtype: object', '>>> s.convert_dtypes()\n0       a\n1       b\n2    <NA>\ndtype: string']"
44,..\pandas\reference\api\pandas.Series.to_json.html,pandas.Series.to_json,"Series.to_json(path_or_buf=None, *, orient=None, date_format=None, double_precision=10, force_ascii=True, date_unit='ms', default_handler=None, lines=False, compression='infer', index=None, indent=None, storage_options=None, mode='w')[source]# Convert the object to a JSON string. Note NaN’s and None will be converted to null and datetime objects will be converted to UNIX timestamps. Notes The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release. orient='table' contains a ‘pandas_version’ field under ‘schema’. This stores the version of pandas used in the latest revision of the schema.","Parameters: path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. orientstrIndication of expected JSON string format. Series: default is ‘index’ allowed values are: {‘split’, ‘records’, ‘index’, ‘table’}. DataFrame: default is ‘columns’ allowed values are: {‘split’, ‘records’, ‘index’, ‘columns’, ‘values’, ‘table’}. The format of the JSON string: ‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]} ‘records’ : list like [{column -> value}, … , {column -> value}] ‘index’ : dict like {index -> {column -> value}} ‘columns’ : dict like {column -> {index -> value}} ‘values’ : just the values array ‘table’ : dict like {‘schema’: {schema}, ‘data’: {data}} Describing the data, where data component is like orient='records'. date_format{None, ‘epoch’, ‘iso’}Type of date conversion. ‘epoch’ = epoch milliseconds, ‘iso’ = ISO8601. The default depends on the orient. For orient='table', the default is ‘iso’. For all other orients, the default is ‘epoch’. double_precisionint, default 10The number of decimal places to use when encoding floating point values. The possible maximal value is 15. Passing double_precision greater than 15 will raise a ValueError. force_asciibool, default TrueForce encoded string to be ASCII. date_unitstr, default ‘ms’ (milliseconds)The time unit to encode to, governs timestamp and ISO8601 precision.  One of ‘s’, ‘ms’, ‘us’, ‘ns’ for second, millisecond, microsecond, and nanosecond respectively. default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. linesbool, default FalseIf ‘orient’ is ‘records’ write out line-delimited json format. Will throw ValueError if incorrect ‘orient’ since others are not list-like. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. indexbool or None, default NoneThe index is only used when ‘orient’ is ‘split’, ‘index’, ‘column’, or ‘table’. Of these, ‘index’ and ‘column’ do not support index=False. indentint, optionalLength of whitespace used to indent each record. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. modestr, default ‘w’ (writing)Specify the IO mode for output when supplying a path_or_buf. Accepted args are ‘w’ (writing) and ‘a’ (append) only. mode=’a’ is only supported when lines is True and orient is ‘records’. Returns: None or strIf path_or_buf is None, returns the resulting json format as a string. Otherwise returns None.","['>>> from json import loads, dumps\n>>> df = pd.DataFrame(\n...     [[""a"", ""b""], [""c"", ""d""]],\n...     index=[""row 1"", ""row 2""],\n...     columns=[""col 1"", ""col 2""],\n... )', '>>> result = df.to_json(orient=""split"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""columns"": [\n        ""col 1"",\n        ""col 2""\n    ],\n    ""index"": [\n        ""row 1"",\n        ""row 2""\n    ],\n    ""data"": [\n        [\n            ""a"",\n            ""b""\n        ],\n        [\n            ""c"",\n            ""d""\n        ]\n    ]\n}', '>>> result = df.to_json(orient=""records"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n[\n    {\n        ""col 1"": ""a"",\n        ""col 2"": ""b""\n    },\n    {\n        ""col 1"": ""c"",\n        ""col 2"": ""d""\n    }\n]', '>>> result = df.to_json(orient=""index"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""row 1"": {\n        ""col 1"": ""a"",\n        ""col 2"": ""b""\n    },\n    ""row 2"": {\n        ""col 1"": ""c"",\n        ""col 2"": ""d""\n    }\n}', '>>> result = df.to_json(orient=""columns"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""col 1"": {\n        ""row 1"": ""a"",\n        ""row 2"": ""c""\n    },\n    ""col 2"": {\n        ""row 1"": ""b"",\n        ""row 2"": ""d""\n    }\n}', '>>> result = df.to_json(orient=""values"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n[\n    [\n        ""a"",\n        ""b""\n    ],\n    [\n        ""c"",\n        ""d""\n    ]\n]', '>>> result = df.to_json(orient=""table"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""schema"": {\n        ""fields"": [\n            {\n                ""name"": ""index"",\n                ""type"": ""string""\n            },\n            {\n                ""name"": ""col 1"",\n                ""type"": ""string""\n            },\n            {\n                ""name"": ""col 2"",\n                ""type"": ""string""\n            }\n        ],\n        ""primaryKey"": [\n            ""index""\n        ],\n        ""pandas_version"": ""1.4.0""\n    },\n    ""data"": [\n        {\n            ""index"": ""row 1"",\n            ""col 1"": ""a"",\n            ""col 2"": ""b""\n        },\n        {\n            ""index"": ""row 2"",\n            ""col 1"": ""c"",\n            ""col 2"": ""d""\n        }\n    ]\n}']"
45,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.equals.html,pandas.api.extensions.ExtensionArray.equals,"ExtensionArray.equals(other)[source]# Return if another array is equivalent to this array. Equivalent means that both arrays have the same shape and dtype, and all values compare equal. Missing values in the same location are considered equal (in contrast with normal equality).",Parameters: otherExtensionArrayArray to compare to this Array. Returns: booleanWhether the arrays are equivalent.,"['>>> arr1 = pd.array([1, 2, np.nan])\n>>> arr2 = pd.array([1, 2, np.nan])\n>>> arr1.equals(arr2)\nTrue']"
46,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.ohlc.html,pandas.core.groupby.SeriesGroupBy.ohlc,"SeriesGroupBy.ohlc()[source]# Compute open, high, low and close values of a group, excluding missing values. For multiple groupings, the result index will be a MultiIndex","Returns: DataFrameOpen, high, low and close values within each group.","["">>> lst = ['SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC',]\n>>> ser = pd.Series([3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 0.1, 0.5], index=lst)\n>>> ser\nSPX     3.4\nCAC     9.0\nSPX     7.2\nCAC     5.2\nSPX     8.8\nCAC     9.4\nSPX     0.1\nCAC     0.5\ndtype: float64\n>>> ser.groupby(level=0).ohlc()\n     open  high  low  close\nCAC   9.0   9.4  0.5    0.5\nSPX   3.4   8.8  0.1    0.1"", "">>> data = {2022: [1.2, 2.3, 8.9, 4.5, 4.4, 3, 2 , 1],\n...         2023: [3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 8.2, 1.0]}\n>>> df = pd.DataFrame(data, index=['SPX', 'CAC', 'SPX', 'CAC',\n...                   'SPX', 'CAC', 'SPX', 'CAC'])\n>>> df\n     2022  2023\nSPX   1.2   3.4\nCAC   2.3   9.0\nSPX   8.9   7.2\nCAC   4.5   5.2\nSPX   4.4   8.8\nCAC   3.0   9.4\nSPX   2.0   8.2\nCAC   1.0   1.0\n>>> df.groupby(level=0).ohlc()\n    2022                 2023\n    open high  low close open high  low close\nCAC  2.3  4.5  1.0   1.0  9.0  9.4  1.0   1.0\nSPX  1.2  8.9  1.2   2.0  3.4  8.8  3.4   8.2"", "">>> ser = pd.Series([1, 3, 2, 4, 3, 5],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').ohlc()\n            open  high  low  close\n2023-01-01     1     3    1      2\n2023-02-01     4     5    3      5""]"
47,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_on_offset.html,pandas.tseries.offsets.BYearBegin.is_on_offset,BYearBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
48,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.pct_change.html,pandas.core.groupby.SeriesGroupBy.pct_change,"SeriesGroupBy.pct_change(periods=1, fill_method=<no_default>, limit=<no_default>, freq=None, axis=<no_default>)[source]# Calculate pct_change of each value to previous entry in group.",Returns: Series or DataFramePercentage changes within each group.,"["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).pct_change()\na         NaN\na    1.000000\nb         NaN\nb    0.333333\ndtype: float64"", '>>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a  b  c\n    tuna   1  2  3\n  salmon   1  5  6\n catfish   2  5  8\ngoldfish   2  6  9\n>>> df.groupby(""a"").pct_change()\n            b  c\n    tuna    NaN    NaN\n  salmon    1.5  1.000\n catfish    NaN    NaN\ngoldfish    0.2  0.125']"
49,..\pandas\reference\api\pandas.Series.to_latex.html,pandas.Series.to_latex,"Series.to_latex(buf=None, *, columns=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None, decimal='.', multicolumn=None, multicolumn_format=None, multirow=None, caption=None, label=None, position=None)[source]# Render object to a LaTeX tabular, longtable, or nested table. Requires \usepackage{{booktabs}}.  The output can be copy/pasted into a main LaTeX document or read from an external file with \input{{table.tex}}. Changed in version 2.0.0: Refactored to use the Styler implementation via jinja2 templating. Notes As of v2.0.0 this method has changed to use the Styler implementation as part of Styler.to_latex() via jinja2 templating. This means that jinja2 is a requirement, and needs to be installed, for this method to function. It is advised that users switch to using Styler, since that implementation is more frequently updated and contains much more flexibility with the output.","Parameters: bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string. columnslist of label, optionalThe subset of columns to write. Writes all columns by default. headerbool or list of str, default TrueWrite out the column names. If a list of strings is given, it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). na_repstr, default ‘NaN’Missing data representation. formatterslist of functions or dict of {{str: function}}, optionalFormatter functions to apply to columns’ elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_formatone-parameter function or str, optional, default NoneFormatter for floating point numbers. For example float_format=""%.2f"" and float_format=""{{:0.2f}}"".format will both result in 0.1234 being formatted as 0.12. sparsifybool, optionalSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module. index_namesbool, default TruePrints the names of the indexes. bold_rowsbool, default FalseMake the row labels bold in the output. column_formatstr, optionalThe columns format as specified in LaTeX table format e.g. ‘rcl’ for 3 columns. By default, ‘l’ will be used for all columns except columns of numbers, which default to ‘r’. longtablebool, optionalUse a longtable environment instead of tabular. Requires adding a usepackage{{longtable}} to your LaTeX preamble. By default, the value will be read from the pandas config module, and set to True if the option styler.latex.environment is “longtable”. Changed in version 2.0.0: The pandas option affecting this argument has changed. escapebool, optionalBy default, the value will be read from the pandas config module and set to True if the option styler.format.escape is “latex”. When set to False prevents from escaping latex special characters in column names. Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to False. encodingstr, optionalA string representing the encoding to use in the output file, defaults to ‘utf-8’. decimalstr, default ‘.’Character recognized as decimal separator, e.g. ‘,’ in Europe. multicolumnbool, default TrueUse multicolumn to enhance MultiIndex columns. The default will be read from the config module, and is set as the option styler.sparse.columns. Changed in version 2.0.0: The pandas option affecting this argument has changed. multicolumn_formatstr, default ‘r’The alignment for multicolumns, similar to column_format The default will be read from the config module, and is set as the option styler.latex.multicol_align. Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to “r”. multirowbool, default TrueUse multirow to enhance MultiIndex rows. Requires adding a usepackage{{multirow}} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module, and is set as the option styler.sparse.index. Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to True. captionstr or tuple, optionalTuple (full_caption, short_caption), which results in \caption[short_caption]{{full_caption}}; if a single string is passed, no short caption will be set. labelstr, optionalThe LaTeX label to be placed inside \label{{}} in the output. This is used with \ref{{}} in the main .tex file. positionstr, optionalThe LaTeX positional argument for tables, to be placed after \begin{{}} in the output. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","['>>> df = pd.DataFrame(dict(name=[\'Raphael\', \'Donatello\'],\n...                        age=[26, 45],\n...                        height=[181.23, 177.65]))\n>>> print(df.to_latex(index=False,\n...                   formatters={""name"": str.upper},\n...                   float_format=""{:.1f}"".format,\n... ))  \n\\begin{tabular}{lrr}\n\\toprule\nname & age & height \\\\\n\\midrule\nRAPHAEL & 26 & 181.2 \\\\\nDONATELLO & 45 & 177.7 \\\\\n\\bottomrule\n\\end{tabular}']"
50,..\pandas\reference\api\pandas.DataFrame.rpow.html,pandas.DataFrame.rpow,"DataFrame.rpow(other, axis='columns', level=None, fill_value=None)[source]# Get Exponential power of dataframe and other, element-wise (binary operator rpow). Equivalent to other ** dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, pow. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
51,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.factorize.html,pandas.api.extensions.ExtensionArray.factorize,ExtensionArray.factorize(use_na_sentinel=True)[source]# Encode the extension array as an enumerated type. Notes pandas.factorize() offers a sort keyword as well.,"Parameters: use_na_sentinelbool, default TrueIf True, the sentinel -1 will be used for NaN values. If False, NaN values will be encoded as non-negative integers and will not drop the NaN from the uniques of the values. Added in version 1.5.0. Returns: codesndarrayAn integer NumPy array that’s an indexer into the original ExtensionArray. uniquesExtensionArrayAn ExtensionArray containing the unique values of self. Note uniques will not contain an entry for the NA value of the ExtensionArray if there are any missing values present in self.","['>>> idx1 = pd.PeriodIndex([""2014-01"", ""2014-01"", ""2014-02"", ""2014-02"",\n...                       ""2014-03"", ""2014-03""], freq=""M"")\n>>> arr, idx = idx1.factorize()\n>>> arr\narray([0, 0, 1, 1, 2, 2])\n>>> idx\nPeriodIndex([\'2014-01\', \'2014-02\', \'2014-03\'], dtype=\'period[M]\')']"
52,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_month_end.html,pandas.tseries.offsets.Milli.is_month_end,Milli.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
53,..\pandas\reference\api\pandas.Series.copy.html,pandas.Series.copy,"Series.copy(deep=True)[source]# Make a copy of this object’s indices and data. When deep=True (default), a new object will be created with a copy of the calling object’s data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When deep=False, a new object will be created without copying the calling object’s data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). Note The deep=False behaviour as described above will change in pandas 3.0. Copy-on-Write will be enabled by default, which means that the “shallow” copy is that is returned with deep=False will still avoid making an eager copy, but changes to the data of the original will no longer be reflected in the shallow copy (or vice versa). Instead, it makes use of a lazy (deferred) copy mechanism that will copy the data only when any changes to the original or shallow copy is made. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Notes When deep=True, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below). While Index objects are copied when deep=True, the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed. Since pandas is not thread safe, see the gotchas when copying in a threading environment. When copy_on_write in pandas config is set to True, the copy_on_write config takes effect even when deep=False. This means that any changes to the copied data would make a new copy of the data upon write (and vice versa). Changes made to either the original or copied variable would not be reflected in the counterpart. See Copy_on_Write for more information.","Parameters: deepbool, default TrueMake a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied. Returns: Series or DataFrameObject type matches caller.","['>>> s = pd.Series([1, 2], index=[""a"", ""b""])\n>>> s\na    1\nb    2\ndtype: int64', '>>> s_copy = s.copy()\n>>> s_copy\na    1\nb    2\ndtype: int64', '>>> s = pd.Series([1, 2], index=[""a"", ""b""])\n>>> deep = s.copy()\n>>> shallow = s.copy(deep=False)', '>>> s is shallow\nFalse\n>>> s.values is shallow.values and s.index is shallow.index\nTrue', '>>> s is deep\nFalse\n>>> s.values is deep.values or s.index is deep.index\nFalse', '>>> s.iloc[0] = 3\n>>> shallow.iloc[1] = 4\n>>> s\na    3\nb    4\ndtype: int64\n>>> shallow\na    3\nb    4\ndtype: int64\n>>> deep\na    1\nb    2\ndtype: int64', '>>> s = pd.Series([[1, 2], [3, 4]])\n>>> deep = s.copy()\n>>> s[0][0] = 10\n>>> s\n0    [10, 2]\n1     [3, 4]\ndtype: object\n>>> deep\n0    [10, 2]\n1     [3, 4]\ndtype: object', '>>> with pd.option_context(""mode.copy_on_write"", True):\n...     s = pd.Series([1, 2], index=[""a"", ""b""])\n...     copy = s.copy(deep=False)\n...     s.iloc[0] = 100\n...     s\na    100\nb      2\ndtype: int64\n>>> copy\na    1\nb    2\ndtype: int64']"
54,..\pandas\reference\api\pandas.IndexSlice.html,pandas.IndexSlice,pandas.IndexSlice = <pandas.core.indexing._IndexSlice object># Create an object to more easily perform multi-index slicing. Notes See Defined Levels for further info on slicing a MultiIndex.,No parameters found,"["">>> midx = pd.MultiIndex.from_product([['A0','A1'], ['B0','B1','B2','B3']])\n>>> columns = ['foo', 'bar']\n>>> dfmi = pd.DataFrame(np.arange(16).reshape((len(midx), len(columns))),\n...                     index=midx, columns=columns)"", "">>> dfmi.loc[(slice(None), slice('B0', 'B1')), :]\n           foo  bar\n    A0 B0    0    1\n       B1    2    3\n    A1 B0    8    9\n       B1   10   11"", "">>> idx = pd.IndexSlice\n>>> dfmi.loc[idx[:, 'B0':'B1'], :]\n           foo  bar\n    A0 B0    0    1\n       B1    2    3\n    A1 B0    8    9\n       B1   10   11""]"
55,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_quarter_end.html,pandas.tseries.offsets.BYearBegin.is_quarter_end,BYearBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
56,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.pipe.html,pandas.core.groupby.SeriesGroupBy.pipe,"SeriesGroupBy.pipe(func, *args, **kwargs)[source]# Apply a func with arguments to this GroupBy object and return its result. Use .pipe when you want to improve readability by chaining together functions that expect Series, DataFrames, GroupBy or Resampler objects. Instead of writing You can write which is much more readable. Notes See more here","Parameters: funccallable or tuple of (callable, str)Function to apply to this GroupBy object or, alternatively, a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the GroupBy object. argsiterable, optionalPositional arguments passed into func. kwargsdict, optionalA dictionary of keyword arguments passed into func. Returns: the return type of func.","['>>> h = lambda x, arg2, arg3: x + 1 - arg2 * arg3\n>>> g = lambda x, arg1: x * 5 / arg1\n>>> f = lambda x: x ** 4\n>>> df = pd.DataFrame([[""a"", 4], [""b"", 5]], columns=[""group"", ""value""])\n>>> h(g(f(df.groupby(\'group\')), arg1=1), arg2=2, arg3=3)', "">>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=1)\n...    .pipe(h, arg2=2, arg3=3))"", "">>> df = pd.DataFrame({'A': 'a b a b'.split(), 'B': [1, 2, 3, 4]})\n>>> df\n   A  B\n0  a  1\n1  b  2\n2  a  3\n3  b  4"", "">>> df.groupby('A').pipe(lambda x: x.max() - x.min())\n   B\nA\na  2\nb  2""]"
57,..\pandas\reference\api\pandas.Series.to_list.html,pandas.Series.to_list,"Series.to_list()[source]# Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)",Returns: list,"['>>> s = pd.Series([1, 2, 3])\n>>> s.to_list()\n[1, 2, 3]', "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')"", '>>> idx.to_list()\n[1, 2, 3]']"
58,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.fillna.html,pandas.api.extensions.ExtensionArray.fillna,"ExtensionArray.fillna(value=None, method=None, limit=None, copy=True)[source]# Fill NA/NaN values using the specified method.","Parameters: valuescalar, array-likeIf a scalar value is passed it is used to fill all missing values. Alternatively, an array-like “value” can be given. It’s expected that the array-like have the same length as ‘self’. method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default NoneMethod to use for filling holes in reindexed Series: pad / ffill: propagate last valid observation forward to next valid. backfill / bfill: use NEXT valid observation to fill gap. Deprecated since version 2.1.0. limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Deprecated since version 2.1.0. copybool, default TrueWhether to make a copy of the data before filling. If False, then the original should be modified and no new memory should be allocated. For ExtensionArray subclasses that cannot do this, it is at the author’s discretion whether to ignore “copy=False” or to raise. The base class implementation ignores the keyword in pad/backfill cases. Returns: ExtensionArrayWith NA/NaN filled.","['>>> arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])\n>>> arr.fillna(0)\n<IntegerArray>\n[0, 0, 2, 3, 0, 0]\nLength: 6, dtype: Int64']"
59,..\pandas\reference\api\pandas.Series.corr.html,pandas.Series.corr,"Series.corr(other, method='pearson', min_periods=None)[source]# Compute correlation with other Series, excluding missing values. The two Series objects are not required to be the same length and will be aligned internally before the correlation function is applied. Notes Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations. Pearson correlation coefficient Kendall rank correlation coefficient Spearman’s rank correlation coefficient Automatic data alignment: as with all pandas operations, automatic data alignment is performed for this method. corr() automatically considers values with matching indices.","Parameters: otherSeriesSeries with which to compute the correlation. method{‘pearson’, ‘kendall’, ‘spearman’} or callableMethod used to compute correlation: pearson : Standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: Callable with input two 1d ndarrays and returning a float. Warning Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable’s behavior. min_periodsint, optionalMinimum number of observations needed to have a valid result. Returns: floatCorrelation with other.","['>>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> s1 = pd.Series([.2, .0, .6, .2])\n>>> s2 = pd.Series([.3, .6, .0, .1])\n>>> s1.corr(s2, method=histogram_intersection)\n0.3', '>>> s1 = pd.Series([1, 2, 3], index=[0, 1, 2])\n>>> s2 = pd.Series([1, 2, 3], index=[2, 1, 0])\n>>> s1.corr(s2)\n-1.0']"
60,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_quarter_start.html,pandas.tseries.offsets.BYearBegin.is_quarter_start,BYearBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
61,..\pandas\reference\api\pandas.infer_freq.html,pandas.infer_freq,pandas.infer_freq(index)[source]# Infer the most likely frequency given the input index.,"Parameters: indexDatetimeIndex, TimedeltaIndex, Series or array-likeIf passed a Series will use the values of the series (NOT THE INDEX). Returns: str or NoneNone if no discernible frequency. Raises: TypeErrorIf the index is not datetime-like. ValueErrorIf there are fewer than three values.","["">>> idx = pd.date_range(start='2020/12/01', end='2020/12/30', periods=30)\n>>> pd.infer_freq(idx)\n'D'""]"
62,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.plot.html,pandas.core.groupby.SeriesGroupBy.plot,"property SeriesGroupBy.plot[source]# Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used. Notes See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)","Parameters: dataSeries or DataFrameThe object for which the method is called. xlabel or position, default NoneOnly used if data is a DataFrame. ylabel, position or list of label, positions, default NoneAllows plotting of one column versus another. Only used if data is a DataFrame. kindstrThe kind of plot to produce: ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only) axmatplotlib axes object, default NoneAn axes of the current figure. subplotsbool or sequence of iterables, default FalseWhether to group columns into subplots: False : No subplots will be used True : Make separate subplots for each column. sequence of iterables of column labels: Create a subplot for each group of columns. For example [(‘a’, ‘c’), (‘b’, ‘d’)] will create 2 subplots: one with columns ‘a’ and ‘c’, and one with columns ‘b’ and ‘d’. Remaining columns that aren’t specified will be plotted in additional subplots (one per column). Added in version 1.5.0. sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure. shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to invisible. layouttuple, optional(rows, columns) for the layout of subplots. figsizea tuple (width, height) in inchesSize of a figure object. use_indexbool, default TrueUse index as ticks for x axis. titlestr or listTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot. gridbool, default None (matlab style default)Axis grid lines. legendbool or {‘reverse’}Place legend on axis subplots. stylelist or dictThe matplotlib line style per column. logxbool or ‘sym’, default FalseUse log scaling or symlog scaling on x axis. logybool or ‘sym’ default FalseUse log scaling or symlog scaling on y axis. loglogbool or ‘sym’, default FalseUse log scaling or symlog scaling on both x and y axes. xtickssequenceValues to use for the xticks. ytickssequenceValues to use for the yticks. xlim2-tuple/listSet the x limits of the current axes. ylim2-tuple/listSet the y limits of the current axes. xlabellabel, optionalName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. ylabellabel, optionalName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. rotfloat, default NoneRotation for ticks (xticks for vertical, yticks for horizontal plots). fontsizefloat, default NoneFont size for xticks and yticks. colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If string, load colormap with that name from matplotlib. colorbarbool, optionalIf True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots). positionfloatSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center). tablebool, Series or DataFrame, default FalseIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table. yerrDataFrame, Series, array-like, dict and strSee Plotting with Error Bars for detail. xerrDataFrame, Series, array-like, dict and strEquivalent to yerr. stackedbool, default False in line and bar plots, and True in area plotIf True, create stacked plot. secondary_ybool or sequence, default FalseWhether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis. mark_rightbool, default TrueWhen using a secondary_y axis, automatically mark the column labels with “(right)” in the legend. include_boolbool, default is FalseIf True, boolean values can be plotted. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes or numpy.ndarray of themIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.","['>>> ser = pd.Series([1, 2, 3, 3])\n>>> plot = ser.plot(kind=\'hist\', title=""My plot"")', '>>> df = pd.DataFrame({\'length\': [1.5, 0.5, 1.2, 0.9, 3],\n...                   \'width\': [0.7, 0.2, 0.15, 0.2, 1.1]},\n...                   index=[\'pig\', \'rabbit\', \'duck\', \'chicken\', \'horse\'])\n>>> plot = df.plot(title=""DataFrame Plot"")', '>>> lst = [-1, -2, -3, 1, 2, 3]\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> plot = ser.groupby(lambda x: x > 0).plot(title=""SeriesGroupBy Plot"")', '>>> df = pd.DataFrame({""col1"" : [1, 2, 3, 4],\n...                   ""col2"" : [""A"", ""B"", ""A"", ""B""]})\n>>> plot = df.groupby(""col2"").plot(kind=""bar"", title=""DataFrameGroupBy Plot"")']"
63,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_month_start.html,pandas.tseries.offsets.Milli.is_month_start,Milli.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
64,..\pandas\reference\api\pandas.Series.to_markdown.html,pandas.Series.to_markdown,"Series.to_markdown(buf=None, mode='wt', index=True, storage_options=None, **kwargs)[source]# Print Series in Markdown-friendly format. Notes Requires the tabulate package.","Parameters: bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string. modestr, optionalMode in which file is opened, “wt” by default. indexbool, optional, default TrueAdd index (row) labels. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. **kwargsThese parameters will be passed to tabulate. Returns: strSeries in Markdown-friendly format.","['>>> s = pd.Series([""elk"", ""pig"", ""dog"", ""quetzal""], name=""animal"")\n>>> print(s.to_markdown())\n|    | animal   |\n|---:|:---------|\n|  0 | elk      |\n|  1 | pig      |\n|  2 | dog      |\n|  3 | quetzal  |', '>>> print(s.to_markdown(tablefmt=""grid""))\n+----+----------+\n|    | animal   |\n+====+==========+\n|  0 | elk      |\n+----+----------+\n|  1 | pig      |\n+----+----------+\n|  2 | dog      |\n+----+----------+\n|  3 | quetzal  |\n+----+----------+']"
65,..\pandas\reference\api\pandas.DataFrame.rsub.html,pandas.DataFrame.rsub,"DataFrame.rsub(other, axis='columns', level=None, fill_value=None)[source]# Get Subtraction of dataframe and other, element-wise (binary operator rsub). Equivalent to other - dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, sub. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
66,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.html,pandas.api.extensions.ExtensionArray,"class pandas.api.extensions.ExtensionArray[source]# Abstract base class for custom 1-D array types. pandas will recognize instances of this class as proper arrays with a custom type and will not attempt to coerce them to objects. They may be stored directly inside a DataFrame or Series. Attributes dtype An instance of ExtensionDtype. nbytes The number of bytes needed to store this object in memory. ndim Extension Arrays are only allowed to be 1-dimensional. shape Return a tuple of the array dimensions. Methods argsort(*[, ascending, kind, na_position]) Return the indices that would sort this array. astype(dtype[, copy]) Cast to a NumPy array or ExtensionArray with 'dtype'. copy() Return a copy of the array. dropna() Return ExtensionArray without NA values. duplicated([keep]) Return boolean ndarray denoting duplicate values. factorize([use_na_sentinel]) Encode the extension array as an enumerated type. fillna([value, method, limit, copy]) Fill NA/NaN values using the specified method. equals(other) Return if another array is equivalent to this array. insert(loc, item) Insert an item at the given position. interpolate(*, method, axis, index, limit, ...) See DataFrame.interpolate.__doc__. isin(values) Pointwise comparison for set containment in the given values. isna() A 1-D array indicating if each value is missing. ravel([order]) Return a flattened view on this array. repeat(repeats[, axis]) Repeat elements of a ExtensionArray. searchsorted(value[, side, sorter]) Find indices where elements should be inserted to maintain order. shift([periods, fill_value]) Shift values by desired number. take(indices, *[, allow_fill, fill_value]) Take elements from an array. tolist() Return a list of the values. unique() Compute the ExtensionArray of unique values. view([dtype]) Return a view on the array. _accumulate(name, *[, skipna]) Return an ExtensionArray performing an accumulation operation. _concat_same_type(to_concat) Concatenate multiple array of this dtype. _explode() Transform each element of list-like to a row. _formatter([boxed]) Formatting function for scalar values. _from_factorized(values, original) Reconstruct an ExtensionArray after factorization. _from_sequence(scalars, *[, dtype, copy]) Construct a new ExtensionArray from a sequence of scalars. _from_sequence_of_strings(strings, *[, ...]) Construct a new ExtensionArray from a sequence of strings. _hash_pandas_object(*, encoding, hash_key, ...) Hook for hash_pandas_object. _pad_or_backfill(*, method[, limit, ...]) Pad or backfill values, used by Series/DataFrame ffill and bfill. _reduce(name, *[, skipna, keepdims]) Return a scalar result of performing the reduction operation. _values_for_argsort() Return values for sorting. _values_for_factorize() Return an array and missing value suitable for factorization. Notes The interface includes the following abstract methods that must be implemented by subclasses: _from_sequence _from_factorized __getitem__ __len__ __eq__ dtype nbytes isna take copy _concat_same_type interpolate A default repr displaying the type, (truncated) data, length, and dtype is provided. It can be customized or replaced by by overriding: __repr__ : A default repr for the ExtensionArray. _formatter : Print scalars inside a Series or DataFrame. Some methods require casting the ExtensionArray to an ndarray of Python objects with self.astype(object), which may be expensive. When performance is a concern, we highly recommend overriding the following methods: fillna _pad_or_backfill dropna unique factorize / _values_for_factorize argsort, argmax, argmin / _values_for_argsort searchsorted map The remaining methods implemented on this class should be performant, as they only compose abstract methods. Still, a more efficient implementation may be available, and these methods can be overridden. One can implement methods to handle array accumulations or reductions. _accumulate _reduce One can implement methods to handle parsing from strings that will be used in methods such as pandas.io.parsers.read_csv. _from_sequence_of_strings This class does not inherit from ‘abc.ABCMeta’ for performance reasons. Methods and properties required by the interface raise pandas.errors.AbstractMethodError and no register method is provided for registering virtual subclasses. ExtensionArrays are limited to 1 dimension. They may be backed by none, one, or many NumPy arrays. For example, pandas.Categorical is an extension array backed by two arrays, one for codes and one for categories. An array of IPv6 address may be backed by a NumPy structured array with two fields, one for the lower 64 bits and one for the upper 64 bits. Or they may be backed by some other storage type, like Python lists. Pandas makes no assumptions on how the data are stored, just that it can be converted to a NumPy array. The ExtensionArray interface does not impose any rules on how this data is stored. However, currently, the backing data cannot be stored in attributes called .values or ._values to ensure full compatibility with pandas internals. But other names as .data, ._data, ._items, … can be freely used. If implementing NumPy’s __array_ufunc__ interface, pandas expects that You defer by returning NotImplemented when any Series are present in inputs. Pandas will extract the arrays and call the ufunc again. You define a _HANDLED_TYPES tuple as an attribute on the class. Pandas inspect this to determine whether the ufunc is valid for the types present. See NumPy universal functions for more. By default, ExtensionArrays are not hashable.  Immutable subclasses may override this behavior.",No parameters found,[]
67,..\pandas\reference\api\pandas.Series.count.html,pandas.Series.count,Series.count()[source]# Return number of non-NA/null observations in the Series.,Returns: intNumber of non-null values in the Series.,"['>>> s = pd.Series([0.0, 1.0, np.nan])\n>>> s.count()\n2']"
68,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_year_end.html,pandas.tseries.offsets.BYearBegin.is_year_end,BYearBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
69,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.prod.html,pandas.core.groupby.SeriesGroupBy.prod,"SeriesGroupBy.prod(numeric_only=False, min_count=0)[source]# Compute prod of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. Returns: Series or DataFrameComputed prod of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).prod()\na    2\nb   12\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").prod()\n     b    c\na\n1   16   10\n2   30   72']"
70,..\pandas\reference\api\pandas.Series.cov.html,pandas.Series.cov,"Series.cov(other, min_periods=None, ddof=1)[source]# Compute covariance with Series, excluding missing values. The two Series objects are not required to be the same length and will be aligned internally before the covariance is calculated.","Parameters: otherSeriesSeries with which to compute the covariance. min_periodsint, optionalMinimum number of observations needed to have a valid result. ddofint, default 1Delta degrees of freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. Returns: floatCovariance between Series and other normalized by N-1 (unbiased estimator).","['>>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n>>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n>>> s1.cov(s2)\n-0.01685762652715874']"
71,..\pandas\reference\api\pandas.DataFrame.rtruediv.html,pandas.DataFrame.rtruediv,"DataFrame.rtruediv(other, axis='columns', level=None, fill_value=None)[source]# Get Floating division of dataframe and other, element-wise (binary operator rtruediv). Equivalent to other / dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
72,..\pandas\reference\api\pandas.Int16Dtype.html,pandas.Int16Dtype,"class pandas.Int16Dtype[source]# An ExtensionDtype for int16 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
72,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.insert.html,pandas.api.extensions.ExtensionArray.insert,"ExtensionArray.insert(loc, item)[source]# Insert an item at the given position. Notes This method should be both type and dtype-preserving.  If the item cannot be held in an array of this type/dtype, either ValueError or TypeError should be raised. The default implementation relies on _from_sequence to raise on invalid items.",Parameters: locint itemscalar-like Returns: same type as self,"['>>> arr = pd.array([1, 2, 3])\n>>> arr.insert(2, -1)\n<IntegerArray>\n[1, 2, -1, 3]\nLength: 4, dtype: Int64']"
74,..\pandas\reference\api\pandas.Series.to_numpy.html,pandas.Series.to_numpy,"Series.to_numpy(dtype=None, copy=False, na_value=<no_default>, **kwargs)[source]# A NumPy ndarray representing the values in this Series or Index. Notes The returned array will be the same up to equality (values equal in self will be equal in the returned array; likewise for values that are not equal). When self contains an ExtensionArray, the dtype may be different. For example, for a category-dtype Series, to_numpy() will return a NumPy array and the categorical dtype will be lost. For NumPy dtypes, this will be a reference to the actual data stored in this Series or Index (assuming copy=False). Modifying the result in place will modify the data stored in the Series or Index (not that we recommend doing that). For extension types, to_numpy() may require copying data and coercing the result to a NumPy type (possibly object), which may be expensive. When you need a no-copy reference to the underlying data, Series.array should be used instead. This table lays out the different dtypes and default return types of to_numpy() for various dtypes within pandas. dtype array type category[T] ndarray[T] (same dtype as input) period ndarray[object] (Periods) interval ndarray[object] (Intervals) IntegerNA ndarray[object] datetime64[ns] datetime64[ns] datetime64[ns, tz] ndarray[object] (Timestamps)","Parameters: dtypestr or numpy.dtype, optionalThe dtype to pass to numpy.asarray(). copybool, default FalseWhether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary. na_valueAny, optionalThe value to use for missing values. The default value depends on dtype and the type of the array. **kwargsAdditional keywords passed through to the to_numpy method of the underlying array (for extension arrays). Returns: numpy.ndarray","["">>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n>>> ser.to_numpy()\narray(['a', 'b', 'a'], dtype=object)"", '>>> ser = pd.Series(pd.date_range(\'2000\', periods=2, tz=""CET""))\n>>> ser.to_numpy(dtype=object)\narray([Timestamp(\'2000-01-01 00:00:00+0100\', tz=\'CET\'),\n       Timestamp(\'2000-01-02 00:00:00+0100\', tz=\'CET\')],\n      dtype=object)', '>>> ser.to_numpy(dtype=""datetime64[ns]"")\n... \narray([\'1999-12-31T23:00:00.000000000\', \'2000-01-01T23:00:00...\'],\n      dtype=\'datetime64[ns]\')']"
75,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_on_offset.html,pandas.tseries.offsets.Milli.is_on_offset,Milli.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
76,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.is_year_start.html,pandas.tseries.offsets.BYearBegin.is_year_start,BYearBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
77,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.quantile.html,pandas.core.groupby.SeriesGroupBy.quantile,"SeriesGroupBy.quantile(q=0.5, interpolation='linear', numeric_only=False)[source]# Return group values at the given quantile, a la numpy.percentile.","Parameters: qfloat or array-like, default 0.5 (50% quantile)Value(s) between 0 and 1 providing the quantile(s) to compute. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}Method to use when the desired quantile falls between two points. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameReturn type determined by caller of GroupBy object.","["">>> df = pd.DataFrame([\n...     ['a', 1], ['a', 2], ['a', 3],\n...     ['b', 1], ['b', 3], ['b', 5]\n... ], columns=['key', 'val'])\n>>> df.groupby('key').quantile()\n    val\nkey\na    2.0\nb    3.0""]"
78,..\pandas\reference\api\pandas.Series.cummax.html,pandas.Series.cummax,"Series.cummax(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative maximum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative maximum.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: scalar or SeriesReturn cumulative maximum of scalar or Series.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cummax()\n0    2.0\n1    NaN\n2    5.0\n3    5.0\n4    5.0\ndtype: float64', '>>> s.cummax(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cummax()\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  3.0  1.0', '>>> df.cummax(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  1.0']"
79,..\pandas\reference\api\pandas.DataFrame.sample.html,pandas.DataFrame.sample,"DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)[source]# Return a random sample of items from an axis of object. You can use random_state for reproducibility. Notes If frac > 1, replacement should be set to True.","Parameters: nint, optionalNumber of items from axis to return. Cannot be used with frac. Default = 1 if frac = None. fracfloat, optionalFraction of axis items to return. Cannot be used with n. replacebool, default FalseAllow or disallow sampling of the same row more than once. weightsstr or ndarray-like, optionalDefault ‘None’ results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_stateint, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optionalIf int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given. Changed in version 1.4.0: np.random.Generator objects now accepted axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneAxis to sample. Accepts axis number or name. Default is stat axis for given data type. For Series this parameter is unused and defaults to None. ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, …, n - 1. Added in version 1.3.0. Returns: Series or DataFrameA new object of same type as caller containing n items randomly sampled from the caller object.","["">>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n...                    'num_wings': [2, 0, 0, 0],\n...                    'num_specimen_seen': [10, 2, 1, 8]},\n...                   index=['falcon', 'dog', 'spider', 'fish'])\n>>> df\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\ndog            4          0                  2\nspider         8          0                  1\nfish           0          0                  8"", "">>> df['num_legs'].sample(n=3, random_state=1)\nfish      0\nspider    8\nfalcon    2\nName: num_legs, dtype: int64"", '>>> df.sample(frac=0.5, replace=True, random_state=1)\n      num_legs  num_wings  num_specimen_seen\ndog          4          0                  2\nfish         0          0                  8', '>>> df.sample(frac=2, replace=True, random_state=1)\n        num_legs  num_wings  num_specimen_seen\ndog            4          0                  2\nfish           0          0                  8\nfalcon         2          2                 10\nfalcon         2          2                 10\nfish           0          0                  8\ndog            4          0                  2\nfish           0          0                  8\ndog            4          0                  2', "">>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\nfish           0          0                  8""]"
80,..\pandas\reference\api\pandas.Series.to_period.html,pandas.Series.to_period,"Series.to_period(freq=None, copy=None)[source]# Convert Series from DatetimeIndex to PeriodIndex.","Parameters: freqstr, default NoneFrequency associated with the PeriodIndex. copybool, default TrueWhether or not to return a copy. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: SeriesSeries with index converted to PeriodIndex.","["">>> idx = pd.DatetimeIndex(['2023', '2024', '2025'])\n>>> s = pd.Series([1, 2, 3], index=idx)\n>>> s = s.to_period()\n>>> s\n2023    1\n2024    2\n2025    3\nFreq: Y-DEC, dtype: int64"", "">>> s.index\nPeriodIndex(['2023', '2024', '2025'], dtype='period[Y-DEC]')""]"
81,..\pandas\reference\api\pandas.Series.cummin.html,pandas.Series.cummin,"Series.cummin(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative minimum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative minimum.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: scalar or SeriesReturn cumulative minimum of scalar or Series.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cummin()\n0    2.0\n1    NaN\n2    2.0\n3   -1.0\n4   -1.0\ndtype: float64', '>>> s.cummin(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cummin()\n     A    B\n0  2.0  1.0\n1  2.0  NaN\n2  1.0  0.0', '>>> df.cummin(axis=1)\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0']"
82,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_quarter_end.html,pandas.tseries.offsets.Milli.is_quarter_end,Milli.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
83,..\pandas\reference\api\pandas.Int32Dtype.html,pandas.Int32Dtype,"class pandas.Int32Dtype[source]# An ExtensionDtype for int32 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
84,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.interpolate.html,pandas.api.extensions.ExtensionArray.interpolate,"ExtensionArray.interpolate(*, method, axis, index, limit, limit_direction, limit_area, copy, **kwargs)[source]# See DataFrame.interpolate.__doc__.",No parameters found,"['>>> arr = pd.arrays.NumpyExtensionArray(np.array([0, 1, np.nan, 3]))\n>>> arr.interpolate(method=""linear"",\n...                 limit=3,\n...                 limit_direction=""forward"",\n...                 index=pd.Index([1, 2, 3, 4]),\n...                 fill_value=1,\n...                 copy=False,\n...                 axis=0,\n...                 limit_area=""inside""\n...                 )\n<NumpyExtensionArray>\n[0.0, 1.0, 2.0, 3.0]\nLength: 4, dtype: float64']"
85,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.rank.html,pandas.core.groupby.SeriesGroupBy.rank,"SeriesGroupBy.rank(method='average', ascending=True, na_option='keep', pct=False, axis=<no_default>)[source]# Provide the rank of values within each group.","Parameters: method{‘average’, ‘min’, ‘max’, ‘first’, ‘dense’}, default ‘average’ average: average rank of group. min: lowest rank in group. max: highest rank in group. first: ranks assigned in order they appear in the array. dense: like ‘min’, but rank always increases by 1 between groups. ascendingbool, default TrueFalse for ranks by high (1) to low (N). na_option{‘keep’, ‘top’, ‘bottom’}, default ‘keep’ keep: leave NA values where they are. top: smallest rank if ascending. bottom: smallest rank if descending. pctbool, default FalseCompute percentage rank of data within each group. axisint, default 0The axis of the object over which to compute the rank. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. Returns: DataFrame with ranking of values within each group","['>>> df = pd.DataFrame(\n...     {\n...         ""group"": [""a"", ""a"", ""a"", ""a"", ""a"", ""b"", ""b"", ""b"", ""b"", ""b""],\n...         ""value"": [2, 4, 2, 3, 5, 1, 2, 4, 1, 5],\n...     }\n... )\n>>> df\n  group  value\n0     a      2\n1     a      4\n2     a      2\n3     a      3\n4     a      5\n5     b      1\n6     b      2\n7     b      4\n8     b      1\n9     b      5\n>>> for method in [\'average\', \'min\', \'max\', \'dense\', \'first\']:\n...     df[f\'{method}_rank\'] = df.groupby(\'group\')[\'value\'].rank(method)\n>>> df\n  group  value  average_rank  min_rank  max_rank  dense_rank  first_rank\n0     a      2           1.5       1.0       2.0         1.0         1.0\n1     a      4           4.0       4.0       4.0         3.0         4.0\n2     a      2           1.5       1.0       2.0         1.0         2.0\n3     a      3           3.0       3.0       3.0         2.0         3.0\n4     a      5           5.0       5.0       5.0         4.0         5.0\n5     b      1           1.5       1.0       2.0         1.0         1.0\n6     b      2           3.0       3.0       3.0         2.0         3.0\n7     b      4           4.0       4.0       4.0         3.0         4.0\n8     b      1           1.5       1.0       2.0         1.0         2.0\n9     b      5           5.0       5.0       5.0         4.0         5.0']"
86,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.kwds.html,pandas.tseries.offsets.BYearBegin.kwds,BYearBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
87,..\pandas\reference\api\pandas.Series.to_pickle.html,pandas.Series.to_pickle,"Series.to_pickle(path, *, compression='infer', protocol=5, storage_options=None)[source]# Pickle (serialize) object to file.","Parameters: pathstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. File path where the pickled object will be stored. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. protocolintInt which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4, 5. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. [1] https://docs.python.org/3/library/pickle.html. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here.","['>>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)})  \n>>> original_df  \n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n>>> original_df.to_pickle(""./dummy.pkl"")', '>>> unpickled_df = pd.read_pickle(""./dummy.pkl"")  \n>>> unpickled_df  \n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9']"
88,..\pandas\reference\api\pandas.DataFrame.select_dtypes.html,pandas.DataFrame.select_dtypes,"DataFrame.select_dtypes(include=None, exclude=None)[source]# Return a subset of the DataFrame’s columns based on the column dtypes. Notes To select all numeric types, use np.number or 'number' To select strings you must use the object dtype, but note that this will return all object dtype columns See the numpy dtype hierarchy To select datetimes, use np.datetime64, 'datetime' or 'datetime64' To select timedeltas, use np.timedelta64, 'timedelta' or 'timedelta64' To select Pandas categorical dtypes, use 'category' To select Pandas datetimetz dtypes, use 'datetimetz' or 'datetime64[ns, tz]'","Parameters: include, excludescalar or list-likeA selection of dtypes or strings to be included/excluded. At least one of these parameters must be supplied. Returns: DataFrameThe subset of the frame including the dtypes in include and excluding the dtypes in exclude. Raises: ValueError If both of include and exclude are empty If include and exclude have overlapping elements If any kind of string dtype is passed in.","["">>> df = pd.DataFrame({'a': [1, 2] * 3,\n...                    'b': [True, False] * 3,\n...                    'c': [1.0, 2.0] * 3})\n>>> df\n        a      b  c\n0       1   True  1.0\n1       2  False  2.0\n2       1   True  1.0\n3       2  False  2.0\n4       1   True  1.0\n5       2  False  2.0"", "">>> df.select_dtypes(include='bool')\n   b\n0  True\n1  False\n2  True\n3  False\n4  True\n5  False"", "">>> df.select_dtypes(include=['float64'])\n   c\n0  1.0\n1  2.0\n2  1.0\n3  2.0\n4  1.0\n5  2.0"", "">>> df.select_dtypes(exclude=['int64'])\n       b    c\n0   True  1.0\n1  False  2.0\n2   True  1.0\n3  False  2.0\n4   True  1.0\n5  False  2.0""]"
89,..\pandas\reference\api\pandas.Series.cumprod.html,pandas.Series.cumprod,"Series.cumprod(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative product over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative product.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: scalar or SeriesReturn cumulative product of scalar or Series.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cumprod()\n0     2.0\n1     NaN\n2    10.0\n3   -10.0\n4    -0.0\ndtype: float64', '>>> s.cumprod(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cumprod()\n     A    B\n0  2.0  1.0\n1  6.0  NaN\n2  6.0  0.0', '>>> df.cumprod(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  0.0']"
90,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_quarter_start.html,pandas.tseries.offsets.Milli.is_quarter_start,Milli.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
91,..\pandas\reference\api\pandas.DataFrame.sem.html,pandas.DataFrame.sem,"DataFrame.sem(axis=0, skipna=True, ddof=1, numeric_only=False, **kwargs)[source]# Return unbiased standard error of the mean over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument","Parameters: axis{index (0), columns (1)}For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.sem with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. Returns: Series or DataFrame (if level specified)","['>>> s = pd.Series([1, 2, 3])\n>>> s.sem().round(6)\n0.57735', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n>>> df\n       a   b\ntiger  1   2\nzebra  2   3\n>>> df.sem()\na   0.5\nb   0.5\ndtype: float64"", '>>> df.sem(axis=1)\ntiger   0.5\nzebra   0.5\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n...                   index=['tiger', 'zebra'])\n>>> df.sem(numeric_only=True)\na   0.5\ndtype: float64""]"
92,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.isin.html,pandas.api.extensions.ExtensionArray.isin,ExtensionArray.isin(values)[source]# Pointwise comparison for set containment in the given values. Roughly equivalent to np.array([x in values for x in self]),Parameters: valuesnp.ndarray or ExtensionArray Returns: np.ndarray[bool],"['>>> arr = pd.array([1, 2, 3])\n>>> arr.isin([1])\n<BooleanArray>\n[True, False, False]\nLength: 3, dtype: boolean']"
93,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.month.html,pandas.tseries.offsets.BYearBegin.month,BYearBegin.month#,No parameters found,[]
94,..\pandas\reference\api\pandas.Int64Dtype.html,pandas.Int64Dtype,"class pandas.Int64Dtype[source]# An ExtensionDtype for int64 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
95,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.resample.html,pandas.core.groupby.SeriesGroupBy.resample,"SeriesGroupBy.resample(rule, *args, include_groups=True, **kwargs)[source]# Provide resampling when using a TimeGrouper. Given a grouper, the function resamples it according to a string “string” -> “frequency”. See the frequency aliases documentation for more details.","Parameters: rulestr or DateOffsetThe offset string or object representing target grouper conversion. *argsPossible arguments are how, fill_method, limit, kind and on, and other arguments of TimeGrouper. include_groupsbool, default TrueWhen True, will attempt to include the groupings in the operation in the case that they are columns of the DataFrame. If this raises a TypeError, the result will be computed with the groupings excluded. When False, the groupings will be excluded when applying func. Added in version 2.2.0. Deprecated since version 2.2.0: Setting include_groups to True is deprecated. Only the value False will be allowed in a future version of pandas. **kwargsPossible arguments are how, fill_method, limit, kind and on, and other arguments of TimeGrouper. Returns: pandas.api.typing.DatetimeIndexResamplerGroupby, pandas.api.typing.PeriodIndexResamplerGroupby, or pandas.api.typing.TimedeltaIndexResamplerGroupbyReturn a new groupby object, with type depending on the data being resampled.","["">>> idx = pd.date_range('1/1/2000', periods=4, freq='min')\n>>> df = pd.DataFrame(data=4 * [range(2)],\n...                   index=idx,\n...                   columns=['a', 'b'])\n>>> df.iloc[2, 0] = 5\n>>> df\n                    a  b\n2000-01-01 00:00:00  0  1\n2000-01-01 00:01:00  0  1\n2000-01-01 00:02:00  5  1\n2000-01-01 00:03:00  0  1"", "">>> df.groupby('a').resample('3min', include_groups=False).sum()\n                         b\na\n0   2000-01-01 00:00:00  2\n    2000-01-01 00:03:00  1\n5   2000-01-01 00:00:00  1"", "">>> df.groupby('a').resample('30s', include_groups=False).sum()\n                    b\na\n0   2000-01-01 00:00:00  1\n    2000-01-01 00:00:30  0\n    2000-01-01 00:01:00  1\n    2000-01-01 00:01:30  0\n    2000-01-01 00:02:00  0\n    2000-01-01 00:02:30  0\n    2000-01-01 00:03:00  1\n5   2000-01-01 00:02:00  1"", "">>> df.groupby('a').resample('ME', include_groups=False).sum()\n            b\na\n0   2000-01-31  3\n5   2000-01-31  1"", "">>> (\n...     df.groupby('a')\n...     .resample('3min', closed='right', include_groups=False)\n...     .sum()\n... )\n                         b\na\n0   1999-12-31 23:57:00  1\n    2000-01-01 00:00:00  2\n5   2000-01-01 00:00:00  1"", "">>> (\n...     df.groupby('a')\n...     .resample('3min', closed='right', label='right', include_groups=False)\n...     .sum()\n... )\n                         b\na\n0   2000-01-01 00:00:00  1\n    2000-01-01 00:03:00  2\n5   2000-01-01 00:03:00  1""]"
96,..\pandas\reference\api\pandas.Series.to_sql.html,pandas.Series.to_sql,"Series.to_sql(name, con, *, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]# Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten. Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. Not all datastores support method=""multi"". Oracle, for example, does not support multi-value insert. References [1] https://docs.sqlalchemy.org [2] https://www.python.org/dev/peps/pep-0249/","Parameters: namestrName of SQL table. consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable. See here. If passing a sqlalchemy.engine.Connection which is already in a transaction, the transaction will not be committed.  If passing a sqlite3.Connection, it will not be possible to roll back the record insertion. schemastr, optionalSpecify the schema (if database flavor supports this). If None, use default schema. if_exists{‘fail’, ‘replace’, ‘append’}, default ‘fail’How to behave if the table already exists. fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table. indexbool, default TrueWrite DataFrame index as a column. Uses index_label as the column name in the table. Creates a table index for this column. index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksizeint, optionalSpecify the number of rows in each batch to be written at a time. By default, all rows will be written at once. dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method{None, ‘multi’, callable}, optionalControls the SQL insertion clause used: None : Uses standard SQL INSERT clause (one per row). ‘multi’: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter). Details and a sample callable implementation can be found in the section insert method. Returns: None or intNumber of rows affected by to_sql. None is returned if the callable passed into method does not return an integer number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy. Added in version 1.4.0. Raises: ValueErrorWhen the table already exists and if_exists is ‘fail’ (the default).","["">>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite://', echo=False)"", "">>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3"", '>>> df.to_sql(name=\'users\', con=engine)\n3\n>>> from sqlalchemy import text\n>>> with engine.connect() as conn:\n...    conn.execute(text(""SELECT * FROM users"")).fetchall()\n[(0, \'User 1\'), (1, \'User 2\'), (2, \'User 3\')]', "">>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql(name='users', con=connection, if_exists='append')\n2"", '>>> df2 = pd.DataFrame({\'name\' : [\'User 6\', \'User 7\']})\n>>> df2.to_sql(name=\'users\', con=engine, if_exists=\'append\')\n2\n>>> with engine.connect() as conn:\n...    conn.execute(text(""SELECT * FROM users"")).fetchall()\n[(0, \'User 1\'), (1, \'User 2\'), (2, \'User 3\'),\n (0, \'User 4\'), (1, \'User 5\'), (0, \'User 6\'),\n (1, \'User 7\')]', '>>> df2.to_sql(name=\'users\', con=engine, if_exists=\'replace\',\n...            index_label=\'id\')\n2\n>>> with engine.connect() as conn:\n...    conn.execute(text(""SELECT * FROM users"")).fetchall()\n[(0, \'User 6\'), (1, \'User 7\')]', '>>> from sqlalchemy.dialects.postgresql import insert\n>>> def insert_on_conflict_nothing(table, conn, keys, data_iter):\n...     # ""a"" is the primary key in ""conflict_table""\n...     data = [dict(zip(keys, row)) for row in data_iter]\n...     stmt = insert(table.table).values(data).on_conflict_do_nothing(index_elements=[""a""])\n...     result = conn.execute(stmt)\n...     return result.rowcount\n>>> df_conflict.to_sql(name=""conflict_table"", con=conn, if_exists=""append"", method=insert_on_conflict_nothing)  \n0', '>>> from sqlalchemy.dialects.mysql import insert\n>>> def insert_on_conflict_update(table, conn, keys, data_iter):\n...     # update columns ""b"" and ""c"" on primary key conflict\n...     data = [dict(zip(keys, row)) for row in data_iter]\n...     stmt = (\n...         insert(table.table)\n...         .values(data)\n...     )\n...     stmt = stmt.on_duplicate_key_update(b=stmt.inserted.b, c=stmt.inserted.c)\n...     result = conn.execute(stmt)\n...     return result.rowcount\n>>> df_conflict.to_sql(name=""conflict_table"", con=conn, if_exists=""append"", method=insert_on_conflict_update)  \n2', '>>> df = pd.DataFrame({""A"": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0', '>>> from sqlalchemy.types import Integer\n>>> df.to_sql(name=\'integers\', con=engine, index=False,\n...           dtype={""A"": Integer()})\n3', '>>> with engine.connect() as conn:\n...   conn.execute(text(""SELECT * FROM integers"")).fetchall()\n[(1,), (None,), (2,)]']"
97,..\pandas\reference\api\pandas.Series.cumsum.html,pandas.Series.cumsum,"Series.cumsum(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative sum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative sum.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: scalar or SeriesReturn cumulative sum of scalar or Series.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cumsum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64', '>>> s.cumsum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cumsum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0', '>>> df.cumsum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0']"
98,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_year_end.html,pandas.tseries.offsets.Milli.is_year_end,Milli.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
99,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.isna.html,pandas.api.extensions.ExtensionArray.isna,"ExtensionArray.isna()[source]# A 1-D array indicating if each value is missing. Notes If returning an ExtensionArray, then na_values._is_boolean should be True na_values should implement ExtensionArray._reduce() na_values.any and na_values.all should be implemented","Returns: numpy.ndarray or pandas.api.extensions.ExtensionArrayIn most cases, this should return a NumPy ndarray. For exceptional cases like SparseArray, where returning an ndarray would be expensive, an ExtensionArray may be returned.","['>>> arr = pd.array([1, 2, np.nan, np.nan])\n>>> arr.isna()\narray([False, False,  True,  True])']"
100,..\pandas\reference\api\pandas.DataFrame.set_axis.html,pandas.DataFrame.set_axis,"DataFrame.set_axis(labels, *, axis=0, copy=None)[source]# Assign desired index to given axis. Indexes for column or row labels can be changed by assigning a list-like or Index.","Parameters: labelslist-like, IndexThe values for the new index. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to update. The value 0 identifies the rows. For Series this parameter is unused and defaults to 0. copybool, default TrueWhether to make a copy of the underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: DataFrameAn object of type DataFrame.","['>>> df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})', "">>> df.set_axis(['a', 'b', 'c'], axis='index')\n   A  B\na  1  4\nb  2  5\nc  3  6"", "">>> df.set_axis(['I', 'II'], axis='columns')\n   I  II\n0  1   4\n1  2   5\n2  3   6""]"
101,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.n.html,pandas.tseries.offsets.BYearBegin.n,BYearBegin.n#,No parameters found,[]
102,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.rolling.html,pandas.core.groupby.SeriesGroupBy.rolling,"SeriesGroupBy.rolling(*args, **kwargs)[source]# Return a rolling grouper, providing rolling functionality per group.","Parameters: windowint, timedelta, str, offset, or BaseIndexer subclassSize of the moving window. If an integer, the fixed number of observations used for each window. If a timedelta, str, or offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link. If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods, center, closed and step will be passed to get_window_bounds. min_periodsint, default NoneMinimum number of observations in window required to have a value; otherwise, result is np.nan. For a window that is specified by an offset, min_periods will default to 1. For a window that is specified by an integer, min_periods will default to the size of the window. centerbool, default FalseIf False, set the window labels as the right edge of the window index. If True, set the window labels as the center of the window index. win_typestr, default NoneIf None, all points are evenly weighted. If a string, it must be a valid scipy.signal window function. Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature. onstr, optionalFor a DataFrame, a column label or Index level on which to calculate the rolling window, rather than the DataFrame’s index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axisint or str, default 0If 0 or 'index', roll across the rows. If 1 or 'columns', roll across the columns. For Series this parameter is unused and defaults to 0. closedstr, default NoneIf 'right', the first point in the window is excluded from calculations. If 'left', the last point in the window is excluded from calculations. If 'both', no points in the window are excluded from calculations. If 'neither', the first and last points in the window are excluded from calculations. Default None ('right'). methodstr {‘single’, ‘table’}, default ‘single’Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Returns: pandas.api.typing.RollingGroupbyReturn a new grouper with our rolling appended.","["">>> df = pd.DataFrame({'A': [1, 1, 2, 2],\n...                    'B': [1, 2, 3, 4],\n...                    'C': [0.362, 0.227, 1.267, -0.562]})\n>>> df\n      A  B      C\n0     1  1  0.362\n1     1  2  0.227\n2     2  3  1.267\n3     2  4 -0.562"", "">>> df.groupby('A').rolling(2).sum()\n    B      C\nA\n1 0  NaN    NaN\n  1  3.0  0.589\n2 2  NaN    NaN\n  3  7.0  0.705"", "">>> df.groupby('A').rolling(2, min_periods=1).sum()\n    B      C\nA\n1 0  1.0  0.362\n  1  3.0  0.589\n2 2  3.0  1.267\n  3  7.0  0.705"", "">>> df.groupby('A').rolling(2, on='B').sum()\n    B      C\nA\n1 0  1    NaN\n  1  2  0.589\n2 2  3    NaN\n  3  4  0.705""]"
103,..\pandas\reference\api\pandas.DataFrame.set_flags.html,pandas.DataFrame.set_flags,"DataFrame.set_flags(*, copy=False, allows_duplicate_labels=None)[source]# Return a new object with updated flags. Notes This method returns a new object that’s a view on the same data as the input. Mutating the input or the output values will be reflected in the other. This method is intended to be used in method chains. “Flags” differ from “metadata”. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.","Parameters: copybool, default FalseSpecify if a copy of the object should be made. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True allows_duplicate_labelsbool, optionalWhether the returned object allows duplicate labels. Returns: Series or DataFrameThe same type as the caller.","['>>> df = pd.DataFrame({""A"": [1, 2]})\n>>> df.flags.allows_duplicate_labels\nTrue\n>>> df2 = df.set_flags(allows_duplicate_labels=False)\n>>> df2.flags.allows_duplicate_labels\nFalse']"
104,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.nbytes.html,pandas.api.extensions.ExtensionArray.nbytes,property ExtensionArray.nbytes[source]# The number of bytes needed to store this object in memory.,No parameters found,"['>>> pd.array([1, 2, 3]).nbytes\n27']"
105,..\pandas\reference\api\pandas.Series.describe.html,pandas.Series.describe,"Series.describe(percentiles=None, include=None, exclude=None)[source]# Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Notes For numeric data, the result’s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value’s frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.","Parameters: percentileslist-like of numbers, optionalThe percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles. include‘all’, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored for Series. Here are the options: ‘all’ : All columns of the input will be included in the output. A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category' None (default) : The result will include all numeric columns. excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored for Series. Here are the options: A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category' None (default) : The result will exclude nothing. Returns: Series or DataFrameSummary statistics of the Series or Dataframe provided.","['>>> s = pd.Series([1, 2, 3])\n>>> s.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\ndtype: float64', "">>> s = pd.Series(['a', 'a', 'b', 'c'])\n>>> s.describe()\ncount     4\nunique    3\ntop       a\nfreq      2\ndtype: object"", '>>> s = pd.Series([\n...     np.datetime64(""2000-01-01""),\n...     np.datetime64(""2010-01-01""),\n...     np.datetime64(""2010-01-01"")\n... ])\n>>> s.describe()\ncount                      3\nmean     2006-09-01 08:00:00\nmin      2000-01-01 00:00:00\n25%      2004-12-31 12:00:00\n50%      2010-01-01 00:00:00\n75%      2010-01-01 00:00:00\nmax      2010-01-01 00:00:00\ndtype: object', "">>> df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n...                    'numeric': [1, 2, 3],\n...                    'object': ['a', 'b', 'c']\n...                    })\n>>> df.describe()\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0"", "">>> df.describe(include='all')  \n       categorical  numeric object\ncount            3      3.0      3\nunique           3      NaN      3\ntop              f      NaN      a\nfreq             1      NaN      1\nmean           NaN      2.0    NaN\nstd            NaN      1.0    NaN\nmin            NaN      1.0    NaN\n25%            NaN      1.5    NaN\n50%            NaN      2.0    NaN\n75%            NaN      2.5    NaN\nmax            NaN      3.0    NaN"", '>>> df.numeric.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\nName: numeric, dtype: float64', '>>> df.describe(include=[np.number])\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0', '>>> df.describe(include=[object])  \n       object\ncount       3\nunique      3\ntop         a\nfreq        1', "">>> df.describe(include=['category'])\n       categorical\ncount            3\nunique           3\ntop              d\nfreq             1"", '>>> df.describe(exclude=[np.number])  \n       categorical object\ncount            3      3\nunique           3      3\ntop              f      a\nfreq             1      1', '>>> df.describe(exclude=[object])  \n       categorical  numeric\ncount            3      3.0\nunique           3      NaN\ntop              f      NaN\nfreq             1      NaN\nmean           NaN      2.0\nstd            NaN      1.0\nmin            NaN      1.0\n25%            NaN      1.5\n50%            NaN      2.0\n75%            NaN      2.5\nmax            NaN      3.0']"
106,..\pandas\reference\api\pandas.Series.to_string.html,pandas.Series.to_string,"Series.to_string(buf=None, na_rep='NaN', float_format=None, header=True, index=True, length=False, dtype=False, name=False, max_rows=None, min_rows=None)[source]# Render a string representation of the Series.","Parameters: bufStringIO-like, optionalBuffer to write to. na_repstr, optionalString representation of NaN to use, default ‘NaN’. float_formatone-parameter function, optionalFormatter function to apply to columns’ elements if they are floats, default None. headerbool, default TrueAdd the Series header (index name). indexbool, optionalAdd index (row) labels, default True. lengthbool, default FalseAdd the Series length. dtypebool, default FalseAdd the Series dtype. namebool, default FalseAdd the Series name if not None. max_rowsint, optionalMaximum number of rows to show before truncating. If None, show all. min_rowsint, optionalThe number of rows to display in a truncated repr (when number of rows is above max_rows). Returns: str or NoneString representation of Series if buf=None, otherwise None.","["">>> ser = pd.Series([1, 2, 3]).to_string()\n>>> ser\n'0    1\\n1    2\\n2    3'""]"
107,..\pandas\reference\api\pandas.Int8Dtype.html,pandas.Int8Dtype,"class pandas.Int8Dtype[source]# An ExtensionDtype for int8 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
108,..\pandas\reference\api\pandas.tseries.offsets.Milli.is_year_start.html,pandas.tseries.offsets.Milli.is_year_start,Milli.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
109,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.name.html,pandas.tseries.offsets.BYearBegin.name,BYearBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
110,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.sample.html,pandas.core.groupby.SeriesGroupBy.sample,"SeriesGroupBy.sample(n=None, frac=None, replace=False, weights=None, random_state=None)[source]# Return a random sample of items from each group. You can use random_state for reproducibility.","Parameters: nint, optionalNumber of items to return for each group. Cannot be used with frac and must be no larger than the smallest group unless replace is True. Default is one if frac is None. fracfloat, optionalFraction of items to return. Cannot be used with n. replacebool, default FalseAllow or disallow sampling of the same row more than once. weightslist-like, optionalDefault None results in equal probability weighting. If passed a list-like then values must have the same length as the underlying DataFrame or Series object and will be used as sampling probabilities after normalization within each group. Values must be non-negative with at least one positive element within each group. random_stateint, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optionalIf int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given. Changed in version 1.4.0: np.random.Generator objects now accepted Returns: Series or DataFrameA new object of same type as caller containing items randomly sampled within each group from the caller object.","['>>> df = pd.DataFrame(\n...     {""a"": [""red""] * 2 + [""blue""] * 2 + [""black""] * 2, ""b"": range(6)}\n... )\n>>> df\n       a  b\n0    red  0\n1    red  1\n2   blue  2\n3   blue  3\n4  black  4\n5  black  5', '>>> df.groupby(""a"").sample(n=1, random_state=1)\n       a  b\n4  black  4\n2   blue  2\n1    red  1', '>>> df.groupby(""a"")[""b""].sample(frac=0.5, random_state=2)\n5    5\n2    2\n0    0\nName: b, dtype: int64', '>>> df.groupby(""a"").sample(\n...     n=1,\n...     weights=[1, 1, 1, 0, 0, 1],\n...     random_state=1,\n... )\n       a  b\n5  black  5\n2   blue  2\n0    red  0']"
111,..\pandas\reference\api\pandas.DataFrame.set_index.html,pandas.DataFrame.set_index,"DataFrame.set_index(keys, *, drop=True, append=False, inplace=False, verify_integrity=False)[source]# Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it.","Parameters: keyslabel or array-like or list of labels/arraysThis parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, “array” encompasses Series, Index, np.ndarray, and instances of Iterator. dropbool, default TrueDelete columns to be used as the new index. appendbool, default FalseWhether to append columns to existing index. inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one. verify_integritybool, default FalseCheck the new index for duplicates. Otherwise defer the check until necessary. Setting to False will improve the performance of this method. Returns: DataFrame or NoneChanged row labels or None if inplace=True.","["">>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n...                    'year': [2012, 2014, 2013, 2014],\n...                    'sale': [55, 40, 84, 31]})\n>>> df\n   month  year  sale\n0      1  2012    55\n1      4  2014    40\n2      7  2013    84\n3     10  2014    31"", "">>> df.set_index('month')\n       year  sale\nmonth\n1      2012    55\n4      2014    40\n7      2013    84\n10     2014    31"", "">>> df.set_index(['year', 'month'])\n            sale\nyear  month\n2012  1     55\n2014  4     40\n2013  7     84\n2014  10    31"", "">>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n         month  sale\n   year\n1  2012  1      55\n2  2014  4      40\n3  2013  7      84\n4  2014  10     31"", '>>> s = pd.Series([1, 2, 3, 4])\n>>> df.set_index([s, s**2])\n      month  year  sale\n1 1       1  2012    55\n2 4       4  2014    40\n3 9       7  2013    84\n4 16     10  2014    31']"
112,..\pandas\reference\api\pandas.Interval.closed.html,pandas.Interval.closed,"Interval.closed# String describing the inclusive side the intervals. Either left, right, both or neither.",No parameters found,"["">>> interval = pd.Interval(left=1, right=2, closed='left')\n>>> interval\nInterval(1, 2, closed='left')\n>>> interval.closed\n'left'""]"
113,..\pandas\reference\api\pandas.Series.to_timestamp.html,pandas.Series.to_timestamp,"Series.to_timestamp(freq=None, how='start', copy=None)[source]# Cast to DatetimeIndex of Timestamps, at beginning of period.","Parameters: freqstr, default frequency of PeriodIndexDesired frequency. how{‘s’, ‘e’, ‘start’, ‘end’}Convention for converting period to timestamp; start of period vs. end. copybool, default TrueWhether or not to return a copy. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: Series with DatetimeIndex","["">>> idx = pd.PeriodIndex(['2023', '2024', '2025'], freq='Y')\n>>> s1 = pd.Series([1, 2, 3], index=idx)\n>>> s1\n2023    1\n2024    2\n2025    3\nFreq: Y-DEC, dtype: int64"", '>>> s1 = s1.to_timestamp()\n>>> s1\n2023-01-01    1\n2024-01-01    2\n2025-01-01    3\nFreq: YS-JAN, dtype: int64', "">>> s2 = pd.Series([1, 2, 3], index=idx)\n>>> s2 = s2.to_timestamp(freq='M')\n>>> s2\n2023-01-31    1\n2024-01-31    2\n2025-01-31    3\nFreq: YE-JAN, dtype: int64""]"
114,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.ndim.html,pandas.api.extensions.ExtensionArray.ndim,property ExtensionArray.ndim[source]# Extension Arrays are only allowed to be 1-dimensional.,No parameters found,"['>>> arr = pd.array([1, 2, 3])\n>>> arr.ndim\n1']"
115,..\pandas\reference\api\pandas.Series.diff.html,pandas.Series.diff,"Series.diff(periods=1)[source]# First discrete difference of element. Calculates the difference of a Series element compared with another element in the Series (default is element in previous row). Notes For boolean dtypes, this uses operator.xor() rather than operator.sub(). The result is calculated according to current dtype in Series, however dtype of the result is always float64.","Parameters: periodsint, default 1Periods to shift for calculating difference, accepts negative values. Returns: SeriesFirst differences of the Series.","['>>> s = pd.Series([1, 1, 2, 3, 5, 8])\n>>> s.diff()\n0    NaN\n1    0.0\n2    1.0\n3    1.0\n4    2.0\n5    3.0\ndtype: float64', '>>> s.diff(periods=3)\n0    NaN\n1    NaN\n2    NaN\n3    2.0\n4    4.0\n5    6.0\ndtype: float64', '>>> s.diff(periods=-1)\n0    0.0\n1   -1.0\n2   -1.0\n3   -2.0\n4   -3.0\n5    NaN\ndtype: float64', '>>> s = pd.Series([1, 0], dtype=np.uint8)\n>>> s.diff()\n0      NaN\n1    255.0\ndtype: float64']"
116,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.nanos.html,pandas.tseries.offsets.BYearBegin.nanos,BYearBegin.nanos#,No parameters found,[]
117,..\pandas\reference\api\pandas.tseries.offsets.Milli.kwds.html,pandas.tseries.offsets.Milli.kwds,Milli.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
118,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.sem.html,pandas.core.groupby.SeriesGroupBy.sem,"SeriesGroupBy.sem(ddof=1, numeric_only=False)[source]# Compute standard error of the mean of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameStandard error of the mean of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([5, 10, 8, 14], index=lst)\n>>> ser\na     5\na    10\nb     8\nb    14\ndtype: int64\n>>> ser.groupby(level=0).sem()\na    2.5\nb    3.0\ndtype: float64"", '>>> data = [[1, 12, 11], [1, 15, 2], [2, 5, 8], [2, 6, 12]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a   b   c\n    tuna   1  12  11\n  salmon   1  15   2\n catfish   2   5   8\ngoldfish   2   6  12\n>>> df.groupby(""a"").sem()\n      b  c\na\n1    1.5  4.5\n2    0.5  2.0', "">>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').sem()\n2023-01-01    0.577350\n2023-02-01    1.527525\nFreq: MS, dtype: float64""]"
119,..\pandas\reference\api\pandas.DataFrame.shape.html,pandas.DataFrame.shape,property DataFrame.shape[source]# Return a tuple representing the dimensionality of the DataFrame.,No parameters found,"["">>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n>>> df.shape\n(2, 2)"", "">>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n...                    'col3': [5, 6]})\n>>> df.shape\n(2, 3)""]"
120,..\pandas\reference\api\pandas.Interval.closed_left.html,pandas.Interval.closed_left,Interval.closed_left# Check if the interval is closed on the left side. For the meaning of closed and open see Interval.,Returns: boolTrue if the Interval is closed on the left-side.,"["">>> iv = pd.Interval(0, 5, closed='left')\n>>> iv.closed_left\nTrue"", "">>> iv = pd.Interval(0, 5, closed='right')\n>>> iv.closed_left\nFalse""]"
121,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.ravel.html,pandas.api.extensions.ExtensionArray.ravel,"ExtensionArray.ravel(order='C')[source]# Return a flattened view on this array. Notes Because ExtensionArrays are 1D-only, this is a no-op. The “order” argument is ignored, is for compatibility with NumPy.","Parameters: order{None, ‘C’, ‘F’, ‘A’, ‘K’}, default ‘C’ Returns: ExtensionArray","['>>> pd.array([1, 2, 3]).ravel()\n<IntegerArray>\n[1, 2, 3]\nLength: 3, dtype: Int64']"
122,..\pandas\reference\api\pandas.Series.div.html,pandas.Series.div,"Series.div(other, level=None, fill_value=None, axis=0)[source]# Return Floating division of series and other, element-wise (binary operator truediv). Equivalent to series / other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.divide(b, fill_value=0)\na    1.0\nb    inf\nc    inf\nd    0.0\ne    NaN\ndtype: float64""]"
123,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.shift.html,pandas.core.groupby.SeriesGroupBy.shift,"SeriesGroupBy.shift(periods=1, freq=None, axis=<no_default>, fill_value=<no_default>, suffix=None)[source]# Shift each group by periods observations. If freq is passed, the index will be increased using the periods and the freq.","Parameters: periodsint | Sequence[int], default 1Number of periods to shift. If a list of values, shift each group by each period. freqstr, optionalFrequency string. axisaxis to shift, default 0Shift direction. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. fill_valueoptionalThe scalar value to use for newly introduced missing values. Changed in version 2.1.0: Will raise a ValueError if freq is provided too. suffixstr, optionalA string to add to each shifted column if there are multiple periods. Ignored otherwise. Returns: Series or DataFrameObject shifted within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).shift(1)\na    NaN\na    1.0\nb    NaN\nb    3.0\ndtype: float64"", '>>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a  b  c\n    tuna   1  2  3\n  salmon   1  5  6\n catfish   2  5  8\ngoldfish   2  6  9\n>>> df.groupby(""a"").shift(1)\n              b    c\n    tuna    NaN  NaN\n  salmon    2.0  3.0\n catfish    NaN  NaN\ngoldfish    5.0  8.0']"
124,..\pandas\reference\api\pandas.Series.to_xarray.html,pandas.Series.to_xarray,Series.to_xarray()[source]# Return an xarray object from the pandas object. Notes See the xarray docs,"Returns: xarray.DataArray or xarray.DatasetData in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series.","["">>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n...                    ('parrot', 'bird', 24.0, 2),\n...                    ('lion', 'mammal', 80.5, 4),\n...                    ('monkey', 'mammal', np.nan, 4)],\n...                   columns=['name', 'class', 'max_speed',\n...                            'num_legs'])\n>>> df\n     name   class  max_speed  num_legs\n0  falcon    bird      389.0         2\n1  parrot    bird       24.0         2\n2    lion  mammal       80.5         4\n3  monkey  mammal        NaN         4"", "">>> df.to_xarray()  \n<xarray.Dataset>\nDimensions:    (index: 4)\nCoordinates:\n  * index      (index) int64 32B 0 1 2 3\nData variables:\n    name       (index) object 32B 'falcon' 'parrot' 'lion' 'monkey'\n    class      (index) object 32B 'bird' 'bird' 'mammal' 'mammal'\n    max_speed  (index) float64 32B 389.0 24.0 80.5 nan\n    num_legs   (index) int64 32B 2 2 4 4"", "">>> df['max_speed'].to_xarray()  \n<xarray.DataArray 'max_speed' (index: 4)>\narray([389. ,  24. ,  80.5,   nan])\nCoordinates:\n  * index    (index) int64 0 1 2 3"", "">>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n...                         '2018-01-02', '2018-01-02'])\n>>> df_multiindex = pd.DataFrame({'date': dates,\n...                               'animal': ['falcon', 'parrot',\n...                                          'falcon', 'parrot'],\n...                               'speed': [350, 18, 361, 15]})\n>>> df_multiindex = df_multiindex.set_index(['date', 'animal'])"", '>>> df_multiindex\n                   speed\ndate       animal\n2018-01-01 falcon    350\n           parrot     18\n2018-01-02 falcon    361\n           parrot     15', "">>> df_multiindex.to_xarray()  \n<xarray.Dataset>\nDimensions:  (date: 2, animal: 2)\nCoordinates:\n  * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n  * animal   (animal) object 'falcon' 'parrot'\nData variables:\n    speed    (date, animal) int64 350 18 361 15""]"
125,..\pandas\reference\api\pandas.tseries.offsets.Milli.n.html,pandas.tseries.offsets.Milli.n,Milli.n#,No parameters found,[]
126,..\pandas\reference\api\pandas.DataFrame.shift.html,pandas.DataFrame.shift,"DataFrame.shift(periods=1, freq=None, axis=0, fill_value=<no_default>, suffix=None)[source]# Shift index by desired number of periods with an optional time freq. When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError), the index will be increased using the periods and the freq. freq can be inferred when specified as “infer” as long as either freq or inferred_freq attribute is set in the index.","Parameters: periodsint or SequenceNumber of periods to shift. Can be positive or negative. If an iterable of ints, the data will be shifted once by each int. This is equivalent to shifting by one value at a time and concatenating all resulting frames. The resulting columns will have the shift suffixed to their column names. For multiple periods, axis must not be 1. freqDateOffset, tseries.offsets, timedelta, or str, optionalOffset to use from the tseries module or time rule (e.g. ‘EOM’). If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as “infer” then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneShift direction. For Series this parameter is unused and defaults to 0. fill_valueobject, optionalThe scalar value to use for newly introduced missing values. the default depends on the dtype of self. For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. NaT is used. For extension dtypes, self.dtype.na_value is used. suffixstr, optionalIf str and periods is an iterable, this is added after the column name and before the shift value for each shifted column name. Returns: DataFrameCopy of input object, shifted.","['>>> df = pd.DataFrame({""Col1"": [10, 20, 15, 30, 45],\n...                    ""Col2"": [13, 23, 18, 33, 48],\n...                    ""Col3"": [17, 27, 22, 37, 52]},\n...                   index=pd.date_range(""2020-01-01"", ""2020-01-05""))\n>>> df\n            Col1  Col2  Col3\n2020-01-01    10    13    17\n2020-01-02    20    23    27\n2020-01-03    15    18    22\n2020-01-04    30    33    37\n2020-01-05    45    48    52', '>>> df.shift(periods=3)\n            Col1  Col2  Col3\n2020-01-01   NaN   NaN   NaN\n2020-01-02   NaN   NaN   NaN\n2020-01-03   NaN   NaN   NaN\n2020-01-04  10.0  13.0  17.0\n2020-01-05  20.0  23.0  27.0', '>>> df.shift(periods=1, axis=""columns"")\n            Col1  Col2  Col3\n2020-01-01   NaN    10    13\n2020-01-02   NaN    20    23\n2020-01-03   NaN    15    18\n2020-01-04   NaN    30    33\n2020-01-05   NaN    45    48', '>>> df.shift(periods=3, fill_value=0)\n            Col1  Col2  Col3\n2020-01-01     0     0     0\n2020-01-02     0     0     0\n2020-01-03     0     0     0\n2020-01-04    10    13    17\n2020-01-05    20    23    27', '>>> df.shift(periods=3, freq=""D"")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52', '>>> df.shift(periods=3, freq=""infer"")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52', "">>> df['Col1'].shift(periods=[0, 1, 2])\n            Col1_0  Col1_1  Col1_2\n2020-01-01      10     NaN     NaN\n2020-01-02      20    10.0     NaN\n2020-01-03      15    20.0    10.0\n2020-01-04      30    15.0    20.0\n2020-01-05      45    30.0    15.0""]"
127,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.normalize.html,pandas.tseries.offsets.BYearBegin.normalize,BYearBegin.normalize#,No parameters found,[]
128,..\pandas\reference\api\pandas.Interval.closed_right.html,pandas.Interval.closed_right,Interval.closed_right# Check if the interval is closed on the right side. For the meaning of closed and open see Interval.,Returns: boolTrue if the Interval is closed on the left-side.,"["">>> iv = pd.Interval(0, 5, closed='both')\n>>> iv.closed_right\nTrue"", "">>> iv = pd.Interval(0, 5, closed='left')\n>>> iv.closed_right\nFalse""]"
129,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.repeat.html,pandas.api.extensions.ExtensionArray.repeat,"ExtensionArray.repeat(repeats, axis=None)[source]# Repeat elements of a ExtensionArray. Returns a new ExtensionArray where each element of the current ExtensionArray is repeated consecutively a given number of times.",Parameters: repeatsint or array of intsThe number of repetitions for each element. This should be a non-negative integer. Repeating 0 times will return an empty ExtensionArray. axisNoneMust be None. Has no effect but is accepted for compatibility with numpy. Returns: ExtensionArrayNewly created ExtensionArray with repeated elements.,"["">>> cat = pd.Categorical(['a', 'b', 'c'])\n>>> cat\n['a', 'b', 'c']\nCategories (3, object): ['a', 'b', 'c']\n>>> cat.repeat(2)\n['a', 'a', 'b', 'b', 'c', 'c']\nCategories (3, object): ['a', 'b', 'c']\n>>> cat.repeat([1, 2, 3])\n['a', 'b', 'b', 'c', 'c', 'c']\nCategories (3, object): ['a', 'b', 'c']""]"
130,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.searchsorted.html,pandas.api.extensions.ExtensionArray.searchsorted,"ExtensionArray.searchsorted(value, side='left', sorter=None)[source]# Find indices where elements should be inserted to maintain order. Find the indices into a sorted array self (a) such that, if the corresponding elements in value were inserted before the indices, the order of self would be preserved. Assuming that self is sorted: side returned index i satisfies left self[i-1] < value <= self[i] right self[i-1] <= value < self[i]","Parameters: valuearray-like, list or scalarValue(s) to insert into self. side{‘left’, ‘right’}, optionalIf ‘left’, the index of the first suitable location found is given. If ‘right’, return the last such index.  If there is no suitable index, return either 0 or N (where N is the length of self). sorter1-D array-like, optionalOptional array of integer indices that sort array a into ascending order. They are typically the result of argsort. Returns: array of ints or intIf value is array-like, array of insertion points. If value is scalar, a single integer.","['>>> arr = pd.array([1, 2, 3, 5])\n>>> arr.searchsorted([4])\narray([3])']"
131,..\pandas\reference\api\pandas.Interval.html,pandas.Interval,"class pandas.Interval# Immutable object implementing an Interval, a bounded slice-like interval. Notes The parameters left and right must be from the same type, you must be able to compare them and they must satisfy left <= right. A closed interval (in mathematics denoted by square brackets) contains its endpoints, i.e. the closed interval [0, 5] is characterized by the conditions 0 <= x <= 5. This is what closed='both' stands for. An open interval (in mathematics denoted by parentheses) does not contain its endpoints, i.e. the open interval (0, 5) is characterized by the conditions 0 < x < 5. This is what closed='neither' stands for. Intervals can also be half-open or half-closed, i.e. [0, 5) is described by 0 <= x < 5 (closed='left') and (0, 5] is described by 0 < x <= 5 (closed='right'). Examples It is possible to build Intervals of different types, like numeric ones: You can check if an element belongs to it, or if it contains another interval: You can test the bounds (closed='right', so 0 < x <= 5): Calculate its length You can operate with + and * over an Interval and the operation is applied to each of its bounds, so the result depends on the type of the bound elements To create a time interval you can use Timestamps as the bounds Attributes closed String describing the inclusive side the intervals. closed_left Check if the interval is closed on the left side. closed_right Check if the interval is closed on the right side. is_empty Indicates if an interval is empty, meaning it contains no points. left Left bound for the interval. length Return the length of the Interval. mid Return the midpoint of the Interval. open_left Check if the interval is open on the left side. open_right Check if the interval is open on the right side. right Right bound for the interval.","Parameters: leftorderable scalarLeft bound for the interval. rightorderable scalarRight bound for the interval. closed{‘right’, ‘left’, ‘both’, ‘neither’}, default ‘right’Whether the interval is closed on the left-side, right-side, both or neither. See the Notes for more detailed explanation.","["">>> iv = pd.Interval(left=0, right=5)\n>>> iv\nInterval(0, 5, closed='right')"", "">>> 2.5 in iv\nTrue\n>>> pd.Interval(left=2, right=5, closed='both') in iv\nTrue"", '>>> 0 in iv\nFalse\n>>> 5 in iv\nTrue\n>>> 0.0001 in iv\nTrue', '>>> iv.length\n5', "">>> shifted_iv = iv + 3\n>>> shifted_iv\nInterval(3, 8, closed='right')\n>>> extended_iv = iv * 10.0\n>>> extended_iv\nInterval(0.0, 50.0, closed='right')"", "">>> year_2017 = pd.Interval(pd.Timestamp('2017-01-01 00:00:00'),\n...                         pd.Timestamp('2018-01-01 00:00:00'),\n...                         closed='left')\n>>> pd.Timestamp('2017-01-01 00:00') in year_2017\nTrue\n>>> year_2017.length\nTimedelta('365 days 00:00:00')""]"
132,..\pandas\reference\api\pandas.DataFrame.size.html,pandas.DataFrame.size,property DataFrame.size[source]# Return an int representing the number of elements in this object. Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame.,No parameters found,"["">>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n>>> s.size\n3"", "">>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n>>> df.size\n4""]"
133,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.size.html,pandas.core.groupby.SeriesGroupBy.size,SeriesGroupBy.size()[source]# Compute group sizes.,Returns: DataFrame or SeriesNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na     1\na     2\nb     3\ndtype: int64\n>>> ser.groupby(level=0).size()\na    2\nb    1\ndtype: int64"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(""a"").size()\na\n1    2\n7    1\ndtype: int64', "">>> ser = pd.Series([1, 2, 3], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\ndtype: int64\n>>> ser.resample('MS').size()\n2023-01-01    2\n2023-02-01    1\nFreq: MS, dtype: int64""]"
134,..\pandas\reference\api\pandas.Series.transform.html,pandas.Series.transform,"Series.transform(func, axis=0, *args, **kwargs)[source]# Call func on self producing a Series with the same axis shape as self. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funcfunction, str, list-like or dict-likeFunction to use for transforming the data. If a function, must either work when passed a Series or when passed to Series.apply. If func is both list-like and dict-like, dict-like behavior takes precedence. Accepted combinations are: function string function name list-like of functions and/or function names, e.g. [np.exp, 'sqrt'] dict-like of axis labels -> functions, function names or list-like of such. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: SeriesA Series that must have the same length as self. Raises: ValueErrorIf the returned Series has a different length than self.","["">>> df = pd.DataFrame({'A': range(3), 'B': range(1, 4)})\n>>> df\n   A  B\n0  0  1\n1  1  2\n2  2  3\n>>> df.transform(lambda x: x + 1)\n   A  B\n0  1  2\n1  2  3\n2  3  4"", '>>> s = pd.Series(range(3))\n>>> s\n0    0\n1    1\n2    2\ndtype: int64\n>>> s.transform([np.sqrt, np.exp])\n       sqrt        exp\n0  0.000000   1.000000\n1  1.000000   2.718282\n2  1.414214   7.389056', '>>> df = pd.DataFrame({\n...     ""Date"": [\n...         ""2015-05-08"", ""2015-05-07"", ""2015-05-06"", ""2015-05-05"",\n...         ""2015-05-08"", ""2015-05-07"", ""2015-05-06"", ""2015-05-05""],\n...     ""Data"": [5, 8, 6, 1, 50, 100, 60, 120],\n... })\n>>> df\n         Date  Data\n0  2015-05-08     5\n1  2015-05-07     8\n2  2015-05-06     6\n3  2015-05-05     1\n4  2015-05-08    50\n5  2015-05-07   100\n6  2015-05-06    60\n7  2015-05-05   120\n>>> df.groupby(\'Date\')[\'Data\'].transform(\'sum\')\n0     55\n1    108\n2     66\n3    121\n4     55\n5    108\n6     66\n7    121\nName: Data, dtype: int64', '>>> df = pd.DataFrame({\n...     ""c"": [1, 1, 1, 2, 2, 2, 2],\n...     ""type"": [""m"", ""n"", ""o"", ""m"", ""m"", ""n"", ""n""]\n... })\n>>> df\n   c type\n0  1    m\n1  1    n\n2  1    o\n3  2    m\n4  2    m\n5  2    n\n6  2    n\n>>> df[\'size\'] = df.groupby(\'c\')[\'type\'].transform(len)\n>>> df\n   c type size\n0  1    m    3\n1  1    n    3\n2  1    o    3\n3  2    m    4\n4  2    m    4\n5  2    n    4\n6  2    n    4']"
135,..\pandas\reference\api\pandas.tseries.offsets.Milli.name.html,pandas.tseries.offsets.Milli.name,Milli.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
136,..\pandas\reference\api\pandas.Series.dot.html,pandas.Series.dot,"Series.dot(other)[source]# Compute the dot product between the Series and the columns of other. This method computes the dot product between the Series and another one, or the Series and each columns of a DataFrame, or the Series and each columns of an array. It can also be called using self @ other. Notes The Series and other has to share the same index if other is a Series or a DataFrame.","Parameters: otherSeries, DataFrame or array-likeThe other object to compute the dot product with its columns. Returns: scalar, Series or numpy.ndarrayReturn the dot product of the Series and other if other is a Series, the Series of the dot product of Series and each rows of other if other is a DataFrame or a numpy.ndarray between the Series and each columns of the numpy array.","['>>> s = pd.Series([0, 1, 2, 3])\n>>> other = pd.Series([-1, 2, -3, 4])\n>>> s.dot(other)\n8\n>>> s @ other\n8\n>>> df = pd.DataFrame([[0, 1], [-2, 3], [4, -5], [6, 7]])\n>>> s.dot(df)\n0    24\n1    14\ndtype: int64\n>>> arr = np.array([[0, 1], [-2, 3], [4, -5], [6, 7]])\n>>> s.dot(arr)\narray([24, 14])']"
137,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.rule_code.html,pandas.tseries.offsets.BYearBegin.rule_code,BYearBegin.rule_code#,No parameters found,[]
138,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.shape.html,pandas.api.extensions.ExtensionArray.shape,property ExtensionArray.shape[source]# Return a tuple of the array dimensions.,No parameters found,"['>>> arr = pd.array([1, 2, 3])\n>>> arr.shape\n(3,)']"
139,..\pandas\reference\api\pandas.Interval.is_empty.html,pandas.Interval.is_empty,"Interval.is_empty# Indicates if an interval is empty, meaning it contains no points.","Returns: bool or ndarrayA boolean indicating if a scalar Interval is empty, or a boolean ndarray positionally indicating if an Interval in an IntervalArray or IntervalIndex is empty.","["">>> pd.Interval(0, 1, closed='right').is_empty\nFalse"", "">>> pd.Interval(0, 0, closed='right').is_empty\nTrue\n>>> pd.Interval(0, 0, closed='left').is_empty\nTrue\n>>> pd.Interval(0, 0, closed='neither').is_empty\nTrue"", "">>> pd.Interval(0, 0, closed='both').is_empty\nFalse"", "">>> ivs = [pd.Interval(0, 0, closed='neither'),\n...        pd.Interval(1, 2, closed='neither')]\n>>> pd.arrays.IntervalArray(ivs).is_empty\narray([ True, False])"", "">>> ivs = [pd.Interval(0, 0, closed='neither'), np.nan]\n>>> pd.IntervalIndex(ivs).is_empty\narray([ True, False])""]"
140,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.copy.html,pandas.tseries.offsets.BYearEnd.copy,BYearEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
141,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.skew.html,pandas.core.groupby.SeriesGroupBy.skew,"SeriesGroupBy.skew(axis=<no_default>, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased skew within groups. Normalized by N-1.","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Axis for the function to be applied on. This parameter is only for compatibility with DataFrame and is unused. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series","['>>> ser = pd.Series([390., 350., 357., np.nan, 22., 20., 30.],\n...                 index=[\'Falcon\', \'Falcon\', \'Falcon\', \'Falcon\',\n...                        \'Parrot\', \'Parrot\', \'Parrot\'],\n...                 name=""Max Speed"")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nFalcon    357.0\nFalcon      NaN\nParrot     22.0\nParrot     20.0\nParrot     30.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).skew()\nFalcon    1.525174\nParrot    1.457863\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).skew(skipna=False)\nFalcon         NaN\nParrot    1.457863\nName: Max Speed, dtype: float64']"
142,..\pandas\reference\api\pandas.Series.truediv.html,pandas.Series.truediv,"Series.truediv(other, level=None, fill_value=None, axis=0)[source]# Return Floating division of series and other, element-wise (binary operator truediv). Equivalent to series / other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.divide(b, fill_value=0)\na    1.0\nb    inf\nc    inf\nd    0.0\ne    NaN\ndtype: float64""]"
143,..\pandas\reference\api\pandas.DataFrame.skew.html,pandas.DataFrame.skew,"DataFrame.skew(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased skew over requested axis. Normalized by N-1.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","['>>> s = pd.Series([1, 2, 3])\n>>> s.skew()\n0.0', "">>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4], 'c': [1, 3, 5]},\n...                   index=['tiger', 'zebra', 'cow'])\n>>> df\n        a   b   c\ntiger   1   2   1\nzebra   2   3   3\ncow     3   4   5\n>>> df.skew()\na   0.0\nb   0.0\nc   0.0\ndtype: float64"", '>>> df.skew(axis=1)\ntiger   1.732051\nzebra  -1.732051\ncow     0.000000\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2, 3], 'b': ['T', 'Z', 'X']},\n...                   index=['tiger', 'zebra', 'cow'])\n>>> df.skew(numeric_only=True)\na   0.0\ndtype: float64""]"
144,..\pandas\reference\api\pandas.tseries.offsets.Milli.nanos.html,pandas.tseries.offsets.Milli.nanos,Milli.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
145,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.shift.html,pandas.api.extensions.ExtensionArray.shift,"ExtensionArray.shift(periods=1, fill_value=None)[source]# Shift values by desired number. Newly introduced missing values are filled with self.dtype.na_value. Notes If self is empty or periods is 0, a copy of self is returned. If periods > len(self), then an array of size len(self) is returned, with all values filled with self.dtype.na_value. For 2-dimensional ExtensionArrays, we are always shifting along axis=0.","Parameters: periodsint, default 1The number of periods to shift. Negative values are allowed for shifting backwards. fill_valueobject, optionalThe scalar value to use for newly introduced missing values. The default is self.dtype.na_value. Returns: ExtensionArrayShifted.","['>>> arr = pd.array([1, 2, 3])\n>>> arr.shift(2)\n<IntegerArray>\n[<NA>, <NA>, 1]\nLength: 3, dtype: Int64']"
146,..\pandas\reference\api\pandas.Series.drop.html,pandas.Series.drop,"Series.drop(labels=None, *, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]# Return Series with specified index labels removed. Remove elements of a Series based on specifying the index labels. When using a multi-index, labels on different levels can be removed by specifying the level.","Parameters: labelssingle label or list-likeIndex labels to drop. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. indexsingle label or list-likeRedundant for application on Series, but ‘index’ can be used instead of ‘labels’. columnssingle label or list-likeNo change is made to the Series; use ‘index’ or ‘labels’ instead. levelint or level name, optionalFor MultiIndex, level for which the labels will be removed. inplacebool, default FalseIf True, do operation inplace and return None. errors{‘ignore’, ‘raise’}, default ‘raise’If ‘ignore’, suppress error and only existing labels are dropped. Returns: Series or NoneSeries with specified index labels removed or None if inplace=True. Raises: KeyErrorIf none of the labels are found in the index.","["">>> s = pd.Series(data=np.arange(3), index=['A', 'B', 'C'])\n>>> s\nA  0\nB  1\nC  2\ndtype: int64"", "">>> s.drop(labels=['B', 'C'])\nA  0\ndtype: int64"", "">>> midx = pd.MultiIndex(levels=[['llama', 'cow', 'falcon'],\n...                              ['speed', 'weight', 'length']],\n...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n>>> s = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n...               index=midx)\n>>> s\nllama   speed      45.0\n        weight    200.0\n        length      1.2\ncow     speed      30.0\n        weight    250.0\n        length      1.5\nfalcon  speed     320.0\n        weight      1.0\n        length      0.3\ndtype: float64"", "">>> s.drop(labels='weight', level=1)\nllama   speed      45.0\n        length      1.2\ncow     speed      30.0\n        length      1.5\nfalcon  speed     320.0\n        length      0.3\ndtype: float64""]"
147,..\pandas\reference\api\pandas.Interval.left.html,pandas.Interval.left,Interval.left# Left bound for the interval.,No parameters found,"["">>> interval = pd.Interval(left=1, right=2, closed='left')\n>>> interval\nInterval(1, 2, closed='left')\n>>> interval.left\n1""]"
148,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.freqstr.html,pandas.tseries.offsets.BYearEnd.freqstr,BYearEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
149,..\pandas\reference\api\pandas.Series.truncate.html,pandas.Series.truncate,"Series.truncate(before=None, after=None, axis=None, copy=None)[source]# Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Notes If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps.","Parameters: beforedate, str, intTruncate all rows before this index value. afterdate, str, intTruncate all rows after this index value. axis{0 or ‘index’, 1 or ‘columns’}, optionalAxis to truncate. Truncates the index (rows) by default. For Series this parameter is unused and defaults to 0. copybool, default is True,Return a copy of the truncated section. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: type of callerThe truncated Series or DataFrame.","["">>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n...                    'B': ['f', 'g', 'h', 'i', 'j'],\n...                    'C': ['k', 'l', 'm', 'n', 'o']},\n...                   index=[1, 2, 3, 4, 5])\n>>> df\n   A  B  C\n1  a  f  k\n2  b  g  l\n3  c  h  m\n4  d  i  n\n5  e  j  o"", '>>> df.truncate(before=2, after=4)\n   A  B  C\n2  b  g  l\n3  c  h  m\n4  d  i  n', '>>> df.truncate(before=""A"", after=""B"", axis=""columns"")\n   A  B\n1  a  f\n2  b  g\n3  c  h\n4  d  i\n5  e  j', "">>> df['A'].truncate(before=2, after=4)\n2    b\n3    c\n4    d\nName: A, dtype: object"", "">>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n>>> df = pd.DataFrame(index=dates, data={'A': 1})\n>>> df.tail()\n                     A\n2016-01-31 23:59:56  1\n2016-01-31 23:59:57  1\n2016-01-31 23:59:58  1\n2016-01-31 23:59:59  1\n2016-02-01 00:00:00  1"", "">>> df.truncate(before=pd.Timestamp('2016-01-05'),\n...             after=pd.Timestamp('2016-01-10')).tail()\n                     A\n2016-01-09 23:59:56  1\n2016-01-09 23:59:57  1\n2016-01-09 23:59:58  1\n2016-01-09 23:59:59  1\n2016-01-10 00:00:00  1"", "">>> df.truncate('2016-01-05', '2016-01-10').tail()\n                     A\n2016-01-09 23:59:56  1\n2016-01-09 23:59:57  1\n2016-01-09 23:59:58  1\n2016-01-09 23:59:59  1\n2016-01-10 00:00:00  1"", "">>> df.loc['2016-01-05':'2016-01-10', :].tail()\n                     A\n2016-01-10 23:59:55  1\n2016-01-10 23:59:56  1\n2016-01-10 23:59:57  1\n2016-01-10 23:59:58  1\n2016-01-10 23:59:59  1""]"
150,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.std.html,pandas.core.groupby.SeriesGroupBy.std,"SeriesGroupBy.std(ddof=1, engine=None, engine_kwargs=None, numeric_only=False)[source]# Compute standard deviation of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}} Added in version 1.4.0. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameStandard deviation of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).std()\na    3.21455\nb    0.57735\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).std()\n              a         b\ndog    2.000000  3.511885\nmouse  2.217356  1.500000""]"
151,..\pandas\reference\api\pandas.Series.droplevel.html,pandas.Series.droplevel,"Series.droplevel(level, axis=0)[source]# Return Series/DataFrame with requested index / column level(s) removed.","Parameters: levelint, str, or list-likeIf a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels. axis{0 or ‘index’, 1 or ‘columns’}, default 0Axis along which the level(s) is removed: 0 or ‘index’: remove level(s) in column. 1 or ‘columns’: remove level(s) in row. For Series this parameter is unused and defaults to 0. Returns: Series/DataFrameSeries/DataFrame with requested index / column level(s) removed.","["">>> df = pd.DataFrame([\n...     [1, 2, 3, 4],\n...     [5, 6, 7, 8],\n...     [9, 10, 11, 12]\n... ]).set_index([0, 1]).rename_axis(['a', 'b'])"", "">>> df.columns = pd.MultiIndex.from_tuples([\n...     ('c', 'e'), ('d', 'f')\n... ], names=['level_1', 'level_2'])"", '>>> df\nlevel_1   c   d\nlevel_2   e   f\na b\n1 2      3   4\n5 6      7   8\n9 10    11  12', "">>> df.droplevel('a')\nlevel_1   c   d\nlevel_2   e   f\nb\n2        3   4\n6        7   8\n10      11  12"", "">>> df.droplevel('level_2', axis=1)\nlevel_1   c   d\na b\n1 2      3   4\n5 6      7   8\n9 10    11  12""]"
152,..\pandas\reference\api\pandas.Interval.length.html,pandas.Interval.length,Interval.length# Return the length of the Interval.,No parameters found,"["">>> interval = pd.Interval(left=1, right=2, closed='left')\n>>> interval\nInterval(1, 2, closed='left')\n>>> interval.length\n1""]"
153,..\pandas\reference\api\pandas.DataFrame.sort_index.html,pandas.DataFrame.sort_index,"DataFrame.sort_index(*, axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)[source]# Sort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False, otherwise updates the original DataFrame and returns None.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis along which to sort.  The value 0 identifies the rows, and 1 identifies the columns. levelint or level name or list of ints or list of level namesIf not None, sort on values in specified index level(s). ascendingbool or list-like of bools, default TrueSort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually. inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one. kind{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, default ‘quicksort’Choice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position{‘first’, ‘last’}, default ‘last’Puts NaNs at the beginning if first; last puts NaNs at the end. Not implemented for MultiIndex. sort_remainingbool, default TrueIf True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. keycallable, optionalIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level. Returns: DataFrame or NoneThe original DataFrame sorted by the labels or None if inplace=True.","["">>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n...                   columns=['A'])\n>>> df.sort_index()\n     A\n1    4\n29   2\n100  1\n150  5\n234  3"", '>>> df.sort_index(ascending=False)\n     A\n234  3\n150  5\n100  1\n29   2\n1    4', '>>> df = pd.DataFrame({""a"": [1, 2, 3, 4]}, index=[\'A\', \'b\', \'C\', \'d\'])\n>>> df.sort_index(key=lambda x: x.str.lower())\n   a\nA  1\nb  2\nC  3\nd  4']"
154,..\pandas\reference\api\pandas.tseries.offsets.Milli.normalize.html,pandas.tseries.offsets.Milli.normalize,Milli.normalize#,No parameters found,[]
155,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.take.html,pandas.api.extensions.ExtensionArray.take,"ExtensionArray.take(indices, *, allow_fill=False, fill_value=None)[source]# Take elements from an array. Notes ExtensionArray.take is called by Series.__getitem__, .loc, iloc, when indices is a sequence of values. Additionally, it’s called by Series.reindex(), or any other method that causes realignment, with a fill_value.","Parameters: indicessequence of int or one-dimensional np.ndarray of intIndices to be taken. allow_fillbool, default FalseHow to handle negative values in indices. False: negative values in indices indicate positional indices from the right (the default). This is similar to numpy.take(). True: negative values in indices indicate missing values. These values are set to fill_value. Any other other negative values raise a ValueError. fill_valueany, optionalFill value to use for NA-indices when allow_fill is True. This may be None, in which case the default NA value for the type, self.dtype.na_value, is used. For many ExtensionArrays, there will be two representations of fill_value: a user-facing “boxed” scalar, and a low-level physical NA value. fill_value should be the user-facing version, and the implementation should handle translating that to the physical version for processing the take if necessary. Returns: ExtensionArray Raises: IndexErrorWhen the indices are out of bounds for the array. ValueErrorWhen indices contains negative values other than -1 and allow_fill is True.",[]
156,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.html,pandas.tseries.offsets.BYearEnd,class pandas.tseries.offsets.BYearEnd# DateOffset increments between the last business day of the year. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. month n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of years represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. monthint, default 12A specific integer for the month of the year.","["">>> from pandas.tseries.offsets import BYearEnd\n>>> ts = pd.Timestamp('2020-05-24 05:01:15')\n>>> ts - BYearEnd()\nTimestamp('2019-12-31 05:01:15')\n>>> ts + BYearEnd()\nTimestamp('2020-12-31 05:01:15')\n>>> ts + BYearEnd(3)\nTimestamp('2022-12-30 05:01:15')\n>>> ts + BYearEnd(-3)\nTimestamp('2017-12-29 05:01:15')\n>>> ts + BYearEnd(month=11)\nTimestamp('2020-11-30 05:01:15')""]"
157,..\pandas\reference\api\pandas.Series.tz_convert.html,pandas.Series.tz_convert,"Series.tz_convert(tz, axis=0, level=None, copy=None)[source]# Convert tz-aware axis to target time zone.","Parameters: tzstr or tzinfo object or NoneTarget time zone. Passing None will convert to UTC and remove the timezone information. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to convert levelint, str, default NoneIf axis is a MultiIndex, convert a specific level. Otherwise must be None. copybool, default TrueAlso make a copy of the underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: Series/DataFrameObject with time zone converted axis. Raises: TypeErrorIf the axis is tz-naive.","["">>> s = pd.Series(\n...     [1],\n...     index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']),\n... )\n>>> s.tz_convert('Asia/Shanghai')\n2018-09-15 07:30:00+08:00    1\ndtype: int64"", "">>> s = pd.Series([1],\n...               index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n>>> s.tz_convert(None)\n2018-09-14 23:30:00    1\ndtype: int64""]"
158,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.tolist.html,pandas.api.extensions.ExtensionArray.tolist,"ExtensionArray.tolist()[source]# Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)",Returns: list,"['>>> arr = pd.array([1, 2, 3])\n>>> arr.tolist()\n[1, 2, 3]']"
159,..\pandas\reference\api\pandas.Interval.mid.html,pandas.Interval.mid,Interval.mid# Return the midpoint of the Interval.,No parameters found,"['>>> iv = pd.Interval(0, 5)\n>>> iv.mid\n2.5']"
160,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.sum.html,pandas.core.groupby.SeriesGroupBy.sum,"SeriesGroupBy.sum(numeric_only=False, min_count=0, engine=None, engine_kwargs=None)[source]# Compute sum of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. enginestr, default None None 'cython' : Runs rolling apply through C-extensions from cython. 'numba'Runs rolling apply through JIT compiled code from numba.Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogiland parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply groupby aggregation. Returns: Series or DataFrameComputed sum of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).sum()\na    3\nb    7\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").sum()\n     b   c\na\n1   10   7\n2   11  17']"
161,..\pandas\reference\api\pandas.DataFrame.sort_values.html,pandas.DataFrame.sort_values,"DataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]# Sort by the values along either axis.","Parameters: bystr or list of strName or list of names to sort by. if axis is 0 or ‘index’ then by may contain index levels and/or column labels. if axis is 1 or ‘columns’ then by may contain column levels and/or index labels. axis“{0 or ‘index’, 1 or ‘columns’}”, default 0Axis to be sorted. ascendingbool or list of bool, default TrueSort ascending vs. descending. Specify list for multiple sort orders.  If this is a list of bools, must match the length of the by. inplacebool, default FalseIf True, perform operation in-place. kind{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, default ‘quicksort’Choice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position{‘first’, ‘last’}, default ‘last’Puts NaNs at the beginning if first; last puts NaNs at the end. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. keycallable, optionalApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently. Returns: DataFrame or NoneDataFrame with sorted values or None if inplace=True.","["">>> df = pd.DataFrame({\n...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n...     'col2': [2, 1, 9, 8, 7, 4],\n...     'col3': [0, 1, 9, 4, 2, 3],\n...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n... })\n>>> df\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F"", "">>> df.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D"", "">>> df.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D"", "">>> df.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D"", "">>> df.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B"", "">>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F"", '>>> df = pd.DataFrame({\n...    ""time"": [\'0hr\', \'128hr\', \'72hr\', \'48hr\', \'96hr\'],\n...    ""value"": [10, 20, 30, 40, 50]\n... })\n>>> df\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\n>>> from natsort import index_natsorted\n>>> df.sort_values(\n...     by=""time"",\n...     key=lambda x: np.argsort(index_natsorted(df[""time""]))\n... )\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20']"
162,..\pandas\reference\api\pandas.Series.dropna.html,pandas.Series.dropna,"Series.dropna(*, axis=0, inplace=False, how=None, ignore_index=False)[source]# Return a new Series with missing values removed. See the User Guide for more on which values are considered missing, and how to work with missing data.","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. inplacebool, default FalseIf True, do operation inplace and return None. howstr, optionalNot in use. Kept for compatibility. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. Added in version 2.0.0. Returns: Series or NoneSeries with NA entries dropped from it or None if inplace=True.","['>>> ser = pd.Series([1., 2., np.nan])\n>>> ser\n0    1.0\n1    2.0\n2    NaN\ndtype: float64', '>>> ser.dropna()\n0    1.0\n1    2.0\ndtype: float64', "">>> ser = pd.Series([np.nan, 2, pd.NaT, '', None, 'I stay'])\n>>> ser\n0       NaN\n1         2\n2       NaT\n3\n4      None\n5    I stay\ndtype: object\n>>> ser.dropna()\n1         2\n3\n5    I stay\ndtype: object""]"
163,..\pandas\reference\api\pandas.tseries.offsets.Milli.rule_code.html,pandas.tseries.offsets.Milli.rule_code,Milli.rule_code#,No parameters found,[]
164,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_anchored.html,pandas.tseries.offsets.BYearEnd.is_anchored,BYearEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
165,..\pandas\reference\api\pandas.Series.tz_localize.html,pandas.Series.tz_localize,"Series.tz_localize(tz, axis=0, level=None, copy=None, ambiguous='raise', nonexistent='raise')[source]# Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use Series.dt.tz_localize().","Parameters: tzstr or tzinfo or NoneTime zone to localize. Passing None will remove the time zone information and preserve local time. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to localize levelint, str, default NoneIf axis ia a MultiIndex, localize a specific level. Otherwise must be None. copybool, default TrueAlso make a copy of the underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistentstr, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are: ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: Series/DataFrameSame type as the input. Raises: TypeErrorIf the TimeSeries is tz-aware and tz is not None.","["">>> s = pd.Series(\n...     [1],\n...     index=pd.DatetimeIndex(['2018-09-15 01:30:00']),\n... )\n>>> s.tz_localize('CET')\n2018-09-15 01:30:00+02:00    1\ndtype: int64"", "">>> s = pd.Series([1],\n...               index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n>>> s.tz_localize(None)\n2018-09-15 01:30:00    1\ndtype: int64"", "">>> s = pd.Series(range(7),\n...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',\n...                                       '2018-10-28 02:00:00',\n...                                       '2018-10-28 02:30:00',\n...                                       '2018-10-28 02:00:00',\n...                                       '2018-10-28 02:30:00',\n...                                       '2018-10-28 03:00:00',\n...                                       '2018-10-28 03:30:00']))\n>>> s.tz_localize('CET', ambiguous='infer')\n2018-10-28 01:30:00+02:00    0\n2018-10-28 02:00:00+02:00    1\n2018-10-28 02:30:00+02:00    2\n2018-10-28 02:00:00+01:00    3\n2018-10-28 02:30:00+01:00    4\n2018-10-28 03:00:00+01:00    5\n2018-10-28 03:30:00+01:00    6\ndtype: int64"", "">>> s = pd.Series(range(3),\n...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',\n...                                       '2018-10-28 02:36:00',\n...                                       '2018-10-28 03:46:00']))\n>>> s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n2018-10-28 01:20:00+02:00    0\n2018-10-28 02:36:00+02:00    1\n2018-10-28 03:46:00+01:00    2\ndtype: int64"", "">>> s = pd.Series(range(2),\n...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',\n...                                       '2015-03-29 03:30:00']))\n>>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n2015-03-29 03:00:00+02:00    0\n2015-03-29 03:30:00+02:00    1\ndtype: int64\n>>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n2015-03-29 01:59:59.999999999+01:00    0\n2015-03-29 03:30:00+02:00              1\ndtype: int64\n>>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1h'))\n2015-03-29 03:30:00+02:00    0\n2015-03-29 03:30:00+02:00    1\ndtype: int64""]"
166,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.unique.html,pandas.api.extensions.ExtensionArray.unique,ExtensionArray.unique()[source]# Compute the ExtensionArray of unique values.,Returns: pandas.api.extensions.ExtensionArray,"['>>> arr = pd.array([1, 2, 3, 1, 2, 3])\n>>> arr.unique()\n<IntegerArray>\n[1, 2, 3]\nLength: 3, dtype: Int64']"
167,..\pandas\reference\api\pandas.DataFrame.sparse.density.html,pandas.DataFrame.sparse.density,DataFrame.sparse.density[source]# Ratio of non-sparse points to total (dense) data points.,No parameters found,"['>>> df = pd.DataFrame({""A"": pd.arrays.SparseArray([0, 1, 0, 1])})\n>>> df.sparse.density\n0.5']"
168,..\pandas\reference\api\pandas.Interval.open_left.html,pandas.Interval.open_left,Interval.open_left# Check if the interval is open on the left side. For the meaning of closed and open see Interval.,Returns: boolTrue if the Interval is not closed on the left-side.,"["">>> iv = pd.Interval(0, 5, closed='neither')\n>>> iv.open_left\nTrue"", "">>> iv = pd.Interval(0, 5, closed='both')\n>>> iv.open_left\nFalse""]"
169,..\pandas\reference\api\pandas.tseries.offsets.Minute.copy.html,pandas.tseries.offsets.Minute.copy,Minute.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
170,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.tail.html,pandas.core.groupby.SeriesGroupBy.tail,"SeriesGroupBy.tail(n=5)[source]# Return last n rows of each group. Similar to .apply(lambda x: x.tail(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).",Parameters: nintIf positive: number of entries to include from end of each group. If negative: number of entries to exclude from start of each group. Returns: Series or DataFrameSubset of original Series or DataFrame as determined by n.,"["">>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').tail(1)\n   A  B\n1  a  2\n3  b  2\n>>> df.groupby('A').tail(-1)\n   A  B\n1  a  2\n3  b  2""]"
171,..\pandas\reference\api\pandas.Series.drop_duplicates.html,pandas.Series.drop_duplicates,"Series.drop_duplicates(*, keep='first', inplace=False, ignore_index=False)[source]# Return Series with duplicate values removed.","Parameters: keep{‘first’, ‘last’, False}, default ‘first’Method to handle dropping duplicates: ‘first’ : Drop duplicates except for the first occurrence. ‘last’ : Drop duplicates except for the last occurrence. False : Drop all duplicates. inplacebool, default FalseIf True, performs operation inplace and returns None. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. Added in version 2.0.0. Returns: Series or NoneSeries with duplicates dropped or None if inplace=True.","["">>> s = pd.Series(['llama', 'cow', 'llama', 'beetle', 'llama', 'hippo'],\n...               name='animal')\n>>> s\n0     llama\n1       cow\n2     llama\n3    beetle\n4     llama\n5     hippo\nName: animal, dtype: object"", '>>> s.drop_duplicates()\n0     llama\n1       cow\n3    beetle\n5     hippo\nName: animal, dtype: object', "">>> s.drop_duplicates(keep='last')\n1       cow\n3    beetle\n4     llama\n5     hippo\nName: animal, dtype: object"", '>>> s.drop_duplicates(keep=False)\n1       cow\n3    beetle\n5     hippo\nName: animal, dtype: object']"
172,..\pandas\reference\api\pandas.api.extensions.ExtensionArray.view.html,pandas.api.extensions.ExtensionArray.view,ExtensionArray.view(dtype=None)[source]# Return a view on the array.,"Parameters: dtypestr, np.dtype, or ExtensionDtype, optionalDefault None. Returns: ExtensionArray or np.ndarrayA view on the ExtensionArray’s data.","['>>> arr = pd.array([1, 2, 3])\n>>> arr2 = arr.view()\n>>> arr[0] = 2\n>>> arr2\n<IntegerArray>\n[2, 2, 3]\nLength: 3, dtype: Int64']"
173,..\pandas\reference\api\pandas.Series.unique.html,pandas.Series.unique,"Series.unique()[source]# Return unique values of Series object. Uniques are returned in order of appearance. Hash table-based unique, therefore does NOT sort. Notes Returns the unique values as a NumPy array. In case of an extension-array backed Series, a new ExtensionArray of that type with just the unique values is returned. This includes Categorical Period Datetime with Timezone Datetime without Timezone Timedelta Interval Sparse IntegerNA See Examples section.",Returns: ndarray or ExtensionArrayThe unique values returned as a NumPy array. See Notes.,"["">>> pd.Series([2, 1, 3, 3], name='A').unique()\narray([2, 1, 3])"", "">>> pd.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n<DatetimeArray>\n['2016-01-01 00:00:00']\nLength: 1, dtype: datetime64[ns]"", "">>> pd.Series([pd.Timestamp('2016-01-01', tz='US/Eastern')\n...            for _ in range(3)]).unique()\n<DatetimeArray>\n['2016-01-01 00:00:00-05:00']\nLength: 1, dtype: datetime64[ns, US/Eastern]"", "">>> pd.Series(pd.Categorical(list('baabc'))).unique()\n['b', 'a', 'c']\nCategories (3, object): ['a', 'b', 'c']\n>>> pd.Series(pd.Categorical(list('baabc'), categories=list('abc'),\n...                          ordered=True)).unique()\n['b', 'a', 'c']\nCategories (3, object): ['a' < 'b' < 'c']""]"
174,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_month_end.html,pandas.tseries.offsets.BYearEnd.is_month_end,BYearEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
175,..\pandas\reference\api\pandas.DataFrame.sparse.from_spmatrix.html,pandas.DataFrame.sparse.from_spmatrix,"classmethod DataFrame.sparse.from_spmatrix(data, index=None, columns=None)[source]# Create a new DataFrame from a scipy sparse matrix.","Parameters: datascipy.sparse.spmatrixMust be convertible to csc format. index, columnsIndex, optionalRow and column labels to use for the resulting DataFrame. Defaults to a RangeIndex. Returns: DataFrameEach column of the DataFrame is stored as a arrays.SparseArray.","['>>> import scipy.sparse\n>>> mat = scipy.sparse.eye(3, dtype=float)\n>>> pd.DataFrame.sparse.from_spmatrix(mat)\n     0    1    2\n0  1.0    0    0\n1    0  1.0    0\n2    0    0  1.0']"
176,..\pandas\reference\api\pandas.Interval.open_right.html,pandas.Interval.open_right,Interval.open_right# Check if the interval is open on the right side. For the meaning of closed and open see Interval.,Returns: boolTrue if the Interval is not closed on the left-side.,"["">>> iv = pd.Interval(0, 5, closed='left')\n>>> iv.open_right\nTrue"", '>>> iv = pd.Interval(0, 5)\n>>> iv.open_right\nFalse']"
177,..\pandas\reference\api\pandas.Series.dt.as_unit.html,pandas.Series.dt.as_unit,"Series.dt.as_unit(*args, **kwargs)[source]#",No parameters found,[]
178,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._accumulate.html,pandas.api.extensions.ExtensionArray._accumulate,"ExtensionArray._accumulate(name, *, skipna=True, **kwargs)[source]# Return an ExtensionArray performing an accumulation operation. The underlying data type might change.","Parameters: namestrName of the function, supported values are: - cummin - cummax - cumsum - cumprod skipnabool, default TrueIf True, skip NA values. **kwargsAdditional keyword arguments passed to the accumulation function. Currently, there is no supported kwarg. Returns: array Raises: NotImplementedErrorsubclass does not define accumulations","["">>> arr = pd.array([1, 2, 3])\n>>> arr._accumulate(name='cumsum')\n<IntegerArray>\n[1, 3, 6]\nLength: 3, dtype: Int64""]"
179,..\pandas\reference\api\pandas.Series.unstack.html,pandas.Series.unstack,"Series.unstack(level=-1, fill_value=None, sort=True)[source]# Unstack, also known as pivot, Series with MultiIndex to produce DataFrame. Notes Reference the user guide for more examples.","Parameters: levelint, str, or list of these, default last levelLevel(s) to unstack, can pass level name. fill_valuescalar value, default NoneValue to use when replacing NaN values. sortbool, default TrueSort the level(s) in the resulting MultiIndex columns. Returns: DataFrameUnstacked Series.","["">>> s = pd.Series([1, 2, 3, 4],\n...               index=pd.MultiIndex.from_product([['one', 'two'],\n...                                                 ['a', 'b']]))\n>>> s\none  a    1\n     b    2\ntwo  a    3\n     b    4\ndtype: int64"", '>>> s.unstack(level=-1)\n     a  b\none  1  2\ntwo  3  4', '>>> s.unstack(level=0)\n   one  two\na    1    3\nb    2    4']"
180,..\pandas\reference\api\pandas.DataFrame.sparse.html,pandas.DataFrame.sparse,DataFrame.sparse()[source]# DataFrame accessor for sparse data.,No parameters found,"['>>> df = pd.DataFrame({""a"": [1, 2, 0, 0],\n...                   ""b"": [3, 0, 0, 4]}, dtype=""Sparse[int]"")\n>>> df.sparse.density\n0.5']"
181,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.take.html,pandas.core.groupby.SeriesGroupBy.take,"SeriesGroupBy.take(indices, axis=<no_default>, **kwargs)[source]# Return the elements in the given positional indices in each group. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. If a requested index does not exist for some group, this method will raise. To get similar behavior that ignores indices that don’t exist, see SeriesGroupBy.nth().","Parameters: indicesarray-likeAn array of ints indicating which positions to take in each group. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns. For SeriesGroupBy this parameter is unused and defaults to 0. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. **kwargsFor compatibility with numpy.take(). Has no effect on the output. Returns: SeriesA Series containing the elements taken from each group.","['>>> df = pd.DataFrame([(\'falcon\', \'bird\', 389.0),\n...                    (\'parrot\', \'bird\', 24.0),\n...                    (\'lion\', \'mammal\', 80.5),\n...                    (\'monkey\', \'mammal\', np.nan),\n...                    (\'rabbit\', \'mammal\', 15.0)],\n...                   columns=[\'name\', \'class\', \'max_speed\'],\n...                   index=[4, 3, 2, 1, 0])\n>>> df\n     name   class  max_speed\n4  falcon    bird      389.0\n3  parrot    bird       24.0\n2    lion  mammal       80.5\n1  monkey  mammal        NaN\n0  rabbit  mammal       15.0\n>>> gb = df[""name""].groupby([1, 1, 2, 2, 2])', '>>> gb.take([0, 1])\n1  4    falcon\n   3    parrot\n2  2      lion\n   1    monkey\nName: name, dtype: object', '>>> gb.take([-1, -2])\n1  3    parrot\n   4    falcon\n2  0    rabbit\n   1    monkey\nName: name, dtype: object']"
182,..\pandas\reference\api\pandas.Interval.overlaps.html,pandas.Interval.overlaps,"Interval.overlaps(other)# Check whether two Interval objects overlap. Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.",Parameters: otherIntervalInterval to check against for an overlap. Returns: boolTrue if the two intervals overlap.,"['>>> i1 = pd.Interval(0, 2)\n>>> i2 = pd.Interval(1, 3)\n>>> i1.overlaps(i2)\nTrue\n>>> i3 = pd.Interval(4, 5)\n>>> i1.overlaps(i3)\nFalse', "">>> i4 = pd.Interval(0, 1, closed='both')\n>>> i5 = pd.Interval(1, 2, closed='both')\n>>> i4.overlaps(i5)\nTrue"", "">>> i6 = pd.Interval(1, 2, closed='neither')\n>>> i4.overlaps(i6)\nFalse""]"
183,..\pandas\reference\api\pandas.tseries.offsets.Minute.delta.html,pandas.tseries.offsets.Minute.delta,Minute.delta#,No parameters found,[]
184,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_month_start.html,pandas.tseries.offsets.BYearEnd.is_month_start,BYearEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
185,..\pandas\reference\api\pandas.Series.dt.ceil.html,pandas.Series.dt.ceil,"Series.dt.ceil(*args, **kwargs)[source]# Perform ceil operation on the data to the specified freq. Notes If the timestamps have a timezone, ceiling will take place relative to the local (“wall”) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to ceil the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.ceil('h')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 13:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.ceil(""h"")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 13:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 01:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.ceil(""h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.ceil(""h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
186,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._concat_same_type.html,pandas.api.extensions.ExtensionArray._concat_same_type,classmethod ExtensionArray._concat_same_type(to_concat)[source]# Concatenate multiple array of this dtype.,Parameters: to_concatsequence of this type Returns: ExtensionArray,"['>>> arr1 = pd.array([1, 2, 3])\n>>> arr2 = pd.array([4, 5, 6])\n>>> pd.arrays.IntegerArray._concat_same_type([arr1, arr2])\n<IntegerArray>\n[1, 2, 3, 4, 5, 6]\nLength: 6, dtype: Int64']"
187,..\pandas\reference\api\pandas.Series.update.html,pandas.Series.update,Series.update(other)[source]# Modify Series in place using values from passed Series. Uses non-NA values from passed Series to make updates. Aligns on index.,"Parameters: otherSeries, or object coercible into Series","['>>> s = pd.Series([1, 2, 3])\n>>> s.update(pd.Series([4, 5, 6]))\n>>> s\n0    4\n1    5\n2    6\ndtype: int64', "">>> s = pd.Series(['a', 'b', 'c'])\n>>> s.update(pd.Series(['d', 'e'], index=[0, 2]))\n>>> s\n0    d\n1    b\n2    e\ndtype: object"", '>>> s = pd.Series([1, 2, 3])\n>>> s.update(pd.Series([4, 5, 6, 7, 8]))\n>>> s\n0    4\n1    5\n2    6\ndtype: int64', '>>> s = pd.Series([1, 2, 3])\n>>> s.update(pd.Series([4, np.nan, 6]))\n>>> s\n0    4\n1    2\n2    6\ndtype: int64', '>>> s = pd.Series([1, 2, 3])\n>>> s.update([4, np.nan, 6])\n>>> s\n0    4\n1    2\n2    6\ndtype: int64', '>>> s = pd.Series([1, 2, 3])\n>>> s.update({1: 9})\n>>> s\n0    1\n1    9\n2    3\ndtype: int64']"
188,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.transform.html,pandas.core.groupby.SeriesGroupBy.transform,"SeriesGroupBy.transform(func, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Call function producing a same-indexed Series on each group. Returns a Series having the same indexes as the original object filled with the transformed values. Notes Each group is endowed the attribute ‘name’ in case you need to know which group you are working on. The current implementation imposes three requirements on f: f must return a value that either has the same shape as the input subframe or can be broadcast to the shape of the input subframe. For example, if f returns a scalar it will be broadcast to have the same shape as the input subframe. if this is a DataFrame, f must support application column-by-column in the subframe. If f also supports application to the entire subframe, then a fast path is used starting from the second chunk. f must not mutate groups. Mutation is not supported and may produce unexpected results. See Mutating with User Defined Function (UDF) methods for more details. When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below. Changed in version 2.0.0: When using .transform on a grouped DataFrame and the transformation function returns a DataFrame, pandas now aligns the result’s index with the input’s index. You can call .to_numpy() on the result of the transformation function to avoid alignment.","Parameters: ffunction, strFunction to apply to each group. See the Notes section below for requirements. Accepted inputs are: String Python function Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine. If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group’s index will be passed to the user defined function and optionally available for use. If a string is chosen, then it needs to be the name of the groupby method you want to use. *argsPositional arguments to pass to func. enginestr, default None 'cython' : Runs the function through C-extensions from cython. 'numba' : Runs the function through JIT compiled code from numba. None : Defaults to 'cython' or the global setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function **kwargsKeyword arguments to be passed into func. Returns: Series","['>>> ser = pd.Series([390.0, 350.0, 30.0, 20.0],\n...                 index=[""Falcon"", ""Falcon"", ""Parrot"", ""Parrot""],\n...                 name=""Max Speed"")\n>>> grouped = ser.groupby([1, 1, 2, 2])\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n    Falcon    0.707107\n    Falcon   -0.707107\n    Parrot    0.707107\n    Parrot   -0.707107\n    Name: Max Speed, dtype: float64', '>>> grouped.transform(lambda x: x.max() - x.min())\nFalcon    40.0\nFalcon    40.0\nParrot    10.0\nParrot    10.0\nName: Max Speed, dtype: float64', '>>> grouped.transform(""mean"")\nFalcon    370.0\nFalcon    370.0\nParrot     25.0\nParrot     25.0\nName: Max Speed, dtype: float64', '>>> grouped.transform(lambda x: x.astype(int).max())\nFalcon    390\nFalcon    390\nParrot     30\nParrot     30\nName: Max Speed, dtype: int64']"
189,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._explode.html,pandas.api.extensions.ExtensionArray._explode,ExtensionArray._explode()[source]# Transform each element of list-like to a row. For arrays that do not contain list-like elements the default implementation of this method just returns a copy and an array of ones (unchanged index).,Returns: ExtensionArrayArray with the exploded values. np.ndarray[uint64]The original lengths of each list-like for determining the resulting index.,"['>>> import pyarrow as pa\n>>> a = pd.array([[1, 2, 3], [4], [5, 6]],\n...              dtype=pd.ArrowDtype(pa.list_(pa.int64())))\n>>> a._explode()\n(<ArrowExtensionArray>\n[1, 2, 3, 4, 5, 6]\nLength: 6, dtype: int64[pyarrow], array([3, 1, 2], dtype=int32))']"
190,..\pandas\reference\api\pandas.Interval.right.html,pandas.Interval.right,Interval.right# Right bound for the interval.,No parameters found,"["">>> interval = pd.Interval(left=1, right=2, closed='left')\n>>> interval\nInterval(1, 2, closed='left')\n>>> interval.right\n2""]"
191,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_on_offset.html,pandas.tseries.offsets.BYearEnd.is_on_offset,BYearEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
192,..\pandas\reference\api\pandas.DataFrame.sparse.to_coo.html,pandas.DataFrame.sparse.to_coo,"DataFrame.sparse.to_coo()[source]# Return the contents of the frame as a sparse SciPy COO matrix. Notes The dtype will be the lowest-common-denominator type (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. e.g. If the dtypes are float16 and float32, dtype will be upcast to float32. By numpy.find_common_type convention, mixing int64 and and uint64 will result in a float64 dtype.","Returns: scipy.sparse.spmatrixIf the caller is heterogeneous and contains booleans or objects, the result will be of dtype=object. See Notes.","['>>> df = pd.DataFrame({""A"": pd.arrays.SparseArray([0, 1, 0, 1])})\n>>> df.sparse.to_coo()\n<COOrdinate sparse matrix of dtype \'int64\'\n    with 2 stored elements and shape (4, 1)>']"
193,..\pandas\reference\api\pandas.Series.dt.components.html,pandas.Series.dt.components,Series.dt.components[source]# Return a Dataframe of the components of the Timedeltas.,Returns: DataFrame,"["">>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='s'))\n>>> s\n0   0 days 00:00:00\n1   0 days 00:00:01\n2   0 days 00:00:02\n3   0 days 00:00:03\n4   0 days 00:00:04\ndtype: timedelta64[ns]\n>>> s.dt.components\n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     0      0        0        0             0             0            0\n1     0      0        0        1             0             0            0\n2     0      0        0        2             0             0            0\n3     0      0        0        3             0             0            0\n4     0      0        0        4             0             0            0""]"
194,..\pandas\reference\api\pandas.tseries.offsets.Minute.freqstr.html,pandas.tseries.offsets.Minute.freqstr,Minute.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
195,..\pandas\reference\api\pandas.Series.values.html,pandas.Series.values,"property Series.values[source]# Return Series as ndarray or ndarray-like depending on the dtype. Warning We recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array.",Returns: numpy.ndarray or ndarray-like,"['>>> pd.Series([1, 2, 3]).values\narray([1, 2, 3])', "">>> pd.Series(list('aabc')).values\narray(['a', 'a', 'b', 'c'], dtype=object)"", "">>> pd.Series(list('aabc')).astype('category').values\n['a', 'a', 'b', 'c']\nCategories (3, object): ['a', 'b', 'c']"", "">>> pd.Series(pd.date_range('20130101', periods=3,\n...                         tz='US/Eastern')).values\narray(['2013-01-01T05:00:00.000000000',\n       '2013-01-02T05:00:00.000000000',\n       '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')""]"
196,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.unique.html,pandas.core.groupby.SeriesGroupBy.unique,"SeriesGroupBy.unique()[source]# Return unique values for each group. It returns unique values for each of the grouped values. Returned in order of appearance. Hash table-based unique, therefore does NOT sort.",Returns: SeriesUnique values for each of the grouped values.,"["">>> df = pd.DataFrame([('Chihuahua', 'dog', 6.1),\n...                    ('Beagle', 'dog', 15.2),\n...                    ('Chihuahua', 'dog', 6.9),\n...                    ('Persian', 'cat', 9.2),\n...                    ('Chihuahua', 'dog', 7),\n...                    ('Persian', 'cat', 8.8)],\n...                   columns=['breed', 'animal', 'height_in'])\n>>> df\n       breed     animal   height_in\n0  Chihuahua        dog         6.1\n1     Beagle        dog        15.2\n2  Chihuahua        dog         6.9\n3    Persian        cat         9.2\n4  Chihuahua        dog         7.0\n5    Persian        cat         8.8\n>>> ser = df.groupby('animal')['breed'].unique()\n>>> ser\nanimal\ncat              [Persian]\ndog    [Chihuahua, Beagle]\nName: breed, dtype: object""]"
197,..\pandas\reference\api\pandas.Series.dt.date.html,pandas.Series.dt.date,"Series.dt.date[source]# Returns numpy array of python datetime.date objects. Namely, the date part of Timestamps without time and timezone information.",No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.date\n0    2020-01-01\n1    2020-02-01\ndtype: object', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.date\narray([datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)], dtype=object)']"
198,..\pandas\reference\api\pandas.DataFrame.sparse.to_dense.html,pandas.DataFrame.sparse.to_dense,DataFrame.sparse.to_dense()[source]# Convert a DataFrame with sparse values to dense.,Returns: DataFrameA DataFrame with the same values stored as dense arrays.,"['>>> df = pd.DataFrame({""A"": pd.arrays.SparseArray([0, 1, 0])})\n>>> df.sparse.to_dense()\n   A\n0  0\n1  1\n2  0']"
199,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._formatter.html,pandas.api.extensions.ExtensionArray._formatter,ExtensionArray._formatter(boxed=False)[source]# Formatting function for scalar values. This is used in the default ‘__repr__’. The returned formatting function receives instances of your scalar type.,"Parameters: boxedbool, default FalseAn indicated for whether or not your array is being printed within a Series, DataFrame, or Index (True), or just by itself (False). This may be useful if you want scalar values to appear differently within a Series versus on its own (e.g. quoted or not). Returns: Callable[[Any], str]A callable that gets instances of the scalar type and returns a string. By default, repr() is used when boxed=False and str() is used when boxed=True.","["">>> class MyExtensionArray(pd.arrays.NumpyExtensionArray):\n...     def _formatter(self, boxed=False):\n...         return lambda x: '*' + str(x) + '*' if boxed else repr(x) + '*'\n>>> MyExtensionArray(np.array([1, 2, 3, 4]))\n<MyExtensionArray>\n[1*, 2*, 3*, 4*]\nLength: 4, dtype: int64""]"
200,..\pandas\reference\api\pandas.IntervalDtype.html,pandas.IntervalDtype,"class pandas.IntervalDtype(subtype=None, closed=None)[source]# An ExtensionDtype for Interval data. This is not an actual numpy dtype, but a duck type. Attributes subtype The dtype of the Interval bounds. Methods None","Parameters: subtypestr, np.dtypeThe dtype of the Interval bounds.","["">>> pd.IntervalDtype(subtype='int64', closed='both')\ninterval[int64, both]""]"
201,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_quarter_end.html,pandas.tseries.offsets.BYearEnd.is_quarter_end,BYearEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
202,..\pandas\reference\api\pandas.Series.value_counts.html,pandas.Series.value_counts,"Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]# Return a Series containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default.","Parameters: normalizebool, default FalseIf True then the object returned will contain the relative frequencies of the unique values. sortbool, default TrueSort by frequencies when True. Preserve the order of the data when False. ascendingbool, default FalseSort in ascending order. binsint, optionalRather than count values, group them into half-open bins, a convenience for pd.cut, only works with numeric data. dropnabool, default TrueDon’t include counts of NaN. Returns: Series","['>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n>>> index.value_counts()\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nName: count, dtype: int64', '>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n>>> s.value_counts(normalize=True)\n3.0    0.4\n1.0    0.2\n2.0    0.2\n4.0    0.2\nName: proportion, dtype: float64', '>>> s.value_counts(bins=3)\n(0.996, 2.0]    2\n(2.0, 3.0]      2\n(3.0, 4.0]      1\nName: count, dtype: int64', '>>> s.value_counts(dropna=False)\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nNaN    1\nName: count, dtype: int64']"
203,..\pandas\reference\api\pandas.tseries.offsets.Minute.html,pandas.tseries.offsets.Minute,class pandas.tseries.offsets.Minute# Offset n minutes. Examples You can use the parameter n to represent a shift of n minutes. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of minutes represented.","["">>> from pandas.tseries.offsets import Minute\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Minute(n=10)\nTimestamp('2022-12-09 15:10:00')\n>>> ts - Minute(n=10)\nTimestamp('2022-12-09 14:50:00')"", "">>> ts + Minute(n=-10)\nTimestamp('2022-12-09 14:50:00')""]"
204,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.value_counts.html,pandas.core.groupby.SeriesGroupBy.value_counts,"SeriesGroupBy.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)[source]#",No parameters found,[]
205,..\pandas\reference\api\pandas.Series.dt.day.html,pandas.Series.dt.day,Series.dt.day[source]# The day of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""D"")\n... )\n>>> datetime_series\n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\ndtype: datetime64[ns]\n>>> datetime_series.dt.day\n0    1\n1    2\n2    3\ndtype: int32']"
206,..\pandas\reference\api\pandas.DataFrame.squeeze.html,pandas.DataFrame.squeeze,"DataFrame.squeeze(axis=None)[source]# Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don’t know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series.","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneA specific axis to squeeze. By default, all length-1 axes are squeezed. For Series this parameter is unused and defaults to None. Returns: DataFrame, Series, or scalarThe projection after squeezing axis or all the axes.","['>>> primes = pd.Series([2, 3, 5, 7])', '>>> even_primes = primes[primes % 2 == 0]\n>>> even_primes\n0    2\ndtype: int64', '>>> even_primes.squeeze()\n2', '>>> odd_primes = primes[primes % 2 == 1]\n>>> odd_primes\n1    3\n2    5\n3    7\ndtype: int64', '>>> odd_primes.squeeze()\n1    3\n2    5\n3    7\ndtype: int64', "">>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n>>> df\n   a  b\n0  1  2\n1  3  4"", "">>> df_a = df[['a']]\n>>> df_a\n   a\n0  1\n1  3"", "">>> df_a.squeeze('columns')\n0    1\n1    3\nName: a, dtype: int64"", "">>> df_0a = df.loc[df.index < 1, ['a']]\n>>> df_0a\n   a\n0  1"", "">>> df_0a.squeeze('rows')\na    1\nName: 0, dtype: int64"", '>>> df_0a.squeeze()\n1']"
207,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._from_factorized.html,pandas.api.extensions.ExtensionArray._from_factorized,"classmethod ExtensionArray._from_factorized(values, original)[source]# Reconstruct an ExtensionArray after factorization.",Parameters: valuesndarrayAn integer ndarray with the factorized values. originalExtensionArrayThe original ExtensionArray that factorize was called on.,"['>>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1),\n...                                      pd.Interval(1, 5), pd.Interval(1, 5)])\n>>> codes, uniques = pd.factorize(interv_arr)\n>>> pd.arrays.IntervalArray._from_factorized(uniques, interv_arr)\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]']"
208,..\pandas\reference\api\pandas.IntervalDtype.subtype.html,pandas.IntervalDtype.subtype,property IntervalDtype.subtype[source]# The dtype of the Interval bounds.,No parameters found,"["">>> dtype = pd.IntervalDtype(subtype='int64', closed='both')\n>>> dtype.subtype\ndtype('int64')""]"
209,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_anchored.html,pandas.tseries.offsets.Minute.is_anchored,Minute.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
210,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_quarter_start.html,pandas.tseries.offsets.BYearEnd.is_quarter_start,BYearEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
211,..\pandas\reference\api\pandas.Series.var.html,pandas.Series.var,"Series.var(axis=None, skipna=True, ddof=1, numeric_only=False, **kwargs)[source]# Return unbiased variance over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument.","Parameters: axis{index (0)}For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.var with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. Returns: scalar or Series (if level specified)","["">>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                    'age': [21, 25, 62, 43],\n...                    'height': [1.61, 1.87, 1.49, 2.01]}\n...                   ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01"", '>>> df.var()\nage       352.916667\nheight      0.056367\ndtype: float64', '>>> df.var(ddof=0)\nage       264.687500\nheight      0.042275\ndtype: float64']"
212,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.var.html,pandas.core.groupby.SeriesGroupBy.var,"SeriesGroupBy.var(ddof=1, engine=None, engine_kwargs=None, numeric_only=False)[source]# Compute variance of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}} Added in version 1.4.0. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameVariance of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).var()\na    10.333333\nb     0.333333\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).var()\n              a          b\ndog    4.000000  12.333333\nmouse  4.916667   2.250000""]"
213,..\pandas\reference\api\pandas.Series.dt.dayofweek.html,pandas.Series.dt.dayofweek,"Series.dt.dayofweek[source]# The day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.",Returns: Series or IndexContaining integers indicating the day number.,"["">>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int32""]"
214,..\pandas\reference\api\pandas.DataFrame.stack.html,pandas.DataFrame.stack,"DataFrame.stack(level=-1, dropna=<no_default>, sort=<no_default>, future_stack=False)[source]# Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: if the columns have a single level, the output is a Series; if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame. Notes The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe). Reference the user guide for more examples.","Parameters: levelint, str, list, default -1Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels. dropnabool, default TrueWhether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section. sortbool, default TrueWhether to sort the levels of the resulting MultiIndex. future_stackbool, default FalseWhether to use the new implementation that will replace the current implementation in pandas 3.0. When True, dropna and sort have no impact on the result and must remain unspecified. See pandas 2.1.0 Release notes for more details. Returns: DataFrame or SeriesStacked dataframe or series.","["">>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n...                                     index=['cat', 'dog'],\n...                                     columns=['weight', 'height'])"", '>>> df_single_level_cols\n     weight height\ncat       0      1\ndog       2      3\n>>> df_single_level_cols.stack(future_stack=True)\ncat  weight    0\n     height    1\ndog  weight    2\n     height    3\ndtype: int64', "">>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n...                                        ('weight', 'pounds')])\n>>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n...                                     index=['cat', 'dog'],\n...                                     columns=multicol1)"", '>>> df_multi_level_cols1\n     weight\n         kg    pounds\ncat       1        2\ndog       2        4\n>>> df_multi_level_cols1.stack(future_stack=True)\n            weight\ncat kg           1\n    pounds       2\ndog kg           2\n    pounds       4', "">>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n...                                        ('height', 'm')])\n>>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n...                                     index=['cat', 'dog'],\n...                                     columns=multicol2)"", '>>> df_multi_level_cols2\n    weight height\n        kg      m\ncat    1.0    2.0\ndog    3.0    4.0\n>>> df_multi_level_cols2.stack(future_stack=True)\n        weight  height\ncat kg     1.0     NaN\n    m      NaN     2.0\ndog kg     3.0     NaN\n    m      NaN     4.0', '>>> df_multi_level_cols2.stack(0, future_stack=True)\n             kg    m\ncat weight  1.0  NaN\n    height  NaN  2.0\ndog weight  3.0  NaN\n    height  NaN  4.0\n>>> df_multi_level_cols2.stack([0, 1], future_stack=True)\ncat  weight  kg    1.0\n     height  m     2.0\ndog  weight  kg    3.0\n     height  m     4.0\ndtype: float64']"
215,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._from_sequence.html,pandas.api.extensions.ExtensionArray._from_sequence,"classmethod ExtensionArray._from_sequence(scalars, *, dtype=None, copy=False)[source]# Construct a new ExtensionArray from a sequence of scalars.","Parameters: scalarsSequenceEach element will be an instance of the scalar type for this array, cls.dtype.type or be converted into this type in this method. dtypedtype, optionalConstruct for this particular dtype. This should be a Dtype compatible with the ExtensionArray. copybool, default FalseIf True, copy the underlying data. Returns: ExtensionArray","['>>> pd.arrays.IntegerArray._from_sequence([4, 5])\n<IntegerArray>\n[4, 5]\nLength: 2, dtype: Int64']"
216,..\pandas\reference\api\pandas.IntervalIndex.closed.html,pandas.IntervalIndex.closed,"IntervalIndex.closed[source]# String describing the inclusive side the intervals. Either left, right, both or neither.",No parameters found,"["">>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.closed\n'right'"", "">>> interv_idx = pd.interval_range(start=0, end=2)\n>>> interv_idx\nIntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')\n>>> interv_idx.closed\n'right'""]"
217,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_month_end.html,pandas.tseries.offsets.Minute.is_month_end,Minute.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
218,..\pandas\reference\api\pandas.Series.view.html,pandas.Series.view,"Series.view(dtype=None)[source]# Create a new view of the Series. Deprecated since version 2.2.0: Series.view is deprecated and will be removed in a future version. Use Series.astype() as an alternative to change the dtype. This function will return a new Series with a view of the same underlying values in memory, optionally reinterpreted with a new data type. The new data type must preserve the same size in bytes as to not cause index misalignment. Notes Series are instantiated with dtype=float64 by default. While numpy.ndarray.view() will return a view with the same data type as the original array, Series.view() (without specified dtype) will try using float64 and may fail if the original data type size in bytes is not the same.",Parameters: dtypedata typeData type object or one of their string representations. Returns: SeriesA new Series object as a view of the same data in memory.,[]
219,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._from_sequence_of_strings.html,pandas.api.extensions.ExtensionArray._from_sequence_of_strings,"classmethod ExtensionArray._from_sequence_of_strings(strings, *, dtype=None, copy=False)[source]# Construct a new ExtensionArray from a sequence of strings.","Parameters: stringsSequenceEach element will be an instance of the scalar type for this array, cls.dtype.type. dtypedtype, optionalConstruct for this particular dtype. This should be a Dtype compatible with the ExtensionArray. copybool, default FalseIf True, copy the underlying data. Returns: ExtensionArray","['>>> pd.arrays.IntegerArray._from_sequence_of_strings([""1"", ""2"", ""3""])\n<IntegerArray>\n[1, 2, 3]\nLength: 3, dtype: Int64']"
220,..\pandas\reference\api\pandas.DataFrame.std.html,pandas.DataFrame.std,"DataFrame.std(axis=0, skipna=True, ddof=1, numeric_only=False, **kwargs)[source]# Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument. Notes To have the same behaviour as numpy.std, use ddof=0 (instead of the default ddof=1)","Parameters: axis{index (0), columns (1)}For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.std with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. Returns: Series or DataFrame (if level specified)","["">>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                    'age': [21, 25, 62, 43],\n...                    'height': [1.61, 1.87, 1.49, 2.01]}\n...                   ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01"", '>>> df.std()\nage       18.786076\nheight     0.237417\ndtype: float64', '>>> df.std(ddof=0)\nage       16.269219\nheight     0.205609\ndtype: float64']"
221,..\pandas\reference\api\pandas.IntervalIndex.contains.html,pandas.IntervalIndex.contains,"IntervalIndex.contains(*args, **kwargs)[source]# Check elementwise if the Intervals contain the value. Return a boolean mask whether the value is contained in the Intervals of the IntervalArray.",Parameters: otherscalarThe value to check whether it is contained in the Intervals. Returns: boolean array,"['>>> intervals = pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 3), (2, 4)])\n>>> intervals\n<IntervalArray>\n[(0, 1], (1, 3], (2, 4]]\nLength: 3, dtype: interval[int64, right]', '>>> intervals.contains(0.5)\narray([ True, False, False])']"
222,..\pandas\reference\api\pandas.Series.dt.dayofyear.html,pandas.Series.dt.dayofyear,Series.dt.dayofyear[source]# The ordinal day of the year.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.dayofyear\n0    1\n1   32\ndtype: int32', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.dayofyear\nIndex([1, 32], dtype=\'int32\')']"
223,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_year_end.html,pandas.tseries.offsets.BYearEnd.is_year_end,BYearEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
224,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.__iter__.html,pandas.core.groupby.SeriesGroupBy.__iter__,SeriesGroupBy.__iter__()[source]# Groupby iterator.,"Returns: Generator yielding sequence of (name, subsetted object) for each group","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> for x, y in ser.groupby(level=0):\n...     print(f'{x}\\n{y}\\n')\na\na    1\na    2\ndtype: int64\nb\nb    3\ndtype: int64"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""])\n>>> df\n   a  b  c\n0  1  2  3\n1  1  5  6\n2  7  8  9\n>>> for x, y in df.groupby(by=[""a""]):\n...     print(f\'{x}\\n{y}\\n\')\n(1,)\n   a  b  c\n0  1  2  3\n1  1  5  6\n(7,)\n   a  b  c\n2  7  8  9', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> for x, y in ser.resample('MS'):\n...     print(f'{x}\\n{y}\\n')\n2023-01-01 00:00:00\n2023-01-01    1\n2023-01-15    2\ndtype: int64\n2023-02-01 00:00:00\n2023-02-01    3\n2023-02-15    4\ndtype: int64""]"
225,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._hash_pandas_object.html,pandas.api.extensions.ExtensionArray._hash_pandas_object,"ExtensionArray._hash_pandas_object(*, encoding, hash_key, categorize)[source]# Hook for hash_pandas_object. Default is to use the values returned by _values_for_factorize.",Parameters: encodingstrEncoding for data & key when strings. hash_keystrHash_key for string key to encode. categorizeboolWhether to first categorize object arrays before hashing. This is more efficient when the array contains duplicate values. Returns: np.ndarray[uint64],"['>>> pd.array([1, 2])._hash_pandas_object(encoding=\'utf-8\',\n...                                      hash_key=""1000000000000000"",\n...                                      categorize=False\n...                                      )\narray([ 6238072747940578789, 15839785061582574730], dtype=uint64)']"
226,..\pandas\reference\api\pandas.IntervalIndex.from_arrays.html,pandas.IntervalIndex.from_arrays,"classmethod IntervalIndex.from_arrays(left, right, closed='right', name=None, copy=False, dtype=None)[source]# Construct from two arrays defining the left and right bounds. Notes Each element of left must be less than or equal to the right element at the same position. If an element is missing, it must be missing in both left and right. A TypeError is raised when using an unsupported type for left or right. At the moment, ‘category’, ‘object’, and ‘string’ subtypes are not supported.","Parameters: leftarray-like (1-dimensional)Left bounds for each interval. rightarray-like (1-dimensional)Right bounds for each interval. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. namestr, optionalName of the resulting IntervalIndex. copybool, default FalseCopy the data. dtypedtype, optionalIf None, dtype will be inferred. Returns: IntervalIndex Raises: ValueErrorWhen a value is missing in only one of left or right. When a value in left is greater than the corresponding value in right.","["">>> pd.IntervalIndex.from_arrays([0, 1, 2], [1, 2, 3])\nIntervalIndex([(0, 1], (1, 2], (2, 3]],\n              dtype='interval[int64, right]')""]"
227,..\pandas\reference\api\pandas.Series.dt.days.html,pandas.Series.dt.days,Series.dt.days[source]# Number of days for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='d'))\n>>> ser\n0   1 days\n1   2 days\n2   3 days\ndtype: timedelta64[ns]\n>>> ser.dt.days\n0    1\n1    2\n2    3\ndtype: int64"", '>>> tdelta_idx = pd.to_timedelta([""0 days"", ""10 days"", ""20 days""])\n>>> tdelta_idx\nTimedeltaIndex([\'0 days\', \'10 days\', \'20 days\'],\n                dtype=\'timedelta64[ns]\', freq=None)\n>>> tdelta_idx.days\nIndex([0, 10, 20], dtype=\'int64\')']"
228,..\pandas\reference\api\pandas.DataFrame.style.html,pandas.DataFrame.style,property DataFrame.style[source]# Returns a Styler object. Contains methods for building a styled HTML representation of the DataFrame.,No parameters found,"["">>> df = pd.DataFrame({'A': [1, 2, 3]})\n>>> df.style""]"
229,..\pandas\reference\api\pandas.core.resample.Resampler.aggregate.html,pandas.core.resample.Resampler.aggregate,"final Resampler.aggregate(func=None, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","["">>> s = pd.Series([1, 2, 3, 4, 5],\n...               index=pd.date_range('20130101', periods=5, freq='s'))\n>>> s\n2013-01-01 00:00:00    1\n2013-01-01 00:00:01    2\n2013-01-01 00:00:02    3\n2013-01-01 00:00:03    4\n2013-01-01 00:00:04    5\nFreq: s, dtype: int64"", "">>> r = s.resample('2s')"", '>>> r.agg(""sum"")\n2013-01-01 00:00:00    3\n2013-01-01 00:00:02    7\n2013-01-01 00:00:04    5\nFreq: 2s, dtype: int64', "">>> r.agg(['sum', 'mean', 'max'])\n                     sum  mean  max\n2013-01-01 00:00:00    3   1.5    2\n2013-01-01 00:00:02    7   3.5    4\n2013-01-01 00:00:04    5   5.0    5"", '>>> r.agg({\'result\': lambda x: x.mean() / x.std(),\n...        \'total\': ""sum""})\n                       result  total\n2013-01-01 00:00:00  2.121320      3\n2013-01-01 00:00:02  4.949747      7\n2013-01-01 00:00:04       NaN      5', '>>> r.agg(average=""mean"", total=""sum"")\n                         average  total\n2013-01-01 00:00:00      1.5      3\n2013-01-01 00:00:02      3.5      7\n2013-01-01 00:00:04      5.0      5']"
230,..\pandas\reference\api\pandas.Series.where.html,pandas.Series.where,"Series.where(cond, other=nan, *, inplace=False, axis=None, level=None)[source]# Replace values where the condition is False. Notes The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with False. The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2). For further details and examples see the where documentation in indexing. The dtype of the object takes precedence. The fill value is casted to the object’s dtype, if this can be done losslessly.","Parameters: condbool Series/DataFrame, array-like, or callableWhere cond is True, keep the original value. Where False, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn’t check it). otherscalar, Series/DataFrame, or callableEntries where cond is False are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn’t check it). If not specified, entries will be filled with the corresponding NULL value (np.nan for numpy dtypes, pd.NA for extension dtypes). inplacebool, default FalseWhether to perform the operation in place on the data. axisint, default NoneAlignment axis if needed. For Series this parameter is unused and defaults to 0. levelint, default NoneAlignment level if needed. Returns: Same type as caller or None if inplace=True.","['>>> s = pd.Series(range(5))\n>>> s.where(s > 0)\n0    NaN\n1    1.0\n2    2.0\n3    3.0\n4    4.0\ndtype: float64\n>>> s.mask(s > 0)\n0    0.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', '>>> s = pd.Series(range(5))\n>>> t = pd.Series([True, False])\n>>> s.where(t, 99)\n0     0\n1    99\n2    99\n3    99\n4    99\ndtype: int64\n>>> s.mask(t, 99)\n0    99\n1     1\n2    99\n3    99\n4    99\ndtype: int64', '>>> s.where(s > 1, 10)\n0    10\n1    10\n2    2\n3    3\n4    4\ndtype: int64\n>>> s.mask(s > 1, 10)\n0     0\n1     1\n2    10\n3    10\n4    10\ndtype: int64', "">>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n>>> df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n>>> m = df % 3 == 0\n>>> df.where(m, -df)\n   A  B\n0  0 -1\n1 -2  3\n2 -4 -5\n3  6 -7\n4 -8  9\n>>> df.where(m, -df) == np.where(m, df, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n>>> df.where(m, -df) == df.mask(~m, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True""]"
231,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.is_year_start.html,pandas.tseries.offsets.BYearEnd.is_year_start,BYearEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
232,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_month_start.html,pandas.tseries.offsets.Minute.is_month_start,Minute.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
233,..\pandas\reference\api\pandas.IntervalIndex.from_breaks.html,pandas.IntervalIndex.from_breaks,"classmethod IntervalIndex.from_breaks(breaks, closed='right', name=None, copy=False, dtype=None)[source]# Construct an IntervalIndex from an array of splits.","Parameters: breaksarray-like (1-dimensional)Left and right bounds for each interval. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. namestr, optionalName of the resulting IntervalIndex. copybool, default FalseCopy the data. dtypedtype or None, default NoneIf None, dtype will be inferred. Returns: IntervalIndex","["">>> pd.IntervalIndex.from_breaks([0, 1, 2, 3])\nIntervalIndex([(0, 1], (1, 2], (2, 3]],\n              dtype='interval[int64, right]')""]"
234,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._pad_or_backfill.html,pandas.api.extensions.ExtensionArray._pad_or_backfill,"ExtensionArray._pad_or_backfill(*, method, limit=None, limit_area=None, copy=True)[source]# Pad or backfill values, used by Series/DataFrame ffill and bfill.","Parameters: method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’}Method to use for filling holes in reindexed Series: pad / ffill: propagate last valid observation forward to next valid. backfill / bfill: use NEXT valid observation to fill gap. limitint, default NoneThis is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. copybool, default TrueWhether to make a copy of the data before filling. If False, then the original should be modified and no new memory should be allocated. For ExtensionArray subclasses that cannot do this, it is at the author’s discretion whether to ignore “copy=False” or to raise. The base class implementation ignores the keyword if any NAs are present. Returns: Same type as self","['>>> arr = pd.array([np.nan, np.nan, 2, 3, np.nan, np.nan])\n>>> arr._pad_or_backfill(method=""backfill"", limit=1)\n<IntegerArray>\n[<NA>, 2, 2, 3, <NA>, <NA>]\nLength: 6, dtype: Int64']"
235,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._reduce.html,pandas.api.extensions.ExtensionArray._reduce,"ExtensionArray._reduce(name, *, skipna=True, keepdims=False, **kwargs)[source]# Return a scalar result of performing the reduction operation.","Parameters: namestrName of the function, supported values are: { any, all, min, max, sum, mean, median, prod, std, var, sem, kurt, skew }. skipnabool, default TrueIf True, skip NaN values. keepdimsbool, default FalseIf False, a scalar is returned. If True, the result has dimension with size one along the reduced axis. Added in version 2.1: This parameter is not required in the _reduce signature to keep backward compatibility, but will become required in the future. If the parameter is not found in the method signature, a FutureWarning will be emitted. **kwargsAdditional keyword arguments passed to the reduction function. Currently, ddof is the only supported kwarg. Returns: scalar Raises: TypeErrorsubclass does not define reductions","['>>> pd.array([1, 2, 3])._reduce(""min"")\n1']"
236,..\pandas\reference\api\pandas.IntervalIndex.from_tuples.html,pandas.IntervalIndex.from_tuples,"classmethod IntervalIndex.from_tuples(data, closed='right', name=None, copy=False, dtype=None)[source]# Construct an IntervalIndex from an array-like of tuples.","Parameters: dataarray-like (1-dimensional)Array of tuples. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. namestr, optionalName of the resulting IntervalIndex. copybool, default FalseBy-default copy the data, this is compat only and ignored. dtypedtype or None, default NoneIf None, dtype will be inferred. Returns: IntervalIndex","["">>> pd.IntervalIndex.from_tuples([(0, 1), (1, 2)])\nIntervalIndex([(0, 1], (1, 2]],\n               dtype='interval[int64, right]')""]"
237,..\pandas\reference\api\pandas.core.resample.Resampler.apply.html,pandas.core.resample.Resampler.apply,"Resampler.apply(func=None, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","["">>> s = pd.Series([1, 2, 3, 4, 5],\n...               index=pd.date_range('20130101', periods=5, freq='s'))\n>>> s\n2013-01-01 00:00:00    1\n2013-01-01 00:00:01    2\n2013-01-01 00:00:02    3\n2013-01-01 00:00:03    4\n2013-01-01 00:00:04    5\nFreq: s, dtype: int64"", "">>> r = s.resample('2s')"", '>>> r.agg(""sum"")\n2013-01-01 00:00:00    3\n2013-01-01 00:00:02    7\n2013-01-01 00:00:04    5\nFreq: 2s, dtype: int64', "">>> r.agg(['sum', 'mean', 'max'])\n                     sum  mean  max\n2013-01-01 00:00:00    3   1.5    2\n2013-01-01 00:00:02    7   3.5    4\n2013-01-01 00:00:04    5   5.0    5"", '>>> r.agg({\'result\': lambda x: x.mean() / x.std(),\n...        \'total\': ""sum""})\n                       result  total\n2013-01-01 00:00:00  2.121320      3\n2013-01-01 00:00:02  4.949747      7\n2013-01-01 00:00:04       NaN      5', '>>> r.agg(average=""mean"", total=""sum"")\n                         average  total\n2013-01-01 00:00:00      1.5      3\n2013-01-01 00:00:02      3.5      7\n2013-01-01 00:00:04      5.0      5']"
238,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.kwds.html,pandas.tseries.offsets.BYearEnd.kwds,BYearEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
239,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_on_offset.html,pandas.tseries.offsets.Minute.is_on_offset,Minute.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
240,..\pandas\reference\api\pandas.DataFrame.sub.html,pandas.DataFrame.sub,"DataFrame.sub(other, axis='columns', level=None, fill_value=None)[source]# Get Subtraction of dataframe and other, element-wise (binary operator sub). Equivalent to dataframe - other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
241,..\pandas\reference\api\pandas.Series.dt.daysinmonth.html,pandas.Series.dt.daysinmonth,Series.dt.daysinmonth[source]# The number of days in the month.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.daysinmonth\n0    31\n1    29\ndtype: int32']"
242,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._values_for_argsort.html,pandas.api.extensions.ExtensionArray._values_for_argsort,"ExtensionArray._values_for_argsort()[source]# Return values for sorting. Notes The caller is responsible for not modifying these values in-place, so it is safe for implementers to give views on self. Functions that use this (e.g. ExtensionArray.argsort) should ignore entries with missing values in the original array (according to self.isna()). This means that the corresponding entries in the returned array don’t need to be modified to sort correctly.",Returns: ndarrayThe transformed values should maintain the ordering between values within the array.,"['>>> arr = pd.array([1, 2, 3])\n>>> arr._values_for_argsort()\narray([1, 2, 3])']"
243,..\pandas\reference\api\pandas.IntervalIndex.get_indexer.html,pandas.IntervalIndex.get_indexer,"IntervalIndex.get_indexer(target, method=None, limit=None, tolerance=None)[source]# Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index. Notes Returns -1 for unmatched values, for further explanation see the example below.","Parameters: targetIndex method{None, ‘pad’/’ffill’, ‘backfill’/’bfill’, ‘nearest’}, optional default: exact matches only. pad / ffill: find the PREVIOUS index value if no exact match. backfill / bfill: use NEXT index value if no exact match nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value. limitint, optionalMaximum number of consecutive labels in target to match for inexact matches. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: np.ndarray[np.intp]Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.","["">>> index = pd.Index(['c', 'a', 'b'])\n>>> index.get_indexer(['a', 'b', 'x'])\narray([ 1,  2, -1])""]"
244,..\pandas\reference\api\pandas.Series.xs.html,pandas.Series.xs,"Series.xs(key, axis=0, level=None, drop_level=True)[source]# Return cross-section from the Series/DataFrame. This method takes a key argument to select data at a particular level of a MultiIndex. Notes xs can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see MultiIndex Slicers.","Parameters: keylabel or tuple of labelLabel contained in the index, or partially in a MultiIndex. axis{0 or ‘index’, 1 or ‘columns’}, default 0Axis to retrieve cross-section on. levelobject, defaults to first n levels (n=1 or len(key))In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position. drop_levelbool, default TrueIf False, returns object with same levels as self. Returns: Series or DataFrameCross-section from the original Series or DataFrame corresponding to the selected index levels.","["">>> d = {'num_legs': [4, 4, 2, 2],\n...      'num_wings': [0, 0, 2, 2],\n...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n>>> df = pd.DataFrame(data=d)\n>>> df = df.set_index(['class', 'animal', 'locomotion'])\n>>> df\n                           num_legs  num_wings\nclass  animal  locomotion\nmammal cat     walks              4          0\n       dog     walks              4          0\n       bat     flies              2          2\nbird   penguin walks              2          2"", "">>> df.xs('mammal')\n                   num_legs  num_wings\nanimal locomotion\ncat    walks              4          0\ndog    walks              4          0\nbat    flies              2          2"", "">>> df.xs(('mammal', 'dog', 'walks'))\nnum_legs     4\nnum_wings    0\nName: (mammal, dog, walks), dtype: int64"", "">>> df.xs('cat', level=1)\n                   num_legs  num_wings\nclass  locomotion\nmammal walks              4          0"", "">>> df.xs(('bird', 'walks'),\n...       level=[0, 'locomotion'])\n         num_legs  num_wings\nanimal\npenguin         2          2"", "">>> df.xs('num_wings', axis=1)\nclass   animal   locomotion\nmammal  cat      walks         0\n        dog      walks         0\n        bat      flies         2\nbird    penguin  walks         2\nName: num_wings, dtype: int64""]"
245,..\pandas\reference\api\pandas.core.resample.Resampler.asfreq.html,pandas.core.resample.Resampler.asfreq,"final Resampler.asfreq(fill_value=None)[source]# Return the values at the new freq, essentially a reindex.","Parameters: fill_valuescalar, optionalValue to use for missing values, applied during upsampling (note this does not fill NaNs that already were present). Returns: DataFrame or SeriesValues at the specified freq.","["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-31', '2023-02-01', '2023-02-28']))\n>>> ser\n2023-01-01    1\n2023-01-31    2\n2023-02-01    3\n2023-02-28    4\ndtype: int64\n>>> ser.resample('MS').asfreq()\n2023-01-01    1\n2023-02-01    3\nFreq: MS, dtype: int64""]"
246,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.month.html,pandas.tseries.offsets.BYearEnd.month,BYearEnd.month#,No parameters found,[]
247,..\pandas\reference\api\pandas.DataFrame.sum.html,pandas.DataFrame.sum,"DataFrame.sum(axis=0, skipna=True, numeric_only=False, min_count=0, **kwargs)[source]# Return the sum of the values over the requested axis. This is equivalent to the method numpy.sum.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","["">>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64"", '>>> s.sum()\n14', '>>> pd.Series([], dtype=""float64"").sum()  # min_count=0 is the default\n0.0', '>>> pd.Series([], dtype=""float64"").sum(min_count=1)\nnan', '>>> pd.Series([np.nan]).sum()\n0.0', '>>> pd.Series([np.nan]).sum(min_count=1)\nnan']"
248,..\pandas\reference\api\pandas.Series.__array__.html,pandas.Series.__array__,"Series.__array__(dtype=None, copy=None)[source]# Return the values as a NumPy array. Users should not call this directly. Rather, it is invoked by numpy.array() and numpy.asarray().","Parameters: dtypestr or numpy.dtype, optionalThe dtype to use for the resulting NumPy array. By default, the dtype is inferred from the data. copybool or None, optionalUnused. Returns: numpy.ndarrayThe values in the series converted to a numpy.ndarray with the specified dtype.","['>>> ser = pd.Series([1, 2, 3])\n>>> np.asarray(ser)\narray([1, 2, 3])', '>>> tzser = pd.Series(pd.date_range(\'2000\', periods=2, tz=""CET""))\n>>> np.asarray(tzser, dtype=""object"")\narray([Timestamp(\'2000-01-01 00:00:00+0100\', tz=\'CET\'),\n       Timestamp(\'2000-01-02 00:00:00+0100\', tz=\'CET\')],\n      dtype=object)', '>>> np.asarray(tzser, dtype=""datetime64[ns]"")  \narray([\'1999-12-31T23:00:00.000000000\', ...],\n      dtype=\'datetime64[ns]\')']"
249,..\pandas\reference\api\pandas.api.extensions.ExtensionArray._values_for_factorize.html,pandas.api.extensions.ExtensionArray._values_for_factorize,"ExtensionArray._values_for_factorize()[source]# Return an array and missing value suitable for factorization. Notes The values returned by this method are also used in pandas.util.hash_pandas_object(). If needed, this can be overridden in the self._hash_pandas_object() method.","Returns: valuesndarrayAn array suitable for factorization. This should maintain order and be a supported dtype (Float64, Int64, UInt64, String, Object). By default, the extension array is cast to object dtype. na_valueobjectThe value in values to consider missing. This will be treated as NA in the factorization routines, so it will be coded as -1 and not included in uniques. By default, np.nan is used.","['>>> pd.array([1, 2, 3])._values_for_factorize()\n(array([1, 2, 3], dtype=object), nan)']"
250,..\pandas\reference\api\pandas.Series.dt.days_in_month.html,pandas.Series.dt.days_in_month,Series.dt.days_in_month[source]# The number of days in the month.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.daysinmonth\n0    31\n1    29\ndtype: int32']"
251,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_quarter_end.html,pandas.tseries.offsets.Minute.is_quarter_end,Minute.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
252,..\pandas\reference\api\pandas.core.resample.Resampler.bfill.html,pandas.core.resample.Resampler.bfill,"final Resampler.bfill(limit=None)[source]# Backward fill the new missing values in the resampled data. In statistics, imputation is the process of replacing missing data with substituted values [1]. When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency). The backward fill will replace NaN values that appeared in the resampled data with the next value in the original sequence. Missing values that existed in the original data will not be modified. References [1] https://en.wikipedia.org/wiki/Imputation_(statistics)","Parameters: limitint, optionalLimit of how many values to fill. Returns: Series, DataFrameAn upsampled Series or DataFrame with backward filled NaN values.","["">>> s = pd.Series([1, 2, 3],\n...               index=pd.date_range('20180101', periods=3, freq='h'))\n>>> s\n2018-01-01 00:00:00    1\n2018-01-01 01:00:00    2\n2018-01-01 02:00:00    3\nFreq: h, dtype: int64"", "">>> s.resample('30min').bfill()\n2018-01-01 00:00:00    1\n2018-01-01 00:30:00    2\n2018-01-01 01:00:00    2\n2018-01-01 01:30:00    3\n2018-01-01 02:00:00    3\nFreq: 30min, dtype: int64"", "">>> s.resample('15min').bfill(limit=2)\n2018-01-01 00:00:00    1.0\n2018-01-01 00:15:00    NaN\n2018-01-01 00:30:00    2.0\n2018-01-01 00:45:00    2.0\n2018-01-01 01:00:00    2.0\n2018-01-01 01:15:00    NaN\n2018-01-01 01:30:00    3.0\n2018-01-01 01:45:00    3.0\n2018-01-01 02:00:00    3.0\nFreq: 15min, dtype: float64"", "">>> df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},\n...                   index=pd.date_range('20180101', periods=3,\n...                                       freq='h'))\n>>> df\n                       a  b\n2018-01-01 00:00:00  2.0  1\n2018-01-01 01:00:00  NaN  3\n2018-01-01 02:00:00  6.0  5"", "">>> df.resample('30min').bfill()\n                       a  b\n2018-01-01 00:00:00  2.0  1\n2018-01-01 00:30:00  NaN  3\n2018-01-01 01:00:00  NaN  3\n2018-01-01 01:30:00  6.0  5\n2018-01-01 02:00:00  6.0  5"", "">>> df.resample('15min').bfill(limit=2)\n                       a    b\n2018-01-01 00:00:00  2.0  1.0\n2018-01-01 00:15:00  NaN  NaN\n2018-01-01 00:30:00  NaN  3.0\n2018-01-01 00:45:00  NaN  3.0\n2018-01-01 01:00:00  NaN  3.0\n2018-01-01 01:15:00  NaN  NaN\n2018-01-01 01:30:00  6.0  5.0\n2018-01-01 01:45:00  6.0  5.0\n2018-01-01 02:00:00  6.0  5.0""]"
253,..\pandas\reference\api\pandas.IntervalIndex.get_loc.html,pandas.IntervalIndex.get_loc,"IntervalIndex.get_loc(key)[source]# Get integer location, slice or boolean mask for requested label.","Parameters: keylabel Returns: int if unique index, slice if monotonic index, else mask","['>>> i1, i2 = pd.Interval(0, 1), pd.Interval(1, 2)\n>>> index = pd.IntervalIndex([i1, i2])\n>>> index.get_loc(1)\n0', '>>> index.get_loc(1.5)\n1', '>>> i3 = pd.Interval(0, 2)\n>>> overlapping_index = pd.IntervalIndex([i1, i2, i3])\n>>> overlapping_index.get_loc(0.5)\narray([ True, False,  True])', '>>> index.get_loc(pd.Interval(0, 1))\n0']"
254,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.n.html,pandas.tseries.offsets.BYearEnd.n,BYearEnd.n#,No parameters found,[]
255,..\pandas\reference\api\pandas.DataFrame.swapaxes.html,pandas.DataFrame.swapaxes,"DataFrame.swapaxes(axis1, axis2, copy=None)[source]# Interchange axes and swap values axes appropriately. Deprecated since version 2.1.0: swapaxes is deprecated and will be removed. Please use transpose instead.",Returns: same as input,[]
256,..\pandas\reference\api\pandas.Series.__iter__.html,pandas.Series.__iter__,"Series.__iter__()[source]# Return an iterator of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)",Returns: iterator,"['>>> s = pd.Series([1, 2, 3])\n>>> for x in s:\n...     print(x)\n1\n2\n3']"
257,..\pandas\reference\api\pandas.api.extensions.ExtensionDtype.html,pandas.api.extensions.ExtensionDtype,"class pandas.api.extensions.ExtensionDtype[source]# A custom data type, to be paired with an ExtensionArray. Notes The interface includes the following abstract methods that must be implemented by subclasses: type name construct_array_type The following attributes and methods influence the behavior of the dtype in pandas operations _is_numeric _is_boolean _get_common_dtype The na_value class attribute can be used to set the default NA value for this type. numpy.nan is used by default. ExtensionDtypes are required to be hashable. The base class provides a default implementation, which relies on the _metadata class attribute. _metadata should be a tuple containing the strings that define your data type. For example, with PeriodDtype that’s the freq attribute. If you have a parametrized dtype you should set the ``_metadata`` class property. Ideally, the attributes in _metadata will match the parameters to your ExtensionDtype.__init__ (if any). If any of the attributes in _metadata don’t implement the standard __eq__ or __hash__, the default implementations here will not work. Examples For interaction with Apache Arrow (pyarrow), a __from_arrow__ method can be implemented: this method receives a pyarrow Array or ChunkedArray as only argument and is expected to return the appropriate pandas ExtensionArray for this dtype and the passed values: This class does not inherit from ‘abc.ABCMeta’ for performance reasons. Methods and properties required by the interface raise pandas.errors.AbstractMethodError and no register method is provided for registering virtual subclasses. Attributes index_class The Index subclass to return from Index.__new__ when this dtype is encountered. kind A character code (one of 'biufcmMOSUV'), default 'O' na_value Default NA value to use for this type. name A string identifying the data type. names Ordered list of field names, or None if there are no fields. type The scalar type for the array, e.g. int.",No parameters found,"['>>> import pyarrow\n>>> from pandas.api.extensions import ExtensionArray\n>>> class ExtensionDtype:\n...     def __from_arrow__(\n...         self,\n...         array: pyarrow.Array | pyarrow.ChunkedArray\n...     ) -> ExtensionArray:\n...         ...']"
258,..\pandas\reference\api\pandas.Series.dt.day_name.html,pandas.Series.dt.day_name,"Series.dt.day_name(*args, **kwargs)[source]# Return the day names with specified locale.","Parameters: localestr, optionalLocale determining the language in which to return the day name. Default is English locale ('en_US.utf8'). Use the command locale -a on your terminal on Unix systems to find your locale language code. Returns: Series or IndexSeries or Index of day names.","["">>> s = pd.Series(pd.date_range(start='2018-01-01', freq='D', periods=3))\n>>> s\n0   2018-01-01\n1   2018-01-02\n2   2018-01-03\ndtype: datetime64[ns]\n>>> s.dt.day_name()\n0       Monday\n1      Tuesday\n2    Wednesday\ndtype: object"", "">>> idx = pd.date_range(start='2018-01-01', freq='D', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.day_name()\nIndex(['Monday', 'Tuesday', 'Wednesday'], dtype='object')"", "">>> idx = pd.date_range(start='2018-01-01', freq='D', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.day_name(locale='pt_BR.utf8') \nIndex(['Segunda', 'Terça', 'Quarta'], dtype='object')""]"
259,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_quarter_start.html,pandas.tseries.offsets.Minute.is_quarter_start,Minute.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
260,..\pandas\reference\api\pandas.DataFrame.swaplevel.html,pandas.DataFrame.swaplevel,"DataFrame.swaplevel(i=-2, j=-1, axis=0)[source]# Swap levels i and j in a MultiIndex. Default is to swap the two innermost levels of the index.","Parameters: i, jint or strLevels of the indices to be swapped. Can pass level name as string. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to swap levels on. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. Returns: DataFrameDataFrame with levels swapped in MultiIndex.","['>>> df = pd.DataFrame(\n...     {""Grade"": [""A"", ""B"", ""A"", ""C""]},\n...     index=[\n...         [""Final exam"", ""Final exam"", ""Coursework"", ""Coursework""],\n...         [""History"", ""Geography"", ""History"", ""Geography""],\n...         [""January"", ""February"", ""March"", ""April""],\n...     ],\n... )\n>>> df\n                                    Grade\nFinal exam  History     January      A\n            Geography   February     B\nCoursework  History     March        A\n            Geography   April        C', '>>> df.swaplevel()\n                                    Grade\nFinal exam  January     History         A\n            February    Geography       B\nCoursework  March       History         A\n            April       Geography       C', '>>> df.swaplevel(0)\n                                    Grade\nJanuary     History     Final exam      A\nFebruary    Geography   Final exam      B\nMarch       History     Coursework      A\nApril       Geography   Coursework      C', '>>> df.swaplevel(0, 1)\n                                    Grade\nHistory     Final exam  January         A\nGeography   Final exam  February        B\nHistory     Coursework  March           A\nGeography   Coursework  April           C']"
261,..\pandas\reference\api\pandas.Series.dt.day_of_week.html,pandas.Series.dt.day_of_week,"Series.dt.day_of_week[source]# The day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.",Returns: Series or IndexContaining integers indicating the day number.,"["">>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int32""]"
262,..\pandas\reference\api\pandas.set_eng_float_format.html,pandas.set_eng_float_format,"pandas.set_eng_float_format(accuracy=3, use_eng_prefix=False)[source]# Format float representation in DataFrame with SI notation.","Parameters: accuracyint, default 3Number of decimal digits after the floating point. use_eng_prefixbool, default FalseWhether to represent a value with SI prefixes. Returns: None","['>>> df = pd.DataFrame([1e-9, 1e-3, 1, 1e3, 1e6])\n>>> df\n              0\n0  1.000000e-09\n1  1.000000e-03\n2  1.000000e+00\n3  1.000000e+03\n4  1.000000e+06', '>>> pd.set_eng_float_format(accuracy=1)\n>>> df\n         0\n0  1.0E-09\n1  1.0E-03\n2  1.0E+00\n3  1.0E+03\n4  1.0E+06', '>>> pd.set_eng_float_format(use_eng_prefix=True)\n>>> df\n        0\n0  1.000n\n1  1.000m\n2   1.000\n3  1.000k\n4  1.000M', '>>> pd.set_eng_float_format(accuracy=1, use_eng_prefix=True)\n>>> df\n      0\n0  1.0n\n1  1.0m\n2   1.0\n3  1.0k\n4  1.0M', '>>> pd.set_option(""display.float_format"", None)  # unset option']"
263,..\pandas\reference\api\pandas.core.resample.Resampler.count.html,pandas.core.resample.Resampler.count,"final Resampler.count()[source]# Compute count of group, excluding missing values.",Returns: Series or DataFrameCount of values within each group.,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, np.nan], index=lst)\n>>> ser\na    1.0\na    2.0\nb    NaN\ndtype: float64\n>>> ser.groupby(level=0).count()\na    2\nb    0\ndtype: int64"", '>>> data = [[1, np.nan, 3], [1, np.nan, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a         b     c\ncow     1       NaN     3\nhorse   1       NaN     6\nbull    7       8.0     9\n>>> df.groupby(""a"").count()\n    b   c\na\n1   0   2\n7   1   1', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').count()\n2023-01-01    2\n2023-02-01    2\nFreq: MS, dtype: int64""]"
264,..\pandas\reference\api\pandas.api.extensions.register_dataframe_accessor.html,pandas.api.extensions.register_dataframe_accessor,"pandas.api.extensions.register_dataframe_accessor(name)[source]# Register a custom accessor on DataFrame objects. Notes When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be def __init__(self, pandas_object):  # noqa: E999     ... For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype.",Parameters: namestrName under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute. Returns: callableA class decorator.,"["">>> pd.Series(['a', 'b']).dt\nTraceback (most recent call last):\n...\nAttributeError: Can only use .dt accessor with datetimelike values""]"
265,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.name.html,pandas.tseries.offsets.BYearEnd.name,BYearEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
266,..\pandas\reference\api\pandas.IntervalIndex.html,pandas.IntervalIndex,"class pandas.IntervalIndex(data, closed=None, dtype=None, copy=False, name=None, verify_integrity=True)[source]# Immutable index of intervals that are closed on the same side. Attributes closed String describing the inclusive side the intervals. is_empty Indicates if an interval is empty, meaning it contains no points. is_non_overlapping_monotonic Return a boolean whether the IntervalArray is non-overlapping and monotonic. is_overlapping Return True if the IntervalIndex has overlapping intervals, else False. values Return an array representing the data in the Index. left right mid length Methods from_arrays(left, right[, closed, name, ...]) Construct from two arrays defining the left and right bounds. from_tuples(data[, closed, name, copy, dtype]) Construct an IntervalIndex from an array-like of tuples. from_breaks(breaks[, closed, name, copy, dtype]) Construct an IntervalIndex from an array of splits. contains(*args, **kwargs) Check elementwise if the Intervals contain the value. overlaps(*args, **kwargs) Check elementwise if an Interval overlaps the values in the IntervalArray. set_closed(*args, **kwargs) Return an identical IntervalArray closed on the specified side. to_tuples(*args, **kwargs) Return an ndarray (if self is IntervalArray) or Index (if self is IntervalIndex) of tuples of the form (left, right). Notes See the user guide for more.","Parameters: dataarray-like (1-dimensional)Array-like (ndarray, DateTimeArray, TimeDeltaArray) containing Interval objects from which to build the IntervalIndex. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. dtypedtype or None, default NoneIf None, dtype will be inferred. copybool, default FalseCopy the input data. nameobject, optionalName to be stored in the index. verify_integritybool, default TrueVerify that the IntervalIndex is valid.","["">>> pd.interval_range(start=0, end=5)\nIntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],\n              dtype='interval[int64, right]')""]"
267,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_year_end.html,pandas.tseries.offsets.Minute.is_year_end,Minute.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
268,..\pandas\reference\api\pandas.DataFrame.T.html,pandas.DataFrame.T,property DataFrame.T[source]# The transpose of the DataFrame.,Returns: DataFrameThe transposed DataFrame.,"["">>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n>>> df\n   col1  col2\n0     1     3\n1     2     4"", '>>> df.T\n      0  1\ncol1  1  2\ncol2  3  4']"
269,..\pandas\reference\api\pandas.core.resample.Resampler.ffill.html,pandas.core.resample.Resampler.ffill,final Resampler.ffill(limit=None)[source]# Forward fill the values.,"Parameters: limitint, optionalLimit of how many values to fill. Returns: An upsampled Series.","["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64"", "">>> ser.resample('MS').ffill()\n2023-01-01    1\n2023-02-01    3\nFreq: MS, dtype: int64"", "">>> ser.resample('W').ffill()\n2023-01-01    1\n2023-01-08    1\n2023-01-15    2\n2023-01-22    2\n2023-01-29    2\n2023-02-05    3\n2023-02-12    3\n2023-02-19    4\nFreq: W-SUN, dtype: int64"", "">>> ser.resample('W').ffill(limit=1)\n2023-01-01    1.0\n2023-01-08    1.0\n2023-01-15    2.0\n2023-01-22    2.0\n2023-01-29    NaN\n2023-02-05    3.0\n2023-02-12    NaN\n2023-02-19    4.0\nFreq: W-SUN, dtype: float64""]"
270,..\pandas\reference\api\pandas.api.extensions.register_extension_dtype.html,pandas.api.extensions.register_extension_dtype,pandas.api.extensions.register_extension_dtype(cls)[source]# Register an ExtensionType with pandas as class decorator. This enables operations like .astype(name) for the name of the ExtensionDtype.,Returns: callableA class decorator.,"['>>> from pandas.api.extensions import register_extension_dtype, ExtensionDtype\n>>> @register_extension_dtype\n... class MyExtensionDtype(ExtensionDtype):\n...     name = ""myextension""']"
271,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.nanos.html,pandas.tseries.offsets.BYearEnd.nanos,BYearEnd.nanos#,No parameters found,[]
272,..\pandas\reference\api\pandas.Series.dt.day_of_year.html,pandas.Series.dt.day_of_year,Series.dt.day_of_year[source]# The ordinal day of the year.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.dayofyear\n0    1\n1   32\ndtype: int32', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.dayofyear\nIndex([1, 32], dtype=\'int32\')']"
273,..\pandas\reference\api\pandas.set_option.html,pandas.set_option,"pandas.set_option(pat, value) = <pandas._config.config.CallableDynamicDoc object># Sets the value of the specified option. Available options: compute.[use_bottleneck, use_numba, use_numexpr] display.[chop_threshold, colheader_justify, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format] display.html.[border, table_schema, use_mathjax] display.[large_repr, max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions] display.unicode.[ambiguous_as_wide, east_asian_width] display.[width] future.[infer_string, no_silent_downcasting] io.excel.ods.[reader, writer] io.excel.xls.[reader] io.excel.xlsb.[reader] io.excel.xlsm.[reader, writer] io.excel.xlsx.[reader, writer] io.hdf.[default_format, dropna_table] io.parquet.[engine] io.sql.[engine] mode.[chained_assignment, copy_on_write, data_manager, sim_interactive, string_storage, use_inf_as_na] plotting.[backend] plotting.matplotlib.[register_converters] styler.format.[decimal, escape, formatter, na_rep, precision, thousands] styler.html.[mathjax] styler.latex.[environment, hrules, multicol_align, multirow_align] styler.render.[encoding, max_columns, max_elements, max_rows, repr] styler.sparse.[columns, index] Notes Please reference the User Guide for more information. The available options with its descriptions: compute.use_bottleneckboolUse the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True] compute.use_numbaboolUse the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False] compute.use_numexprboolUse the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True] display.chop_thresholdfloat or Noneif set to a float value, all float values smaller than the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None] display.colheader_justify‘left’/’right’Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right] display.date_dayfirstbooleanWhen True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False] display.date_yearfirstbooleanWhen True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False] display.encodingstr/unicodeDefaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8] display.expand_frame_reprbooleanWhether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple “pages” if its width exceeds display.width. [default: True] [currently: True] display.float_formatcallableThe callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None] display.html.borderintA border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1] display.html.table_schemabooleanWhether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False] display.html.use_mathjaxbooleanWhen True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True] display.large_repr‘truncate’/’info’For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table, or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate] display.max_categoriesintThis sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype “category”. [default: 8] [currently: 8] display.max_columnsintIf max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 or None and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection and defaults to 20. [default: 0] [currently: 0] display.max_colwidthint or NoneThe maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a “…” placeholder is embedded in the output. A ‘None’ value means unlimited. [default: 50] [currently: 50] display.max_dir_itemsintThe number of items that will be added to dir(…). ‘None’ value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added. This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100] display.max_info_columnsintmax_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100] display.max_info_rowsintdf.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785] display.max_rowsintIf max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60] display.max_seq_itemsint or NoneWhen pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of “…” to the resulting string. If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100] display.memory_usagebool, string or NoneThis specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,’deep’ [default: True] [currently: True] display.min_rowsintThe numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10] display.multi_sparseboolean“sparsify” MultiIndex display (don’t display repeated elements in outer levels within groups) [default: True] [currently: True] display.notebook_repr_htmlbooleanWhen True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True] display.pprint_nest_depthintControls the number of nested levels to process when pretty-printing [default: 3] [currently: 3] display.precisionintFloating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6] display.show_dimensionsboolean or ‘truncate’Whether to print out dimensions at the end of DataFrame repr. If ‘truncate’ is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate] display.unicode.ambiguous_as_widebooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.unicode.east_asian_widthbooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.widthintWidth of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80] future.infer_string Whether to infer sequence of str objects as pyarrow string dtype, which will be the default in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] future.no_silent_downcasting Whether to opt-in to the future behavior which will not silently downcast results from Series and DataFrame where, mask, and clip methods. Silent downcasting will be removed in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] io.excel.ods.readerstringThe default Excel reader engine for ‘ods’ files. Available options: auto, odf, calamine. [default: auto] [currently: auto] io.excel.ods.writerstringThe default Excel writer engine for ‘ods’ files. Available options: auto, odf. [default: auto] [currently: auto] io.excel.xls.readerstringThe default Excel reader engine for ‘xls’ files. Available options: auto, xlrd, calamine. [default: auto] [currently: auto] io.excel.xlsb.readerstringThe default Excel reader engine for ‘xlsb’ files. Available options: auto, pyxlsb, calamine. [default: auto] [currently: auto] io.excel.xlsm.readerstringThe default Excel reader engine for ‘xlsm’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsm.writerstringThe default Excel writer engine for ‘xlsm’ files. Available options: auto, openpyxl. [default: auto] [currently: auto] io.excel.xlsx.readerstringThe default Excel reader engine for ‘xlsx’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsx.writerstringThe default Excel writer engine for ‘xlsx’ files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto] io.hdf.default_formatformatdefault format writing format, if None, then put will default to ‘fixed’ and append will default to ‘table’ [default: None] [currently: None] io.hdf.dropna_tablebooleandrop ALL nan rows when appending to a table [default: False] [currently: False] io.parquet.enginestringThe default parquet reader/writer engine. Available options: ‘auto’, ‘pyarrow’, ‘fastparquet’, the default is ‘auto’ [default: auto] [currently: auto] io.sql.enginestringThe default sql reader/writer engine. Available options: ‘auto’, ‘sqlalchemy’, the default is ‘auto’ [default: auto] [currently: auto] mode.chained_assignmentstringRaise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn] mode.copy_on_writeboolUse new copy-view behaviour using Copy-on-Write. Defaults to False, unless overridden by the ‘PANDAS_COPY_ON_WRITE’ environment variable (if set to “1” for True, needs to be set before pandas is imported). [default: False] [currently: False] mode.data_managerstringInternal data manager type; can be “block” or “array”. Defaults to “block”, unless overridden by the ‘PANDAS_DATA_MANAGER’ environment variable (needs to be set before pandas is imported). [default: block] [currently: block] (Deprecated, use `` instead.) mode.sim_interactivebooleanWhether to simulate interactive mode for purposes of testing [default: False] [currently: False] mode.string_storagestringThe default storage for StringDtype. This option is ignored if future.infer_string is set to True. [default: python] [currently: python] mode.use_inf_as_nabooleanTrue means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). This option is deprecated in pandas 2.1.0 and will be removed in 3.0. [default: False] [currently: False] (Deprecated, use `` instead.) plotting.backendstrThe plotting backend to use. The default value is “matplotlib”, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib] plotting.matplotlib.register_convertersbool or ‘auto’.Whether to register converters with matplotlib’s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto] styler.format.decimalstrThe character representation for the decimal separator for floats and complex. [default: .] [currently: .] styler.format.escapestr, optionalWhether to escape certain characters according to the given context; html or latex. [default: None] [currently: None] styler.format.formatterstr, callable, dict, optionalA formatter object to be used as default within Styler.format. [default: None] [currently: None] styler.format.na_repstr, optionalThe string representation for values identified as missing. [default: None] [currently: None] styler.format.precisionintThe precision for floats and complex numbers. [default: 6] [currently: 6] styler.format.thousandsstr, optionalThe character representation for thousands separator for floats, int and complex. [default: None] [currently: None] styler.html.mathjaxboolIf False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True] styler.latex.environmentstrThe environment to replace \begin{table}. If “longtable” is used results in a specific longtable environment format. [default: None] [currently: None] styler.latex.hrulesboolWhether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False] styler.latex.multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. “|r” will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r] styler.latex.multirow_align{“c”, “t”, “b”}The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c] styler.render.encodingstrThe encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8] styler.render.max_columnsint, optionalThe maximum number of columns that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.max_elementsintThe maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144] styler.render.max_rowsint, optionalThe maximum number of rows that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.reprstrDetermine which output to use in Jupyter Notebook in {“html”, “latex”}. [default: html] [currently: html] styler.sparse.columnsboolWhether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True] styler.sparse.indexboolWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]","Parameters: patstrRegexp which should match a single option. Note: partial matches are supported for convenience, but unless you use the full option name (e.g. x.y.z.option_name), your code may break in future versions if new options with similar names are introduced. valueobjectNew value of option. Returns: None Raises: OptionError if no such option exists","["">>> pd.set_option('display.max_columns', 4)\n>>> df = pd.DataFrame([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> df\n   0  1  ...  3   4\n0  1  2  ...  4   5\n1  6  7  ...  9  10\n[2 rows x 5 columns]\n>>> pd.reset_option('display.max_columns')""]"
274,..\pandas\reference\api\pandas.core.resample.Resampler.fillna.html,pandas.core.resample.Resampler.fillna,"final Resampler.fillna(method, limit=None)[source]# Fill missing values introduced by upsampling. In statistics, imputation is the process of replacing missing data with substituted values [1]. When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency). Missing values that existed in the original data will not be modified. References [1] https://en.wikipedia.org/wiki/Imputation_(statistics)","Parameters: method{‘pad’, ‘backfill’, ‘ffill’, ‘bfill’, ‘nearest’}Method to use for filling holes in resampled data ‘pad’ or ‘ffill’: use previous valid observation to fill gap (forward fill). ‘backfill’ or ‘bfill’: use next valid observation to fill gap. ‘nearest’: use nearest valid observation to fill gap. limitint, optionalLimit of how many consecutive missing values to fill. Returns: Series or DataFrameAn upsampled Series or DataFrame with missing values filled.","["">>> s = pd.Series([1, 2, 3],\n...               index=pd.date_range('20180101', periods=3, freq='h'))\n>>> s\n2018-01-01 00:00:00    1\n2018-01-01 01:00:00    2\n2018-01-01 02:00:00    3\nFreq: h, dtype: int64"", '>>> s.resample(""30min"").asfreq()\n2018-01-01 00:00:00    1.0\n2018-01-01 00:30:00    NaN\n2018-01-01 01:00:00    2.0\n2018-01-01 01:30:00    NaN\n2018-01-01 02:00:00    3.0\nFreq: 30min, dtype: float64', '>>> s.resample(\'30min\').fillna(""backfill"")\n2018-01-01 00:00:00    1\n2018-01-01 00:30:00    2\n2018-01-01 01:00:00    2\n2018-01-01 01:30:00    3\n2018-01-01 02:00:00    3\nFreq: 30min, dtype: int64', '>>> s.resample(\'15min\').fillna(""backfill"", limit=2)\n2018-01-01 00:00:00    1.0\n2018-01-01 00:15:00    NaN\n2018-01-01 00:30:00    2.0\n2018-01-01 00:45:00    2.0\n2018-01-01 01:00:00    2.0\n2018-01-01 01:15:00    NaN\n2018-01-01 01:30:00    3.0\n2018-01-01 01:45:00    3.0\n2018-01-01 02:00:00    3.0\nFreq: 15min, dtype: float64', '>>> s.resample(\'30min\').fillna(""pad"")\n2018-01-01 00:00:00    1\n2018-01-01 00:30:00    1\n2018-01-01 01:00:00    2\n2018-01-01 01:30:00    2\n2018-01-01 02:00:00    3\nFreq: 30min, dtype: int64', '>>> s.resample(\'30min\').fillna(""nearest"")\n2018-01-01 00:00:00    1\n2018-01-01 00:30:00    2\n2018-01-01 01:00:00    2\n2018-01-01 01:30:00    3\n2018-01-01 02:00:00    3\nFreq: 30min, dtype: int64', "">>> sm = pd.Series([1, None, 3],\n...                index=pd.date_range('20180101', periods=3, freq='h'))\n>>> sm\n2018-01-01 00:00:00    1.0\n2018-01-01 01:00:00    NaN\n2018-01-01 02:00:00    3.0\nFreq: h, dtype: float64"", "">>> sm.resample('30min').fillna('backfill')\n2018-01-01 00:00:00    1.0\n2018-01-01 00:30:00    NaN\n2018-01-01 01:00:00    NaN\n2018-01-01 01:30:00    3.0\n2018-01-01 02:00:00    3.0\nFreq: 30min, dtype: float64"", "">>> sm.resample('30min').fillna('pad')\n2018-01-01 00:00:00    1.0\n2018-01-01 00:30:00    1.0\n2018-01-01 01:00:00    NaN\n2018-01-01 01:30:00    NaN\n2018-01-01 02:00:00    3.0\nFreq: 30min, dtype: float64"", "">>> sm.resample('30min').fillna('nearest')\n2018-01-01 00:00:00    1.0\n2018-01-01 00:30:00    NaN\n2018-01-01 01:00:00    NaN\n2018-01-01 01:30:00    3.0\n2018-01-01 02:00:00    3.0\nFreq: 30min, dtype: float64"", "">>> df = pd.DataFrame({'a': [2, np.nan, 6], 'b': [1, 3, 5]},\n...                   index=pd.date_range('20180101', periods=3,\n...                                       freq='h'))\n>>> df\n                       a  b\n2018-01-01 00:00:00  2.0  1\n2018-01-01 01:00:00  NaN  3\n2018-01-01 02:00:00  6.0  5"", '>>> df.resample(\'30min\').fillna(""bfill"")\n                       a  b\n2018-01-01 00:00:00  2.0  1\n2018-01-01 00:30:00  NaN  3\n2018-01-01 01:00:00  NaN  3\n2018-01-01 01:30:00  6.0  5\n2018-01-01 02:00:00  6.0  5']"
275,..\pandas\reference\api\pandas.IntervalIndex.is_empty.html,pandas.IntervalIndex.is_empty,"property IntervalIndex.is_empty[source]# Indicates if an interval is empty, meaning it contains no points.","Returns: bool or ndarrayA boolean indicating if a scalar Interval is empty, or a boolean ndarray positionally indicating if an Interval in an IntervalArray or IntervalIndex is empty.","["">>> pd.Interval(0, 1, closed='right').is_empty\nFalse"", "">>> pd.Interval(0, 0, closed='right').is_empty\nTrue\n>>> pd.Interval(0, 0, closed='left').is_empty\nTrue\n>>> pd.Interval(0, 0, closed='neither').is_empty\nTrue"", "">>> pd.Interval(0, 0, closed='both').is_empty\nFalse"", "">>> ivs = [pd.Interval(0, 0, closed='neither'),\n...        pd.Interval(1, 2, closed='neither')]\n>>> pd.arrays.IntervalArray(ivs).is_empty\narray([ True, False])"", "">>> ivs = [pd.Interval(0, 0, closed='neither'), np.nan]\n>>> pd.IntervalIndex(ivs).is_empty\narray([ True, False])""]"
276,..\pandas\reference\api\pandas.tseries.offsets.Minute.is_year_start.html,pandas.tseries.offsets.Minute.is_year_start,Minute.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
277,..\pandas\reference\api\pandas.DataFrame.tail.html,pandas.DataFrame.tail,"DataFrame.tail(n=5)[source]# Return the last n rows. This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of n, this function returns all rows except the first |n| rows, equivalent to df[|n|:]. If n is larger than the number of rows, this function returns all rows.","Parameters: nint, default 5Number of rows to select. Returns: type of callerThe last n rows of the caller object.","["">>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n>>> df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra"", '>>> df.tail()\n   animal\n4  monkey\n5  parrot\n6   shark\n7   whale\n8   zebra', '>>> df.tail(3)\n  animal\n6  shark\n7  whale\n8  zebra', '>>> df.tail(-3)\n   animal\n3    lion\n4  monkey\n5  parrot\n6   shark\n7   whale\n8   zebra']"
278,..\pandas\reference\api\pandas.api.extensions.register_index_accessor.html,pandas.api.extensions.register_index_accessor,"pandas.api.extensions.register_index_accessor(name)[source]# Register a custom accessor on Index objects. Notes When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be def __init__(self, pandas_object):  # noqa: E999     ... For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype.",Parameters: namestrName under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute. Returns: callableA class decorator.,"["">>> pd.Series(['a', 'b']).dt\nTraceback (most recent call last):\n...\nAttributeError: Can only use .dt accessor with datetimelike values""]"
279,..\pandas\reference\api\pandas.show_versions.html,pandas.show_versions,"pandas.show_versions(as_json=False)[source]# Provide useful information, important for bug reports. It comprises info about hosting operation system, pandas version, and versions of other installed relative packages.","Parameters: as_jsonstr or bool, default False If False, outputs info in a human readable form to the console. If str, it will be considered as a path to a file. Info will be written to that file in JSON format. If True, outputs info in JSON format to the console.",['>>> pd.show_versions()  \nYour output may look something like this:\nINSTALLED VERSIONS\n------------------\ncommit           : 37ea63d540fd27274cad6585082c91b1283f963d\npython           : 3.10.6.final.0\npython-bits      : 64\nOS               : Linux\nOS-release       : 5.10.102.1-microsoft-standard-WSL2\nVersion          : #1 SMP Wed Mar 2 00:30:59 UTC 2022\nmachine          : x86_64\nprocessor        : x86_64\nbyteorder        : little\nLC_ALL           : None\nLANG             : en_GB.UTF-8\nLOCALE           : en_GB.UTF-8\npandas           : 2.0.1\nnumpy            : 1.24.3\n...']
280,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.normalize.html,pandas.tseries.offsets.BYearEnd.normalize,BYearEnd.normalize#,No parameters found,[]
281,..\pandas\reference\api\pandas.api.extensions.register_series_accessor.html,pandas.api.extensions.register_series_accessor,"pandas.api.extensions.register_series_accessor(name)[source]# Register a custom accessor on Series objects. Notes When accessed, your accessor will be initialized with the pandas object the user is interacting with. So the signature must be def __init__(self, pandas_object):  # noqa: E999     ... For consistency with pandas methods, you should raise an AttributeError if the data passed to your accessor has an incorrect dtype.",Parameters: namestrName under which the accessor should be registered. A warning is issued if this name conflicts with a preexisting attribute. Returns: callableA class decorator.,"["">>> pd.Series(['a', 'b']).dt\nTraceback (most recent call last):\n...\nAttributeError: Can only use .dt accessor with datetimelike values""]"
282,..\pandas\reference\api\pandas.IntervalIndex.is_non_overlapping_monotonic.html,pandas.IntervalIndex.is_non_overlapping_monotonic,"IntervalIndex.is_non_overlapping_monotonic[source]# Return a boolean whether the IntervalArray is non-overlapping and monotonic. Non-overlapping means (no Intervals share points), and monotonic means either monotonic increasing or monotonic decreasing.",No parameters found,"['>>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.is_non_overlapping_monotonic\nTrue', '>>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1),\n...                                       pd.Interval(-1, 0.1)])\n>>> interv_arr\n<IntervalArray>\n[(0.0, 1.0], (-1.0, 0.1]]\nLength: 2, dtype: interval[float64, right]\n>>> interv_arr.is_non_overlapping_monotonic\nFalse', "">>> interv_idx = pd.interval_range(start=0, end=2)\n>>> interv_idx\nIntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')\n>>> interv_idx.is_non_overlapping_monotonic\nTrue"", "">>> interv_idx = pd.interval_range(start=0, end=2, closed='both')\n>>> interv_idx\nIntervalIndex([[0, 1], [1, 2]], dtype='interval[int64, both]')\n>>> interv_idx.is_non_overlapping_monotonic\nFalse""]"
283,..\pandas\reference\api\pandas.core.resample.Resampler.first.html,pandas.core.resample.Resampler.first,"final Resampler.first(numeric_only=False, min_count=0, skipna=True, *args, **kwargs)[source]# Compute the first entry of each column within each group. Defaults to skipping NA elements.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count valid values are present the result will be NA. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Added in version 2.2.1. Returns: Series or DataFrameFirst values within each group.","['>>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[None, 5, 6], C=[1, 2, 3],\n...                        D=[\'3/11/2000\', \'3/12/2000\', \'3/13/2000\']))\n>>> df[\'D\'] = pd.to_datetime(df[\'D\'])\n>>> df.groupby(""A"").first()\n     B  C          D\nA\n1  5.0  1 2000-03-11\n3  6.0  3 2000-03-13\n>>> df.groupby(""A"").first(min_count=2)\n    B    C          D\nA\n1 NaN  1.0 2000-03-11\n3 NaN  NaN        NaT\n>>> df.groupby(""A"").first(numeric_only=True)\n     B  C\nA\n1  5.0  1\n3  6.0  3']"
284,..\pandas\reference\api\pandas.tseries.offsets.Minute.kwds.html,pandas.tseries.offsets.Minute.kwds,Minute.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
285,..\pandas\reference\api\pandas.Series.dt.end_time.html,pandas.Series.dt.end_time,Series.dt.end_time[source]# Get the Timestamp for the end of the period.,Returns: Timestamp,"["">>> pd.Period('2020-01', 'D').end_time\nTimestamp('2020-01-01 23:59:59.999999999')"", "">>> period_index = pd.period_range('2020-1-1 00:00', '2020-3-1 00:00', freq='M')\n>>> s = pd.Series(period_index)\n>>> s\n0   2020-01\n1   2020-02\n2   2020-03\ndtype: period[M]\n>>> s.dt.end_time\n0   2020-01-31 23:59:59.999999999\n1   2020-02-29 23:59:59.999999999\n2   2020-03-31 23:59:59.999999999\ndtype: datetime64[ns]"", '>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.end_time\nDatetimeIndex([\'2023-01-31 23:59:59.999999999\',\n               \'2023-02-28 23:59:59.999999999\',\n               \'2023-03-31 23:59:59.999999999\'],\n               dtype=\'datetime64[ns]\', freq=None)']"
286,..\pandas\reference\api\pandas.DataFrame.take.html,pandas.DataFrame.take,"DataFrame.take(indices, axis=0, **kwargs)[source]# Return the elements in the given positional indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object.","Parameters: indicesarray-likeAn array of ints indicating which positions to take. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns. For Series this parameter is unused and defaults to 0. **kwargsFor compatibility with numpy.take(). Has no effect on the output. Returns: same type as callerAn array-like containing the elements taken from the object.","["">>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n...                    ('parrot', 'bird', 24.0),\n...                    ('lion', 'mammal', 80.5),\n...                    ('monkey', 'mammal', np.nan)],\n...                   columns=['name', 'class', 'max_speed'],\n...                   index=[0, 2, 3, 1])\n>>> df\n     name   class  max_speed\n0  falcon    bird      389.0\n2  parrot    bird       24.0\n3    lion  mammal       80.5\n1  monkey  mammal        NaN"", '>>> df.take([0, 3])\n     name   class  max_speed\n0  falcon    bird      389.0\n1  monkey  mammal        NaN', '>>> df.take([1, 2], axis=1)\n    class  max_speed\n0    bird      389.0\n2    bird       24.0\n3  mammal       80.5\n1  mammal        NaN', '>>> df.take([-1, -2])\n     name   class  max_speed\n1  monkey  mammal        NaN\n3    lion  mammal       80.5']"
287,..\pandas\reference\api\pandas.SparseDtype.html,pandas.SparseDtype,"class pandas.SparseDtype(dtype=<class 'numpy.float64'>, fill_value=None)[source]# Dtype for data stored in SparseArray. This dtype implements the pandas ExtensionDtype interface. Attributes None Methods None","Parameters: dtypestr, ExtensionDtype, numpy.dtype, type, default numpy.float64The dtype of the underlying array storing the non-fill value values. fill_valuescalar, optionalThe scalar value not stored in the SparseArray. By default, this depends on dtype. dtype na_value float np.nan int 0 bool False datetime64 pd.NaT timedelta64 pd.NaT The default value may be overridden by specifying a fill_value.","['>>> ser = pd.Series([1, 0, 0], dtype=pd.SparseDtype(dtype=int, fill_value=0))\n>>> ser\n0    1\n1    0\n2    0\ndtype: Sparse[int64, 0]\n>>> ser.sparse.density\n0.3333333333333333']"
288,..\pandas\reference\api\pandas.api.indexers.BaseIndexer.html,pandas.api.indexers.BaseIndexer,"class pandas.api.indexers.BaseIndexer(index_array=None, window_size=0, **kwargs)[source]# Base class for window bounds calculations. Examples",No parameters found,"['>>> from pandas.api.indexers import BaseIndexer\n>>> class CustomIndexer(BaseIndexer):\n...     def get_window_bounds(self, num_values, min_periods, center, closed, step):\n...         start = np.empty(num_values, dtype=np.int64)\n...         end = np.empty(num_values, dtype=np.int64)\n...         for i in range(num_values):\n...             start[i] = i\n...             end[i] = i + self.window_size\n...         return start, end\n>>> df = pd.DataFrame({""values"": range(5)})\n>>> indexer = CustomIndexer(window_size=2)\n>>> df.rolling(indexer).sum()\n    values\n0   1.0\n1   3.0\n2   5.0\n3   7.0\n4   4.0']"
289,..\pandas\reference\api\pandas.DataFrame.to_clipboard.html,pandas.DataFrame.to_clipboard,"DataFrame.to_clipboard(*, excel=True, sep=None, **kwargs)[source]# Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Notes Requirements for your platform. Linux : xclip, or xsel (with PyQt4 modules) Windows : none macOS : none This method uses the processes developed for the package pyperclip. A solution to render any output string format is given in the examples.","Parameters: excelbool, default TrueProduce output in a csv format for easy pasting into excel. True, use the provided separator for csv pasting. False, write a string representation of the object to the clipboard. sepstr, default '\t'Field delimiter. **kwargsThese parameters will be passed to DataFrame.to_csv.","["">>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])"", "">>> df.to_clipboard(sep=',')  \n... # Wrote the following to the system clipboard:\n... # ,A,B,C\n... # 0,1,2,3\n... # 1,4,5,6"", "">>> df.to_clipboard(sep=',', index=False)  \n... # Wrote the following to the system clipboard:\n... # A,B,C\n... # 1,2,3\n... # 4,5,6""]"
290,..\pandas\reference\api\pandas.tseries.offsets.Minute.n.html,pandas.tseries.offsets.Minute.n,Minute.n#,No parameters found,[]
291,..\pandas\reference\api\pandas.core.resample.Resampler.get_group.html,pandas.core.resample.Resampler.get_group,"Resampler.get_group(name, obj=None)[source]# Construct DataFrame from group with provided name.","Parameters: nameobjectThe name of the group to get as a DataFrame. objDataFrame, default NoneThe DataFrame to take the DataFrame out of.  If it is None, the object groupby was called on will be used. Deprecated since version 2.1.0: The obj is deprecated and will be removed in a future version. Do df.iloc[gb.indices.get(name)] instead of gb.get_group(name, obj=df). Returns: same type as obj","['>>> lst = [\'a\', \'a\', \'b\']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).get_group(""a"")\na    1\na    2\ndtype: int64', '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(by=[""a""]).get_group((1,))\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').get_group('2023-01-01')\n2023-01-01    1\n2023-01-15    2\ndtype: int64""]"
292,..\pandas\reference\api\pandas.Series.dt.floor.html,pandas.Series.dt.floor,"Series.dt.floor(*args, **kwargs)[source]# Perform floor operation on the data to the specified freq. Notes If the timestamps have a timezone, flooring will take place relative to the local (“wall”) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to floor the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.floor('h')\nDatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.floor(""h"")\n0   2018-01-01 11:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 12:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 03:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.floor(""2h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n             dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.floor(""2h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
293,..\pandas\reference\api\pandas.tseries.offsets.BYearEnd.rule_code.html,pandas.tseries.offsets.BYearEnd.rule_code,BYearEnd.rule_code#,No parameters found,[]
294,..\pandas\reference\api\pandas.IntervalIndex.is_overlapping.html,pandas.IntervalIndex.is_overlapping,"property IntervalIndex.is_overlapping[source]# Return True if the IntervalIndex has overlapping intervals, else False. Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.",Returns: boolBoolean indicating if the IntervalIndex has overlapping intervals.,"["">>> index = pd.IntervalIndex.from_tuples([(0, 2), (1, 3), (4, 5)])\n>>> index\nIntervalIndex([(0, 2], (1, 3], (4, 5]],\n      dtype='interval[int64, right]')\n>>> index.is_overlapping\nTrue"", "">>> index = pd.interval_range(0, 3, closed='both')\n>>> index\nIntervalIndex([[0, 1], [1, 2], [2, 3]],\n      dtype='interval[int64, both]')\n>>> index.is_overlapping\nTrue"", "">>> index = pd.interval_range(0, 3, closed='left')\n>>> index\nIntervalIndex([[0, 1), [1, 2), [2, 3)],\n      dtype='interval[int64, left]')\n>>> index.is_overlapping\nFalse""]"
295,..\pandas\reference\api\pandas.api.indexers.check_array_indexer.html,pandas.api.indexers.check_array_indexer,"pandas.api.indexers.check_array_indexer(array, indexer)[source]# Check if indexer is a valid array indexer for array. For a boolean mask, array and indexer are checked to have the same length. The dtype is validated, and if it is an integer or boolean ExtensionArray, it is checked if there are missing values present, and it is converted to the appropriate numpy array. Other dtypes will raise an error. Non-array indexers (integer, slice, Ellipsis, tuples, ..) are passed through as is.",Parameters: arrayarray-likeThe array that is being indexed (only used for the length). indexerarray-like or list-likeThe array-like that’s used to index. List-like input that is not yet a numpy array or an ExtensionArray is converted to one. Other input types are passed through as is. Returns: numpy.ndarrayThe validated indexer as a numpy array that can be used to index. Raises: IndexErrorWhen the lengths don’t match. ValueErrorWhen indexer cannot be converted to a numpy ndarray to index (e.g. presence of missing values).,"['>>> mask = pd.array([True, False])\n>>> arr = pd.array([1, 2])\n>>> pd.api.indexers.check_array_indexer(arr, mask)\narray([ True, False])', '>>> mask = pd.array([True, False, True])\n>>> pd.api.indexers.check_array_indexer(arr, mask)\nTraceback (most recent call last):\n...\nIndexError: Boolean index has wrong length: 3 instead of 2.', '>>> mask = pd.array([True, pd.NA])\n>>> pd.api.indexers.check_array_indexer(arr, mask)\narray([ True, False])', '>>> mask = np.array([True, False])\n>>> pd.api.indexers.check_array_indexer(arr, mask)\narray([ True, False])', '>>> indexer = pd.array([0, 2], dtype=""Int64"")\n>>> arr = pd.array([1, 2, 3])\n>>> pd.api.indexers.check_array_indexer(arr, indexer)\narray([0, 2])', '>>> indexer = pd.array([0, pd.NA], dtype=""Int64"")\n>>> pd.api.indexers.check_array_indexer(arr, indexer)\nTraceback (most recent call last):\n...\nValueError: Cannot index with an integer indexer containing NA values', '>>> indexer = np.array([0., 2.], dtype=""float64"")\n>>> pd.api.indexers.check_array_indexer(arr, indexer)\nTraceback (most recent call last):\n...\nIndexError: arrays used as indices must be of integer or boolean type']"
296,..\pandas\reference\api\pandas.StringDtype.html,pandas.StringDtype,class pandas.StringDtype(storage=None)[source]# Extension dtype for string data. Warning StringDtype is considered experimental. The implementation and parts of the API may change without warning. Attributes None Methods None,"Parameters: storage{“python”, “pyarrow”, “pyarrow_numpy”}, optionalIf not given, the value of pd.options.mode.string_storage.","['>>> pd.StringDtype()\nstring[python]', '>>> pd.StringDtype(storage=""pyarrow"")\nstring[pyarrow]']"
297,..\pandas\reference\api\pandas.DataFrame.to_csv.html,pandas.DataFrame.to_csv,"DataFrame.to_csv(path_or_buf=None, *, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='""', lineterminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.', errors='strict', storage_options=None)[source]# Write object to a comma-separated values (csv) file.","Parameters: path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. If a non-binary file object is passed, it should be opened with newline=’’, disabling universal newlines. If a binary file object is passed, mode might need to contain a ‘b’. sepstr, default ‘,’String of length 1. Field delimiter for the output file. na_repstr, default ‘’Missing data representation. float_formatstr, Callable, default NoneFormat string for floating point numbers. If a Callable is given, it takes precedence over other numeric formatting parameters, like decimal. columnssequence, optionalColumns to write. headerbool or list of str, default TrueWrite out the column names. If a list of strings is given it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). index_labelstr or sequence, or False, default NoneColumn label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode{‘w’, ‘x’, ‘a’}, default ‘w’Forwarded to either open(mode=) or fsspec.open(mode=) to control the file opening. Typical values include: ‘w’, truncate the file first. ‘x’, exclusive creation, failing if the file already exists. ‘a’, append to the end of file if it exists. encodingstr, optionalA string representing the encoding to use in the output file, defaults to ‘utf-8’. encoding is not supported if path_or_buf is a non-binary file object. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. May be a dict with key ‘method’ as compression mode and other entries as additional compression options if compression mode is ‘zip’. Passing compression options as keys in dict is supported for compression modes ‘gzip’, ‘bz2’, ‘zstd’, and ‘zip’. quotingoptional constant from csv moduleDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotecharstr, default ‘""’String of length 1. Character used to quote fields. lineterminatorstr, optionalThe newline character or character sequence to use in the output file. Defaults to os.linesep, which depends on the OS in which this method is called (’\n’ for linux, ‘\r\n’ for Windows, i.e.). Changed in version 1.5.0: Previously was line_terminator, changed for consistency with read_csv and the standard library ‘csv’ module. chunksizeint or NoneRows to write at a time. date_formatstr, default NoneFormat string for datetime objects. doublequotebool, default TrueControl quoting of quotechar inside a field. escapecharstr, default NoneString of length 1. Character used to escape sep and quotechar when appropriate. decimalstr, default ‘.’Character recognized as decimal separator. E.g. use ‘,’ for European data. errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Returns: None or strIf path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.","["">>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n...                    'mask': ['red', 'purple'],\n...                    'weapon': ['sai', 'bo staff']})\n>>> df.to_csv('out.csv', index=False)"", "">>> df.to_csv(index=False)\n'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n>>> compression_opts = dict(method='zip',\n...                         archive_name='out.csv')  \n>>> df.to_csv('out.zip', index=False,\n...           compression=compression_opts)"", "">>> from pathlib import Path  \n>>> filepath = Path('folder/subfolder/out.csv')  \n>>> filepath.parent.mkdir(parents=True, exist_ok=True)  \n>>> df.to_csv(filepath)"", "">>> import os  \n>>> os.makedirs('folder/subfolder', exist_ok=True)  \n>>> df.to_csv('folder/subfolder/out.csv')""]"
298,..\pandas\reference\api\pandas.tseries.offsets.Minute.name.html,pandas.tseries.offsets.Minute.name,Minute.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
299,..\pandas\reference\api\pandas.tseries.offsets.CBMonthBegin.html,pandas.tseries.offsets.CBMonthBegin,pandas.tseries.offsets.CBMonthBegin# alias of CustomBusinessMonthBegin,No parameters found,[]
300,..\pandas\reference\api\pandas.core.resample.Resampler.groups.html,pandas.core.resample.Resampler.groups,property Resampler.groups[source]# Dict {group name -> group labels}.,No parameters found,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).groups\n{'a': ['a', 'a'], 'b': ['b']}"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""])\n>>> df\n   a  b  c\n0  1  2  3\n1  1  5  6\n2  7  8  9\n>>> df.groupby(by=[""a""]).groups\n{1: [0, 1], 7: [2]}', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').groups\n{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}""]"
301,..\pandas\reference\api\pandas.DataFrame.to_dict.html,pandas.DataFrame.to_dict,"DataFrame.to_dict(orient='dict', *, into=<class 'dict'>, index=True)[source]# Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below).","Parameters: orientstr {‘dict’, ‘list’, ‘series’, ‘split’, ‘tight’, ‘records’, ‘index’}Determines the type of the values of the dictionary. ‘dict’ (default) : dict like {column -> {index -> value}} ‘list’ : dict like {column -> [values]} ‘series’ : dict like {column -> Series(values)} ‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]} ‘tight’ : dict like {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values], ‘index_names’ -> [index.names], ‘column_names’ -> [column.names]} ‘records’ : list like [{column -> value}, … , {column -> value}] ‘index’ : dict like {index -> {column -> value}} Added in version 1.4.0: ‘tight’ as an allowed value for the orient argument intoclass, default dictThe collections.abc.MutableMapping subclass used for all Mappings in the return value.  Can be the actual class or an empty instance of the mapping type you want.  If you want a collections.defaultdict, you must pass it initialized. indexbool, default TrueWhether to include the index item (and index_names item if orient is ‘tight’) in the returned dictionary. Can only be False when orient is ‘split’ or ‘tight’. Added in version 2.0.0. Returns: dict, list or collections.abc.MutableMappingReturn a collections.abc.MutableMapping object representing the DataFrame. The resulting transformation depends on the orient parameter.","["">>> df = pd.DataFrame({'col1': [1, 2],\n...                    'col2': [0.5, 0.75]},\n...                   index=['row1', 'row2'])\n>>> df\n      col1  col2\nrow1     1  0.50\nrow2     2  0.75\n>>> df.to_dict()\n{'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}}"", "">>> df.to_dict('series')\n{'col1': row1    1\n         row2    2\nName: col1, dtype: int64,\n'col2': row1    0.50\n        row2    0.75\nName: col2, dtype: float64}"", "">>> df.to_dict('split')\n{'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n 'data': [[1, 0.5], [2, 0.75]]}"", "">>> df.to_dict('records')\n[{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}]"", "">>> df.to_dict('index')\n{'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}}"", "">>> df.to_dict('tight')\n{'index': ['row1', 'row2'], 'columns': ['col1', 'col2'],\n 'data': [[1, 0.5], [2, 0.75]], 'index_names': [None], 'column_names': [None]}"", "">>> from collections import OrderedDict, defaultdict\n>>> df.to_dict(into=OrderedDict)\nOrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])),\n             ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))])"", "">>> dd = defaultdict(list)\n>>> df.to_dict('records', into=dd)\n[defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}),\n defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]""]"
302,..\pandas\reference\api\pandas.Series.dt.freq.html,pandas.Series.dt.freq,Series.dt.freq[source]#,No parameters found,[]
303,..\pandas\reference\api\pandas.api.indexers.FixedForwardWindowIndexer.html,pandas.api.indexers.FixedForwardWindowIndexer,"class pandas.api.indexers.FixedForwardWindowIndexer(index_array=None, window_size=0, **kwargs)[source]# Creates window boundaries for fixed-length windows that include the current row. Examples",No parameters found,"["">>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0"", '>>> indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2)\n>>> df.rolling(window=indexer, min_periods=1).sum()\n     B\n0  1.0\n1  3.0\n2  2.0\n3  4.0\n4  4.0']"
304,..\pandas\reference\api\pandas.test.html,pandas.test,"pandas.test(extra_args=None, run_doctests=False)[source]# Run the pandas test suite using pytest. By default, runs with the marks -m “not slow and not network and not db”","Parameters: extra_argslist[str], default NoneExtra marks to run the tests. run_doctestsbool, default FalseWhether to only run the Python and Cython doctests. If you would like to run both doctests/regular tests, just append “–doctest-modules”/”–doctest-cython” to extra_args.",['>>> pd.test()  \nrunning: pytest...']
305,..\pandas\reference\api\pandas.IntervalIndex.left.html,pandas.IntervalIndex.left,IntervalIndex.left[source]#,No parameters found,[]
306,..\pandas\reference\api\pandas.tseries.offsets.Minute.nanos.html,pandas.tseries.offsets.Minute.nanos,Minute.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
307,..\pandas\reference\api\pandas.tseries.offsets.CBMonthEnd.html,pandas.tseries.offsets.CBMonthEnd,pandas.tseries.offsets.CBMonthEnd# alias of CustomBusinessMonthEnd,No parameters found,[]
308,..\pandas\reference\api\pandas.core.resample.Resampler.indices.html,pandas.core.resample.Resampler.indices,property Resampler.indices[source]# Dict {group name -> group indices}.,No parameters found,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).indices\n{'a': array([0, 1]), 'b': array([2])}"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(by=[""a""]).indices\n{1: array([0, 1]), 7: array([2])}', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').indices\ndefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],\nTimestamp('2023-02-01 00:00:00'): [2, 3]})""]"
309,..\pandas\reference\api\pandas.DataFrame.to_excel.html,pandas.DataFrame.to_excel,"DataFrame.to_excel(excel_writer, *, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, inf_rep='inf', freeze_panes=None, storage_options=None, engine_kwargs=None)[source]# Write object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased. Notes For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook.","Parameters: excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter. sheet_namestr, default ‘Sheet1’Name of sheet which will contain DataFrame. na_repstr, default ‘’Missing data representation. float_formatstr, optionalFormat string for floating point numbers. For example float_format=""%.2f"" will format 0.1234 to 0.12. columnssequence or list of str, optionalColumns to write. headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrowint, default 0Upper left cell row to dump data frame. startcolint, default 0Upper left cell column to dump data frame. enginestr, optionalWrite engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this via the options io.excel.xlsx.writer or io.excel.xlsm.writer. merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells. inf_repstr, default ‘inf’Representation for infinity (there is no native representation for infinity in Excel). freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that is to be frozen. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Added in version 1.2.0. engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.","['>>> df1 = pd.DataFrame([[\'a\', \'b\'], [\'c\', \'d\']],\n...                    index=[\'row 1\', \'row 2\'],\n...                    columns=[\'col 1\', \'col 2\'])\n>>> df1.to_excel(""output.xlsx"")', '>>> df1.to_excel(""output.xlsx"",\n...              sheet_name=\'Sheet_name_1\')', "">>> df2 = df1.copy()\n>>> with pd.ExcelWriter('output.xlsx') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n...     df2.to_excel(writer, sheet_name='Sheet_name_2')"", "">>> with pd.ExcelWriter('output.xlsx',\n...                     mode='a') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_3')"", "">>> df1.to_excel('output1.xlsx', engine='xlsxwriter')""]"
310,..\pandas\reference\api\pandas.Series.dt.hour.html,pandas.Series.dt.hour,Series.dt.hour[source]# The hours of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""h"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 01:00:00\n2   2000-01-01 02:00:00\ndtype: datetime64[ns]\n>>> datetime_series.dt.hour\n0    0\n1    1\n2    2\ndtype: int32']"
311,..\pandas\reference\api\pandas.testing.assert_extension_array_equal.html,pandas.testing.assert_extension_array_equal,"pandas.testing.assert_extension_array_equal(left, right, check_dtype=True, index_values=None, check_exact=<no_default>, rtol=<no_default>, atol=<no_default>, obj='ExtensionArray')[source]# Check that left and right ExtensionArrays are equal. Notes Missing values are checked separately from valid values. A mask of missing values is computed for each and checked to match. The remaining all-valid values are cast to object dtype and checked.","Parameters: left, rightExtensionArrayThe two arrays to compare. check_dtypebool, default TrueWhether to check if the ExtensionArray dtypes are identical. index_valuesIndex | numpy.ndarray, default NoneOptional index (shared by both left and right), used in output. check_exactbool, default FalseWhether to compare number exactly. Changed in version 2.2.0: Defaults to True for integer dtypes if none of check_exact, rtol and atol are specified. rtolfloat, default 1e-5Relative tolerance. Only used when check_exact is False. atolfloat, default 1e-8Absolute tolerance. Only used when check_exact is False. objstr, default ‘ExtensionArray’Specify object name being compared, internally used to show appropriate assertion message. Added in version 2.0.0.","['>>> from pandas import testing as tm\n>>> a = pd.Series([1, 2, 3, 4])\n>>> b, c = a.array, a.array\n>>> tm.assert_extension_array_equal(b, c)']"
312,..\pandas\reference\api\pandas.tseries.offsets.CDay.html,pandas.tseries.offsets.CDay,pandas.tseries.offsets.CDay# alias of CustomBusinessDay,No parameters found,[]
313,..\pandas\reference\api\pandas.core.resample.Resampler.interpolate.html,pandas.core.resample.Resampler.interpolate,"final Resampler.interpolate(method='linear', *, axis=0, limit=None, inplace=False, limit_direction='forward', limit_area=None, downcast=<no_default>, **kwargs)[source]# Interpolate values between target timestamps according to different methods. The original index is first reindexed to target timestamps (see core.resample.Resampler.asfreq()), then the interpolation of NaN values via DataFrame.interpolate() happens. Notes For high-frequent or non-equidistant time-series with timestamps the reindexing followed by interpolation may lead to information loss as shown in the last example.","Parameters: methodstr, default ‘linear’Interpolation technique to use. One of: ‘linear’: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. ‘time’: Works on daily and higher resolution data to interpolate given length of interval. ‘index’, ‘values’: use the actual numerical values of the index. ‘pad’: Fill in NaNs using existing values. ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘barycentric’, ‘polynomial’: Passed to scipy.interpolate.interp1d, whereas ‘spline’ is passed to scipy.interpolate.UnivariateSpline. These methods use the numerical values of the index.  Both ‘polynomial’ and ‘spline’ require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5). Note that, slinear method in Pandas refers to the Scipy first order spline instead of Pandas first order spline. ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’, ‘akima’, ‘cubicspline’: Wrappers around the SciPy interpolation methods of similar names. See Notes. ‘from_derivatives’: Refers to scipy.interpolate.BPoly.from_derivatives. axis{{0 or ‘index’, 1 or ‘columns’, None}}, default NoneAxis to interpolate along. For Series this parameter is unused and defaults to 0. limitint, optionalMaximum number of consecutive NaNs to fill. Must be greater than 0. inplacebool, default FalseUpdate the data in place if possible. limit_direction{{‘forward’, ‘backward’, ‘both’}}, OptionalConsecutive NaNs will be filled in this direction. If limit is specified: If ‘method’ is ‘pad’ or ‘ffill’, ‘limit_direction’ must be ‘forward’. If ‘method’ is ‘backfill’ or ‘bfill’, ‘limit_direction’ must be ‘backwards’. If ‘limit’ is not specified: If ‘method’ is ‘backfill’ or ‘bfill’, the default is ‘backward’ else the default is ‘forward’ raises ValueError if limit_direction is ‘forward’ or ‘both’ andmethod is ‘backfill’ or ‘bfill’. raises ValueError if limit_direction is ‘backward’ or ‘both’ andmethod is ‘pad’ or ‘ffill’. limit_area{{None, ‘inside’, ‘outside’}}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). downcastoptional, ‘infer’ or None, defaults to NoneDowncast dtypes if possible. Deprecated since version 2.1.0. ``**kwargs``optionalKeyword arguments to pass on to the interpolating function. Returns: DataFrame or SeriesInterpolated values at the specified freq.","['>>> start = ""2023-03-01T07:00:00""\n>>> timesteps = pd.date_range(start, periods=5, freq=""s"")\n>>> series = pd.Series(data=[1, -1, 2, 1, 3], index=timesteps)\n>>> series\n2023-03-01 07:00:00    1\n2023-03-01 07:00:01   -1\n2023-03-01 07:00:02    2\n2023-03-01 07:00:03    1\n2023-03-01 07:00:04    3\nFreq: s, dtype: int64', '>>> series.resample(""2s"").interpolate(""linear"")\n2023-03-01 07:00:00    1\n2023-03-01 07:00:02    2\n2023-03-01 07:00:04    3\nFreq: 2s, dtype: int64', '>>> series.resample(""500ms"").interpolate(""linear"")\n2023-03-01 07:00:00.000    1.0\n2023-03-01 07:00:00.500    0.0\n2023-03-01 07:00:01.000   -1.0\n2023-03-01 07:00:01.500    0.5\n2023-03-01 07:00:02.000    2.0\n2023-03-01 07:00:02.500    1.5\n2023-03-01 07:00:03.000    1.0\n2023-03-01 07:00:03.500    2.0\n2023-03-01 07:00:04.000    3.0\nFreq: 500ms, dtype: float64', '>>> series.resample(""400ms"").interpolate(""linear"")\n2023-03-01 07:00:00.000    1.0\n2023-03-01 07:00:00.400    1.2\n2023-03-01 07:00:00.800    1.4\n2023-03-01 07:00:01.200    1.6\n2023-03-01 07:00:01.600    1.8\n2023-03-01 07:00:02.000    2.0\n2023-03-01 07:00:02.400    2.2\n2023-03-01 07:00:02.800    2.4\n2023-03-01 07:00:03.200    2.6\n2023-03-01 07:00:03.600    2.8\n2023-03-01 07:00:04.000    3.0\nFreq: 400ms, dtype: float64']"
314,..\pandas\reference\api\pandas.IntervalIndex.length.html,pandas.IntervalIndex.length,property IntervalIndex.length[source]#,No parameters found,[]
315,..\pandas\reference\api\pandas.api.indexers.VariableOffsetWindowIndexer.html,pandas.api.indexers.VariableOffsetWindowIndexer,"class pandas.api.indexers.VariableOffsetWindowIndexer(index_array=None, window_size=0, index=None, offset=None, **kwargs)[source]# Calculate window boundaries based on a non-fixed offset such as a BusinessDay. Examples",No parameters found,"['>>> from pandas.api.indexers import VariableOffsetWindowIndexer\n>>> df = pd.DataFrame(range(10), index=pd.date_range(""2020"", periods=10))\n>>> offset = pd.offsets.BDay(1)\n>>> indexer = VariableOffsetWindowIndexer(index=df.index, offset=offset)\n>>> df\n            0\n2020-01-01  0\n2020-01-02  1\n2020-01-03  2\n2020-01-04  3\n2020-01-05  4\n2020-01-06  5\n2020-01-07  6\n2020-01-08  7\n2020-01-09  8\n2020-01-10  9\n>>> df.rolling(indexer).sum()\n               0\n2020-01-01   0.0\n2020-01-02   1.0\n2020-01-03   2.0\n2020-01-04   3.0\n2020-01-05   7.0\n2020-01-06  12.0\n2020-01-07   6.0\n2020-01-08   7.0\n2020-01-09   8.0\n2020-01-10   9.0']"
316,..\pandas\reference\api\pandas.DataFrame.to_feather.html,pandas.DataFrame.to_feather,"DataFrame.to_feather(path, **kwargs)[source]# Write a DataFrame to the binary Feather format. Notes This function writes the dataframe as a feather file. Requires a default index. For saving the DataFrame with your custom index use a method that supports custom indices e.g. to_parquet.","Parameters: pathstr, path object, file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. If a string or a path, it will be used as Root Directory path when writing a partitioned dataset. **kwargsAdditional keywords passed to pyarrow.feather.write_feather(). This includes the compression, compression_level, chunksize and version keywords.","['>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]])\n>>> df.to_feather(""file.feather"")']"
317,..\pandas\reference\api\pandas.Series.dt.html,pandas.Series.dt,Series.dt()[source]# Accessor object for datetimelike properties of the Series values.,No parameters found,"['>>> seconds_series = pd.Series(pd.date_range(""2000-01-01"", periods=3, freq=""s""))\n>>> seconds_series\n0   2000-01-01 00:00:00\n1   2000-01-01 00:00:01\n2   2000-01-01 00:00:02\ndtype: datetime64[ns]\n>>> seconds_series.dt.second\n0    0\n1    1\n2    2\ndtype: int32', '>>> hours_series = pd.Series(pd.date_range(""2000-01-01"", periods=3, freq=""h""))\n>>> hours_series\n0   2000-01-01 00:00:00\n1   2000-01-01 01:00:00\n2   2000-01-01 02:00:00\ndtype: datetime64[ns]\n>>> hours_series.dt.hour\n0    0\n1    1\n2    2\ndtype: int32', '>>> quarters_series = pd.Series(pd.date_range(""2000-01-01"", periods=3, freq=""QE""))\n>>> quarters_series\n0   2000-03-31\n1   2000-06-30\n2   2000-09-30\ndtype: datetime64[ns]\n>>> quarters_series.dt.quarter\n0    1\n1    2\n2    3\ndtype: int32']"
318,..\pandas\reference\api\pandas.tseries.offsets.Minute.normalize.html,pandas.tseries.offsets.Minute.normalize,Minute.normalize#,No parameters found,[]
319,..\pandas\reference\api\pandas.core.resample.Resampler.last.html,pandas.core.resample.Resampler.last,"final Resampler.last(numeric_only=False, min_count=0, skipna=True, *args, **kwargs)[source]# Compute the last entry of each column within each group. Defaults to skipping NA elements.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count valid values are present the result will be NA. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Added in version 2.2.1. Returns: Series or DataFrameLast of values within each group.","['>>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))\n>>> df.groupby(""A"").last()\n     B  C\nA\n1  5.0  2\n3  6.0  3']"
320,..\pandas\reference\api\pandas.DataFrame.to_gbq.html,pandas.DataFrame.to_gbq,"DataFrame.to_gbq(destination_table, *, project_id=None, chunksize=None, reauth=False, if_exists='fail', auth_local_webserver=True, table_schema=None, location=None, progress_bar=True, credentials=None)[source]# Write a DataFrame to a Google BigQuery table. Deprecated since version 2.2.0: Please use pandas_gbq.to_gbq instead. This function requires the pandas-gbq package. See the How to authenticate with Google BigQuery guide for authentication instructions.","Parameters: destination_tablestrName of table to be written, in the form dataset.tablename. project_idstr, optionalGoogle BigQuery Account project ID. Optional when available from the environment. chunksizeint, optionalNumber of rows to be inserted in each chunk from the dataframe. Set to None to load the whole dataframe at once. reauthbool, default FalseForce Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used. if_existsstr, default ‘fail’Behavior when the destination table exists. Value can be one of: 'fail'If table exists raise pandas_gbq.gbq.TableCreationError. 'replace'If table exists, drop it, recreate it, and insert data. 'append'If table exists, insert data. Create if does not exist. auth_local_webserverbool, default TrueUse the local webserver flow instead of the console flow when getting user credentials. New in version 0.2.0 of pandas-gbq. Changed in version 1.5.0: Default value is changed to True. Google has deprecated the auth_local_webserver = False “out of band” (copy-paste) flow. table_schemalist of dicts, optionalList of BigQuery table fields to which according DataFrame columns conform to, e.g. [{'name': 'col1', 'type': 'STRING'},...]. If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. New in version 0.3.1 of pandas-gbq. locationstr, optionalLocation where the load job should run. See the BigQuery locations documentation for a list of available locations. The location must match that of the target dataset. New in version 0.5.0 of pandas-gbq. progress_barbool, default TrueUse the library tqdm to show the progress bar for the upload, chunk by chunk. New in version 0.5.0 of pandas-gbq. credentialsgoogle.auth.credentials.Credentials, optionalCredentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine google.auth.compute_engine.Credentials or Service Account google.oauth2.service_account.Credentials directly. New in version 0.8.0 of pandas-gbq.","['>>> project_id = ""my-project""\n>>> table_id = \'my_dataset.my_table\'\n>>> df = pd.DataFrame({\n...                   ""my_string"": [""a"", ""b"", ""c""],\n...                   ""my_int64"": [1, 2, 3],\n...                   ""my_float64"": [4.0, 5.0, 6.0],\n...                   ""my_bool1"": [True, False, True],\n...                   ""my_bool2"": [False, True, False],\n...                   ""my_dates"": pd.date_range(""now"", periods=3),\n...                   }\n...                   )', '>>> df.to_gbq(table_id, project_id=project_id)']"
321,..\pandas\reference\api\pandas.api.interchange.from_dataframe.html,pandas.api.interchange.from_dataframe,"pandas.api.interchange.from_dataframe(df, allow_copy=True)[source]# Build a pd.DataFrame from any DataFrame supporting the interchange protocol.","Parameters: dfDataFrameXchgObject supporting the interchange protocol, i.e. __dataframe__ method. allow_copybool, default: TrueWhether to allow copying the memory to perform the conversion (if false then zero-copy approach is requested). Returns: pd.DataFrame","["">>> df_not_necessarily_pandas = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n>>> interchange_object = df_not_necessarily_pandas.__dataframe__()\n>>> interchange_object.column_names()\nIndex(['A', 'B'], dtype='object')\n>>> df_pandas = (pd.api.interchange.from_dataframe\n...              (interchange_object.select_columns_by_name(['A'])))\n>>> df_pandas\n     A\n0    1\n1    2""]"
322,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.calendar.html,pandas.tseries.offsets.CustomBusinessDay.calendar,CustomBusinessDay.calendar#,No parameters found,[]
323,..\pandas\reference\api\pandas.Series.dt.isocalendar.html,pandas.Series.dt.isocalendar,"Series.dt.isocalendar()[source]# Calculate year, week, and day according to the ISO 8601 standard.","Returns: DataFrameWith columns year, week and day.","['>>> ser = pd.to_datetime(pd.Series([""2010-01-01"", pd.NaT]))\n>>> ser.dt.isocalendar()\n   year  week  day\n0  2009    53     5\n1  <NA>  <NA>  <NA>\n>>> ser.dt.isocalendar().week\n0      53\n1    <NA>\nName: week, dtype: UInt32']"
324,..\pandas\reference\api\pandas.IntervalIndex.mid.html,pandas.IntervalIndex.mid,IntervalIndex.mid[source]#,No parameters found,[]
325,..\pandas\reference\api\pandas.testing.assert_frame_equal.html,pandas.testing.assert_frame_equal,"pandas.testing.assert_frame_equal(left, right, check_dtype=True, check_index_type='equiv', check_column_type='equiv', check_frame_type=True, check_names=True, by_blocks=False, check_exact=<no_default>, check_datetimelike_compat=False, check_categorical=True, check_like=False, check_freq=True, check_flags=True, rtol=<no_default>, atol=<no_default>, obj='DataFrame')[source]# Check that left and right DataFrame are equal. This function is intended to compare two DataFrames and output any differences. It is mostly intended for use in unit tests. Additional parameters allow varying the strictness of the equality checks performed.","Parameters: leftDataFrameFirst DataFrame to compare. rightDataFrameSecond DataFrame to compare. check_dtypebool, default TrueWhether to check the DataFrame dtype is identical. check_index_typebool or {‘equiv’}, default ‘equiv’Whether to check the Index class, dtype and inferred_type are identical. check_column_typebool or {‘equiv’}, default ‘equiv’Whether to check the columns class, dtype and inferred_type are identical. Is passed as the exact argument of assert_index_equal(). check_frame_typebool, default TrueWhether to check the DataFrame class is identical. check_namesbool, default TrueWhether to check that the names attribute for both the index and column attributes of the DataFrame is identical. by_blocksbool, default FalseSpecify how to compare internal data. If False, compare by columns. If True, compare by blocks. check_exactbool, default FalseWhether to compare number exactly. Changed in version 2.2.0: Defaults to True for integer dtypes if none of check_exact, rtol and atol are specified. check_datetimelike_compatbool, default FalseCompare datetime-like which is comparable ignoring dtype. check_categoricalbool, default TrueWhether to compare internal Categorical exactly. check_likebool, default FalseIf True, ignore the order of index & columns. Note: index labels must match their respective rows (same as in columns) - same labels must be with the same data. check_freqbool, default TrueWhether to check the freq attribute on a DatetimeIndex or TimedeltaIndex. check_flagsbool, default TrueWhether to check the flags attribute. rtolfloat, default 1e-5Relative tolerance. Only used when check_exact is False. atolfloat, default 1e-8Absolute tolerance. Only used when check_exact is False. objstr, default ‘DataFrame’Specify object name being compared, internally used to show appropriate assertion message.","["">>> from pandas.testing import assert_frame_equal\n>>> df1 = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n>>> df2 = pd.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})"", '>>> assert_frame_equal(df1, df1)', '>>> assert_frame_equal(df1, df2)\nTraceback (most recent call last):\n...\nAssertionError: Attributes of DataFrame.iloc[:, 1] (column name=""b"") are different', '>>> assert_frame_equal(df1, df2, check_dtype=False)']"
326,..\pandas\reference\api\pandas.tseries.offsets.Minute.rule_code.html,pandas.tseries.offsets.Minute.rule_code,Minute.rule_code#,No parameters found,[]
327,..\pandas\reference\api\pandas.core.resample.Resampler.max.html,pandas.core.resample.Resampler.max,"final Resampler.max(numeric_only=False, min_count=0, *args, **kwargs)[source]# Compute max value of group.",Returns: Series or DataFrame,"["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').max()\n2023-01-01    2\n2023-02-01    4\nFreq: MS, dtype: int64""]"
328,..\pandas\reference\api\pandas.core.resample.Resampler.mean.html,pandas.core.resample.Resampler.mean,"final Resampler.mean(numeric_only=False, *args, **kwargs)[source]# Compute mean of groups, excluding missing values.","Parameters: numeric_onlybool, default FalseInclude only float, int or boolean data. Changed in version 2.0.0: numeric_only now defaults to False. Returns: DataFrame or SeriesMean of values within each group.","["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').mean()\n2023-01-01    1.5\n2023-02-01    3.5\nFreq: MS, dtype: float64""]"
329,..\pandas\reference\api\pandas.DataFrame.to_hdf.html,pandas.DataFrame.to_hdf,"DataFrame.to_hdf(path_or_buf, *, key, mode='a', complevel=None, complib=None, append=False, format=None, index=True, min_itemsize=None, nan_rep=None, dropna=None, data_columns=None, errors='strict', encoding='UTF-8')[source]# Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. Warning One can store a subclass of DataFrame or Series to HDF5, but the type of the subclass is lost upon storing. For more information see the user guide.","Parameters: path_or_bufstr or pandas.HDFStoreFile path or HDFStore object. keystrIdentifier for the group in the store. mode{‘a’, ‘w’, ‘r+’}, default ‘a’Mode to open file: ‘w’: write, a new file is created (an existing file with the same name would be deleted). ‘a’: append, an existing file is opened for reading and writing, and if the file does not exist it is created. ‘r+’: similar to ‘a’, but the file must already exist. complevel{0-9}, default NoneSpecifies a compression level for data. A value of 0 or None disables compression. complib{‘zlib’, ‘lzo’, ‘bzip2’, ‘blosc’}, default ‘zlib’Specifies the compression library to be used. These additional compressors for Blosc are supported (default if no compressor specified: ‘blosc:blosclz’): {‘blosc:blosclz’, ‘blosc:lz4’, ‘blosc:lz4hc’, ‘blosc:snappy’, ‘blosc:zlib’, ‘blosc:zstd’}. Specifying a compression library which is not available issues a ValueError. appendbool, default FalseFor Table formats, append the input data to the existing. format{‘fixed’, ‘table’, None}, default ‘fixed’Possible values: ‘fixed’: Fixed format. Fast writing/reading. Not-appendable, nor searchable. ‘table’: Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. If None, pd.get_option(‘io.hdf.default_format’) is checked, followed by fallback to “fixed”. indexbool, default TrueWrite DataFrame index as a column. min_itemsizedict or int, optionalMap column names to minimum string sizes for columns. nan_repAny, optionalHow to represent null values as str. Not allowed with append=True. dropnabool, default False, optionalRemove missing values. data_columnslist of columns or True, optionalList of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See Query via data columns. for more information. Applicable only to format=’table’. errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options. encodingstr, default “UTF-8”","["">>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]},\n...                   index=['a', 'b', 'c'])  \n>>> df.to_hdf('data.h5', key='df', mode='w')"", "">>> s = pd.Series([1, 2, 3, 4])  \n>>> s.to_hdf('data.h5', key='s')"", "">>> pd.read_hdf('data.h5', 'df')  \nA  B\na  1  4\nb  2  5\nc  3  6\n>>> pd.read_hdf('data.h5', 's')  \n0    1\n1    2\n2    3\n3    4\ndtype: int64""]"
330,..\pandas\reference\api\pandas.IntervalIndex.overlaps.html,pandas.IntervalIndex.overlaps,"IntervalIndex.overlaps(*args, **kwargs)[source]# Check elementwise if an Interval overlaps the values in the IntervalArray. Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.",Parameters: otherIntervalArrayInterval to check against for an overlap. Returns: ndarrayBoolean array positionally indicating where an overlap occurs.,"['>>> data = [(0, 1), (1, 3), (2, 4)]\n>>> intervals = pd.arrays.IntervalArray.from_tuples(data)\n>>> intervals\n<IntervalArray>\n[(0, 1], (1, 3], (2, 4]]\nLength: 3, dtype: interval[int64, right]', '>>> intervals.overlaps(pd.Interval(0.5, 1.5))\narray([ True,  True, False])', "">>> intervals.overlaps(pd.Interval(1, 3, closed='left'))\narray([ True,  True, True])"", "">>> intervals.overlaps(pd.Interval(1, 2, closed='right'))\narray([False,  True, False])""]"
331,..\pandas\reference\api\pandas.Series.dt.is_leap_year.html,pandas.Series.dt.is_leap_year,"Series.dt.is_leap_year[source]# Boolean indicator if the date belongs to a leap year. A leap year is a year, which has 366 days (instead of 365) including 29th of February as an intercalary day. Leap years are years which are multiples of four with the exception of years divisible by 100 but not by 400.",Returns: Series or ndarrayBooleans indicating if dates belong to a leap year.,"['>>> idx = pd.date_range(""2012-01-01"", ""2015-01-01"", freq=""YE"")\n>>> idx\nDatetimeIndex([\'2012-12-31\', \'2013-12-31\', \'2014-12-31\'],\n              dtype=\'datetime64[ns]\', freq=\'YE-DEC\')\n>>> idx.is_leap_year\narray([ True, False, False])', '>>> dates_series = pd.Series(idx)\n>>> dates_series\n0   2012-12-31\n1   2013-12-31\n2   2014-12-31\ndtype: datetime64[ns]\n>>> dates_series.dt.is_leap_year\n0     True\n1    False\n2    False\ndtype: bool']"
332,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.copy.html,pandas.tseries.offsets.MonthBegin.copy,MonthBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
333,..\pandas\reference\api\pandas.testing.assert_index_equal.html,pandas.testing.assert_index_equal,"pandas.testing.assert_index_equal(left, right, exact='equiv', check_names=True, check_exact=True, check_categorical=True, check_order=True, rtol=1e-05, atol=1e-08, obj='Index')[source]# Check that left and right Index are equal.","Parameters: leftIndex rightIndex exactbool or {‘equiv’}, default ‘equiv’Whether to check the Index class, dtype and inferred_type are identical. If ‘equiv’, then RangeIndex can be substituted for Index with an int64 dtype as well. check_namesbool, default TrueWhether to check the names attribute. check_exactbool, default TrueWhether to compare number exactly. check_categoricalbool, default TrueWhether to compare internal Categorical exactly. check_orderbool, default TrueWhether to compare the order of index entries as well as their values. If True, both indexes must contain the same elements, in the same order. If False, both indexes must contain the same elements, but in any order. rtolfloat, default 1e-5Relative tolerance. Only used when check_exact is False. atolfloat, default 1e-8Absolute tolerance. Only used when check_exact is False. objstr, default ‘Index’Specify object name being compared, internally used to show appropriate assertion message.","['>>> from pandas import testing as tm\n>>> a = pd.Index([1, 2, 3])\n>>> b = pd.Index([1, 2, 3])\n>>> tm.assert_index_equal(a, b)']"
334,..\pandas\reference\api\pandas.api.types.infer_dtype.html,pandas.api.types.infer_dtype,"pandas.api.types.infer_dtype(value, skipna=True)# Return a string label of the type of a scalar or list-like of values. Notes ‘mixed’ is the catchall for anything that is not otherwise specialized ‘mixed-integer-float’ are floats and integers ‘mixed-integer’ are integers mixed with non-integers ‘unknown-array’ is the catchall for something that is an array (has a dtype attribute), but has a dtype unknown to pandas (e.g. external extension array)","Parameters: valuescalar, list, ndarray, or pandas type skipnabool, default TrueIgnore NaN values when inferring the type. Returns: strDescribing the common type of the input data. Results can include: string bytes floating integer mixed-integer mixed-integer-float decimal complex categorical boolean datetime64 datetime date timedelta64 timedelta time period mixed unknown-array Raises: TypeErrorIf ndarray-like but cannot infer the dtype","["">>> from pandas.api.types import infer_dtype\n>>> infer_dtype(['foo', 'bar'])\n'string'"", "">>> infer_dtype(['a', np.nan, 'b'], skipna=True)\n'string'"", "">>> infer_dtype(['a', np.nan, 'b'], skipna=False)\n'mixed'"", "">>> infer_dtype([b'foo', b'bar'])\n'bytes'"", "">>> infer_dtype([1, 2, 3])\n'integer'"", "">>> infer_dtype([1, 2, 3.5])\n'mixed-integer-float'"", "">>> infer_dtype([1.0, 2.0, 3.5])\n'floating'"", "">>> infer_dtype(['a', 1])\n'mixed-integer'"", "">>> from decimal import Decimal\n>>> infer_dtype([Decimal(1), Decimal(2.0)])\n'decimal'"", "">>> infer_dtype([True, False])\n'boolean'"", "">>> infer_dtype([True, False, np.nan])\n'boolean'"", "">>> infer_dtype([pd.Timestamp('20130101')])\n'datetime'"", "">>> import datetime\n>>> infer_dtype([datetime.date(2013, 1, 1)])\n'date'"", "">>> infer_dtype([np.datetime64('2013-01-01')])\n'datetime64'"", "">>> infer_dtype([datetime.timedelta(0, 1, 1)])\n'timedelta'"", "">>> infer_dtype(pd.Series(list('aabc')).astype('category'))\n'categorical'""]"
335,..\pandas\reference\api\pandas.core.resample.Resampler.median.html,pandas.core.resample.Resampler.median,"final Resampler.median(numeric_only=False, *args, **kwargs)[source]# Compute median of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None and defaults to False. Returns: Series or DataFrameMedian of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).median()\na    7.0\nb    3.0\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).median()\n         a    b\ndog    3.0  4.0\nmouse  7.0  3.0"", "">>> ser = pd.Series([1, 2, 3, 3, 4, 5],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').median()\n2023-01-01    2.0\n2023-02-01    4.0\nFreq: MS, dtype: float64""]"
336,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.copy.html,pandas.tseries.offsets.CustomBusinessDay.copy,CustomBusinessDay.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
337,..\pandas\reference\api\pandas.DataFrame.to_html.html,pandas.DataFrame.to_html,"DataFrame.to_html(buf=None, *, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', bold_rows=True, classes=None, escape=True, notebook=False, border=None, table_id=None, render_links=False, encoding=None)[source]# Render a DataFrame as an HTML table.","Parameters: bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string. columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default. col_spacestr or int, list or dict of int or str, optionalThe minimum width of each column in CSS length units.  An int is assumed to be px units.. headerbool, optionalWhether to print column labels, default True. indexbool, optional, default TrueWhether to print index (row) labels. na_repstr, optional, default ‘NaN’String representation of NaN to use. formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columns’ elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_formatone-parameter function, optional, default NoneFormatter function to apply to columns’ elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep. sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row. index_namesbool, optional, default TruePrints the names of the indexes. justifystr, default NoneHow to justify the column labels. If None uses the option from the print configuration (controlled by set_option), ‘right’ out of the box. Valid values are left right center justify justify-all start end inherit match-parent initial unset. max_rowsint, optionalMaximum number of rows to display in the console. max_colsint, optionalMaximum number of columns to display in the console. show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns). decimalstr, default ‘.’Character recognized as decimal separator, e.g. ‘,’ in Europe. bold_rowsbool, default TrueMake the row labels bold in the output. classesstr or list or tuple, default NoneCSS class(es) to apply to the resulting html table. escapebool, default TrueConvert the characters <, >, and & to HTML-safe sequences. notebook{True, False}, default FalseWhether the generated HTML is for IPython Notebook. borderintA border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border. table_idstr, optionalA css id is included in the opening <table> tag if specified. render_linksbool, default FalseConvert URLs to HTML links. encodingstr, default “utf-8”Set character encoding. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","['>>> df = pd.DataFrame(data={\'col1\': [1, 2], \'col2\': [4, 3]})\n>>> html_string = \'\'\'<table border=""1"" class=""dataframe"">\n...   <thead>\n...     <tr style=""text-align: right;"">\n...       <th></th>\n...       <th>col1</th>\n...       <th>col2</th>\n...     </tr>\n...   </thead>\n...   <tbody>\n...     <tr>\n...       <th>0</th>\n...       <td>1</td>\n...       <td>4</td>\n...     </tr>\n...     <tr>\n...       <th>1</th>\n...       <td>2</td>\n...       <td>3</td>\n...     </tr>\n...   </tbody>\n... </table>\'\'\'\n>>> assert html_string == df.to_html()']"
338,..\pandas\reference\api\pandas.IntervalIndex.right.html,pandas.IntervalIndex.right,IntervalIndex.right[source]#,No parameters found,[]
339,..\pandas\reference\api\pandas.Series.dt.is_month_end.html,pandas.Series.dt.is_month_end,Series.dt.is_month_end[source]# Indicates whether the date is the last day of the month.,"Returns: Series or arrayFor Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.","['>>> s = pd.Series(pd.date_range(""2018-02-27"", periods=3))\n>>> s\n0   2018-02-27\n1   2018-02-28\n2   2018-03-01\ndtype: datetime64[ns]\n>>> s.dt.is_month_start\n0    False\n1    False\n2    True\ndtype: bool\n>>> s.dt.is_month_end\n0    False\n1    True\n2    False\ndtype: bool', '>>> idx = pd.date_range(""2018-02-27"", periods=3)\n>>> idx.is_month_start\narray([False, False, True])\n>>> idx.is_month_end\narray([False, True, False])']"
340,..\pandas\reference\api\pandas.testing.assert_series_equal.html,pandas.testing.assert_series_equal,"pandas.testing.assert_series_equal(left, right, check_dtype=True, check_index_type='equiv', check_series_type=True, check_names=True, check_exact=<no_default>, check_datetimelike_compat=False, check_categorical=True, check_category_order=True, check_freq=True, check_flags=True, rtol=<no_default>, atol=<no_default>, obj='Series', *, check_index=True, check_like=False)[source]# Check that left and right Series are equal.","Parameters: leftSeries rightSeries check_dtypebool, default TrueWhether to check the Series dtype is identical. check_index_typebool or {‘equiv’}, default ‘equiv’Whether to check the Index class, dtype and inferred_type are identical. check_series_typebool, default TrueWhether to check the Series class is identical. check_namesbool, default TrueWhether to check the Series and Index names attribute. check_exactbool, default FalseWhether to compare number exactly. Changed in version 2.2.0: Defaults to True for integer dtypes if none of check_exact, rtol and atol are specified. check_datetimelike_compatbool, default FalseCompare datetime-like which is comparable ignoring dtype. check_categoricalbool, default TrueWhether to compare internal Categorical exactly. check_category_orderbool, default TrueWhether to compare category order of internal Categoricals. check_freqbool, default TrueWhether to check the freq attribute on a DatetimeIndex or TimedeltaIndex. check_flagsbool, default TrueWhether to check the flags attribute. rtolfloat, default 1e-5Relative tolerance. Only used when check_exact is False. atolfloat, default 1e-8Absolute tolerance. Only used when check_exact is False. objstr, default ‘Series’Specify object name being compared, internally used to show appropriate assertion message. check_indexbool, default TrueWhether to check index equivalence. If False, then compare only values. Added in version 1.3.0. check_likebool, default FalseIf True, ignore the order of the index. Must be False if check_index is False. Note: same labels must be with the same data. Added in version 1.5.0.","['>>> from pandas import testing as tm\n>>> a = pd.Series([1, 2, 3, 4])\n>>> b = pd.Series([1, 2, 3, 4])\n>>> tm.assert_series_equal(a, b)']"
341,..\pandas\reference\api\pandas.DataFrame.to_json.html,pandas.DataFrame.to_json,"DataFrame.to_json(path_or_buf=None, *, orient=None, date_format=None, double_precision=10, force_ascii=True, date_unit='ms', default_handler=None, lines=False, compression='infer', index=None, indent=None, storage_options=None, mode='w')[source]# Convert the object to a JSON string. Note NaN’s and None will be converted to null and datetime objects will be converted to UNIX timestamps. Notes The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release. orient='table' contains a ‘pandas_version’ field under ‘schema’. This stores the version of pandas used in the latest revision of the schema.","Parameters: path_or_bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. orientstrIndication of expected JSON string format. Series: default is ‘index’ allowed values are: {‘split’, ‘records’, ‘index’, ‘table’}. DataFrame: default is ‘columns’ allowed values are: {‘split’, ‘records’, ‘index’, ‘columns’, ‘values’, ‘table’}. The format of the JSON string: ‘split’ : dict like {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]} ‘records’ : list like [{column -> value}, … , {column -> value}] ‘index’ : dict like {index -> {column -> value}} ‘columns’ : dict like {column -> {index -> value}} ‘values’ : just the values array ‘table’ : dict like {‘schema’: {schema}, ‘data’: {data}} Describing the data, where data component is like orient='records'. date_format{None, ‘epoch’, ‘iso’}Type of date conversion. ‘epoch’ = epoch milliseconds, ‘iso’ = ISO8601. The default depends on the orient. For orient='table', the default is ‘iso’. For all other orients, the default is ‘epoch’. double_precisionint, default 10The number of decimal places to use when encoding floating point values. The possible maximal value is 15. Passing double_precision greater than 15 will raise a ValueError. force_asciibool, default TrueForce encoded string to be ASCII. date_unitstr, default ‘ms’ (milliseconds)The time unit to encode to, governs timestamp and ISO8601 precision.  One of ‘s’, ‘ms’, ‘us’, ‘ns’ for second, millisecond, microsecond, and nanosecond respectively. default_handlercallable, default NoneHandler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. linesbool, default FalseIf ‘orient’ is ‘records’ write out line-delimited json format. Will throw ValueError if incorrect ‘orient’ since others are not list-like. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buf’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. indexbool or None, default NoneThe index is only used when ‘orient’ is ‘split’, ‘index’, ‘column’, or ‘table’. Of these, ‘index’ and ‘column’ do not support index=False. indentint, optionalLength of whitespace used to indent each record. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. modestr, default ‘w’ (writing)Specify the IO mode for output when supplying a path_or_buf. Accepted args are ‘w’ (writing) and ‘a’ (append) only. mode=’a’ is only supported when lines is True and orient is ‘records’. Returns: None or strIf path_or_buf is None, returns the resulting json format as a string. Otherwise returns None.","['>>> from json import loads, dumps\n>>> df = pd.DataFrame(\n...     [[""a"", ""b""], [""c"", ""d""]],\n...     index=[""row 1"", ""row 2""],\n...     columns=[""col 1"", ""col 2""],\n... )', '>>> result = df.to_json(orient=""split"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""columns"": [\n        ""col 1"",\n        ""col 2""\n    ],\n    ""index"": [\n        ""row 1"",\n        ""row 2""\n    ],\n    ""data"": [\n        [\n            ""a"",\n            ""b""\n        ],\n        [\n            ""c"",\n            ""d""\n        ]\n    ]\n}', '>>> result = df.to_json(orient=""records"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n[\n    {\n        ""col 1"": ""a"",\n        ""col 2"": ""b""\n    },\n    {\n        ""col 1"": ""c"",\n        ""col 2"": ""d""\n    }\n]', '>>> result = df.to_json(orient=""index"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""row 1"": {\n        ""col 1"": ""a"",\n        ""col 2"": ""b""\n    },\n    ""row 2"": {\n        ""col 1"": ""c"",\n        ""col 2"": ""d""\n    }\n}', '>>> result = df.to_json(orient=""columns"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""col 1"": {\n        ""row 1"": ""a"",\n        ""row 2"": ""c""\n    },\n    ""col 2"": {\n        ""row 1"": ""b"",\n        ""row 2"": ""d""\n    }\n}', '>>> result = df.to_json(orient=""values"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n[\n    [\n        ""a"",\n        ""b""\n    ],\n    [\n        ""c"",\n        ""d""\n    ]\n]', '>>> result = df.to_json(orient=""table"")\n>>> parsed = loads(result)\n>>> dumps(parsed, indent=4)  \n{\n    ""schema"": {\n        ""fields"": [\n            {\n                ""name"": ""index"",\n                ""type"": ""string""\n            },\n            {\n                ""name"": ""col 1"",\n                ""type"": ""string""\n            },\n            {\n                ""name"": ""col 2"",\n                ""type"": ""string""\n            }\n        ],\n        ""primaryKey"": [\n            ""index""\n        ],\n        ""pandas_version"": ""1.4.0""\n    },\n    ""data"": [\n        {\n            ""index"": ""row 1"",\n            ""col 1"": ""a"",\n            ""col 2"": ""b""\n        },\n        {\n            ""index"": ""row 2"",\n            ""col 1"": ""c"",\n            ""col 2"": ""d""\n        }\n    ]\n}']"
342,..\pandas\reference\api\pandas.api.types.is_any_real_numeric_dtype.html,pandas.api.types.is_any_real_numeric_dtype,pandas.api.types.is_any_real_numeric_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of a real number dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of a real number dtype.,"['>>> from pandas.api.types import is_any_real_numeric_dtype\n>>> is_any_real_numeric_dtype(int)\nTrue\n>>> is_any_real_numeric_dtype(float)\nTrue\n>>> is_any_real_numeric_dtype(object)\nFalse\n>>> is_any_real_numeric_dtype(str)\nFalse\n>>> is_any_real_numeric_dtype(complex(1, 2))\nFalse\n>>> is_any_real_numeric_dtype(bool)\nFalse']"
343,..\pandas\reference\api\pandas.IntervalIndex.set_closed.html,pandas.IntervalIndex.set_closed,"IntervalIndex.set_closed(*args, **kwargs)[source]# Return an identical IntervalArray closed on the specified side.","Parameters: closed{‘left’, ‘right’, ‘both’, ‘neither’}Whether the intervals are closed on the left-side, right-side, both or neither. Returns: IntervalArray","["">>> index = pd.arrays.IntervalArray.from_breaks(range(4))\n>>> index\n<IntervalArray>\n[(0, 1], (1, 2], (2, 3]]\nLength: 3, dtype: interval[int64, right]\n>>> index.set_closed('both')\n<IntervalArray>\n[[0, 1], [1, 2], [2, 3]]\nLength: 3, dtype: interval[int64, both]""]"
344,..\pandas\reference\api\pandas.core.resample.Resampler.min.html,pandas.core.resample.Resampler.min,"final Resampler.min(numeric_only=False, min_count=0, *args, **kwargs)[source]# Compute min value of group.",Returns: Series or DataFrame,"["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').min()\n2023-01-01    1\n2023-02-01    3\nFreq: MS, dtype: int64""]"
345,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.freqstr.html,pandas.tseries.offsets.MonthBegin.freqstr,MonthBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
346,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.freqstr.html,pandas.tseries.offsets.CustomBusinessDay.freqstr,CustomBusinessDay.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
347,..\pandas\reference\api\pandas.Series.dt.is_month_start.html,pandas.Series.dt.is_month_start,Series.dt.is_month_start[source]# Indicates whether the date is the first day of the month.,"Returns: Series or arrayFor Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.","['>>> s = pd.Series(pd.date_range(""2018-02-27"", periods=3))\n>>> s\n0   2018-02-27\n1   2018-02-28\n2   2018-03-01\ndtype: datetime64[ns]\n>>> s.dt.is_month_start\n0    False\n1    False\n2    True\ndtype: bool\n>>> s.dt.is_month_end\n0    False\n1    True\n2    False\ndtype: bool', '>>> idx = pd.date_range(""2018-02-27"", periods=3)\n>>> idx.is_month_start\narray([False, False, True])\n>>> idx.is_month_end\narray([False, True, False])']"
348,..\pandas\reference\api\pandas.api.types.is_bool.html,pandas.api.types.is_bool,pandas.api.types.is_bool(obj)# Return True if given object is boolean.,Returns: bool,"['>>> pd.api.types.is_bool(True)\nTrue', '>>> pd.api.types.is_bool(1)\nFalse']"
349,..\pandas\reference\api\pandas.DataFrame.to_latex.html,pandas.DataFrame.to_latex,"DataFrame.to_latex(buf=None, *, columns=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None, decimal='.', multicolumn=None, multicolumn_format=None, multirow=None, caption=None, label=None, position=None)[source]# Render object to a LaTeX tabular, longtable, or nested table. Requires \usepackage{{booktabs}}.  The output can be copy/pasted into a main LaTeX document or read from an external file with \input{{table.tex}}. Changed in version 2.0.0: Refactored to use the Styler implementation via jinja2 templating. Notes As of v2.0.0 this method has changed to use the Styler implementation as part of Styler.to_latex() via jinja2 templating. This means that jinja2 is a requirement, and needs to be installed, for this method to function. It is advised that users switch to using Styler, since that implementation is more frequently updated and contains much more flexibility with the output.","Parameters: bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string. columnslist of label, optionalThe subset of columns to write. Writes all columns by default. headerbool or list of str, default TrueWrite out the column names. If a list of strings is given, it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). na_repstr, default ‘NaN’Missing data representation. formatterslist of functions or dict of {{str: function}}, optionalFormatter functions to apply to columns’ elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_formatone-parameter function or str, optional, default NoneFormatter for floating point numbers. For example float_format=""%.2f"" and float_format=""{{:0.2f}}"".format will both result in 0.1234 being formatted as 0.12. sparsifybool, optionalSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module. index_namesbool, default TruePrints the names of the indexes. bold_rowsbool, default FalseMake the row labels bold in the output. column_formatstr, optionalThe columns format as specified in LaTeX table format e.g. ‘rcl’ for 3 columns. By default, ‘l’ will be used for all columns except columns of numbers, which default to ‘r’. longtablebool, optionalUse a longtable environment instead of tabular. Requires adding a usepackage{{longtable}} to your LaTeX preamble. By default, the value will be read from the pandas config module, and set to True if the option styler.latex.environment is “longtable”. Changed in version 2.0.0: The pandas option affecting this argument has changed. escapebool, optionalBy default, the value will be read from the pandas config module and set to True if the option styler.format.escape is “latex”. When set to False prevents from escaping latex special characters in column names. Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to False. encodingstr, optionalA string representing the encoding to use in the output file, defaults to ‘utf-8’. decimalstr, default ‘.’Character recognized as decimal separator, e.g. ‘,’ in Europe. multicolumnbool, default TrueUse multicolumn to enhance MultiIndex columns. The default will be read from the config module, and is set as the option styler.sparse.columns. Changed in version 2.0.0: The pandas option affecting this argument has changed. multicolumn_formatstr, default ‘r’The alignment for multicolumns, similar to column_format The default will be read from the config module, and is set as the option styler.latex.multicol_align. Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to “r”. multirowbool, default TrueUse multirow to enhance MultiIndex rows. Requires adding a usepackage{{multirow}} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module, and is set as the option styler.sparse.index. Changed in version 2.0.0: The pandas option affecting this argument has changed, as has the default value to True. captionstr or tuple, optionalTuple (full_caption, short_caption), which results in \caption[short_caption]{{full_caption}}; if a single string is passed, no short caption will be set. labelstr, optionalThe LaTeX label to be placed inside \label{{}} in the output. This is used with \ref{{}} in the main .tex file. positionstr, optionalThe LaTeX positional argument for tables, to be placed after \begin{{}} in the output. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","['>>> df = pd.DataFrame(dict(name=[\'Raphael\', \'Donatello\'],\n...                        age=[26, 45],\n...                        height=[181.23, 177.65]))\n>>> print(df.to_latex(index=False,\n...                   formatters={""name"": str.upper},\n...                   float_format=""{:.1f}"".format,\n... ))  \n\\begin{tabular}{lrr}\n\\toprule\nname & age & height \\\\\n\\midrule\nRAPHAEL & 26 & 181.2 \\\\\nDONATELLO & 45 & 177.7 \\\\\n\\bottomrule\n\\end{tabular}']"
350,..\pandas\reference\api\pandas.Timedelta.asm8.html,pandas.Timedelta.asm8,"Timedelta.asm8# Return a numpy timedelta64 array scalar view. Provides access to the array scalar view (i.e. a combination of the value and the units) associated with the numpy.timedelta64().view(), including a 64-bit integer representation of the timedelta in nanoseconds (Python int compatible).",Returns: numpy timedelta64 array scalar viewArray scalar view of the timedelta in nanoseconds.,"["">>> td = pd.Timedelta('1 days 2 min 3 us 42 ns')\n>>> td.asm8\nnumpy.timedelta64(86520000003042,'ns')"", "">>> td = pd.Timedelta('2 min 3 s')\n>>> td.asm8\nnumpy.timedelta64(123000000000,'ns')"", "">>> td = pd.Timedelta('3 ms 5 us')\n>>> td.asm8\nnumpy.timedelta64(3005000,'ns')"", "">>> td = pd.Timedelta(42, unit='ns')\n>>> td.asm8\nnumpy.timedelta64(42,'ns')""]"
351,..\pandas\reference\api\pandas.core.resample.Resampler.nearest.html,pandas.core.resample.Resampler.nearest,"final Resampler.nearest(limit=None)[source]# Resample by using the nearest value. When resampling data, missing values may appear (e.g., when the resampling frequency is higher than the original frequency). The nearest method will replace NaN values that appeared in the resampled data with the value from the nearest member of the sequence, based on the index value. Missing values that existed in the original data will not be modified. If limit is given, fill only this many values in each direction for each of the original values.","Parameters: limitint, optionalLimit of how many values to fill. Returns: Series or DataFrameAn upsampled Series or DataFrame with NaN values filled with their nearest value.","["">>> s = pd.Series([1, 2],\n...               index=pd.date_range('20180101',\n...                                   periods=2,\n...                                   freq='1h'))\n>>> s\n2018-01-01 00:00:00    1\n2018-01-01 01:00:00    2\nFreq: h, dtype: int64"", "">>> s.resample('15min').nearest()\n2018-01-01 00:00:00    1\n2018-01-01 00:15:00    1\n2018-01-01 00:30:00    2\n2018-01-01 00:45:00    2\n2018-01-01 01:00:00    2\nFreq: 15min, dtype: int64"", "">>> s.resample('15min').nearest(limit=1)\n2018-01-01 00:00:00    1.0\n2018-01-01 00:15:00    1.0\n2018-01-01 00:30:00    NaN\n2018-01-01 00:45:00    2.0\n2018-01-01 01:00:00    2.0\nFreq: 15min, dtype: float64""]"
352,..\pandas\reference\api\pandas.IntervalIndex.to_tuples.html,pandas.IntervalIndex.to_tuples,"IntervalIndex.to_tuples(*args, **kwargs)[source]# Return an ndarray (if self is IntervalArray) or Index (if self is IntervalIndex) of tuples of the form (left, right).","Parameters: na_tuplebool, default TrueIf True, return NA as a tuple (nan, nan). If False, just return NA as nan. Returns: tuples: ndarray (if self is IntervalArray) or Index (if self is IntervalIndex)","['>>> idx = pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 2)])\n>>> idx\n<IntervalArray>\n[(0, 1], (1, 2]]\nLength: 2, dtype: interval[int64, right]\n>>> idx.to_tuples()\narray([(0, 1), (1, 2)], dtype=object)', "">>> idx = pd.interval_range(start=0, end=2)\n>>> idx\nIntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')\n>>> idx.to_tuples()\nIndex([(0, 1), (1, 2)], dtype='object')""]"
353,..\pandas\reference\api\pandas.Series.dt.is_quarter_end.html,pandas.Series.dt.is_quarter_end,Series.dt.is_quarter_end[source]# Indicator for whether the date is the last day of a quarter.,Returns: is_quarter_endSeries or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> df = pd.DataFrame({\'dates\': pd.date_range(""2017-03-30"",\n...                    periods=4)})\n>>> df.assign(quarter=df.dates.dt.quarter,\n...           is_quarter_end=df.dates.dt.is_quarter_end)\n       dates  quarter    is_quarter_end\n0 2017-03-30        1             False\n1 2017-03-31        1              True\n2 2017-04-01        2             False\n3 2017-04-02        2             False', "">>> idx = pd.date_range('2017-03-30', periods=4)\n>>> idx\nDatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n              dtype='datetime64[ns]', freq='D')"", '>>> idx.is_quarter_end\narray([False,  True, False, False])']"
354,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.html,pandas.tseries.offsets.MonthBegin,class pandas.tseries.offsets.MonthBegin# DateOffset of one month at beginning. MonthBegin goes to the next date which is a start of the month. Examples If you want to get the start of the current month: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range.","["">>> ts = pd.Timestamp(2022, 11, 30)\n>>> ts + pd.offsets.MonthBegin()\nTimestamp('2022-12-01 00:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 1)\n>>> ts + pd.offsets.MonthBegin()\nTimestamp('2023-01-01 00:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 1)\n>>> pd.offsets.MonthBegin().rollback(ts)\nTimestamp('2022-12-01 00:00:00')""]"
355,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.holidays.html,pandas.tseries.offsets.CustomBusinessDay.holidays,CustomBusinessDay.holidays#,No parameters found,[]
356,..\pandas\reference\api\pandas.api.types.is_bool_dtype.html,pandas.api.types.is_bool_dtype,pandas.api.types.is_bool_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of a boolean dtype. Notes An ExtensionArray is considered boolean when the _is_boolean attribute is set to True.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of a boolean dtype.,"["">>> from pandas.api.types import is_bool_dtype\n>>> is_bool_dtype(str)\nFalse\n>>> is_bool_dtype(int)\nFalse\n>>> is_bool_dtype(bool)\nTrue\n>>> is_bool_dtype(np.bool_)\nTrue\n>>> is_bool_dtype(np.array(['a', 'b']))\nFalse\n>>> is_bool_dtype(pd.Series([1, 2]))\nFalse\n>>> is_bool_dtype(np.array([True, False]))\nTrue\n>>> is_bool_dtype(pd.Categorical([True, False]))\nTrue\n>>> is_bool_dtype(pd.arrays.SparseArray([True, False]))\nTrue""]"
357,..\pandas\reference\api\pandas.DataFrame.to_markdown.html,pandas.DataFrame.to_markdown,"DataFrame.to_markdown(buf=None, *, mode='wt', index=True, storage_options=None, **kwargs)[source]# Print DataFrame in Markdown-friendly format. Notes Requires the tabulate package.","Parameters: bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string. modestr, optionalMode in which file is opened, “wt” by default. indexbool, optional, default TrueAdd index (row) labels. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. **kwargsThese parameters will be passed to tabulate. Returns: strDataFrame in Markdown-friendly format.","['>>> df = pd.DataFrame(\n...     data={""animal_1"": [""elk"", ""pig""], ""animal_2"": [""dog"", ""quetzal""]}\n... )\n>>> print(df.to_markdown())\n|    | animal_1   | animal_2   |\n|---:|:-----------|:-----------|\n|  0 | elk        | dog        |\n|  1 | pig        | quetzal    |', '>>> print(df.to_markdown(tablefmt=""grid""))\n+----+------------+------------+\n|    | animal_1   | animal_2   |\n+====+============+============+\n|  0 | elk        | dog        |\n+----+------------+------------+\n|  1 | pig        | quetzal    |\n+----+------------+------------+']"
358,..\pandas\reference\api\pandas.IntervalIndex.values.html,pandas.IntervalIndex.values,"property IntervalIndex.values[source]# Return an array representing the data in the Index. Warning We recommend using Index.array or Index.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array.",Returns: array: numpy.ndarray or ExtensionArray,"["">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.values\narray([1, 2, 3])"", '>>> idx = pd.interval_range(start=0, end=5)\n>>> idx.values\n<IntervalArray>\n[(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]]\nLength: 5, dtype: interval[int64, right]']"
359,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.html,pandas.tseries.offsets.CustomBusinessDay,"class pandas.tseries.offsets.CustomBusinessDay# DateOffset subclass representing possibly n custom business days. In CustomBusinessDay we can use custom weekmask, holidays, and calendar. Examples In the example below the default parameters give the next business day. Business days can be specified by weekmask parameter. To convert the returned datetime object to its string representation the function strftime() is used in the next example. Using NumPy business day calendar you can define custom holidays. If you want to shift the result on n day you can use the parameter offset. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. calendar freqstr Return a string representing the frequency. holidays kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize offset Alias for self._offset. rule_code weekmask","Parameters: nint, default 1The number of days represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekmaskstr, Default ‘Mon Tue Wed Thu Fri’Weekmask of valid business days, passed to numpy.busdaycalendar. holidayslistList/array of dates to exclude from the set of valid business days, passed to numpy.busdaycalendar. calendarnp.busdaycalendarCalendar to integrate. offsettimedelta, default timedelta(0)Time offset to apply.","["">>> ts = pd.Timestamp(2022, 8, 5, 16)\n>>> ts + pd.offsets.CustomBusinessDay()\nTimestamp('2022-08-08 16:00:00')"", '>>> import datetime as dt\n>>> freq = pd.offsets.CustomBusinessDay(weekmask=""Mon Wed Fri"")\n>>> pd.date_range(dt.datetime(2022, 12, 10), dt.datetime(2022, 12, 21),\n...               freq=freq).strftime(\'%a %d %b %Y %H:%M\')\nIndex([\'Mon 12 Dec 2022 00:00\', \'Wed 14 Dec 2022 00:00\',\n       \'Fri 16 Dec 2022 00:00\', \'Mon 19 Dec 2022 00:00\',\n       \'Wed 21 Dec 2022 00:00\'],\n       dtype=\'object\')', "">>> import datetime as dt\n>>> bdc = np.busdaycalendar(holidays=['2022-12-12', '2022-12-14'])\n>>> freq = pd.offsets.CustomBusinessDay(calendar=bdc)\n>>> pd.date_range(dt.datetime(2022, 12, 10), dt.datetime(2022, 12, 25), freq=freq)\nDatetimeIndex(['2022-12-13', '2022-12-15', '2022-12-16', '2022-12-19',\n               '2022-12-20', '2022-12-21', '2022-12-22', '2022-12-23'],\n               dtype='datetime64[ns]', freq='C')"", "">>> pd.Timestamp(2022, 8, 5, 16) + pd.offsets.CustomBusinessDay(1)\nTimestamp('2022-08-08 16:00:00')"", "">>> import datetime as dt\n>>> ts = pd.Timestamp(2022, 8, 5, 16)\n>>> ts + pd.offsets.CustomBusinessDay(1, offset=dt.timedelta(days=1))\nTimestamp('2022-08-09 16:00:00')""]"
360,..\pandas\reference\api\pandas.api.types.is_categorical_dtype.html,pandas.api.types.is_categorical_dtype,"pandas.api.types.is_categorical_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of the Categorical dtype. Deprecated since version 2.2.0: Use isinstance(dtype, pd.CategoricalDtype) instead.",Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of the Categorical dtype.,"['>>> from pandas.api.types import is_categorical_dtype\n>>> from pandas import CategoricalDtype\n>>> is_categorical_dtype(object)\nFalse\n>>> is_categorical_dtype(CategoricalDtype())\nTrue\n>>> is_categorical_dtype([1, 2, 3])\nFalse\n>>> is_categorical_dtype(pd.Categorical([1, 2, 3]))\nTrue\n>>> is_categorical_dtype(pd.CategoricalIndex([1, 2, 3]))\nTrue']"
361,..\pandas\reference\api\pandas.Series.dt.is_quarter_start.html,pandas.Series.dt.is_quarter_start,Series.dt.is_quarter_start[source]# Indicator for whether the date is the first day of a quarter.,Returns: is_quarter_startSeries or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> df = pd.DataFrame({\'dates\': pd.date_range(""2017-03-30"",\n...                   periods=4)})\n>>> df.assign(quarter=df.dates.dt.quarter,\n...           is_quarter_start=df.dates.dt.is_quarter_start)\n       dates  quarter  is_quarter_start\n0 2017-03-30        1             False\n1 2017-03-31        1             False\n2 2017-04-01        2              True\n3 2017-04-02        2             False', "">>> idx = pd.date_range('2017-03-30', periods=4)\n>>> idx\nDatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n              dtype='datetime64[ns]', freq='D')"", '>>> idx.is_quarter_start\narray([False, False,  True, False])']"
362,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_anchored.html,pandas.tseries.offsets.MonthBegin.is_anchored,MonthBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
363,..\pandas\reference\api\pandas.Timedelta.as_unit.html,pandas.Timedelta.as_unit,"Timedelta.as_unit(unit, round_ok=True)# Convert the underlying int64 representation to the given unit.","Parameters: unit{“ns”, “us”, “ms”, “s”} round_okbool, default TrueIf False and the conversion requires rounding, raise. Returns: Timedelta","["">>> td = pd.Timedelta('1001ms')\n>>> td\nTimedelta('0 days 00:00:01.001000')\n>>> td.as_unit('s')\nTimedelta('0 days 00:00:01')""]"
364,..\pandas\reference\api\pandas.core.resample.Resampler.nunique.html,pandas.core.resample.Resampler.nunique,"final Resampler.nunique(*args, **kwargs)[source]# Return number of unique elements in the group.",Returns: SeriesNumber of unique values within each group.,"["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    3\ndtype: int64\n>>> ser.groupby(level=0).nunique()\na    2\nb    1\ndtype: int64"", "">>> ser = pd.Series([1, 2, 3, 3], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    3\ndtype: int64\n>>> ser.resample('MS').nunique()\n2023-01-01    2\n2023-02-01    1\nFreq: MS, dtype: int64""]"
365,..\pandas\reference\api\pandas.DataFrame.to_numpy.html,pandas.DataFrame.to_numpy,"DataFrame.to_numpy(dtype=None, copy=False, na_value=<no_default>)[source]# Convert the DataFrame to a NumPy array. By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32, the results dtype will be float32. This may require copying data and coercing values, which may be expensive.","Parameters: dtypestr or numpy.dtype, optionalThe dtype to pass to numpy.asarray(). copybool, default FalseWhether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary. na_valueAny, optionalThe value to use for missing values. The default value depends on dtype and the dtypes of the DataFrame columns. Returns: numpy.ndarray","['>>> pd.DataFrame({""A"": [1, 2], ""B"": [3, 4]}).to_numpy()\narray([[1, 3],\n       [2, 4]])', '>>> df = pd.DataFrame({""A"": [1, 2], ""B"": [3.0, 4.5]})\n>>> df.to_numpy()\narray([[1. , 3. ],\n       [2. , 4.5]])', "">>> df['C'] = pd.date_range('2000', periods=2)\n>>> df.to_numpy()\narray([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n       [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)""]"
366,..\pandas\reference\api\pandas.interval_range.html,pandas.interval_range,"pandas.interval_range(start=None, end=None, periods=None, freq=None, name=None, closed='right')[source]# Return a fixed frequency IntervalIndex. Notes Of the four parameters start, end, periods, and freq, exactly three must be specified. If freq is omitted, the resulting IntervalIndex will have periods linearly spaced elements between start and end, inclusively. To learn more about datetime-like frequency strings, please see this link.","Parameters: startnumeric or datetime-like, default NoneLeft bound for generating intervals. endnumeric or datetime-like, default NoneRight bound for generating intervals. periodsint, default NoneNumber of periods to generate. freqnumeric, str, Timedelta, datetime.timedelta, or DateOffset, default NoneThe length of each interval. Must be consistent with the type of start and end, e.g. 2 for numeric, or ‘5H’ for datetime-like.  Default is 1 for numeric and ‘D’ for datetime-like. namestr, default NoneName of the resulting IntervalIndex. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. Returns: IntervalIndex","["">>> pd.interval_range(start=0, end=5)\nIntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],\n              dtype='interval[int64, right]')"", "">>> pd.interval_range(start=pd.Timestamp('2017-01-01'),\n...                   end=pd.Timestamp('2017-01-04'))\nIntervalIndex([(2017-01-01 00:00:00, 2017-01-02 00:00:00],\n               (2017-01-02 00:00:00, 2017-01-03 00:00:00],\n               (2017-01-03 00:00:00, 2017-01-04 00:00:00]],\n              dtype='interval[datetime64[ns], right]')"", "">>> pd.interval_range(start=0, periods=4, freq=1.5)\nIntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],\n              dtype='interval[float64, right]')"", "">>> pd.interval_range(start=pd.Timestamp('2017-01-01'),\n...                   periods=3, freq='MS')\nIntervalIndex([(2017-01-01 00:00:00, 2017-02-01 00:00:00],\n               (2017-02-01 00:00:00, 2017-03-01 00:00:00],\n               (2017-03-01 00:00:00, 2017-04-01 00:00:00]],\n              dtype='interval[datetime64[ns], right]')"", "">>> pd.interval_range(start=0, end=6, periods=4)\nIntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],\n          dtype='interval[float64, right]')"", "">>> pd.interval_range(end=5, periods=4, closed='both')\nIntervalIndex([[1, 2], [2, 3], [3, 4], [4, 5]],\n              dtype='interval[int64, both]')""]"
367,..\pandas\reference\api\pandas.api.types.is_complex.html,pandas.api.types.is_complex,pandas.api.types.is_complex(obj)# Return True if given object is complex.,Returns: bool,"['>>> pd.api.types.is_complex(1 + 1j)\nTrue', '>>> pd.api.types.is_complex(1)\nFalse']"
368,..\pandas\reference\api\pandas.DataFrame.to_orc.html,pandas.DataFrame.to_orc,"DataFrame.to_orc(path=None, *, engine='pyarrow', index=None, engine_kwargs=None)[source]# Write a DataFrame to the ORC format. Added in version 1.5.0. Notes Before using this function you should read the user guide about ORC and install optional dependencies. This function requires pyarrow library. For supported dtypes please refer to supported ORC features in Arrow. Currently timezones in datetime columns are not preserved when a dataframe is converted into ORC files.","Parameters: pathstr, file-like object or None, default NoneIf a string, it will be used as Root Directory path when writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handle (e.g. via builtin open function). If path is None, a bytes object is returned. engine{‘pyarrow’}, default ‘pyarrow’ORC library to use. indexbool, optionalIf True, include the dataframe’s index(es) in the file output. If False, they will not be written to the file. If None, similar to infer the dataframe’s index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn’t require much space and is faster. Other indexes will be included as columns in the file output. engine_kwargsdict[str, Any] or None, default NoneAdditional keyword arguments passed to pyarrow.orc.write_table(). Returns: bytes if no path argument is provided else None Raises: NotImplementedErrorDtype of one or more columns is category, unsigned integers, interval, period or sparse. ValueErrorengine is not pyarrow.","["">>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [4, 3]})\n>>> df.to_orc('df.orc')  \n>>> pd.read_orc('df.orc')  \n   col1  col2\n0     1     4\n1     2     3"", '>>> import io\n>>> b = io.BytesIO(df.to_orc())  \n>>> b.seek(0)  \n0\n>>> content = b.read()']"
369,..\pandas\reference\api\pandas.Series.dt.is_year_end.html,pandas.Series.dt.is_year_end,Series.dt.is_year_end[source]# Indicate whether the date is the last day of the year.,Returns: Series or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> dates = pd.Series(pd.date_range(""2017-12-30"", periods=3))\n>>> dates\n0   2017-12-30\n1   2017-12-31\n2   2018-01-01\ndtype: datetime64[ns]', '>>> dates.dt.is_year_end\n0    False\n1     True\n2    False\ndtype: bool', '>>> idx = pd.date_range(""2017-12-30"", periods=3)\n>>> idx\nDatetimeIndex([\'2017-12-30\', \'2017-12-31\', \'2018-01-01\'],\n              dtype=\'datetime64[ns]\', freq=\'D\')', '>>> idx.is_year_end\narray([False,  True, False])']"
370,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_month_end.html,pandas.tseries.offsets.MonthBegin.is_month_end,MonthBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
371,..\pandas\reference\api\pandas.Timedelta.ceil.html,pandas.Timedelta.ceil,Timedelta.ceil(freq)# Return a new Timedelta ceiled to this resolution.,Parameters: freqstrFrequency string indicating the ceiling resolution. It uses the same units as class constructor Timedelta.,"["">>> td = pd.Timedelta('1001ms')\n>>> td\nTimedelta('0 days 00:00:01.001000')\n>>> td.ceil('s')\nTimedelta('0 days 00:00:02')""]"
372,..\pandas\reference\api\pandas.core.resample.Resampler.ohlc.html,pandas.core.resample.Resampler.ohlc,"final Resampler.ohlc(*args, **kwargs)[source]# Compute open, high, low and close values of a group, excluding missing values. For multiple groupings, the result index will be a MultiIndex","Returns: DataFrameOpen, high, low and close values within each group.","["">>> lst = ['SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC',]\n>>> ser = pd.Series([3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 0.1, 0.5], index=lst)\n>>> ser\nSPX     3.4\nCAC     9.0\nSPX     7.2\nCAC     5.2\nSPX     8.8\nCAC     9.4\nSPX     0.1\nCAC     0.5\ndtype: float64\n>>> ser.groupby(level=0).ohlc()\n     open  high  low  close\nCAC   9.0   9.4  0.5    0.5\nSPX   3.4   8.8  0.1    0.1"", "">>> data = {2022: [1.2, 2.3, 8.9, 4.5, 4.4, 3, 2 , 1],\n...         2023: [3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 8.2, 1.0]}\n>>> df = pd.DataFrame(data, index=['SPX', 'CAC', 'SPX', 'CAC',\n...                   'SPX', 'CAC', 'SPX', 'CAC'])\n>>> df\n     2022  2023\nSPX   1.2   3.4\nCAC   2.3   9.0\nSPX   8.9   7.2\nCAC   4.5   5.2\nSPX   4.4   8.8\nCAC   3.0   9.4\nSPX   2.0   8.2\nCAC   1.0   1.0\n>>> df.groupby(level=0).ohlc()\n    2022                 2023\n    open high  low close open high  low close\nCAC  2.3  4.5  1.0   1.0  9.0  9.4  1.0   1.0\nSPX  1.2  8.9  1.2   2.0  3.4  8.8  3.4   8.2"", "">>> ser = pd.Series([1, 3, 2, 4, 3, 5],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').ohlc()\n            open  high  low  close\n2023-01-01     1     3    1      2\n2023-02-01     4     5    3      5""]"
373,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_anchored.html,pandas.tseries.offsets.CustomBusinessDay.is_anchored,CustomBusinessDay.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
374,..\pandas\reference\api\pandas.io.formats.style.Styler.apply.html,pandas.io.formats.style.Styler.apply,"Styler.apply(func, axis=0, subset=None, **kwargs)[source]# Apply a CSS-styling function column-wise, row-wise, or table-wise. Updates the HTML representation with the result. Notes The elements of the output of func should be CSS styles as strings, in the format ‘attribute: value; attribute2: value2; …’ or, if nothing is to be applied to that element, an empty string or None. This is similar to DataFrame.apply, except that axis=None applies the function to the entire DataFrame at once, rather than column-wise or row-wise.","Parameters: funcfunctionfunc should take a Series if axis in [0,1] and return a list-like object of same length, or a Series, not necessarily of same length, with valid index labels considering subset. func should take a DataFrame if axis is None and return either an ndarray with the same shape or a DataFrame, not necessarily of the same shape, with valid index and columns labels considering subset. Changed in version 1.3.0. Changed in version 1.4.0. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None. subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. **kwargsdictPass along to func. Returns: Styler","['>>> def highlight_max(x, color):\n...     return np.where(x == np.nanmax(x.to_numpy()), f""color: {color};"", None)\n>>> df = pd.DataFrame(np.random.randn(5, 2), columns=[""A"", ""B""])\n>>> df.style.apply(highlight_max, color=\'red\')  \n>>> df.style.apply(highlight_max, color=\'blue\', axis=1)  \n>>> df.style.apply(highlight_max, color=\'green\', axis=None)', '>>> df.style.apply(highlight_max, color=\'red\', subset=""A"")\n... \n>>> df.style.apply(highlight_max, color=\'red\', subset=[""A"", ""B""])\n...', '>>> df.style.apply(highlight_max, color=\'red\', subset=([0, 1, 2], slice(None)))\n... \n>>> df.style.apply(highlight_max, color=\'red\', subset=(slice(0, 5, 2), ""A""))\n...', '>>> df = pd.DataFrame([[1, 2], [3, 4], [4, 6]], index=[""A1"", ""A2"", ""Total""])\n>>> total_style = pd.Series(""font-weight: bold;"", index=[""Total""])\n>>> df.style.apply(lambda s: total_style)']"
375,..\pandas\reference\api\pandas.Timedelta.components.html,pandas.Timedelta.components,Timedelta.components# Return a components namedtuple-like.,No parameters found,"["">>> td = pd.Timedelta('2 day 4 min 3 us 42 ns')\n>>> td.components\nComponents(days=2, hours=0, minutes=4, seconds=0, milliseconds=0,\n    microseconds=3, nanoseconds=42)""]"
376,..\pandas\reference\api\pandas.core.resample.Resampler.pipe.html,pandas.core.resample.Resampler.pipe,"final Resampler.pipe(func, *args, **kwargs)[source]# Apply a func with arguments to this Resampler object and return its result. Use .pipe when you want to improve readability by chaining together functions that expect Series, DataFrames, GroupBy or Resampler objects. Instead of writing You can write which is much more readable. Notes See more here","Parameters: funccallable or tuple of (callable, str)Function to apply to this Resampler object or, alternatively, a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Resampler object. argsiterable, optionalPositional arguments passed into func. kwargsdict, optionalA dictionary of keyword arguments passed into func. Returns: the return type of func.","['>>> h = lambda x, arg2, arg3: x + 1 - arg2 * arg3\n>>> g = lambda x, arg1: x * 5 / arg1\n>>> f = lambda x: x ** 4\n>>> df = pd.DataFrame([[""a"", 4], [""b"", 5]], columns=[""group"", ""value""])\n>>> h(g(f(df.groupby(\'group\')), arg1=1), arg2=2, arg3=3)', "">>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=1)\n...    .pipe(h, arg2=2, arg3=3))"", "">>> df = pd.DataFrame({'A': [1, 2, 3, 4]},\n...                   index=pd.date_range('2012-08-02', periods=4))\n>>> df\n            A\n2012-08-02  1\n2012-08-03  2\n2012-08-04  3\n2012-08-05  4"", "">>> df.resample('2D').pipe(lambda x: x.max() - x.min())\n            A\n2012-08-02  1\n2012-08-04  1""]"
377,..\pandas\reference\api\pandas.DataFrame.to_parquet.html,pandas.DataFrame.to_parquet,"DataFrame.to_parquet(path=None, *, engine='auto', compression='snappy', index=None, partition_cols=None, storage_options=None, **kwargs)[source]# Write a DataFrame to the binary parquet format. This function writes the dataframe as a parquet file. You can choose different parquet backends, and have the option of compression. See the user guide for more details. Notes This function requires either the fastparquet or pyarrow library.","Parameters: pathstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. If None, the result is returned as bytes. If a string or path, it will be used as Root Directory path when writing a partitioned dataset. engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’Parquet library to use. If ‘auto’, then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if ‘pyarrow’ is unavailable. compressionstr or None, default ‘snappy’Name of the compression to use. Use None for no compression. Supported options: ‘snappy’, ‘gzip’, ‘brotli’, ‘lz4’, ‘zstd’. indexbool, default NoneIf True, include the dataframe’s index(es) in the file output. If False, they will not be written to the file. If None, similar to True the dataframe’s index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn’t require much space and is faster. Other indexes will be included as columns in the file output. partition_colslist, optional, default NoneColumn names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. **kwargsAdditional arguments passed to the parquet library. See pandas io for more details. Returns: bytes if no path argument is provided else None","["">>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n>>> df.to_parquet('df.parquet.gzip',\n...               compression='gzip')  \n>>> pd.read_parquet('df.parquet.gzip')  \n   col1  col2\n0     1     3\n1     2     4"", '>>> import io\n>>> f = io.BytesIO()\n>>> df.to_parquet(f)\n>>> f.seek(0)\n0\n>>> content = f.read()']"
378,..\pandas\reference\api\pandas.api.types.is_complex_dtype.html,pandas.api.types.is_complex_dtype,pandas.api.types.is_complex_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of a complex dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of a complex dtype.,"["">>> from pandas.api.types import is_complex_dtype\n>>> is_complex_dtype(str)\nFalse\n>>> is_complex_dtype(int)\nFalse\n>>> is_complex_dtype(np.complex128)\nTrue\n>>> is_complex_dtype(np.array(['a', 'b']))\nFalse\n>>> is_complex_dtype(pd.Series([1, 2]))\nFalse\n>>> is_complex_dtype(np.array([1 + 1j, 5]))\nTrue""]"
379,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_month_end.html,pandas.tseries.offsets.CustomBusinessDay.is_month_end,CustomBusinessDay.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
380,..\pandas\reference\api\pandas.Series.dt.is_year_start.html,pandas.Series.dt.is_year_start,Series.dt.is_year_start[source]# Indicate whether the date is the first day of a year.,Returns: Series or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> dates = pd.Series(pd.date_range(""2017-12-30"", periods=3))\n>>> dates\n0   2017-12-30\n1   2017-12-31\n2   2018-01-01\ndtype: datetime64[ns]', '>>> dates.dt.is_year_start\n0    False\n1    False\n2    True\ndtype: bool', '>>> idx = pd.date_range(""2017-12-30"", periods=3)\n>>> idx\nDatetimeIndex([\'2017-12-30\', \'2017-12-31\', \'2018-01-01\'],\n              dtype=\'datetime64[ns]\', freq=\'D\')', '>>> idx.is_year_start\narray([False, False,  True])']"
381,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_month_start.html,pandas.tseries.offsets.MonthBegin.is_month_start,MonthBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
382,..\pandas\reference\api\pandas.Timedelta.days.html,pandas.Timedelta.days,Timedelta.days# Returns the days of the timedelta.,Returns: int,"['>>> td = pd.Timedelta(1, ""d"")\n>>> td.days\n1', "">>> td = pd.Timedelta('4 min 3 us 42 ns')\n>>> td.days\n0""]"
383,..\pandas\reference\api\pandas.Series.dt.microsecond.html,pandas.Series.dt.microsecond,Series.dt.microsecond[source]# The microseconds of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""us"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00.000000\n1   2000-01-01 00:00:00.000001\n2   2000-01-01 00:00:00.000002\ndtype: datetime64[ns]\n>>> datetime_series.dt.microsecond\n0       0\n1       1\n2       2\ndtype: int32']"
384,..\pandas\reference\api\pandas.io.formats.style.Styler.apply_index.html,pandas.io.formats.style.Styler.apply_index,"Styler.apply_index(func, axis=0, level=None, **kwargs)[source]# Apply a CSS-styling function to the index or column headers, level-wise. Updates the HTML representation with the result. Added in version 1.4.0. Added in version 2.1.0: Styler.applymap_index was deprecated and renamed to Styler.map_index. Notes Each input to func will be the index as a Series, if an Index, or a level of a MultiIndex. The output of func should be an identically sized array of CSS styles as strings, in the format ‘attribute: value; attribute2: value2; …’ or, if nothing is to be applied to that element, an empty string or None.","Parameters: funcfunctionfunc should take a Series and return a string array of the same length. axis{0, 1, “index”, “columns”}The headers over which to apply the function. levelint, str, list, optionalIf index is MultiIndex the level(s) over which to apply the function. **kwargsdictPass along to func. Returns: Styler","['>>> df = pd.DataFrame([[1,2], [3,4]], index=[""A"", ""B""])\n>>> def color_b(s):\n...     return np.where(s == ""B"", ""background-color: yellow;"", """")\n>>> df.style.apply_index(color_b)', '>>> midx = pd.MultiIndex.from_product([[\'ix\', \'jy\'], [0, 1], [\'x3\', \'z4\']])\n>>> df = pd.DataFrame([np.arange(8)], columns=midx)\n>>> def highlight_x(s):\n...     return [""background-color: yellow;"" if ""x"" in v else """" for v in s]\n>>> df.style.apply_index(highlight_x, axis=""columns"", level=[0, 2])\n...']"
385,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_month_start.html,pandas.tseries.offsets.CustomBusinessDay.is_month_start,CustomBusinessDay.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
386,..\pandas\reference\api\pandas.core.resample.Resampler.prod.html,pandas.core.resample.Resampler.prod,"final Resampler.prod(numeric_only=False, min_count=0, *args, **kwargs)[source]# Compute prod of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. Returns: Series or DataFrameComputed prod of values within each group.","["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').prod()\n2023-01-01    2\n2023-02-01   12\nFreq: MS, dtype: int64""]"
387,..\pandas\reference\api\pandas.DataFrame.to_period.html,pandas.DataFrame.to_period,"DataFrame.to_period(freq=None, axis=0, copy=None)[source]# Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed).","Parameters: freqstr, defaultFrequency of the PeriodIndex. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to convert (the index by default). copybool, default TrueIf False then underlying input data is not copied. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: DataFrameThe DataFrame has a PeriodIndex.","['>>> idx = pd.to_datetime(\n...     [\n...         ""2001-03-31 00:00:00"",\n...         ""2002-05-31 00:00:00"",\n...         ""2003-08-31 00:00:00"",\n...     ]\n... )', "">>> idx\nDatetimeIndex(['2001-03-31', '2002-05-31', '2003-08-31'],\ndtype='datetime64[ns]', freq=None)"", '>>> idx.to_period(""M"")\nPeriodIndex([\'2001-03\', \'2002-05\', \'2003-08\'], dtype=\'period[M]\')', '>>> idx.to_period(""Y"")\nPeriodIndex([\'2001\', \'2002\', \'2003\'], dtype=\'period[Y-DEC]\')']"
388,..\pandas\reference\api\pandas.api.types.is_datetime64tz_dtype.html,pandas.api.types.is_datetime64tz_dtype,"pandas.api.types.is_datetime64tz_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of a DatetimeTZDtype dtype. Deprecated since version 2.1.0: Use isinstance(dtype, pd.DatetimeTZDtype) instead.",Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of a DatetimeTZDtype dtype.,"['>>> from pandas.api.types import is_datetime64tz_dtype\n>>> is_datetime64tz_dtype(object)\nFalse\n>>> is_datetime64tz_dtype([1, 2, 3])\nFalse\n>>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3]))  # tz-naive\nFalse\n>>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3], tz=""US/Eastern""))\nTrue', '>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype\n>>> dtype = DatetimeTZDtype(""ns"", tz=""US/Eastern"")\n>>> s = pd.Series([], dtype=dtype)\n>>> is_datetime64tz_dtype(dtype)\nTrue\n>>> is_datetime64tz_dtype(s)\nTrue']"
389,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_on_offset.html,pandas.tseries.offsets.MonthBegin.is_on_offset,MonthBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
390,..\pandas\reference\api\pandas.Timedelta.floor.html,pandas.Timedelta.floor,Timedelta.floor(freq)# Return a new Timedelta floored to this resolution.,Parameters: freqstrFrequency string indicating the flooring resolution. It uses the same units as class constructor Timedelta.,"["">>> td = pd.Timedelta('1001ms')\n>>> td\nTimedelta('0 days 00:00:01.001000')\n>>> td.floor('s')\nTimedelta('0 days 00:00:01')""]"
391,..\pandas\reference\api\pandas.Series.dt.microseconds.html,pandas.Series.dt.microseconds,Series.dt.microseconds[source]# Number of microseconds (>= 0 and less than 1 second) for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='us'))\n>>> ser\n0   0 days 00:00:00.000001\n1   0 days 00:00:00.000002\n2   0 days 00:00:00.000003\ndtype: timedelta64[ns]\n>>> ser.dt.microseconds\n0    1\n1    2\n2    3\ndtype: int32"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='us')\n>>> tdelta_idx\nTimedeltaIndex(['0 days 00:00:00.000001', '0 days 00:00:00.000002',\n                '0 days 00:00:00.000003'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.microseconds\nIndex([1, 2, 3], dtype='int32')""]"
392,..\pandas\reference\api\pandas.io.formats.style.Styler.background_gradient.html,pandas.io.formats.style.Styler.background_gradient,"Styler.background_gradient(cmap='PuBu', low=0, high=0, axis=0, subset=None, text_color_threshold=0.408, vmin=None, vmax=None, gmap=None)[source]# Color the background in a gradient style. The background color is determined according to the data in each column, row or frame, or by a given gradient map. Requires matplotlib. Notes When using low and high the range of the gradient, given by the data if gmap is not given or by gmap, is extended at the low end effectively by map.min - low * map.range and at the high end by map.max + high * map.range before the colors are normalized and determined. If combining with vmin and vmax the map.min, map.max and map.range are replaced by values according to the values derived from vmin and vmax. This method will preselect numeric columns and ignore non-numeric columns unless a gmap is supplied in which case no preselection occurs.","Parameters: cmapstr or colormapMatplotlib colormap. lowfloatCompress the color range at the low end. This is a multiple of the data range to extend below the minimum; good values usually in [0, 1], defaults to 0. highfloatCompress the color range at the high end. This is a multiple of the data range to extend above the maximum; good values usually in [0, 1], defaults to 0. axis{0, 1, “index”, “columns”, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None. subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. text_color_thresholdfloat or intLuminance threshold for determining text color in [0, 1]. Facilitates text visibility across varying background colors. All text is dark if 0, and light if 1, defaults to 0.408. vminfloat, optionalMinimum data value that corresponds to colormap minimum value. If not specified the minimum value of the data (or gmap) will be used. vmaxfloat, optionalMaximum data value that corresponds to colormap maximum value. If not specified the maximum value of the data (or gmap) will be used. gmaparray-like, optionalGradient map for determining the background colors. If not supplied will use the underlying data from rows, columns or frame. If given as an ndarray or list-like must be an identical shape to the underlying data considering axis and subset. If given as DataFrame or Series must have same index and column labels considering axis and subset. If supplied, vmin and vmax should be given relative to this gradient map. Added in version 1.3.0. Returns: Styler","['>>> df = pd.DataFrame(columns=[""City"", ""Temp (c)"", ""Rain (mm)"", ""Wind (m/s)""],\n...                   data=[[""Stockholm"", 21.6, 5.0, 3.2],\n...                         [""Oslo"", 22.4, 13.3, 3.1],\n...                         [""Copenhagen"", 24.5, 0.0, 6.7]])', '>>> df.style.background_gradient(axis=0)', '>>> df.style.background_gradient(axis=None)', '>>> df.style.background_gradient(axis=None, low=0.75, high=1.0)', '>>> df.style.background_gradient(axis=None, vmin=6.7, vmax=21.6)', "">>> df.style.background_gradient(axis=0, gmap=df['Temp (c)'], cmap='YlOrRd')\n..."", "">>> gmap = np.array([[1,2,3], [2,3,4], [3,4,5]])\n>>> df.style.background_gradient(axis=None, gmap=gmap,\n...     cmap='YlOrRd', subset=['Temp (c)', 'Rain (mm)', 'Wind (m/s)']\n... )""]"
393,..\pandas\reference\api\pandas.DataFrame.to_pickle.html,pandas.DataFrame.to_pickle,"DataFrame.to_pickle(path, *, compression='infer', protocol=5, storage_options=None)[source]# Pickle (serialize) object to file.","Parameters: pathstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. File path where the pickled object will be stored. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. protocolintInt which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1] paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4, 5. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. [1] https://docs.python.org/3/library/pickle.html. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here.","['>>> original_df = pd.DataFrame({""foo"": range(5), ""bar"": range(5, 10)})  \n>>> original_df  \n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n>>> original_df.to_pickle(""./dummy.pkl"")', '>>> unpickled_df = pd.read_pickle(""./dummy.pkl"")  \n>>> unpickled_df  \n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9']"
394,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_quarter_end.html,pandas.tseries.offsets.MonthBegin.is_quarter_end,MonthBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
395,..\pandas\reference\api\pandas.core.resample.Resampler.quantile.html,pandas.core.resample.Resampler.quantile,"final Resampler.quantile(q=0.5, **kwargs)[source]# Return value at the given quantile.","Parameters: qfloat or array-like, default 0.5 (50% quantile) Returns: DataFrame or SeriesQuantile of values within each group.","["">>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').quantile()\n2023-01-01    2.0\n2023-02-01    4.0\nFreq: MS, dtype: float64"", "">>> ser.resample('MS').quantile(.25)\n2023-01-01    1.5\n2023-02-01    3.5\nFreq: MS, dtype: float64""]"
396,..\pandas\reference\api\pandas.api.types.is_datetime64_any_dtype.html,pandas.api.types.is_datetime64_any_dtype,pandas.api.types.is_datetime64_any_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of the datetime64 dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: boolWhether or not the array or dtype is of the datetime64 dtype.,"['>>> from pandas.api.types import is_datetime64_any_dtype\n>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype\n>>> is_datetime64_any_dtype(str)\nFalse\n>>> is_datetime64_any_dtype(int)\nFalse\n>>> is_datetime64_any_dtype(np.datetime64)  # can be tz-naive\nTrue\n>>> is_datetime64_any_dtype(DatetimeTZDtype(""ns"", ""US/Eastern""))\nTrue\n>>> is_datetime64_any_dtype(np.array([\'a\', \'b\']))\nFalse\n>>> is_datetime64_any_dtype(np.array([1, 2]))\nFalse\n>>> is_datetime64_any_dtype(np.array([], dtype=""datetime64[ns]""))\nTrue\n>>> is_datetime64_any_dtype(pd.DatetimeIndex([1, 2, 3], dtype=""datetime64[ns]""))\nTrue']"
397,..\pandas\reference\api\pandas.Series.dt.minute.html,pandas.Series.dt.minute,Series.dt.minute[source]# The minutes of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""min"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 00:01:00\n2   2000-01-01 00:02:00\ndtype: datetime64[ns]\n>>> datetime_series.dt.minute\n0    0\n1    1\n2    2\ndtype: int32']"
398,..\pandas\reference\api\pandas.Timedelta.html,pandas.Timedelta,"class pandas.Timedelta(value=<object object>, unit=None, **kwargs)# Represents a duration, the difference between two dates or times. Timedelta is the pandas equivalent of python’s datetime.timedelta and is interchangeable with it in most cases. Notes The constructor may take in either both values of value and unit or kwargs as above. Either one of them must be used during initialization The .value attribute is always in ns. If the precision is higher than nanoseconds, the precision of the duration is truncated to nanoseconds. Examples Here we initialize Timedelta object with both value and unit Here we initialize the Timedelta object with kwargs We see that either way we get the same result Attributes asm8 Return a numpy timedelta64 array scalar view. components Return a components namedtuple-like. days Returns the days of the timedelta. max microseconds min nanoseconds Return the number of nanoseconds (n), where 0 <= n < 1 microsecond. resolution resolution_string Return a string representing the lowest timedelta resolution. seconds Return the total hours, minutes, and seconds of the timedelta as seconds. unit value","Parameters: valueTimedelta, timedelta, np.timedelta64, str, or int unitstr, default ‘ns’Denote the unit of the input, if input is an integer. Possible values: ‘W’, or ‘D’ ‘days’, or ‘day’ ‘hours’, ‘hour’, ‘hr’, or ‘h’ ‘minutes’, ‘minute’, ‘min’, or ‘m’ ‘seconds’, ‘second’, ‘sec’, or ‘s’ ‘milliseconds’, ‘millisecond’, ‘millis’, ‘milli’, or ‘ms’ ‘microseconds’, ‘microsecond’, ‘micros’, ‘micro’, or ‘us’ ‘nanoseconds’, ‘nanosecond’, ‘nanos’, ‘nano’, or ‘ns’. Deprecated since version 2.2.0: Values H, T, S, L, U, and N are deprecated in favour of the values h, min, s, ms, us, and ns. **kwargsAvailable kwargs: {days, seconds, microseconds, milliseconds, minutes, hours, weeks}. Values for construction in compat with datetime.timedelta. Numpy ints and floats will be coerced to python ints and floats.","['>>> td = pd.Timedelta(1, ""d"")\n>>> td\nTimedelta(\'1 days 00:00:00\')', "">>> td2 = pd.Timedelta(days=1)\n>>> td2\nTimedelta('1 days 00:00:00')""]"
399,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_on_offset.html,pandas.tseries.offsets.CustomBusinessDay.is_on_offset,CustomBusinessDay.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
400,..\pandas\reference\api\pandas.io.formats.style.Styler.bar.html,pandas.io.formats.style.Styler.bar,"Styler.bar(subset=None, axis=0, *, color=None, cmap=None, width=100, height=100, align='mid', vmin=None, vmax=None, props='width: 10em;')[source]# Draw bar chart in the cell backgrounds. Changed in version 1.4.0. Notes This section of the user guide: Table Visualization gives a number of examples for different settings and color coordination.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None. colorstr or 2-tuple/listIf a str is passed, the color is the same for both negative and positive numbers. If 2-tuple/list is used, the first element is the color_negative and the second is the color_positive (eg: [‘#d65f5f’, ‘#5fba7d’]). cmapstr, matplotlib.cm.ColorMapA string name of a matplotlib Colormap, or a Colormap object. Cannot be used together with color. Added in version 1.4.0. widthfloat, default 100The percentage of the cell, measured from the left, in which to draw the bars, in [0, 100]. heightfloat, default 100The percentage height of the bar in the cell, centrally aligned, in [0,100]. Added in version 1.4.0. alignstr, int, float, callable, default ‘mid’How to align the bars within the cells relative to a width adjusted center. If string must be one of: ‘left’ : bars are drawn rightwards from the minimum data value. ‘right’ : bars are drawn leftwards from the maximum data value. ‘zero’ : a value of zero is located at the center of the cell. ‘mid’ : a value of (max-min)/2 is located at the center of the cell, or if all values are negative (positive) the zero is aligned at the right (left) of the cell. ‘mean’ : the mean value of the data is located at the center of the cell. If a float or integer is given this will indicate the center of the cell. If a callable should take a 1d or 2d array and return a scalar. Changed in version 1.4.0. vminfloat, optionalMinimum bar value, defining the left hand limit of the bar drawing range, lower values are clipped to vmin. When None (default): the minimum value of the data will be used. vmaxfloat, optionalMaximum bar value, defining the right hand limit of the bar drawing range, higher values are clipped to vmax. When None (default): the maximum value of the data will be used. propsstr, optionalThe base CSS of the cell that is extended to add the bar chart. Defaults to “width: 10em;”. Added in version 1.4.0. Returns: Styler","["">>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df.style.bar(subset=['A'], color='gray')""]"
401,..\pandas\reference\api\pandas.DataFrame.to_records.html,pandas.DataFrame.to_records,"DataFrame.to_records(index=True, column_dtypes=None, index_dtypes=None)[source]# Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested.","Parameters: indexbool, default TrueInclude index in resulting record array, stored in ‘index’ field or using the index label, if set. column_dtypesstr, type, dict, default NoneIf a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypesstr, type, dict, default NoneIf a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if index=True. Returns: numpy.rec.recarrayNumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries.","["">>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]},\n...                   index=['a', 'b'])\n>>> df\n   A     B\na  1  0.50\nb  2  0.75\n>>> df.to_records()\nrec.array([('a', 1, 0.5 ), ('b', 2, 0.75)],\n          dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')])"", '>>> df.index = df.index.rename(""I"")\n>>> df.to_records()\nrec.array([(\'a\', 1, 0.5 ), (\'b\', 2, 0.75)],\n          dtype=[(\'I\', \'O\'), (\'A\', \'<i8\'), (\'B\', \'<f8\')])', "">>> df.to_records(index=False)\nrec.array([(1, 0.5 ), (2, 0.75)],\n          dtype=[('A', '<i8'), ('B', '<f8')])"", '>>> df.to_records(column_dtypes={""A"": ""int32""})\nrec.array([(\'a\', 1, 0.5 ), (\'b\', 2, 0.75)],\n          dtype=[(\'I\', \'O\'), (\'A\', \'<i4\'), (\'B\', \'<f8\')])', '>>> df.to_records(index_dtypes=""<S2"")\nrec.array([(b\'a\', 1, 0.5 ), (b\'b\', 2, 0.75)],\n          dtype=[(\'I\', \'S2\'), (\'A\', \'<i8\'), (\'B\', \'<f8\')])', '>>> index_dtypes = f""<S{df.index.str.len().max()}""\n>>> df.to_records(index_dtypes=index_dtypes)\nrec.array([(b\'a\', 1, 0.5 ), (b\'b\', 2, 0.75)],\n          dtype=[(\'I\', \'S1\'), (\'A\', \'<i8\'), (\'B\', \'<f8\')])']"
402,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_quarter_end.html,pandas.tseries.offsets.CustomBusinessDay.is_quarter_end,CustomBusinessDay.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
403,..\pandas\reference\api\pandas.io.formats.style.Styler.clear.html,pandas.io.formats.style.Styler.clear,"Styler.clear()[source]# Reset the Styler, removing any previously applied styles. Returns None.",No parameters found,"["">>> df = pd.DataFrame({'A': [1, 2], 'B': [3, np.nan]})"", "">>> df.style.highlight_null(color='yellow')"", '>>> df.style.clear()']"
404,..\pandas\reference\api\pandas.Series.dt.month.html,pandas.Series.dt.month,"Series.dt.month[source]# The month as January=1, December=12.",No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""ME"")\n... )\n>>> datetime_series\n0   2000-01-31\n1   2000-02-29\n2   2000-03-31\ndtype: datetime64[ns]\n>>> datetime_series.dt.month\n0    1\n1    2\n2    3\ndtype: int32']"
405,..\pandas\reference\api\pandas.core.resample.Resampler.sem.html,pandas.core.resample.Resampler.sem,"final Resampler.sem(ddof=1, numeric_only=False, *args, **kwargs)[source]# Compute standard error of the mean of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameStandard error of the mean of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([5, 10, 8, 14], index=lst)\n>>> ser\na     5\na    10\nb     8\nb    14\ndtype: int64\n>>> ser.groupby(level=0).sem()\na    2.5\nb    3.0\ndtype: float64"", '>>> data = [[1, 12, 11], [1, 15, 2], [2, 5, 8], [2, 6, 12]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a   b   c\n    tuna   1  12  11\n  salmon   1  15   2\n catfish   2   5   8\ngoldfish   2   6  12\n>>> df.groupby(""a"").sem()\n      b  c\na\n1    1.5  4.5\n2    0.5  2.0', "">>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').sem()\n2023-01-01    0.577350\n2023-02-01    1.527525\nFreq: MS, dtype: float64""]"
406,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_quarter_start.html,pandas.tseries.offsets.MonthBegin.is_quarter_start,MonthBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
407,..\pandas\reference\api\pandas.Timedelta.isoformat.html,pandas.Timedelta.isoformat,"Timedelta.isoformat()# Format the Timedelta as ISO 8601 Duration. P[n]Y[n]M[n]DT[n]H[n]M[n]S, where the [n] s are replaced by the values. See https://en.wikipedia.org/wiki/ISO_8601#Durations. Notes The longest component is days, whose value may be larger than 365. Every component is always included, even if its value is 0. Pandas uses nanosecond precision, so up to 9 decimal places may be included in the seconds component. Trailing 0’s are removed from the seconds component after the decimal. We do not 0 pad components, so it’s …T5H…, not …T05H…",Returns: str,"['>>> td = pd.Timedelta(days=6, minutes=50, seconds=3,\n...                   milliseconds=10, microseconds=10, nanoseconds=12)', "">>> td.isoformat()\n'P6DT0H50M3.010010012S'\n>>> pd.Timedelta(hours=1, seconds=10).isoformat()\n'P0DT1H0M10S'\n>>> pd.Timedelta(days=500.5).isoformat()\n'P500DT12H0M0S'""]"
408,..\pandas\reference\api\pandas.api.types.is_datetime64_dtype.html,pandas.api.types.is_datetime64_dtype,pandas.api.types.is_datetime64_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of the datetime64 dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of the datetime64 dtype.,"['>>> from pandas.api.types import is_datetime64_dtype\n>>> is_datetime64_dtype(object)\nFalse\n>>> is_datetime64_dtype(np.datetime64)\nTrue\n>>> is_datetime64_dtype(np.array([], dtype=int))\nFalse\n>>> is_datetime64_dtype(np.array([], dtype=np.datetime64))\nTrue\n>>> is_datetime64_dtype([1, 2, 3])\nFalse']"
409,..\pandas\reference\api\pandas.DataFrame.to_sql.html,pandas.DataFrame.to_sql,"DataFrame.to_sql(name, con, *, schema=None, if_exists='fail', index=True, index_label=None, chunksize=None, dtype=None, method=None)[source]# Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1] are supported. Tables can be newly created, appended to, or overwritten. Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. Not all datastores support method=""multi"". Oracle, for example, does not support multi-value insert. References [1] https://docs.sqlalchemy.org [2] https://www.python.org/dev/peps/pep-0249/","Parameters: namestrName of SQL table. consqlalchemy.engine.(Engine or Connection) or sqlite3.ConnectionUsing SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable. See here. If passing a sqlalchemy.engine.Connection which is already in a transaction, the transaction will not be committed.  If passing a sqlite3.Connection, it will not be possible to roll back the record insertion. schemastr, optionalSpecify the schema (if database flavor supports this). If None, use default schema. if_exists{‘fail’, ‘replace’, ‘append’}, default ‘fail’How to behave if the table already exists. fail: Raise a ValueError. replace: Drop the table before inserting new values. append: Insert new values to the existing table. indexbool, default TrueWrite DataFrame index as a column. Uses index_label as the column name in the table. Creates a table index for this column. index_labelstr or sequence, default NoneColumn label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksizeint, optionalSpecify the number of rows in each batch to be written at a time. By default, all rows will be written at once. dtypedict or scalar, optionalSpecifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method{None, ‘multi’, callable}, optionalControls the SQL insertion clause used: None : Uses standard SQL INSERT clause (one per row). ‘multi’: Pass multiple values in a single INSERT clause. callable with signature (pd_table, conn, keys, data_iter). Details and a sample callable implementation can be found in the section insert method. Returns: None or intNumber of rows affected by to_sql. None is returned if the callable passed into method does not return an integer number of rows. The number of returned rows affected is the sum of the rowcount attribute of sqlite3.Cursor or SQLAlchemy connectable which may not reflect the exact number of written rows as stipulated in the sqlite3 or SQLAlchemy. Added in version 1.4.0. Raises: ValueErrorWhen the table already exists and if_exists is ‘fail’ (the default).","["">>> from sqlalchemy import create_engine\n>>> engine = create_engine('sqlite://', echo=False)"", "">>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n>>> df\n     name\n0  User 1\n1  User 2\n2  User 3"", '>>> df.to_sql(name=\'users\', con=engine)\n3\n>>> from sqlalchemy import text\n>>> with engine.connect() as conn:\n...    conn.execute(text(""SELECT * FROM users"")).fetchall()\n[(0, \'User 1\'), (1, \'User 2\'), (2, \'User 3\')]', "">>> with engine.begin() as connection:\n...     df1 = pd.DataFrame({'name' : ['User 4', 'User 5']})\n...     df1.to_sql(name='users', con=connection, if_exists='append')\n2"", '>>> df2 = pd.DataFrame({\'name\' : [\'User 6\', \'User 7\']})\n>>> df2.to_sql(name=\'users\', con=engine, if_exists=\'append\')\n2\n>>> with engine.connect() as conn:\n...    conn.execute(text(""SELECT * FROM users"")).fetchall()\n[(0, \'User 1\'), (1, \'User 2\'), (2, \'User 3\'),\n (0, \'User 4\'), (1, \'User 5\'), (0, \'User 6\'),\n (1, \'User 7\')]', '>>> df2.to_sql(name=\'users\', con=engine, if_exists=\'replace\',\n...            index_label=\'id\')\n2\n>>> with engine.connect() as conn:\n...    conn.execute(text(""SELECT * FROM users"")).fetchall()\n[(0, \'User 6\'), (1, \'User 7\')]', '>>> from sqlalchemy.dialects.postgresql import insert\n>>> def insert_on_conflict_nothing(table, conn, keys, data_iter):\n...     # ""a"" is the primary key in ""conflict_table""\n...     data = [dict(zip(keys, row)) for row in data_iter]\n...     stmt = insert(table.table).values(data).on_conflict_do_nothing(index_elements=[""a""])\n...     result = conn.execute(stmt)\n...     return result.rowcount\n>>> df_conflict.to_sql(name=""conflict_table"", con=conn, if_exists=""append"", method=insert_on_conflict_nothing)  \n0', '>>> from sqlalchemy.dialects.mysql import insert\n>>> def insert_on_conflict_update(table, conn, keys, data_iter):\n...     # update columns ""b"" and ""c"" on primary key conflict\n...     data = [dict(zip(keys, row)) for row in data_iter]\n...     stmt = (\n...         insert(table.table)\n...         .values(data)\n...     )\n...     stmt = stmt.on_duplicate_key_update(b=stmt.inserted.b, c=stmt.inserted.c)\n...     result = conn.execute(stmt)\n...     return result.rowcount\n>>> df_conflict.to_sql(name=""conflict_table"", con=conn, if_exists=""append"", method=insert_on_conflict_update)  \n2', '>>> df = pd.DataFrame({""A"": [1, None, 2]})\n>>> df\n     A\n0  1.0\n1  NaN\n2  2.0', '>>> from sqlalchemy.types import Integer\n>>> df.to_sql(name=\'integers\', con=engine, index=False,\n...           dtype={""A"": Integer()})\n3', '>>> with engine.connect() as conn:\n...   conn.execute(text(""SELECT * FROM integers"")).fetchall()\n[(1,), (None,), (2,)]']"
410,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_quarter_start.html,pandas.tseries.offsets.CustomBusinessDay.is_quarter_start,CustomBusinessDay.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
411,..\pandas\reference\api\pandas.api.types.is_datetime64_ns_dtype.html,pandas.api.types.is_datetime64_ns_dtype,pandas.api.types.is_datetime64_ns_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of the datetime64[ns] dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: boolWhether or not the array or dtype is of the datetime64[ns] dtype.,"['>>> from pandas.api.types import is_datetime64_ns_dtype\n>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype\n>>> is_datetime64_ns_dtype(str)\nFalse\n>>> is_datetime64_ns_dtype(int)\nFalse\n>>> is_datetime64_ns_dtype(np.datetime64)  # no unit\nFalse\n>>> is_datetime64_ns_dtype(DatetimeTZDtype(""ns"", ""US/Eastern""))\nTrue\n>>> is_datetime64_ns_dtype(np.array([\'a\', \'b\']))\nFalse\n>>> is_datetime64_ns_dtype(np.array([1, 2]))\nFalse\n>>> is_datetime64_ns_dtype(np.array([], dtype=""datetime64""))  # no unit\nFalse\n>>> is_datetime64_ns_dtype(np.array([], dtype=""datetime64[ps]""))  # wrong unit\nFalse\n>>> is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3], dtype=""datetime64[ns]""))\nTrue']"
412,..\pandas\reference\api\pandas.Series.dt.month_name.html,pandas.Series.dt.month_name,"Series.dt.month_name(*args, **kwargs)[source]# Return the month names with specified locale.","Parameters: localestr, optionalLocale determining the language in which to return the month name. Default is English locale ('en_US.utf8'). Use the command locale -a on your terminal on Unix systems to find your locale language code. Returns: Series or IndexSeries or Index of month names.","["">>> s = pd.Series(pd.date_range(start='2018-01', freq='ME', periods=3))\n>>> s\n0   2018-01-31\n1   2018-02-28\n2   2018-03-31\ndtype: datetime64[ns]\n>>> s.dt.month_name()\n0     January\n1    February\n2       March\ndtype: object"", "">>> idx = pd.date_range(start='2018-01', freq='ME', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n              dtype='datetime64[ns]', freq='ME')\n>>> idx.month_name()\nIndex(['January', 'February', 'March'], dtype='object')"", "">>> idx = pd.date_range(start='2018-01', freq='ME', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n              dtype='datetime64[ns]', freq='ME')\n>>> idx.month_name(locale='pt_BR.utf8')  \nIndex(['Janeiro', 'Fevereiro', 'Março'], dtype='object')""]"
413,..\pandas\reference\api\pandas.Timedelta.max.html,pandas.Timedelta.max,Timedelta.max = Timedelta('106751 days 23:47:16.854775807')#,No parameters found,[]
414,..\pandas\reference\api\pandas.DataFrame.to_stata.html,pandas.DataFrame.to_stata,"DataFrame.to_stata(path, *, convert_dates=None, write_index=True, byteorder=None, time_stamp=None, data_label=None, variable_labels=None, version=114, convert_strl=None, compression='infer', storage_options=None, value_labels=None)[source]# Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. “dta” files contain a Stata dataset.","Parameters: pathstr, path object, or bufferString, path object (implementing os.PathLike[str]), or file-like object implementing a binary write() function. convert_datesdictDictionary mapping columns containing datetime types to stata internal format to use when writing the dates. Options are ‘tc’, ‘td’, ‘tm’, ‘tw’, ‘th’, ‘tq’, ‘ty’. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to ‘tc’. Raises NotImplementedError if a datetime column has timezone information. write_indexboolWrite the index to Stata dataset. byteorderstrCan be “>”, “<”, “little”, or “big”. default is sys.byteorder. time_stampdatetimeA datetime to use as file creation date.  Default is the current time. data_labelstr, optionalA label for the data set.  Must be 80 characters or smaller. variable_labelsdictDictionary containing columns as keys and variable labels as values. Each label must be 80 characters or smaller. version{114, 117, 118, 119, None}, default 114Version to use in the output dta file. Set to None to let pandas decide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. Version 119 should usually only be used when the number of variables exceeds the capacity of dta format 118. Exporting smaller datasets in format 119 may have unintended consequences, and, as of November 2020, Stata SE cannot read version 119 files. convert_strllist, optionalList of column names to convert to string columns to Stata StrL format. Only available if version is 117.  Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. value_labelsdict of dictsDictionary containing columns as keys and dictionaries of column value to labels as values. Labels for a single variable must be 32,000 characters or smaller. Added in version 1.4.0. Raises: NotImplementedError If datetimes contain timezone information Column dtype is not representable in Stata ValueError Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime Column listed in convert_dates is not in DataFrame Categorical label contains more than 32,000 characters","["">>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon',\n...                               'parrot'],\n...                    'speed': [350, 18, 361, 15]})\n>>> df.to_stata('animals.dta')""]"
415,..\pandas\reference\api\pandas.io.formats.style.Styler.concat.html,pandas.io.formats.style.Styler.concat,"Styler.concat(other)[source]# Append another Styler to combine the output into a single table. Added in version 1.5.0. Notes The purpose of this method is to extend existing styled dataframes with other metrics that may be useful but may not conform to the original’s structure. For example adding a sub total row, or displaying metrics such as means, variance or counts. Styles that are applied using the apply, map, apply_index and map_index, and formatting applied with format and format_index will be preserved. Warning Only the output methods to_html, to_string and to_latex currently work with concatenated Stylers. Other output methods, including to_excel, do not work with concatenated Stylers. The following should be noted: table_styles, table_attributes, caption and uuid are all inherited from the original Styler and not other. hidden columns and hidden index levels will be inherited from the original Styler css will be inherited from the original Styler, and the value of keys data, row_heading and row will be prepended with foot0_. If more concats are chained, their styles will be prepended with foot1_, ‘’foot_2’’, etc., and if a concatenated style have another concatanated style, the second style will be prepended with foot{parent}_foot{child}_. A common use case is to concatenate user defined functions with DataFrame.agg or with described statistics via DataFrame.describe. See examples.","Parameters: otherStylerThe other Styler object which has already been styled and formatted. The data for this Styler must have the same columns as the original, and the number of index levels must also be the same to render correctly. Returns: Styler","['>>> df = pd.DataFrame([[4, 6], [1, 9], [3, 4], [5, 5], [9, 6]],\n...                   columns=[""Mike"", ""Jim""],\n...                   index=[""Mon"", ""Tue"", ""Wed"", ""Thurs"", ""Fri""])\n>>> styler = df.style.concat(df.agg([""sum""]).style)', '>>> descriptors = df.agg([""sum"", ""mean"", lambda s: s.dtype])\n>>> descriptors.index = [""Total"", ""Average"", ""dtype""]\n>>> other = (descriptors.style\n...          .highlight_max(axis=1, subset=([""Total"", ""Average""], slice(None)))\n...          .format(subset=(""Average"", slice(None)), precision=2, decimal="","")\n...          .map(lambda v: ""font-weight: bold;""))\n>>> styler = (df.style\n...             .highlight_max(color=""salmon"")\n...             .set_table_styles([{""selector"": "".foot_row0"",\n...                                 ""props"": ""border-top: 1px solid black;""}]))\n>>> styler.concat(other)', '>>> df = pd.DataFrame([[1], [2]],\n...                   index=pd.MultiIndex.from_product([[0], [1, 2]]))\n>>> descriptors = df.agg([""sum""])\n>>> descriptors.index = pd.MultiIndex.from_product([[""""], descriptors.index])\n>>> df.style.concat(descriptors.style)']"
416,..\pandas\reference\api\pandas.core.resample.Resampler.size.html,pandas.core.resample.Resampler.size,final Resampler.size()[source]# Compute group sizes.,Returns: DataFrame or SeriesNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na     1\na     2\nb     3\ndtype: int64\n>>> ser.groupby(level=0).size()\na    2\nb    1\ndtype: int64"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(""a"").size()\na\n1    2\n7    1\ndtype: int64', "">>> ser = pd.Series([1, 2, 3], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\ndtype: int64\n>>> ser.resample('MS').size()\n2023-01-01    2\n2023-02-01    1\nFreq: MS, dtype: int64""]"
417,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_year_end.html,pandas.tseries.offsets.MonthBegin.is_year_end,MonthBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
418,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_year_end.html,pandas.tseries.offsets.CustomBusinessDay.is_year_end,CustomBusinessDay.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
419,..\pandas\reference\api\pandas.api.types.is_dict_like.html,pandas.api.types.is_dict_like,pandas.api.types.is_dict_like(obj)[source]# Check if the object is dict-like.,Parameters: objThe object to check Returns: boolWhether obj has dict-like properties.,"['>>> from pandas.api.types import is_dict_like\n>>> is_dict_like({1: 2})\nTrue\n>>> is_dict_like([1, 2, 3])\nFalse\n>>> is_dict_like(dict)\nFalse\n>>> is_dict_like(dict())\nTrue']"
420,..\pandas\reference\api\pandas.Series.dt.nanosecond.html,pandas.Series.dt.nanosecond,Series.dt.nanosecond[source]# The nanoseconds of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""ns"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00.000000000\n1   2000-01-01 00:00:00.000000001\n2   2000-01-01 00:00:00.000000002\ndtype: datetime64[ns]\n>>> datetime_series.dt.nanosecond\n0       0\n1       1\n2       2\ndtype: int32']"
421,..\pandas\reference\api\pandas.Timedelta.microseconds.html,pandas.Timedelta.microseconds,Timedelta.microseconds#,No parameters found,[]
422,..\pandas\reference\api\pandas.DataFrame.to_string.html,pandas.DataFrame.to_string,"DataFrame.to_string(buf=None, *, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True, justify=None, max_rows=None, max_cols=None, show_dimensions=False, decimal='.', line_width=None, min_rows=None, max_colwidth=None, encoding=None)[source]# Render a DataFrame to a console-friendly tabular output.","Parameters: bufstr, Path or StringIO-like, optional, default NoneBuffer to write to. If None, the output is returned as a string. columnsarray-like, optional, default NoneThe subset of columns to write. Writes all columns by default. col_spaceint, list or dict of int, optionalThe minimum width of each column. If a list of ints is given every integers corresponds with one column. If a dict is given, the key references the column, while the value defines the space to use.. headerbool or list of str, optionalWrite out the column names. If a list of columns is given, it is assumed to be aliases for the column names. indexbool, optional, default TrueWhether to print index (row) labels. na_repstr, optional, default ‘NaN’String representation of NaN to use. formatterslist, tuple or dict of one-param. functions, optionalFormatter functions to apply to columns’ elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_formatone-parameter function, optional, default NoneFormatter function to apply to columns’ elements if they are floats. This function must return a unicode string and will be applied only to the non-NaN elements, with NaN being handled by na_rep. sparsifybool, optional, default TrueSet to False for a DataFrame with a hierarchical index to print every multiindex key at each row. index_namesbool, optional, default TruePrints the names of the indexes. justifystr, default NoneHow to justify the column labels. If None uses the option from the print configuration (controlled by set_option), ‘right’ out of the box. Valid values are left right center justify justify-all start end inherit match-parent initial unset. max_rowsint, optionalMaximum number of rows to display in the console. max_colsint, optionalMaximum number of columns to display in the console. show_dimensionsbool, default FalseDisplay DataFrame dimensions (number of rows by number of columns). decimalstr, default ‘.’Character recognized as decimal separator, e.g. ‘,’ in Europe. line_widthint, optionalWidth to wrap a line in characters. min_rowsint, optionalThe number of rows to display in the console in a truncated repr (when number of rows is above max_rows). max_colwidthint, optionalMax width to truncate each column in characters. By default, no limit. encodingstr, default “utf-8”Set character encoding. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","["">>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n>>> df = pd.DataFrame(d)\n>>> print(df.to_string())\n   col1  col2\n0     1     4\n1     2     5\n2     3     6""]"
423,..\pandas\reference\api\pandas.io.formats.style.Styler.env.html,pandas.io.formats.style.Styler.env,Styler.env = <jinja2.environment.Environment object>#,No parameters found,[]
424,..\pandas\reference\api\pandas.Timedelta.min.html,pandas.Timedelta.min,Timedelta.min = Timedelta('-106752 days +00:12:43.145224193')#,No parameters found,[]
425,..\pandas\reference\api\pandas.api.types.is_extension_array_dtype.html,pandas.api.types.is_extension_array_dtype,"pandas.api.types.is_extension_array_dtype(arr_or_dtype)[source]# Check if an object is a pandas extension array type. See the Use Guide for more. Notes This checks whether an object implements the pandas extension array interface. In pandas, this includes: Categorical Sparse Interval Period DatetimeArray TimedeltaArray Third-party libraries may implement arrays or types satisfying this interface as well.","Parameters: arr_or_dtypeobjectFor array-like input, the .dtype attribute will be extracted. Returns: boolWhether the arr_or_dtype is an extension array type.","["">>> from pandas.api.types import is_extension_array_dtype\n>>> arr = pd.Categorical(['a', 'b'])\n>>> is_extension_array_dtype(arr)\nTrue\n>>> is_extension_array_dtype(arr.dtype)\nTrue"", "">>> arr = np.array(['a', 'b'])\n>>> is_extension_array_dtype(arr.dtype)\nFalse""]"
426,..\pandas\reference\api\pandas.Series.dt.nanoseconds.html,pandas.Series.dt.nanoseconds,Series.dt.nanoseconds[source]# Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='ns'))\n>>> ser\n0   0 days 00:00:00.000000001\n1   0 days 00:00:00.000000002\n2   0 days 00:00:00.000000003\ndtype: timedelta64[ns]\n>>> ser.dt.nanoseconds\n0    1\n1    2\n2    3\ndtype: int32"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='ns')\n>>> tdelta_idx\nTimedeltaIndex(['0 days 00:00:00.000000001', '0 days 00:00:00.000000002',\n                '0 days 00:00:00.000000003'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.nanoseconds\nIndex([1, 2, 3], dtype='int32')""]"
427,..\pandas\reference\api\pandas.core.resample.Resampler.std.html,pandas.core.resample.Resampler.std,"final Resampler.std(ddof=1, numeric_only=False, *args, **kwargs)[source]# Compute standard deviation of groups, excluding missing values.","Parameters: ddofint, default 1Degrees of freedom. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: DataFrame or SeriesStandard deviation of values within each group.","["">>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').std()\n2023-01-01    1.000000\n2023-02-01    2.645751\nFreq: MS, dtype: float64""]"
428,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.is_year_start.html,pandas.tseries.offsets.MonthBegin.is_year_start,MonthBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
429,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.is_year_start.html,pandas.tseries.offsets.CustomBusinessDay.is_year_start,CustomBusinessDay.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
430,..\pandas\reference\api\pandas.DataFrame.to_timestamp.html,pandas.DataFrame.to_timestamp,"DataFrame.to_timestamp(freq=None, how='start', axis=0, copy=None)[source]# Cast to DatetimeIndex of timestamps, at beginning of period.","Parameters: freqstr, default frequency of PeriodIndexDesired frequency. how{‘s’, ‘e’, ‘start’, ‘end’}Convention for converting period to timestamp; start of period vs. end. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to convert (the index by default). copybool, default TrueIf False then underlying input data is not copied. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: DataFrameThe DataFrame has a DatetimeIndex.","["">>> idx = pd.PeriodIndex(['2023', '2024'], freq='Y')\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df1 = pd.DataFrame(data=d, index=idx)\n>>> df1\n      col1   col2\n2023     1      3\n2024     2      4"", "">>> df1 = df1.to_timestamp()\n>>> df1\n            col1   col2\n2023-01-01     1      3\n2024-01-01     2      4\n>>> df1.index\nDatetimeIndex(['2023-01-01', '2024-01-01'], dtype='datetime64[ns]', freq=None)"", "">>> df2 = pd.DataFrame(data=d, index=idx)\n>>> df2 = df2.to_timestamp(freq='M')\n>>> df2\n            col1   col2\n2023-01-31     1      3\n2024-01-31     2      4\n>>> df2.index\nDatetimeIndex(['2023-01-31', '2024-01-31'], dtype='datetime64[ns]', freq=None)""]"
431,..\pandas\reference\api\pandas.io.formats.style.Styler.export.html,pandas.io.formats.style.Styler.export,"Styler.export()[source]# Export the styles applied to the current Styler. Can be applied to a second Styler with Styler.use. Notes This method is designed to copy non-data dependent attributes of one Styler to another. It differs from Styler.copy where data and data dependent attributes are also copied. The following items are exported since they are not generally data dependent: Styling functions added by the apply and map Whether axes and names are hidden from the display, if unambiguous. Table attributes Table styles The following attributes are considered data dependent and therefore not exported: Caption UUID Tooltips Any hidden rows or columns identified by Index labels Any formatting applied using Styler.format Any CSS classes added using Styler.set_td_classes",Returns: dict,"['>>> styler = pd.DataFrame([[1, 2], [3, 4]]).style\n>>> styler2 = pd.DataFrame([[9, 9, 9]]).style\n>>> styler.hide(axis=0).highlight_max(axis=1)  \n>>> export = styler.export()\n>>> styler2.use(export)']"
432,..\pandas\reference\api\pandas.Timedelta.nanoseconds.html,pandas.Timedelta.nanoseconds,"Timedelta.nanoseconds# Return the number of nanoseconds (n), where 0 <= n < 1 microsecond.",Returns: intNumber of nanoseconds.,"["">>> td = pd.Timedelta('1 days 2 min 3 us 42 ns')"", '>>> td.nanoseconds\n42', "">>> td = pd.Timedelta(42, unit='ns')\n>>> td.nanoseconds\n42""]"
433,..\pandas\reference\api\pandas.core.resample.Resampler.sum.html,pandas.core.resample.Resampler.sum,"final Resampler.sum(numeric_only=False, min_count=0, *args, **kwargs)[source]# Compute sum of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. Returns: Series or DataFrameComputed sum of values within each group.","["">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').sum()\n2023-01-01    3\n2023-02-01    7\nFreq: MS, dtype: int64""]"
434,..\pandas\reference\api\pandas.api.types.is_file_like.html,pandas.api.types.is_file_like,"pandas.api.types.is_file_like(obj)[source]# Check if the object is a file-like object. For objects to be considered file-like, they must be an iterator AND have either a read and/or write method as an attribute. Note: file-like objects must be iterable, but iterable objects need not be file-like.",Parameters: objThe object to check Returns: boolWhether obj has file-like properties.,"['>>> import io\n>>> from pandas.api.types import is_file_like\n>>> buffer = io.StringIO(""data"")\n>>> is_file_like(buffer)\nTrue\n>>> is_file_like([1, 2, 3])\nFalse']"
435,..\pandas\reference\api\pandas.Series.dt.normalize.html,pandas.Series.dt.normalize,"Series.dt.normalize(*args, **kwargs)[source]# Convert times to midnight. The time component of the date-time is converted to midnight i.e. 00:00:00. This is useful in cases, when the time does not matter. Length is unaltered. The timezones are unaffected. This method is available on Series with datetime values under the .dt accessor, and directly on Datetime Array/Index.","Returns: DatetimeArray, DatetimeIndex or SeriesThe same type as the original data. Series will have the same name and index. DatetimeIndex will have the same name.","["">>> idx = pd.date_range(start='2014-08-01 10:00', freq='h',\n...                     periods=3, tz='Asia/Calcutta')\n>>> idx\nDatetimeIndex(['2014-08-01 10:00:00+05:30',\n               '2014-08-01 11:00:00+05:30',\n               '2014-08-01 12:00:00+05:30'],\n                dtype='datetime64[ns, Asia/Calcutta]', freq='h')\n>>> idx.normalize()\nDatetimeIndex(['2014-08-01 00:00:00+05:30',\n               '2014-08-01 00:00:00+05:30',\n               '2014-08-01 00:00:00+05:30'],\n               dtype='datetime64[ns, Asia/Calcutta]', freq=None)""]"
436,..\pandas\reference\api\pandas.io.formats.style.Styler.format.html,pandas.io.formats.style.Styler.format,"Styler.format(formatter=None, subset=None, na_rep=None, precision=None, decimal='.', thousands=None, escape=None, hyperlinks=None)[source]# Format the text display value of cells. Notes This method assigns a formatting function, formatter, to each cell in the DataFrame. If formatter is None, then the default formatter is used. If a callable then that function should take a data value as input and return a displayable representation, such as a string. If formatter is given as a string this is assumed to be a valid Python format specification and is wrapped to a callable as string.format(x). If a dict is given, keys should correspond to column names, and values should be string or callable, as above. The default formatter currently expresses floats and complex numbers with the pandas display precision unless using the precision argument here. The default formatter does not adjust the representation of missing values unless the na_rep argument is used. The subset argument defines which region to apply the formatting function to. If the formatter argument is given in dict form but does not include all columns within the subset then these columns will have the default formatter applied. Any columns in the formatter dict excluded from the subset will be ignored. When using a formatter string the dtypes must be compatible, otherwise a ValueError will be raised. When instantiating a Styler, default formatting can be applied be setting the pandas.options: styler.format.formatter: default None. styler.format.na_rep: default None. styler.format.precision: default 6. styler.format.decimal: default “.”. styler.format.thousands: default None. styler.format.escape: default None. Warning Styler.format is ignored when using the output format Styler.to_excel, since Excel and Python have inherrently different formatting structures. However, it is possible to use the number-format pseudo CSS attribute to force Excel permissible formatting. See examples.","Parameters: formatterstr, callable, dict or NoneObject to define how values are displayed. See notes. subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. na_repstr, optionalRepresentation for missing values. If na_rep is None, no special formatting is applied. precisionint, optionalFloating point precision to use for display purposes, if not determined by the specified formatter. Added in version 1.3.0. decimalstr, default “.”Character used as decimal separator for floats, complex and integers. Added in version 1.3.0. thousandsstr, optional, default NoneCharacter used as thousands separator for floats, complex and integers. Added in version 1.3.0. escapestr, optionalUse ‘html’ to replace the characters &, <, >, ', and "" in cell display string with HTML-safe sequences. Use ‘latex’ to replace the characters &, %, $, #, _, {, }, ~, ^, and \ in the cell display string with LaTeX-safe sequences. Use ‘latex-math’ to replace the characters the same way as in ‘latex’ mode, except for math substrings, which either are surrounded by two characters $ or start with the character \( and end with \). Escaping is done before formatter. Added in version 1.3.0. hyperlinks{“html”, “latex”}, optionalConvert string patterns containing https://, http://, ftp:// or www. to HTML <a> tags as clickable URL hyperlinks if “html”, or LaTeX href commands if “latex”. Added in version 1.4.0. Returns: Styler","["">>> df = pd.DataFrame([[np.nan, 1.0, 'A'], [2.0, np.nan, 3.0]])\n>>> df.style.format(na_rep='MISS', precision=3)  \n        0       1       2\n0    MISS   1.000       A\n1   2.000    MISS   3.000"", "">>> df.style.format('{:.2f}', na_rep='MISS', subset=[0,1])  \n        0      1          2\n0    MISS   1.00          A\n1    2.00   MISS   3.000000"", "">>> df.style.format({0: '{:.2f}', 1: '£ {:.1f}'}, na_rep='MISS', precision=1)\n...  \n         0      1     2\n0    MISS   £ 1.0     A\n1    2.00    MISS   3.0"", "">>> (df.style.format(na_rep='MISS', precision=1, subset=[0])\n...     .format(na_rep='PASS', precision=2, subset=[1, 2]))  \n        0      1      2\n0    MISS   1.00      A\n1     2.0   PASS   3.00"", "">>> func = lambda s: 'STRING' if isinstance(s, str) else 'FLOAT'\n>>> df.style.format({0: '{:.1f}', 2: func}, precision=4, na_rep='MISS')\n...  \n        0        1        2\n0    MISS   1.0000   STRING\n1     2.0     MISS    FLOAT"", '>>> df = pd.DataFrame([[\'<div></div>\', \'""A&B""\', None]])\n>>> s = df.style.format(\n...     \'<a href=""a.com/{0}"">{0}</a>\', escape=""html"", na_rep=""NA""\n...     )\n>>> s.to_html()  \n...\n<td .. ><a href=""a.com/&lt;div&gt;&lt;/div&gt;"">&lt;div&gt;&lt;/div&gt;</a></td>\n<td .. ><a href=""a.com/&#34;A&amp;B&#34;"">&#34;A&amp;B&#34;</a></td>\n<td .. >NA</td>\n...', '>>> df = pd.DataFrame([[""123""], [""~ ^""], [""$%#""]])\n>>> df.style.format(""\\\\textbf{{{}}}"", escape=""latex"").to_latex()\n...  \n\\begin{tabular}{ll}\n & 0 \\\\\n0 & \\textbf{123} \\\\\n1 & \\textbf{\\textasciitilde \\space \\textasciicircum } \\\\\n2 & \\textbf{\\$\\%\\#} \\\\\n\\end{tabular}', '>>> df = pd.DataFrame([[r""$\\sum_{i=1}^{10} a_i$ a~b $\\alpha \\\n...     = \\frac{\\beta}{\\zeta^2}$""], [""%#^ $ \\$x^2 $""]])\n>>> df.style.format(escape=""latex-math"").to_latex()\n...  \n\\begin{tabular}{ll}\n & 0 \\\\\n0 & $\\sum_{i=1}^{10} a_i$ a\\textasciitilde b $\\alpha = \\frac{\\beta}{\\zeta^2}$ \\\\\n1 & \\%\\#\\textasciicircum \\space $ \\$x^2 $ \\\\\n\\end{tabular}', '>>> df = pd.DataFrame([[r""\\(\\sum_{i=1}^{10} a_i\\) a~b \\(\\alpha \\\n...     = \\frac{\\beta}{\\zeta^2}\\)""], [""%#^ \\( \\$x^2 \\)""]])\n>>> df.style.format(escape=""latex-math"").to_latex()\n...  \n\\begin{tabular}{ll}\n & 0 \\\\\n0 & \\(\\sum_{i=1}^{10} a_i\\) a\\textasciitilde b \\(\\alpha\n= \\frac{\\beta}{\\zeta^2}\\) \\\\\n1 & \\%\\#\\textasciicircum \\space \\( \\$x^2 \\) \\\\\n\\end{tabular}', '>>> df = pd.DataFrame([[r""\\( x^2 \\)  $x^2$""], \\\n...     [r""$\\frac{\\beta}{\\zeta}$ \\(\\frac{\\beta}{\\zeta}\\)""]])\n>>> df.style.format(escape=""latex-math"").to_latex()\n...  \n\\begin{tabular}{ll}\n & 0 \\\\\n0 & \\textbackslash ( x\\textasciicircum 2 \\textbackslash )  $x^2$ \\\\\n1 & $\\frac{\\beta}{\\zeta}$ \\textbackslash (\\textbackslash\nfrac\\{\\textbackslash beta\\}\\{\\textbackslash zeta\\}\\textbackslash ) \\\\\n\\end{tabular}', '>>> df = pd.DataFrame({""A"": [1, 0, -1]})\n>>> pseudo_css = ""number-format: 0§[Red](0)§-§@;""\n>>> filename = ""formatted_file.xlsx""\n>>> df.style.map(lambda v: pseudo_css).to_excel(filename)']"
437,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.kwds.html,pandas.tseries.offsets.MonthBegin.kwds,MonthBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
438,..\pandas\reference\api\pandas.DataFrame.to_xarray.html,pandas.DataFrame.to_xarray,DataFrame.to_xarray()[source]# Return an xarray object from the pandas object. Notes See the xarray docs,"Returns: xarray.DataArray or xarray.DatasetData in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series.","["">>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2),\n...                    ('parrot', 'bird', 24.0, 2),\n...                    ('lion', 'mammal', 80.5, 4),\n...                    ('monkey', 'mammal', np.nan, 4)],\n...                   columns=['name', 'class', 'max_speed',\n...                            'num_legs'])\n>>> df\n     name   class  max_speed  num_legs\n0  falcon    bird      389.0         2\n1  parrot    bird       24.0         2\n2    lion  mammal       80.5         4\n3  monkey  mammal        NaN         4"", "">>> df.to_xarray()  \n<xarray.Dataset>\nDimensions:    (index: 4)\nCoordinates:\n  * index      (index) int64 32B 0 1 2 3\nData variables:\n    name       (index) object 32B 'falcon' 'parrot' 'lion' 'monkey'\n    class      (index) object 32B 'bird' 'bird' 'mammal' 'mammal'\n    max_speed  (index) float64 32B 389.0 24.0 80.5 nan\n    num_legs   (index) int64 32B 2 2 4 4"", "">>> df['max_speed'].to_xarray()  \n<xarray.DataArray 'max_speed' (index: 4)>\narray([389. ,  24. ,  80.5,   nan])\nCoordinates:\n  * index    (index) int64 0 1 2 3"", "">>> dates = pd.to_datetime(['2018-01-01', '2018-01-01',\n...                         '2018-01-02', '2018-01-02'])\n>>> df_multiindex = pd.DataFrame({'date': dates,\n...                               'animal': ['falcon', 'parrot',\n...                                          'falcon', 'parrot'],\n...                               'speed': [350, 18, 361, 15]})\n>>> df_multiindex = df_multiindex.set_index(['date', 'animal'])"", '>>> df_multiindex\n                   speed\ndate       animal\n2018-01-01 falcon    350\n           parrot     18\n2018-01-02 falcon    361\n           parrot     15', "">>> df_multiindex.to_xarray()  \n<xarray.Dataset>\nDimensions:  (date: 2, animal: 2)\nCoordinates:\n  * date     (date) datetime64[ns] 2018-01-01 2018-01-02\n  * animal   (animal) object 'falcon' 'parrot'\nData variables:\n    speed    (date, animal) int64 350 18 361 15""]"
439,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.kwds.html,pandas.tseries.offsets.CustomBusinessDay.kwds,CustomBusinessDay.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
440,..\pandas\reference\api\pandas.Timedelta.resolution.html,pandas.Timedelta.resolution,Timedelta.resolution = Timedelta('0 days 00:00:00.000000001')#,No parameters found,[]
441,..\pandas\reference\api\pandas.core.resample.Resampler.transform.html,pandas.core.resample.Resampler.transform,"final Resampler.transform(arg, *args, **kwargs)[source]# Call function producing a like-indexed Series on each group. Return a Series with the transformed values.",Parameters: argfunctionTo apply to each group. Should return a Series with the same index. Returns: Series,"["">>> s = pd.Series([1, 2],\n...               index=pd.date_range('20180101',\n...                                   periods=2,\n...                                   freq='1h'))\n>>> s\n2018-01-01 00:00:00    1\n2018-01-01 01:00:00    2\nFreq: h, dtype: int64"", "">>> resampled = s.resample('15min')\n>>> resampled.transform(lambda x: (x - x.mean()) / x.std())\n2018-01-01 00:00:00   NaN\n2018-01-01 01:00:00   NaN\nFreq: h, dtype: float64""]"
442,..\pandas\reference\api\pandas.DataFrame.to_xml.html,pandas.DataFrame.to_xml,"DataFrame.to_xml(path_or_buffer=None, *, index=True, root_name='data', row_name='row', na_rep=None, attr_cols=None, elem_cols=None, namespaces=None, prefix=None, encoding='utf-8', xml_declaration=True, pretty_print=True, parser='lxml', stylesheet=None, compression='infer', storage_options=None)[source]# Render a DataFrame to an XML document. Added in version 1.3.0.","Parameters: path_or_bufferstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a write() function. If None, the result is returned as a string. indexbool, default TrueWhether to include index in XML document. root_namestr, default ‘data’The name of root element in XML document. row_namestr, default ‘row’The name of row element in XML document. na_repstr, optionalMissing data representation. attr_colslist-like, optionalList of columns to write as attributes in row element. Hierarchical columns will be flattened with underscore delimiting the different levels. elem_colslist-like, optionalList of columns to write as children in row element. By default, all columns output as children of row element. Hierarchical columns will be flattened with underscore delimiting the different levels. namespacesdict, optionalAll namespaces to be defined in root element. Keys of dict should be prefix names and values of dict corresponding URIs. Default namespaces should be given empty string key. For example, namespaces = {"""": ""https://example.com""} prefixstr, optionalNamespace prefix to be used for every element and/or attribute in document. This should be one of the keys in namespaces dict. encodingstr, default ‘utf-8’Encoding of the resulting document. xml_declarationbool, default TrueWhether to include the XML declaration at start of document. pretty_printbool, default TrueWhether output should be pretty printed with indentation and line breaks. parser{‘lxml’,’etree’}, default ‘lxml’Parser module to use for building of tree. Only ‘lxml’ and ‘etree’ are supported. With ‘lxml’, the ability to use XSLT stylesheet is supported. stylesheetstr, path object or file-like object, optionalA URL, file-like object, or a raw string containing an XSLT script used to transform the raw XML output. Script should use layout of elements and attributes from original output. This argument requires lxml to be installed. Only XSLT 1.0 scripts and not later versions is currently supported. compressionstr or dict, default ‘infer’For on-the-fly compression of the output data. If ‘infer’ and ‘path_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). Set to None for no compression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdCompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Returns: None or strIf io is None, returns the resulting XML format as a string. Otherwise returns None.","["">>> df = pd.DataFrame({'shape': ['square', 'circle', 'triangle'],\n...                    'degrees': [360, 360, 180],\n...                    'sides': [4, np.nan, 3]})"", "">>> df.to_xml()  \n<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row>\n    <index>0</index>\n    <shape>square</shape>\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n  </row>\n  <row>\n    <index>1</index>\n    <shape>circle</shape>\n    <degrees>360</degrees>\n    <sides/>\n  </row>\n  <row>\n    <index>2</index>\n    <shape>triangle</shape>\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n  </row>\n</data>"", '>>> df.to_xml(attr_cols=[\n...           \'index\', \'shape\', \'degrees\', \'sides\'\n...           ])  \n<?xml version=\'1.0\' encoding=\'utf-8\'?>\n<data>\n  <row index=""0"" shape=""square"" degrees=""360"" sides=""4.0""/>\n  <row index=""1"" shape=""circle"" degrees=""360""/>\n  <row index=""2"" shape=""triangle"" degrees=""180"" sides=""3.0""/>\n</data>', '>>> df.to_xml(namespaces={""doc"": ""https://example.com""},\n...           prefix=""doc"")  \n<?xml version=\'1.0\' encoding=\'utf-8\'?>\n<doc:data xmlns:doc=""https://example.com"">\n  <doc:row>\n    <doc:index>0</doc:index>\n    <doc:shape>square</doc:shape>\n    <doc:degrees>360</doc:degrees>\n    <doc:sides>4.0</doc:sides>\n  </doc:row>\n  <doc:row>\n    <doc:index>1</doc:index>\n    <doc:shape>circle</doc:shape>\n    <doc:degrees>360</doc:degrees>\n    <doc:sides/>\n  </doc:row>\n  <doc:row>\n    <doc:index>2</doc:index>\n    <doc:shape>triangle</doc:shape>\n    <doc:degrees>180</doc:degrees>\n    <doc:sides>3.0</doc:sides>\n  </doc:row>\n</doc:data>']"
443,..\pandas\reference\api\pandas.api.types.is_float.html,pandas.api.types.is_float,pandas.api.types.is_float(obj)# Return True if given object is float.,Returns: bool,"['>>> pd.api.types.is_float(1.0)\nTrue', '>>> pd.api.types.is_float(1)\nFalse']"
444,..\pandas\reference\api\pandas.io.formats.style.Styler.format_index.html,pandas.io.formats.style.Styler.format_index,"Styler.format_index(formatter=None, axis=0, level=None, na_rep=None, precision=None, decimal='.', thousands=None, escape=None, hyperlinks=None)[source]# Format the text display value of index labels or column headers. Added in version 1.4.0. Notes This method assigns a formatting function, formatter, to each level label in the DataFrame’s index or column headers. If formatter is None, then the default formatter is used. If a callable then that function should take a label value as input and return a displayable representation, such as a string. If formatter is given as a string this is assumed to be a valid Python format specification and is wrapped to a callable as string.format(x). If a dict is given, keys should correspond to MultiIndex level numbers or names, and values should be string or callable, as above. The default formatter currently expresses floats and complex numbers with the pandas display precision unless using the precision argument here. The default formatter does not adjust the representation of missing values unless the na_rep argument is used. The level argument defines which levels of a MultiIndex to apply the method to. If the formatter argument is given in dict form but does not include all levels within the level argument then these unspecified levels will have the default formatter applied. Any levels in the formatter dict specifically excluded from the level argument will be ignored. When using a formatter string the dtypes must be compatible, otherwise a ValueError will be raised. Warning Styler.format_index is ignored when using the output format Styler.to_excel, since Excel and Python have inherrently different formatting structures. However, it is possible to use the number-format pseudo CSS attribute to force Excel permissible formatting. See documentation for Styler.format.","Parameters: formatterstr, callable, dict or NoneObject to define how values are displayed. See notes. axis{0, “index”, 1, “columns”}Whether to apply the formatter to the index or column headers. levelint, str, listThe level(s) over which to apply the generic formatter. na_repstr, optionalRepresentation for missing values. If na_rep is None, no special formatting is applied. precisionint, optionalFloating point precision to use for display purposes, if not determined by the specified formatter. decimalstr, default “.”Character used as decimal separator for floats, complex and integers. thousandsstr, optional, default NoneCharacter used as thousands separator for floats, complex and integers. escapestr, optionalUse ‘html’ to replace the characters &, <, >, ', and "" in cell display string with HTML-safe sequences. Use ‘latex’ to replace the characters &, %, $, #, _, {, }, ~, ^, and \ in the cell display string with LaTeX-safe sequences. Escaping is done before formatter. hyperlinks{“html”, “latex”}, optionalConvert string patterns containing https://, http://, ftp:// or www. to HTML <a> tags as clickable URL hyperlinks if “html”, or LaTeX href commands if “latex”. Returns: Styler","["">>> df = pd.DataFrame([[1, 2, 3]], columns=[2.0, np.nan, 4.0])\n>>> df.style.format_index(axis=1, na_rep='MISS', precision=3)  \n    2.000    MISS   4.000\n0       1       2       3"", "">>> df.style.format_index('{:.2f}', axis=1, na_rep='MISS')  \n     2.00   MISS    4.00\n0       1      2       3"", '>>> df = pd.DataFrame([[1, 2, 3]],\n...     columns=pd.MultiIndex.from_arrays([[""a"", ""a"", ""b""],[2, np.nan, 4]]))\n>>> df.style.format_index({0: lambda v: v.upper()}, axis=1, precision=1)\n...  \n               A       B\n      2.0    nan     4.0\n0       1      2       3', "">>> func = lambda s: 'STRING' if isinstance(s, str) else 'FLOAT'\n>>> df.style.format_index(func, axis=1, na_rep='MISS')\n...  \n          STRING  STRING\n    FLOAT   MISS   FLOAT\n0       1      2       3"", '>>> df = pd.DataFrame([[1, 2, 3]], columns=[\'""A""\', \'A&B\', None])\n>>> s = df.style.format_index(\'$ {0}\', axis=1, escape=""html"", na_rep=""NA"")\n...  \n<th .. >$ &#34;A&#34;</th>\n<th .. >$ A&amp;B</th>\n<th .. >NA</td>\n...', '>>> df = pd.DataFrame([[1, 2, 3]], columns=[""123"", ""~"", ""$%#""])\n>>> df.style.format_index(""\\\\textbf{{{}}}"", escape=""latex"", axis=1).to_latex()\n...  \n\\begin{tabular}{lrrr}\n{} & {\\textbf{123}} & {\\textbf{\\textasciitilde }} & {\\textbf{\\$\\%\\#}} \\\\\n0 & 1 & 2 & 3 \\\\\n\\end{tabular}']"
445,..\pandas\reference\api\pandas.Timedelta.round.html,pandas.Timedelta.round,Timedelta.round(freq)# Round the Timedelta to the specified resolution.,Parameters: freqstrFrequency string indicating the rounding resolution. It uses the same units as class constructor Timedelta. Returns: a new Timedelta rounded to the given resolution of freq Raises: ValueError if the freq cannot be converted,"["">>> td = pd.Timedelta('1001ms')\n>>> td\nTimedelta('0 days 00:00:01.001000')\n>>> td.round('s')\nTimedelta('0 days 00:00:01')""]"
446,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.n.html,pandas.tseries.offsets.CustomBusinessDay.n,CustomBusinessDay.n#,No parameters found,[]
447,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.n.html,pandas.tseries.offsets.MonthBegin.n,MonthBegin.n#,No parameters found,[]
448,..\pandas\reference\api\pandas.Series.dt.quarter.html,pandas.Series.dt.quarter,Series.dt.quarter[source]# The quarter of the date.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""4/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-04-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.quarter\n0    1\n1    2\ndtype: int32', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.quarter\nIndex([1, 1], dtype=\'int32\')']"
449,..\pandas\reference\api\pandas.DataFrame.transform.html,pandas.DataFrame.transform,"DataFrame.transform(func, axis=0, *args, **kwargs)[source]# Call func on self producing a DataFrame with the same axis shape as self. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funcfunction, str, list-like or dict-likeFunction to use for transforming the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. If func is both list-like and dict-like, dict-like behavior takes precedence. Accepted combinations are: function string function name list-like of functions and/or function names, e.g. [np.exp, 'sqrt'] dict-like of axis labels -> functions, function names or list-like of such. axis{0 or ‘index’, 1 or ‘columns’}, default 0If 0 or ‘index’: apply function to each column. If 1 or ‘columns’: apply function to each row. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: DataFrameA DataFrame that must have the same length as self. Raises: ValueErrorIf the returned DataFrame has a different length than self.","["">>> df = pd.DataFrame({'A': range(3), 'B': range(1, 4)})\n>>> df\n   A  B\n0  0  1\n1  1  2\n2  2  3\n>>> df.transform(lambda x: x + 1)\n   A  B\n0  1  2\n1  2  3\n2  3  4"", '>>> s = pd.Series(range(3))\n>>> s\n0    0\n1    1\n2    2\ndtype: int64\n>>> s.transform([np.sqrt, np.exp])\n       sqrt        exp\n0  0.000000   1.000000\n1  1.000000   2.718282\n2  1.414214   7.389056', '>>> df = pd.DataFrame({\n...     ""Date"": [\n...         ""2015-05-08"", ""2015-05-07"", ""2015-05-06"", ""2015-05-05"",\n...         ""2015-05-08"", ""2015-05-07"", ""2015-05-06"", ""2015-05-05""],\n...     ""Data"": [5, 8, 6, 1, 50, 100, 60, 120],\n... })\n>>> df\n         Date  Data\n0  2015-05-08     5\n1  2015-05-07     8\n2  2015-05-06     6\n3  2015-05-05     1\n4  2015-05-08    50\n5  2015-05-07   100\n6  2015-05-06    60\n7  2015-05-05   120\n>>> df.groupby(\'Date\')[\'Data\'].transform(\'sum\')\n0     55\n1    108\n2     66\n3    121\n4     55\n5    108\n6     66\n7    121\nName: Data, dtype: int64', '>>> df = pd.DataFrame({\n...     ""c"": [1, 1, 1, 2, 2, 2, 2],\n...     ""type"": [""m"", ""n"", ""o"", ""m"", ""m"", ""n"", ""n""]\n... })\n>>> df\n   c type\n0  1    m\n1  1    n\n2  1    o\n3  2    m\n4  2    m\n5  2    n\n6  2    n\n>>> df[\'size\'] = df.groupby(\'c\')[\'type\'].transform(len)\n>>> df\n   c type size\n0  1    m    3\n1  1    n    3\n2  1    o    3\n3  2    m    4\n4  2    m    4\n5  2    n    4\n6  2    n    4']"
450,..\pandas\reference\api\pandas.Timedelta.seconds.html,pandas.Timedelta.seconds,"Timedelta.seconds# Return the total hours, minutes, and seconds of the timedelta as seconds. Timedelta.seconds = hours * 3600 + minutes * 60 + seconds.",Returns: intNumber of seconds.,"["">>> td = pd.Timedelta('1 days 2 min 3 us 42 ns')\n>>> td.seconds\n120"", "">>> td = pd.Timedelta(42, unit='s')\n>>> td.seconds\n42""]"
451,..\pandas\reference\api\pandas.api.types.is_float_dtype.html,pandas.api.types.is_float_dtype,pandas.api.types.is_float_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of a float dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of a float dtype.,"["">>> from pandas.api.types import is_float_dtype\n>>> is_float_dtype(str)\nFalse\n>>> is_float_dtype(int)\nFalse\n>>> is_float_dtype(float)\nTrue\n>>> is_float_dtype(np.array(['a', 'b']))\nFalse\n>>> is_float_dtype(pd.Series([1, 2]))\nFalse\n>>> is_float_dtype(pd.Index([1, 2.]))\nTrue""]"
452,..\pandas\reference\api\pandas.core.resample.Resampler.var.html,pandas.core.resample.Resampler.var,"final Resampler.var(ddof=1, numeric_only=False, *args, **kwargs)[source]# Compute variance of groups, excluding missing values.","Parameters: ddofint, default 1Degrees of freedom. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: DataFrame or SeriesVariance of values within each group.","["">>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').var()\n2023-01-01    1.0\n2023-02-01    7.0\nFreq: MS, dtype: float64"", "">>> ser.resample('MS').var(ddof=0)\n2023-01-01    0.666667\n2023-02-01    4.666667\nFreq: MS, dtype: float64""]"
453,..\pandas\reference\api\pandas.Series.dt.qyear.html,pandas.Series.dt.qyear,Series.dt.qyear[source]#,No parameters found,[]
454,..\pandas\reference\api\pandas.io.formats.style.Styler.from_custom_template.html,pandas.io.formats.style.Styler.from_custom_template,"classmethod Styler.from_custom_template(searchpath, html_table=None, html_style=None)[source]# Factory function for creating a subclass of Styler. Uses custom templates and Jinja environment. Changed in version 1.3.0.","Parameters: searchpathstr or listPath or paths of directories containing the templates. html_tablestrName of your custom template to replace the html_table template. Added in version 1.3.0. html_stylestrName of your custom template to replace the html_style template. Added in version 1.3.0. Returns: MyStylersubclass of StylerHas the correct env,``template_html``, template_html_table and template_html_style class attributes set.","['>>> from pandas.io.formats.style import Styler\n>>> EasyStyler = Styler.from_custom_template(""path/to/template"",\n...                                          ""template.tpl"",\n...                                          )  \n>>> df = pd.DataFrame({""A"": [1, 2]})\n>>> EasyStyler(df)']"
455,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.name.html,pandas.tseries.offsets.MonthBegin.name,MonthBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
456,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.name.html,pandas.tseries.offsets.CustomBusinessDay.name,CustomBusinessDay.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
457,..\pandas\reference\api\pandas.DataFrame.transpose.html,pandas.DataFrame.transpose,"DataFrame.transpose(*args, copy=False)[source]# Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property T is an accessor to the method transpose(). Notes Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the object dtype. In such a case, a copy of the data is always made.","Parameters: *argstuple, optionalAccepted for compatibility with NumPy. copybool, default FalseWhether to copy the data after transposing, even for DataFrames with a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: DataFrameThe transposed DataFrame.","["">>> d1 = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df1 = pd.DataFrame(data=d1)\n>>> df1\n   col1  col2\n0     1     3\n1     2     4"", '>>> df1_transposed = df1.T  # or df1.transpose()\n>>> df1_transposed\n      0  1\ncol1  1  2\ncol2  3  4', '>>> df1.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n>>> df1_transposed.dtypes\n0    int64\n1    int64\ndtype: object', "">>> d2 = {'name': ['Alice', 'Bob'],\n...       'score': [9.5, 8],\n...       'employed': [False, True],\n...       'kids': [0, 0]}\n>>> df2 = pd.DataFrame(data=d2)\n>>> df2\n    name  score  employed  kids\n0  Alice    9.5     False     0\n1    Bob    8.0      True     0"", '>>> df2_transposed = df2.T  # or df2.transpose()\n>>> df2_transposed\n              0     1\nname      Alice   Bob\nscore       9.5   8.0\nemployed  False  True\nkids          0     0', '>>> df2.dtypes\nname         object\nscore       float64\nemployed       bool\nkids          int64\ndtype: object\n>>> df2_transposed.dtypes\n0    object\n1    object\ndtype: object']"
458,..\pandas\reference\api\pandas.io.formats.style.Styler.hide.html,pandas.io.formats.style.Styler.hide,"Styler.hide(subset=None, axis=0, level=None, names=False)[source]# Hide the entire index / column headers, or specific rows / columns from display. Added in version 1.4.0. Notes Warning This method only works with the output methods to_html, to_string and to_latex. Other output methods, including to_excel, ignore this hiding method and will display all data. This method has multiple functionality depending upon the combination of the subset, level and names arguments (see examples). The axis argument is used only to control whether the method is applied to row or column headers: Argument combinations# subset level names Effect None None False The axis-Index is hidden entirely. None None True Only the axis-Index names are hidden. None Int, Str, List False Specified axis-MultiIndex levels are hidden entirely. None Int, Str, List True Specified axis-MultiIndex levels are hidden entirely and the names of remaining axis-MultiIndex levels. Subset None False The specified data rows/columns are hidden, but the axis-Index itself, and names, remain unchanged. Subset None True The specified data rows/columns and axis-Index names are hidden, but the axis-Index itself remains unchanged. Subset Int, Str, List Boolean ValueError: cannot supply subset and level simultaneously. Note this method only hides the identified elements so can be chained to hide multiple elements in sequence.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 1d input or single key along the axis within DataFrame.loc[<subset>, :] or DataFrame.loc[:, <subset>] depending upon axis, to limit data to select hidden rows / columns. axis{“index”, 0, “columns”, 1}Apply to the index or columns. levelint, str, listThe level(s) to hide in a MultiIndex if hiding the entire index / column headers. Cannot be used simultaneously with subset. namesboolWhether to hide the level name(s) of the index / columns headers in the case it (or at least one the levels) remains visible. Returns: Styler","['>>> df = pd.DataFrame([[1,2], [3,4], [5,6]], index=[""a"", ""b"", ""c""])\n>>> df.style.hide([""a"", ""b""])  \n     0    1\nc    5    6', '>>> midx = pd.MultiIndex.from_product([[""x"", ""y""], [""a"", ""b"", ""c""]])\n>>> df = pd.DataFrame(np.random.randn(6,6), index=midx, columns=midx)\n>>> df.style.format(""{:.1f}"").hide()  \n                 x                    y\n   a      b      c      a      b      c\n 0.1    0.0    0.4    1.3    0.6   -1.4\n 0.7    1.0    1.3    1.5   -0.0   -0.2\n 1.4   -0.8    1.6   -0.2   -0.4   -0.3\n 0.4    1.0   -0.2   -0.8   -1.2    1.1\n-0.6    1.2    1.8    1.9    0.3    0.3\n 0.8    0.5   -0.3    1.2    2.2   -0.8', '>>> df.style.format(""{:.1f}"").hide(subset=(slice(None), [""a"", ""c""]))\n...   \n                         x                    y\n           a      b      c      a      b      c\nx   b    0.7    1.0    1.3    1.5   -0.0   -0.2\ny   b   -0.6    1.2    1.8    1.9    0.3    0.3', '>>> df.style.format(""{:.1f}"").hide(subset=(slice(None), [""a"", ""c""])).hide()\n...   \n                 x                    y\n   a      b      c      a      b      c\n 0.7    1.0    1.3    1.5   -0.0   -0.2\n-0.6    1.2    1.8    1.9    0.3    0.3', '>>> df.style.format(""{:,.1f}"").hide(level=1)  \n                     x                    y\n       a      b      c      a      b      c\nx    0.1    0.0    0.4    1.3    0.6   -1.4\n     0.7    1.0    1.3    1.5   -0.0   -0.2\n     1.4   -0.8    1.6   -0.2   -0.4   -0.3\ny    0.4    1.0   -0.2   -0.8   -1.2    1.1\n    -0.6    1.2    1.8    1.9    0.3    0.3\n     0.8    0.5   -0.3    1.2    2.2   -0.8', '>>> df.index.names = [""lev0"", ""lev1""]\n>>> df.style.format(""{:,.1f}"").hide(names=True)  \n                         x                    y\n           a      b      c      a      b      c\nx   a    0.1    0.0    0.4    1.3    0.6   -1.4\n    b    0.7    1.0    1.3    1.5   -0.0   -0.2\n    c    1.4   -0.8    1.6   -0.2   -0.4   -0.3\ny   a    0.4    1.0   -0.2   -0.8   -1.2    1.1\n    b   -0.6    1.2    1.8    1.9    0.3    0.3\n    c    0.8    0.5   -0.3    1.2    2.2   -0.8']"
459,..\pandas\reference\api\pandas.core.resample.Resampler.__iter__.html,pandas.core.resample.Resampler.__iter__,Resampler.__iter__()[source]# Groupby iterator.,"Returns: Generator yielding sequence of (name, subsetted object) for each group","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> for x, y in ser.groupby(level=0):\n...     print(f'{x}\\n{y}\\n')\na\na    1\na    2\ndtype: int64\nb\nb    3\ndtype: int64"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""])\n>>> df\n   a  b  c\n0  1  2  3\n1  1  5  6\n2  7  8  9\n>>> for x, y in df.groupby(by=[""a""]):\n...     print(f\'{x}\\n{y}\\n\')\n(1,)\n   a  b  c\n0  1  2  3\n1  1  5  6\n(7,)\n   a  b  c\n2  7  8  9', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> for x, y in ser.resample('MS'):\n...     print(f'{x}\\n{y}\\n')\n2023-01-01 00:00:00\n2023-01-01    1\n2023-01-15    2\ndtype: int64\n2023-02-01 00:00:00\n2023-02-01    3\n2023-02-15    4\ndtype: int64""]"
460,..\pandas\reference\api\pandas.Series.dt.round.html,pandas.Series.dt.round,"Series.dt.round(*args, **kwargs)[source]# Perform round operation on the data to the specified freq. Notes If the timestamps have a timezone, rounding will take place relative to the local (“wall”) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to round the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.round('h')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.round(""h"")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 12:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 03:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.floor(""2h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.floor(""2h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
461,..\pandas\reference\api\pandas.DataFrame.truediv.html,pandas.DataFrame.truediv,"DataFrame.truediv(other, axis='columns', level=None, fill_value=None)[source]# Get Floating division of dataframe and other, element-wise (binary operator truediv). Equivalent to dataframe / other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
462,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.nanos.html,pandas.tseries.offsets.CustomBusinessDay.nanos,CustomBusinessDay.nanos#,No parameters found,[]
463,..\pandas\reference\api\pandas.api.types.is_hashable.html,pandas.api.types.is_hashable,"pandas.api.types.is_hashable(obj)[source]# Return True if hash(obj) will succeed, False otherwise. Some types will pass a test against collections.abc.Hashable but fail when they are actually hashed with hash(). Distinguish between these and other types by trying the call to hash() and seeing if they raise TypeError.",Returns: bool,"['>>> import collections\n>>> from pandas.api.types import is_hashable\n>>> a = ([],)\n>>> isinstance(a, collections.abc.Hashable)\nTrue\n>>> is_hashable(a)\nFalse']"
464,..\pandas\reference\api\pandas.Timedelta.total_seconds.html,pandas.Timedelta.total_seconds,Timedelta.total_seconds()# Total seconds in the duration.,No parameters found,"["">>> td = pd.Timedelta('1min')\n>>> td\nTimedelta('0 days 00:01:00')\n>>> td.total_seconds()\n60.0""]"
465,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.nanos.html,pandas.tseries.offsets.MonthBegin.nanos,MonthBegin.nanos#,No parameters found,[]
466,..\pandas\reference\api\pandas.io.formats.style.Styler.highlight_between.html,pandas.io.formats.style.Styler.highlight_between,"Styler.highlight_between(subset=None, color='yellow', axis=0, left=None, right=None, inclusive='both', props=None)[source]# Highlight a defined range with a style. Added in version 1.3.0. Notes If left is None only the right bound is applied. If right is None only the left bound is applied. If both are None all values are highlighted. axis is only needed if left or right are provided as a sequence or an array-like object for aligning the shapes. If left and right are both scalars then all axis inputs will give the same result. This function only works with compatible dtypes. For example a datetime-like region can only use equivalent datetime-like left and right arguments. Use subset to control regions which have multiple dtypes.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. colorstr, default ‘yellow’Background color to use for highlighting. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0If left or right given as sequence, axis along which to apply those boundaries. See examples. leftscalar or datetime-like, or sequence or array-like, default NoneLeft bound for defining the range. rightscalar or datetime-like, or sequence or array-like, default NoneRight bound for defining the range. inclusive{‘both’, ‘neither’, ‘left’, ‘right’}Identify whether bounds are closed or open. propsstr, default NoneCSS properties to use for highlighting. If props is given, color is not used. Returns: Styler","["">>> df = pd.DataFrame({\n...     'One': [1.2, 1.6, 1.5],\n...     'Two': [2.9, 2.1, 2.5],\n...     'Three': [3.1, 3.2, 3.8],\n... })\n>>> df.style.highlight_between(left=2.1, right=2.9)"", '>>> df.style.highlight_between(left=[1.4, 2.4, 3.4], right=[1.6, 2.6, 3.6],\n...     axis=1, color=""#fffd75"")', '>>> df.style.highlight_between(left=[[2,2,3],[2,2,3],[3,3,3]], right=3.5,\n...     axis=None, color=""#fffd75"")', "">>> df.style.highlight_between(left=1.5, right=3.5,\n...     props='font-weight:bold;color:#e83e8c')""]"
467,..\pandas\reference\api\pandas.DataFrame.truncate.html,pandas.DataFrame.truncate,"DataFrame.truncate(before=None, after=None, axis=None, copy=None)[source]# Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Notes If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps.","Parameters: beforedate, str, intTruncate all rows before this index value. afterdate, str, intTruncate all rows after this index value. axis{0 or ‘index’, 1 or ‘columns’}, optionalAxis to truncate. Truncates the index (rows) by default. For Series this parameter is unused and defaults to 0. copybool, default is True,Return a copy of the truncated section. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: type of callerThe truncated Series or DataFrame.","["">>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'],\n...                    'B': ['f', 'g', 'h', 'i', 'j'],\n...                    'C': ['k', 'l', 'm', 'n', 'o']},\n...                   index=[1, 2, 3, 4, 5])\n>>> df\n   A  B  C\n1  a  f  k\n2  b  g  l\n3  c  h  m\n4  d  i  n\n5  e  j  o"", '>>> df.truncate(before=2, after=4)\n   A  B  C\n2  b  g  l\n3  c  h  m\n4  d  i  n', '>>> df.truncate(before=""A"", after=""B"", axis=""columns"")\n   A  B\n1  a  f\n2  b  g\n3  c  h\n4  d  i\n5  e  j', "">>> df['A'].truncate(before=2, after=4)\n2    b\n3    c\n4    d\nName: A, dtype: object"", "">>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s')\n>>> df = pd.DataFrame(index=dates, data={'A': 1})\n>>> df.tail()\n                     A\n2016-01-31 23:59:56  1\n2016-01-31 23:59:57  1\n2016-01-31 23:59:58  1\n2016-01-31 23:59:59  1\n2016-02-01 00:00:00  1"", "">>> df.truncate(before=pd.Timestamp('2016-01-05'),\n...             after=pd.Timestamp('2016-01-10')).tail()\n                     A\n2016-01-09 23:59:56  1\n2016-01-09 23:59:57  1\n2016-01-09 23:59:58  1\n2016-01-09 23:59:59  1\n2016-01-10 00:00:00  1"", "">>> df.truncate('2016-01-05', '2016-01-10').tail()\n                     A\n2016-01-09 23:59:56  1\n2016-01-09 23:59:57  1\n2016-01-09 23:59:58  1\n2016-01-09 23:59:59  1\n2016-01-10 00:00:00  1"", "">>> df.loc['2016-01-05':'2016-01-10', :].tail()\n                     A\n2016-01-10 23:59:55  1\n2016-01-10 23:59:56  1\n2016-01-10 23:59:57  1\n2016-01-10 23:59:58  1\n2016-01-10 23:59:59  1""]"
468,..\pandas\reference\api\pandas.Series.dt.second.html,pandas.Series.dt.second,Series.dt.second[source]# The seconds of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""s"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 00:00:01\n2   2000-01-01 00:00:02\ndtype: datetime64[ns]\n>>> datetime_series.dt.second\n0    0\n1    1\n2    2\ndtype: int32']"
469,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.normalize.html,pandas.tseries.offsets.CustomBusinessDay.normalize,CustomBusinessDay.normalize#,No parameters found,[]
470,..\pandas\reference\api\pandas.Timedelta.to_numpy.html,pandas.Timedelta.to_numpy,"Timedelta.to_numpy(dtype=None, copy=False)# Convert the Timedelta to a NumPy timedelta64. This is an alias method for Timedelta.to_timedelta64(). The dtype and copy parameters are available here only for compatibility. Their values will not affect the return value.",Returns: numpy.timedelta64,"["">>> td = pd.Timedelta('3D')\n>>> td\nTimedelta('3 days 00:00:00')\n>>> td.to_numpy()\nnumpy.timedelta64(259200000000000,'ns')""]"
471,..\pandas\reference\api\pandas.core.window.ewm.ExponentialMovingWindow.corr.html,pandas.core.window.ewm.ExponentialMovingWindow.corr,"ExponentialMovingWindow.corr(other=None, pairwise=None, numeric_only=False)[source]# Calculate the ewm (exponential weighted moment) sample correlation.","Parameters: otherSeries or DataFrame, optionalIf not supplied then will default to self and produce pairwise output. pairwisebool, default NoneIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndex DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser1 = pd.Series([1, 2, 3, 4])\n>>> ser2 = pd.Series([10, 11, 13, 16])\n>>> ser1.ewm(alpha=.2).corr(ser2)\n0         NaN\n1    1.000000\n2    0.982821\n3    0.977802\ndtype: float64']"
472,..\pandas\reference\api\pandas.io.formats.style.Styler.highlight_max.html,pandas.io.formats.style.Styler.highlight_max,"Styler.highlight_max(subset=None, color='yellow', axis=0, props=None)[source]# Highlight the maximum with a style.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. colorstr, default ‘yellow’Background color to use for highlighting. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None. propsstr, default NoneCSS properties to use for highlighting. If props is given, color is not used. Added in version 1.3.0. Returns: Styler","["">>> df = pd.DataFrame({'A': [2, 1], 'B': [3, 4]})\n>>> df.style.highlight_max(color='yellow')""]"
473,..\pandas\reference\api\pandas.api.types.is_int64_dtype.html,pandas.api.types.is_int64_dtype,"pandas.api.types.is_int64_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of the int64 dtype. Deprecated since version 2.1.0: is_int64_dtype is deprecated and will be removed in a future version. Use dtype == np.int64 instead. Notes Depending on system architecture, the return value of is_int64_dtype( int) will be True if the OS uses 64-bit integers and False if the OS uses 32-bit integers.",Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of the int64 dtype.,"["">>> from pandas.api.types import is_int64_dtype\n>>> is_int64_dtype(str)  \nFalse\n>>> is_int64_dtype(np.int32)  \nFalse\n>>> is_int64_dtype(np.int64)  \nTrue\n>>> is_int64_dtype('int8')  \nFalse\n>>> is_int64_dtype('Int8')  \nFalse\n>>> is_int64_dtype(pd.Int64Dtype)  \nTrue\n>>> is_int64_dtype(float)  \nFalse\n>>> is_int64_dtype(np.uint64)  # unsigned  \nFalse\n>>> is_int64_dtype(np.array(['a', 'b']))  \nFalse\n>>> is_int64_dtype(np.array([1, 2], dtype=np.int64))  \nTrue\n>>> is_int64_dtype(pd.Index([1, 2.]))  # float  \nFalse\n>>> is_int64_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned  \nFalse""]"
474,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.normalize.html,pandas.tseries.offsets.MonthBegin.normalize,MonthBegin.normalize#,No parameters found,[]
475,..\pandas\reference\api\pandas.DataFrame.tz_convert.html,pandas.DataFrame.tz_convert,"DataFrame.tz_convert(tz, axis=0, level=None, copy=None)[source]# Convert tz-aware axis to target time zone.","Parameters: tzstr or tzinfo object or NoneTarget time zone. Passing None will convert to UTC and remove the timezone information. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to convert levelint, str, default NoneIf axis is a MultiIndex, convert a specific level. Otherwise must be None. copybool, default TrueAlso make a copy of the underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: Series/DataFrameObject with time zone converted axis. Raises: TypeErrorIf the axis is tz-naive.","["">>> s = pd.Series(\n...     [1],\n...     index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']),\n... )\n>>> s.tz_convert('Asia/Shanghai')\n2018-09-15 07:30:00+08:00    1\ndtype: int64"", "">>> s = pd.Series([1],\n...               index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n>>> s.tz_convert(None)\n2018-09-14 23:30:00    1\ndtype: int64""]"
476,..\pandas\reference\api\pandas.io.formats.style.Styler.highlight_min.html,pandas.io.formats.style.Styler.highlight_min,"Styler.highlight_min(subset=None, color='yellow', axis=0, props=None)[source]# Highlight the minimum with a style.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. colorstr, default ‘yellow’Background color to use for highlighting. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None. propsstr, default NoneCSS properties to use for highlighting. If props is given, color is not used. Added in version 1.3.0. Returns: Styler","["">>> df = pd.DataFrame({'A': [2, 1], 'B': [3, 4]})\n>>> df.style.highlight_min(color='yellow')""]"
477,..\pandas\reference\api\pandas.tseries.offsets.MonthBegin.rule_code.html,pandas.tseries.offsets.MonthBegin.rule_code,MonthBegin.rule_code#,No parameters found,[]
478,..\pandas\reference\api\pandas.core.window.ewm.ExponentialMovingWindow.cov.html,pandas.core.window.ewm.ExponentialMovingWindow.cov,"ExponentialMovingWindow.cov(other=None, pairwise=None, bias=False, numeric_only=False)[source]# Calculate the ewm (exponential weighted moment) sample covariance.","Parameters: otherSeries or DataFrame , optionalIf not supplied then will default to self and produce pairwise output. pairwisebool, default NoneIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndex DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used. biasbool, default FalseUse a standard estimation bias correction. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser1 = pd.Series([1, 2, 3, 4])\n>>> ser2 = pd.Series([10, 11, 13, 16])\n>>> ser1.ewm(alpha=.2).cov(ser2)\n0         NaN\n1    0.500000\n2    1.524590\n3    3.408836\ndtype: float64']"
479,..\pandas\reference\api\pandas.Series.dt.seconds.html,pandas.Series.dt.seconds,Series.dt.seconds[source]# Number of seconds (>= 0 and less than 1 day) for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='s'))\n>>> ser\n0   0 days 00:00:01\n1   0 days 00:00:02\n2   0 days 00:00:03\ndtype: timedelta64[ns]\n>>> ser.dt.seconds\n0    1\n1    2\n2    3\ndtype: int32"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='s')\n>>> tdelta_idx\nTimedeltaIndex(['0 days 00:00:01', '0 days 00:00:02', '0 days 00:00:03'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.seconds\nIndex([1, 2, 3], dtype='int32')""]"
480,..\pandas\reference\api\pandas.api.types.is_integer.html,pandas.api.types.is_integer,pandas.api.types.is_integer(obj)# Return True if given object is integer.,Returns: bool,"['>>> pd.api.types.is_integer(1)\nTrue', '>>> pd.api.types.is_integer(1.0)\nFalse']"
481,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.rule_code.html,pandas.tseries.offsets.CustomBusinessDay.rule_code,CustomBusinessDay.rule_code#,No parameters found,[]
482,..\pandas\reference\api\pandas.Timedelta.to_pytimedelta.html,pandas.Timedelta.to_pytimedelta,Timedelta.to_pytimedelta()# Convert a pandas Timedelta object into a python datetime.timedelta object. Timedelta objects are internally saved as numpy datetime64[ns] dtype. Use to_pytimedelta() to convert to object dtype. Notes Any nanosecond resolution will be lost.,Returns: datetime.timedelta or numpy.array of datetime.timedelta,"["">>> td = pd.Timedelta('3D')\n>>> td\nTimedelta('3 days 00:00:00')\n>>> td.to_pytimedelta()\ndatetime.timedelta(days=3)""]"
483,..\pandas\reference\api\pandas.DataFrame.tz_localize.html,pandas.DataFrame.tz_localize,"DataFrame.tz_localize(tz, axis=0, level=None, copy=None, ambiguous='raise', nonexistent='raise')[source]# Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use Series.dt.tz_localize().","Parameters: tzstr or tzinfo or NoneTime zone to localize. Passing None will remove the time zone information and preserve local time. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to localize levelint, str, default NoneIf axis ia a MultiIndex, localize a specific level. Otherwise must be None. copybool, default TrueAlso make a copy of the underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistentstr, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are: ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: Series/DataFrameSame type as the input. Raises: TypeErrorIf the TimeSeries is tz-aware and tz is not None.","["">>> s = pd.Series(\n...     [1],\n...     index=pd.DatetimeIndex(['2018-09-15 01:30:00']),\n... )\n>>> s.tz_localize('CET')\n2018-09-15 01:30:00+02:00    1\ndtype: int64"", "">>> s = pd.Series([1],\n...               index=pd.DatetimeIndex(['2018-09-15 01:30:00+02:00']))\n>>> s.tz_localize(None)\n2018-09-15 01:30:00    1\ndtype: int64"", "">>> s = pd.Series(range(7),\n...               index=pd.DatetimeIndex(['2018-10-28 01:30:00',\n...                                       '2018-10-28 02:00:00',\n...                                       '2018-10-28 02:30:00',\n...                                       '2018-10-28 02:00:00',\n...                                       '2018-10-28 02:30:00',\n...                                       '2018-10-28 03:00:00',\n...                                       '2018-10-28 03:30:00']))\n>>> s.tz_localize('CET', ambiguous='infer')\n2018-10-28 01:30:00+02:00    0\n2018-10-28 02:00:00+02:00    1\n2018-10-28 02:30:00+02:00    2\n2018-10-28 02:00:00+01:00    3\n2018-10-28 02:30:00+01:00    4\n2018-10-28 03:00:00+01:00    5\n2018-10-28 03:30:00+01:00    6\ndtype: int64"", "">>> s = pd.Series(range(3),\n...               index=pd.DatetimeIndex(['2018-10-28 01:20:00',\n...                                       '2018-10-28 02:36:00',\n...                                       '2018-10-28 03:46:00']))\n>>> s.tz_localize('CET', ambiguous=np.array([True, True, False]))\n2018-10-28 01:20:00+02:00    0\n2018-10-28 02:36:00+02:00    1\n2018-10-28 03:46:00+01:00    2\ndtype: int64"", "">>> s = pd.Series(range(2),\n...               index=pd.DatetimeIndex(['2015-03-29 02:30:00',\n...                                       '2015-03-29 03:30:00']))\n>>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n2015-03-29 03:00:00+02:00    0\n2015-03-29 03:30:00+02:00    1\ndtype: int64\n>>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n2015-03-29 01:59:59.999999999+01:00    0\n2015-03-29 03:30:00+02:00              1\ndtype: int64\n>>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1h'))\n2015-03-29 03:30:00+02:00    0\n2015-03-29 03:30:00+02:00    1\ndtype: int64""]"
484,..\pandas\reference\api\pandas.io.formats.style.Styler.highlight_null.html,pandas.io.formats.style.Styler.highlight_null,"Styler.highlight_null(color='red', subset=None, props=None)[source]# Highlight missing values with a style.","Parameters: colorstr, default ‘red’Background color to use for highlighting. Added in version 1.5.0. subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. propsstr, default NoneCSS properties to use for highlighting. If props is given, color is not used. Added in version 1.3.0. Returns: Styler","["">>> df = pd.DataFrame({'A': [1, 2], 'B': [3, np.nan]})\n>>> df.style.highlight_null(color='yellow')""]"
485,..\pandas\reference\api\pandas.api.types.is_integer_dtype.html,pandas.api.types.is_integer_dtype,"pandas.api.types.is_integer_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of an integer dtype. Unlike in is_any_int_dtype, timedelta64 instances will return False. The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as integer by this function.",Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of an integer dtype and not an instance of timedelta64.,"["">>> from pandas.api.types import is_integer_dtype\n>>> is_integer_dtype(str)\nFalse\n>>> is_integer_dtype(int)\nTrue\n>>> is_integer_dtype(float)\nFalse\n>>> is_integer_dtype(np.uint64)\nTrue\n>>> is_integer_dtype('int8')\nTrue\n>>> is_integer_dtype('Int8')\nTrue\n>>> is_integer_dtype(pd.Int8Dtype)\nTrue\n>>> is_integer_dtype(np.datetime64)\nFalse\n>>> is_integer_dtype(np.timedelta64)\nFalse\n>>> is_integer_dtype(np.array(['a', 'b']))\nFalse\n>>> is_integer_dtype(pd.Series([1, 2]))\nTrue\n>>> is_integer_dtype(np.array([], dtype=np.timedelta64))\nFalse\n>>> is_integer_dtype(pd.Index([1, 2.]))  # float\nFalse""]"
486,..\pandas\reference\api\pandas.core.window.ewm.ExponentialMovingWindow.mean.html,pandas.core.window.ewm.ExponentialMovingWindow.mean,"ExponentialMovingWindow.mean(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the ewm (exponential weighted moment) mean. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 2, 3, 4])\n>>> ser.ewm(alpha=.2).mean()\n0    1.000000\n1    1.555556\n2    2.147541\n3    2.775068\ndtype: float64']"
487,..\pandas\reference\api\pandas.DataFrame.unstack.html,pandas.DataFrame.unstack,"DataFrame.unstack(level=-1, fill_value=None, sort=True)[source]# Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). Notes Reference the user guide for more examples.","Parameters: levelint, str, or list of these, default -1 (last level)Level(s) of index to unstack, can pass level name. fill_valueint, str or dictReplace NaN with this value if the unstack produces missing values. sortbool, default TrueSort the level(s) in the resulting MultiIndex columns. Returns: Series or DataFrame","["">>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n...                                    ('two', 'a'), ('two', 'b')])\n>>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n>>> s\none  a   1.0\n     b   2.0\ntwo  a   3.0\n     b   4.0\ndtype: float64"", '>>> s.unstack(level=-1)\n     a   b\none  1.0  2.0\ntwo  3.0  4.0', '>>> s.unstack(level=0)\n   one  two\na  1.0   3.0\nb  2.0   4.0', '>>> df = s.unstack(level=0)\n>>> df.unstack()\none  a  1.0\n     b  2.0\ntwo  a  3.0\n     b  4.0\ndtype: float64']"
488,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessDay.weekmask.html,pandas.tseries.offsets.CustomBusinessDay.weekmask,CustomBusinessDay.weekmask#,No parameters found,[]
489,..\pandas\reference\api\pandas.Series.dt.start_time.html,pandas.Series.dt.start_time,Series.dt.start_time[source]# Get the Timestamp for the start of the period.,Returns: Timestamp,"["">>> period = pd.Period('2012-1-1', freq='D')\n>>> period\nPeriod('2012-01-01', 'D')"", "">>> period.start_time\nTimestamp('2012-01-01 00:00:00')"", "">>> period.end_time\nTimestamp('2012-01-01 23:59:59.999999999')""]"
490,..\pandas\reference\api\pandas.Timedelta.to_timedelta64.html,pandas.Timedelta.to_timedelta64,Timedelta.to_timedelta64()# Return a numpy.timedelta64 object with ‘ns’ precision.,No parameters found,"["">>> td = pd.Timedelta('3D')\n>>> td\nTimedelta('3 days 00:00:00')\n>>> td.to_timedelta64()\nnumpy.timedelta64(259200000000000,'ns')""]"
491,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.copy.html,pandas.tseries.offsets.MonthEnd.copy,MonthEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
492,..\pandas\reference\api\pandas.io.formats.style.Styler.highlight_quantile.html,pandas.io.formats.style.Styler.highlight_quantile,"Styler.highlight_quantile(subset=None, color='yellow', axis=0, q_left=0.0, q_right=1.0, interpolation='linear', inclusive='both', props=None)[source]# Highlight values defined by a quantile with a style. Added in version 1.3.0. Notes This function does not work with str dtypes.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. colorstr, default ‘yellow’Background color to use for highlighting. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Axis along which to determine and highlight quantiles. If None quantiles are measured over the entire DataFrame. See examples. q_leftfloat, default 0Left bound, in [0, q_right), for the target quantile range. q_rightfloat, default 1Right bound, in (q_left, 1], for the target quantile range. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}Argument passed to Series.quantile or DataFrame.quantile for quantile estimation. inclusive{‘both’, ‘neither’, ‘left’, ‘right’}Identify whether quantile bounds are closed or open. propsstr, default NoneCSS properties to use for highlighting. If props is given, color is not used. Returns: Styler","['>>> df = pd.DataFrame(np.arange(10).reshape(2,5) + 1)\n>>> df.style.highlight_quantile(axis=None, q_left=0.8, color=""#fffd75"")\n...', '>>> df.style.highlight_quantile(axis=1, q_left=0.8, color=""#fffd75"")\n...', "">>> df.style.highlight_quantile(axis=None, q_left=0.2, q_right=0.8,\n...     props='font-weight:bold;color:#e83e8c')""]"
493,..\pandas\reference\api\pandas.api.types.is_interval.html,pandas.api.types.is_interval,pandas.api.types.is_interval(obj)#,No parameters found,[]
494,..\pandas\reference\api\pandas.core.window.ewm.ExponentialMovingWindow.std.html,pandas.core.window.ewm.ExponentialMovingWindow.std,"ExponentialMovingWindow.std(bias=False, numeric_only=False)[source]# Calculate the ewm (exponential weighted moment) standard deviation.","Parameters: biasbool, default FalseUse a standard estimation bias correction. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 2, 3, 4])\n>>> ser.ewm(alpha=.2).std()\n0         NaN\n1    0.707107\n2    0.995893\n3    1.277320\ndtype: float64']"
495,..\pandas\reference\api\pandas.Timedelta.unit.html,pandas.Timedelta.unit,Timedelta.unit#,No parameters found,[]
496,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.freqstr.html,pandas.tseries.offsets.MonthEnd.freqstr,MonthEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
497,..\pandas\reference\api\pandas.Series.dt.strftime.html,pandas.Series.dt.strftime,"Series.dt.strftime(*args, **kwargs)[source]# Convert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc. Formats supported by the C strftime API but not by the python string format doc (such as “%R”, “%r”) are not officially supported and should be preferably replaced with their supported equivalents (such as “%H:%M”, “%I:%M:%S %p”). Note that PeriodIndex support additional directives, detailed in Period.strftime.",Parameters: date_formatstrDate format string (e.g. “%Y-%m-%d”). Returns: ndarray[object]NumPy ndarray of formatted strings.,"['>>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),\n...                     periods=3, freq=\'s\')\n>>> rng.strftime(\'%B %d, %Y, %r\')\nIndex([\'March 10, 2018, 09:00:00 AM\', \'March 10, 2018, 09:00:01 AM\',\n       \'March 10, 2018, 09:00:02 AM\'],\n      dtype=\'object\')']"
498,..\pandas\reference\api\pandas.io.formats.style.Styler.html,pandas.io.formats.style.Styler,"class pandas.io.formats.style.Styler(data, precision=None, table_styles=None, uuid=None, caption=None, table_attributes=None, cell_ids=True, na_rep=None, uuid_len=5, decimal=None, thousands=None, escape=None, formatter=None)[source]# Helps style a DataFrame or Series according to the data with HTML and CSS. Attributes env (Jinja2 jinja2.Environment) template_html (Jinja2 Template) template_html_table (Jinja2 Template) template_html_style (Jinja2 Template) template_latex (Jinja2 Template) loader (Jinja2 Loader) Notes Most styling will be done by passing style functions into Styler.apply or Styler.map. Style functions should return values with strings containing CSS 'attr: value' that will be applied to the indicated cells. If using in the Jupyter notebook, Styler has defined a _repr_html_ to automatically render itself. Otherwise call Styler.to_html to get the generated HTML. CSS classes are attached to the generated HTML Index and Column names include index_name and level<k> where k is its level in a MultiIndex Index label cells include row_heading row<n> where n is the numeric position of the row level<k> where k is the level in a MultiIndex Column label cells include * col_heading * col<n> where n is the numeric position of the column * level<k> where k is the level in a MultiIndex Blank cells include blank Data cells include data Trimmed cells include col_trim or row_trim. Any, or all, or these classes can be renamed by using the css_class_names argument in Styler.set_table_classes, giving a value such as {“row”: “MY_ROW_CLASS”, “col_trim”: “”, “row_trim”: “”}. Examples Please see: Table Visualization for more examples. Attributes env loader template_html template_html_style template_html_table template_latex template_string","Parameters: dataSeries or DataFrameData to be styled - either a Series or DataFrame. precisionint, optionalPrecision to round floats to. If not given defaults to pandas.options.styler.format.precision. Changed in version 1.4.0. table_styleslist-like, default NoneList of {selector: (attr, value)} dicts; see Notes. uuidstr, default NoneA unique identifier to avoid CSS collisions; generated automatically. captionstr, tuple, default NoneString caption to attach to the table. Tuple only used for LaTeX dual captions. table_attributesstr, default NoneItems that show up in the opening <table> tag in addition to automatic (by default) id. cell_idsbool, default TrueIf True, each cell will have an id attribute in their HTML tag. The id takes the form T_<uuid>_row<num_row>_col<num_col> where <uuid> is the unique identifier, <num_row> is the row number and <num_col> is the column number. na_repstr, optionalRepresentation for missing values. If na_rep is None, no special formatting is applied, and falls back to pandas.options.styler.format.na_rep. uuid_lenint, default 5If uuid is not specified, the length of the uuid to randomly generate expressed in hex characters, in range [0, 32]. decimalstr, optionalCharacter used as decimal separator for floats, complex and integers. If not given uses pandas.options.styler.format.decimal. Added in version 1.3.0. thousandsstr, optional, default NoneCharacter used as thousands separator for floats, complex and integers. If not given uses pandas.options.styler.format.thousands. Added in version 1.3.0. escapestr, optionalUse ‘html’ to replace the characters &, <, >, ', and "" in cell display string with HTML-safe sequences. Use ‘latex’ to replace the characters &, %, $, #, _, {, }, ~, ^, and \ in the cell display string with LaTeX-safe sequences. Use ‘latex-math’ to replace the characters the same way as in ‘latex’ mode, except for math substrings, which either are surrounded by two characters $ or start with the character \( and end with \). If not given uses pandas.options.styler.format.escape. Added in version 1.3.0. formatterstr, callable, dict, optionalObject to define how values are displayed. See Styler.format. If not given uses pandas.options.styler.format.formatter. Added in version 1.4.0.","['>>> df = pd.DataFrame([[1.0, 2.0, 3.0], [4, 5, 6]], index=[\'a\', \'b\'],\n...                   columns=[\'A\', \'B\', \'C\'])\n>>> pd.io.formats.style.Styler(df, precision=2,\n...                            caption=""My table"")']"
499,..\pandas\reference\api\pandas.api.types.is_interval_dtype.html,pandas.api.types.is_interval_dtype,"pandas.api.types.is_interval_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of the Interval dtype. Deprecated since version 2.2.0: Use isinstance(dtype, pd.IntervalDtype) instead.",Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of the Interval dtype.,"['>>> from pandas.core.dtypes.common import is_interval_dtype\n>>> is_interval_dtype(object)\nFalse\n>>> is_interval_dtype(pd.IntervalDtype())\nTrue\n>>> is_interval_dtype([1, 2, 3])\nFalse\n>>>\n>>> interval = pd.Interval(1, 2, closed=""right"")\n>>> is_interval_dtype(interval)\nFalse\n>>> is_interval_dtype(pd.IntervalIndex([interval]))\nTrue']"
500,..\pandas\reference\api\pandas.DataFrame.update.html,pandas.DataFrame.update,"DataFrame.update(other, join='left', overwrite=True, filter_func=None, errors='ignore')[source]# Modify in place using non-NA values from another DataFrame. Aligns on indices. There is no return value.","Parameters: otherDataFrame, or object coercible into a DataFrameShould have at least one matching index/column label with the original DataFrame. If a Series is passed, its name attribute must be set, and that will be used as the column name to align with the original DataFrame. join{‘left’}, default ‘left’Only left join is implemented, keeping the index and columns of the original object. overwritebool, default TrueHow to handle non-NA values for overlapping keys: True: overwrite original DataFrame’s values with values from other. False: only update values that are NA in the original DataFrame. filter_funccallable(1d-array) -> bool 1d-array, optionalCan choose to replace values other than NA. Return True for values that should be updated. errors{‘raise’, ‘ignore’}, default ‘ignore’If ‘raise’, will raise a ValueError if the DataFrame and other both contain non-NA data in the same place. Returns: NoneThis method directly changes calling object. Raises: ValueError When errors=’raise’ and there’s overlapping non-NA data. When errors is not either ‘ignore’ or ‘raise’ NotImplementedError If join != ‘left’","["">>> df = pd.DataFrame({'A': [1, 2, 3],\n...                    'B': [400, 500, 600]})\n>>> new_df = pd.DataFrame({'B': [4, 5, 6],\n...                        'C': [7, 8, 9]})\n>>> df.update(new_df)\n>>> df\n   A  B\n0  1  4\n1  2  5\n2  3  6"", "">>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n...                    'B': ['x', 'y', 'z']})\n>>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n>>> df.update(new_df)\n>>> df\n   A  B\n0  a  d\n1  b  e\n2  c  f"", "">>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n...                    'B': ['x', 'y', 'z']})\n>>> new_df = pd.DataFrame({'B': ['d', 'f']}, index=[0, 2])\n>>> df.update(new_df)\n>>> df\n   A  B\n0  a  d\n1  b  y\n2  c  f"", "">>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n...                    'B': ['x', 'y', 'z']})\n>>> new_column = pd.Series(['d', 'e', 'f'], name='B')\n>>> df.update(new_column)\n>>> df\n   A  B\n0  a  d\n1  b  e\n2  c  f"", "">>> df = pd.DataFrame({'A': [1, 2, 3],\n...                    'B': [400., 500., 600.]})\n>>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n>>> df.update(new_df)\n>>> df\n   A      B\n0  1    4.0\n1  2  500.0\n2  3    6.0""]"
501,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.calendar.html,pandas.tseries.offsets.CustomBusinessHour.calendar,CustomBusinessHour.calendar#,No parameters found,[]
502,..\pandas\reference\api\pandas.Timedelta.value.html,pandas.Timedelta.value,Timedelta.value#,No parameters found,[]
503,..\pandas\reference\api\pandas.core.window.ewm.ExponentialMovingWindow.sum.html,pandas.core.window.ewm.ExponentialMovingWindow.sum,"ExponentialMovingWindow.sum(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the ewm (exponential weighted moment) sum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 2, 3, 4])\n>>> ser.ewm(alpha=.2).sum()\n0    1.000\n1    2.800\n2    5.240\n3    8.192\ndtype: float64']"
504,..\pandas\reference\api\pandas.core.window.ewm.ExponentialMovingWindow.var.html,pandas.core.window.ewm.ExponentialMovingWindow.var,"ExponentialMovingWindow.var(bias=False, numeric_only=False)[source]# Calculate the ewm (exponential weighted moment) variance.","Parameters: biasbool, default FalseUse a standard estimation bias correction. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 2, 3, 4])\n>>> ser.ewm(alpha=.2).var()\n0         NaN\n1    0.500000\n2    0.991803\n3    1.631547\ndtype: float64']"
505,..\pandas\reference\api\pandas.Series.dt.time.html,pandas.Series.dt.time,Series.dt.time[source]# Returns numpy array of datetime.time objects. The time part of the Timestamps.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.time\n0    10:00:00\n1    11:00:00\ndtype: object', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.time\narray([datetime.time(10, 0), datetime.time(11, 0)], dtype=object)']"
506,..\pandas\reference\api\pandas.io.formats.style.Styler.loader.html,pandas.io.formats.style.Styler.loader,Styler.loader = <jinja2.loaders.PackageLoader object>#,No parameters found,[]
507,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.copy.html,pandas.tseries.offsets.CustomBusinessHour.copy,CustomBusinessHour.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
508,..\pandas\reference\api\pandas.Timedelta.view.html,pandas.Timedelta.view,Timedelta.view(dtype)# Array view compatibility.,Parameters: dtypestr or dtypeThe dtype to view the underlying data as.,"["">>> td = pd.Timedelta('3D')\n>>> td\nTimedelta('3 days 00:00:00')\n>>> td.view(int)\n259200000000000""]"
509,..\pandas\reference\api\pandas.DataFrame.values.html,pandas.DataFrame.values,"property DataFrame.values[source]# Return a Numpy representation of the DataFrame. Warning We recommend using DataFrame.to_numpy() instead. Only the values in the DataFrame will be returned, the axes labels will be removed. Notes The dtype will be a lower-common-denominator dtype (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. Use this with care if you are not dealing with the blocks. e.g. If the dtypes are float16 and float32, dtype will be upcast to float32.  If dtypes are int32 and uint8, dtype will be upcast to int32. By numpy.find_common_type() convention, mixing int64 and uint64 will result in a float64 dtype.",Returns: numpy.ndarrayThe values of the DataFrame.,"["">>> df = pd.DataFrame({'age':    [ 3,  29],\n...                    'height': [94, 170],\n...                    'weight': [31, 115]})\n>>> df\n   age  height  weight\n0    3      94      31\n1   29     170     115\n>>> df.dtypes\nage       int64\nheight    int64\nweight    int64\ndtype: object\n>>> df.values\narray([[  3,  94,  31],\n       [ 29, 170, 115]])"", "">>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n...                     ('lion',     80.5, 1),\n...                     ('monkey', np.nan, None)],\n...                   columns=('name', 'max_speed', 'rank'))\n>>> df2.dtypes\nname          object\nmax_speed    float64\nrank          object\ndtype: object\n>>> df2.values\narray([['parrot', 24.0, 'second'],\n       ['lion', 80.5, 1],\n       ['monkey', nan, None]], dtype=object)""]"
510,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.html,pandas.tseries.offsets.MonthEnd,class pandas.tseries.offsets.MonthEnd# DateOffset of one month end. MonthEnd goes to the next date which is an end of the month. Examples If you want to get the end of the current month: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range.","["">>> ts = pd.Timestamp(2022, 1, 30)\n>>> ts + pd.offsets.MonthEnd()\nTimestamp('2022-01-31 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 31)\n>>> ts + pd.offsets.MonthEnd()\nTimestamp('2022-02-28 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 31)\n>>> pd.offsets.MonthEnd().rollforward(ts)\nTimestamp('2022-01-31 00:00:00')""]"
511,..\pandas\reference\api\pandas.api.types.is_iterator.html,pandas.api.types.is_iterator,"pandas.api.types.is_iterator(obj)# Check if the object is an iterator. This is intended for generators, not list-like objects.",Parameters: objThe object to check Returns: is_iterboolWhether obj is an iterator.,"['>>> import datetime\n>>> from pandas.api.types import is_iterator\n>>> is_iterator((x for x in []))\nTrue\n>>> is_iterator([1, 2, 3])\nFalse\n>>> is_iterator(datetime.datetime(2017, 1, 1))\nFalse\n>>> is_iterator(""foo"")\nFalse\n>>> is_iterator(1)\nFalse']"
512,..\pandas\reference\api\pandas.core.window.expanding.Expanding.aggregate.html,pandas.core.window.expanding.Expanding.aggregate,"Expanding.aggregate(func, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a Series/Dataframe or when passed to Series/Dataframe.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","['>>> df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6], ""C"": [7, 8, 9]})\n>>> df\n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9', '>>> df.ewm(alpha=0.5).mean()\n          A         B         C\n0  1.000000  4.000000  7.000000\n1  1.666667  4.666667  7.666667\n2  2.428571  5.428571  8.428571']"
513,..\pandas\reference\api\pandas.Series.dt.timetz.html,pandas.Series.dt.timetz,Series.dt.timetz[source]# Returns numpy array of datetime.time objects with timezones. The time part of the Timestamps.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.timetz\n0    10:00:00+00:00\n1    11:00:00+00:00\ndtype: object', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.timetz\narray([datetime.time(10, 0, tzinfo=datetime.timezone.utc),\ndatetime.time(11, 0, tzinfo=datetime.timezone.utc)], dtype=object)']"
514,..\pandas\reference\api\pandas.core.window.expanding.Expanding.apply.html,pandas.core.window.expanding.Expanding.apply,"Expanding.apply(func, raw=False, engine=None, engine_kwargs=None, args=None, kwargs=None)[source]# Calculate the expanding custom aggregation function.","Parameters: funcfunctionMust produce a single value from an ndarray input if raw=True or a single value from a Series if raw=False. Can also accept a Numba JIT function with engine='numba' specified. rawbool, default False False : passes each row or column as a Series to the function. True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. enginestr, default None 'cython' : Runs rolling apply through C-extensions from cython. 'numba' : Runs rolling apply through JIT compiled code from numba. Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply rolling aggregation. argstuple, default NonePositional arguments to be passed into func. kwargsdict, default NoneKeyword arguments to be passed into func. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().apply(lambda s: s.max() - 2 * s.min())\na   -1.0\nb    0.0\nc    1.0\nd    2.0\ndtype: float64""]"
515,..\pandas\reference\api\pandas.api.types.is_list_like.html,pandas.api.types.is_list_like,"pandas.api.types.is_list_like(obj, allow_sets=True)# Check if the object is list-like. Objects that are considered list-like are for example Python lists, tuples, sets, NumPy arrays, and Pandas Series. Strings and datetime objects, however, are not considered list-like.","Parameters: objobjectObject to check. allow_setsbool, default TrueIf this parameter is False, sets will not be considered list-like. Returns: boolWhether obj has list-like properties.","['>>> import datetime\n>>> from pandas.api.types import is_list_like\n>>> is_list_like([1, 2, 3])\nTrue\n>>> is_list_like({1, 2, 3})\nTrue\n>>> is_list_like(datetime.datetime(2017, 1, 1))\nFalse\n>>> is_list_like(""foo"")\nFalse\n>>> is_list_like(1)\nFalse\n>>> is_list_like(np.array([2]))\nTrue\n>>> is_list_like(np.array(2))\nFalse']"
516,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_anchored.html,pandas.tseries.offsets.MonthEnd.is_anchored,MonthEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
517,..\pandas\reference\api\pandas.io.formats.style.Styler.map.html,pandas.io.formats.style.Styler.map,"Styler.map(func, subset=None, **kwargs)[source]# Apply a CSS-styling function elementwise. Updates the HTML representation with the result. Notes The elements of the output of func should be CSS styles as strings, in the format ‘attribute: value; attribute2: value2; …’ or, if nothing is to be applied to that element, an empty string or None.","Parameters: funcfunctionfunc should take a scalar and return a string. subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. **kwargsdictPass along to func. Returns: Styler","['>>> def color_negative(v, color):\n...     return f""color: {color};"" if v < 0 else None\n>>> df = pd.DataFrame(np.random.randn(5, 2), columns=[""A"", ""B""])\n>>> df.style.map(color_negative, color=\'red\')', '>>> df.style.map(color_negative, color=\'red\', subset=""A"")\n...  \n>>> df.style.map(color_negative, color=\'red\', subset=[""A"", ""B""])\n...', '>>> df.style.map(color_negative, color=\'red\',\n...  subset=([0,1,2], slice(None)))  \n>>> df.style.map(color_negative, color=\'red\', subset=(slice(0,5,2), ""A""))\n...']"
518,..\pandas\reference\api\pandas.DataFrame.value_counts.html,pandas.DataFrame.value_counts,"DataFrame.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)[source]# Return a Series containing the frequency of each distinct row in the Dataframe. Notes The returned Series will have a MultiIndex with one level per input column but an Index (non-multi) for a single label. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row.","Parameters: subsetlabel or list of labels, optionalColumns to use when counting unique combinations. normalizebool, default FalseReturn proportions rather than frequencies. sortbool, default TrueSort by frequencies when True. Sort by DataFrame column values when False. ascendingbool, default FalseSort in ascending order. dropnabool, default TrueDon’t include counts of rows that contain NA values. Added in version 1.3.0. Returns: Series","["">>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n...                    'num_wings': [2, 0, 0, 0]},\n...                   index=['falcon', 'dog', 'cat', 'ant'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0\ncat            4          0\nant            6          0"", '>>> df.value_counts()\nnum_legs  num_wings\n4         0            2\n2         2            1\n6         0            1\nName: count, dtype: int64', '>>> df.value_counts(sort=False)\nnum_legs  num_wings\n2         2            1\n4         0            2\n6         0            1\nName: count, dtype: int64', '>>> df.value_counts(ascending=True)\nnum_legs  num_wings\n2         2            1\n6         0            1\n4         0            2\nName: count, dtype: int64', '>>> df.value_counts(normalize=True)\nnum_legs  num_wings\n4         0            0.50\n2         2            0.25\n6         0            0.25\nName: proportion, dtype: float64', "">>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n>>> df\n  first_name middle_name\n0       John       Smith\n1       Anne        <NA>\n2       John        <NA>\n3       Beth      Louise"", '>>> df.value_counts()\nfirst_name  middle_name\nBeth        Louise         1\nJohn        Smith          1\nName: count, dtype: int64', '>>> df.value_counts(dropna=False)\nfirst_name  middle_name\nAnne        NaN            1\nBeth        Louise         1\nJohn        Smith          1\n            NaN            1\nName: count, dtype: int64', '>>> df.value_counts(""first_name"")\nfirst_name\nJohn    2\nAnne    1\nBeth    1\nName: count, dtype: int64']"
519,..\pandas\reference\api\pandas.TimedeltaIndex.as_unit.html,pandas.TimedeltaIndex.as_unit,TimedeltaIndex.as_unit(unit)[source]# Convert to a dtype with the given unit resolution.,"Parameters: unit{‘s’, ‘ms’, ‘us’, ‘ns’} Returns: same type as self","["">>> idx = pd.DatetimeIndex(['2020-01-02 01:02:03.004005006'])\n>>> idx\nDatetimeIndex(['2020-01-02 01:02:03.004005006'],\n              dtype='datetime64[ns]', freq=None)\n>>> idx.as_unit('s')\nDatetimeIndex(['2020-01-02 01:02:03'], dtype='datetime64[s]', freq=None)"", "">>> tdelta_idx = pd.to_timedelta(['1 day 3 min 2 us 42 ns'])\n>>> tdelta_idx\nTimedeltaIndex(['1 days 00:03:00.000002042'],\n                dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.as_unit('s')\nTimedeltaIndex(['1 days 00:03:00'], dtype='timedelta64[s]', freq=None)""]"
520,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.end.html,pandas.tseries.offsets.CustomBusinessHour.end,CustomBusinessHour.end#,No parameters found,[]
521,..\pandas\reference\api\pandas.Series.dt.total_seconds.html,pandas.Series.dt.total_seconds,"Series.dt.total_seconds(*args, **kwargs)[source]# Return total duration of each element expressed in seconds. This method is available directly on TimedeltaArray, TimedeltaIndex and on Series containing timedelta values under the .dt namespace.","Returns: ndarray, Index or SeriesWhen the calling object is a TimedeltaArray, the return type is ndarray.  When the calling object is a TimedeltaIndex, the return type is an Index with a float64 dtype. When the calling object is a Series, the return type is Series of type float64 whose index is the same as the original.","["">>> s = pd.Series(pd.to_timedelta(np.arange(5), unit='d'))\n>>> s\n0   0 days\n1   1 days\n2   2 days\n3   3 days\n4   4 days\ndtype: timedelta64[ns]"", '>>> s.dt.total_seconds()\n0         0.0\n1     86400.0\n2    172800.0\n3    259200.0\n4    345600.0\ndtype: float64', "">>> idx = pd.to_timedelta(np.arange(5), unit='d')\n>>> idx\nTimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],\n               dtype='timedelta64[ns]', freq=None)"", "">>> idx.total_seconds()\nIndex([0.0, 86400.0, 172800.0, 259200.0, 345600.0], dtype='float64')""]"
522,..\pandas\reference\api\pandas.core.window.expanding.Expanding.corr.html,pandas.core.window.expanding.Expanding.corr,"Expanding.corr(other=None, pairwise=None, ddof=1, numeric_only=False)[source]# Calculate the expanding correlation. Notes This function uses Pearson’s definition of correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). When other is not specified, the output will be self correlation (e.g. all 1’s), except for DataFrame inputs with pairwise set to True. Function will return NaN for correlations of equal valued sequences; this is the result of a 0/0 division error. When pairwise is set to False, only matching columns between self and other will be used. When pairwise is set to True, the output will be a MultiIndex DataFrame with the original index on the first level, and the other DataFrame columns on the second level. In the case of missing elements, only complete pairwise observations will be used.","Parameters: otherSeries or DataFrame, optionalIf not supplied then will default to self and produce pairwise output. pairwisebool, default NoneIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser2 = pd.Series([10, 11, 13, 16], index=['a', 'b', 'c', 'd'])\n>>> ser1.expanding().corr(ser2)\na         NaN\nb    1.000000\nc    0.981981\nd    0.975900\ndtype: float64""]"
523,..\pandas\reference\api\pandas.api.types.is_named_tuple.html,pandas.api.types.is_named_tuple,pandas.api.types.is_named_tuple(obj)[source]# Check if the object is a named tuple.,Parameters: objThe object to check Returns: boolWhether obj is a named tuple.,"['>>> from collections import namedtuple\n>>> from pandas.api.types import is_named_tuple\n>>> Point = namedtuple(""Point"", [""x"", ""y""])\n>>> p = Point(1, 2)\n>>>\n>>> is_named_tuple(p)\nTrue\n>>> is_named_tuple((1, 2))\nFalse']"
524,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_month_end.html,pandas.tseries.offsets.MonthEnd.is_month_end,MonthEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
525,..\pandas\reference\api\pandas.core.window.expanding.Expanding.count.html,pandas.core.window.expanding.Expanding.count,Expanding.count(numeric_only=False)[source]# Calculate the expanding count of non NaN observations.,Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.,"["">>> ser = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().count()\na    1.0\nb    2.0\nc    3.0\nd    4.0\ndtype: float64""]"
526,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.freqstr.html,pandas.tseries.offsets.CustomBusinessHour.freqstr,CustomBusinessHour.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
527,..\pandas\reference\api\pandas.api.types.is_number.html,pandas.api.types.is_number,"pandas.api.types.is_number(obj)[source]# Check if the object is a number. Returns True when the object is a number, and False if is not.",Parameters: objany typeThe object to check if is a number. Returns: boolWhether obj is a number or not.,"['>>> from pandas.api.types import is_number\n>>> is_number(1)\nTrue\n>>> is_number(7.15)\nTrue', '>>> is_number(False)\nTrue', '>>> is_number(""foo"")\nFalse\n>>> is_number(""5"")\nFalse']"
528,..\pandas\reference\api\pandas.DataFrame.var.html,pandas.DataFrame.var,"DataFrame.var(axis=0, skipna=True, ddof=1, numeric_only=False, **kwargs)[source]# Return unbiased variance over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument.","Parameters: axis{index (0), columns (1)}For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.var with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. Returns: Series or DataFrame (if level specified)","["">>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                    'age': [21, 25, 62, 43],\n...                    'height': [1.61, 1.87, 1.49, 2.01]}\n...                   ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01"", '>>> df.var()\nage       352.916667\nheight      0.056367\ndtype: float64', '>>> df.var(ddof=0)\nage       264.687500\nheight      0.042275\ndtype: float64']"
529,..\pandas\reference\api\pandas.Series.dt.to_period.html,pandas.Series.dt.to_period,"Series.dt.to_period(*args, **kwargs)[source]# Cast to PeriodArray/PeriodIndex at a particular frequency. Converts DatetimeArray/Index to PeriodArray/PeriodIndex.","Parameters: freqstr or Period, optionalOne of pandas’ period aliases or an Period object. Will be inferred by default. Returns: PeriodArray/PeriodIndex Raises: ValueErrorWhen converting a DatetimeArray/Index with non-regular values, so that a frequency cannot be inferred.","['>>> df = pd.DataFrame({""y"": [1, 2, 3]},\n...                   index=pd.to_datetime([""2000-03-31 00:00:00"",\n...                                         ""2000-05-31 00:00:00"",\n...                                         ""2000-08-31 00:00:00""]))\n>>> df.index.to_period(""M"")\nPeriodIndex([\'2000-03\', \'2000-05\', \'2000-08\'],\n            dtype=\'period[M]\')', '>>> idx = pd.date_range(""2017-01-01"", periods=2)\n>>> idx.to_period()\nPeriodIndex([\'2017-01-01\', \'2017-01-02\'],\n            dtype=\'period[D]\')']"
530,..\pandas\reference\api\pandas.TimedeltaIndex.ceil.html,pandas.TimedeltaIndex.ceil,"TimedeltaIndex.ceil(*args, **kwargs)[source]# Perform ceil operation on the data to the specified freq. Notes If the timestamps have a timezone, ceiling will take place relative to the local (“wall”) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to ceil the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.ceil('h')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 13:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.ceil(""h"")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 13:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 01:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.ceil(""h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.ceil(""h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
531,..\pandas\reference\api\pandas.io.formats.style.Styler.map_index.html,pandas.io.formats.style.Styler.map_index,"Styler.map_index(func, axis=0, level=None, **kwargs)[source]# Apply a CSS-styling function to the index or column headers, elementwise. Updates the HTML representation with the result. Added in version 1.4.0. Added in version 2.1.0: Styler.applymap_index was deprecated and renamed to Styler.map_index. Notes Each input to func will be an index value, if an Index, or a level value of a MultiIndex. The output of func should be CSS styles as a string, in the format ‘attribute: value; attribute2: value2; …’ or, if nothing is to be applied to that element, an empty string or None.","Parameters: funcfunctionfunc should take a scalar and return a string. axis{0, 1, “index”, “columns”}The headers over which to apply the function. levelint, str, list, optionalIf index is MultiIndex the level(s) over which to apply the function. **kwargsdictPass along to func. Returns: Styler","['>>> df = pd.DataFrame([[1,2], [3,4]], index=[""A"", ""B""])\n>>> def color_b(s):\n...     return ""background-color: yellow;"" if v == ""B"" else None\n>>> df.style.map_index(color_b)', '>>> midx = pd.MultiIndex.from_product([[\'ix\', \'jy\'], [0, 1], [\'x3\', \'z4\']])\n>>> df = pd.DataFrame([np.arange(8)], columns=midx)\n>>> def highlight_x(v):\n...     return ""background-color: yellow;"" if ""x"" in v else None\n>>> df.style.map_index(highlight_x, axis=""columns"", level=[0, 2])\n...']"
532,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_month_start.html,pandas.tseries.offsets.MonthEnd.is_month_start,MonthEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
533,..\pandas\reference\api\pandas.core.window.expanding.Expanding.cov.html,pandas.core.window.expanding.Expanding.cov,"Expanding.cov(other=None, pairwise=None, ddof=1, numeric_only=False)[source]# Calculate the expanding sample covariance.","Parameters: otherSeries or DataFrame, optionalIf not supplied then will default to self and produce pairwise output. pairwisebool, default NoneIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used. ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser1 = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser2 = pd.Series([10, 11, 13, 16], index=['a', 'b', 'c', 'd'])\n>>> ser1.expanding().cov(ser2)\na         NaN\nb    0.500000\nc    1.500000\nd    3.333333\ndtype: float64""]"
534,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.holidays.html,pandas.tseries.offsets.CustomBusinessHour.holidays,CustomBusinessHour.holidays#,No parameters found,[]
535,..\pandas\reference\api\pandas.core.window.expanding.Expanding.kurt.html,pandas.core.window.expanding.Expanding.kurt,Expanding.kurt(numeric_only=False)[source]# Calculate the expanding Fisher’s definition of kurtosis without bias. Notes A minimum of four periods is required for the calculation.,"Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> arr = [1, 2, 3, 4, 999]\n>>> import scipy.stats\n>>> print(f""{scipy.stats.kurtosis(arr[:-1], bias=False):.6f}"")\n-1.200000\n>>> print(f""{scipy.stats.kurtosis(arr, bias=False):.6f}"")\n4.999874\n>>> s = pd.Series(arr)\n>>> s.expanding(4).kurt()\n0         NaN\n1         NaN\n2         NaN\n3   -1.200000\n4    4.999874\ndtype: float64']"
536,..\pandas\reference\api\pandas.io.formats.style.Styler.pipe.html,pandas.io.formats.style.Styler.pipe,"Styler.pipe(func, *args, **kwargs)[source]# Apply func(self, *args, **kwargs), and return the result. Notes Like DataFrame.pipe(), this method can simplify the application of several user-defined functions to a styler.  Instead of writing: f(g(df.style.format(precision=3), arg1=a), arg2=b, arg3=c) users can write: (df.style.format(precision=3)    .pipe(g, arg1=a)    .pipe(f, arg2=b, arg3=c)) In particular, this allows users to define functions that take a styler object, along with other parameters, and return the styler after making styling changes (such as calling Styler.apply() or Styler.set_properties()).","Parameters: funcfunctionFunction to apply to the Styler.  Alternatively, a (callable, keyword) tuple where keyword is a string indicating the keyword of callable that expects the Styler. *argsoptionalArguments passed to func. **kwargsoptionalA dictionary of keyword arguments passed into func. Returns: objectThe value returned by func.","['>>> def some_highlights(styler, min_color=""red"", max_color=""blue""):\n...      styler.highlight_min(color=min_color, axis=None)\n...      styler.highlight_max(color=max_color, axis=None)\n...      styler.highlight_null()\n...      return styler\n>>> df = pd.DataFrame([[1, 2, 3, pd.NA], [pd.NA, 4, 5, 6]], dtype=""Int64"")\n>>> df.style.pipe(some_highlights, min_color=""green"")', '>>> (df.style.format(""{:.1f}"")\n...         .pipe(some_highlights, min_color=""green"")\n...         .highlight_between(left=2, right=5))', '>>> def highlight_last_level(styler):\n...     return styler.apply_index(\n...         lambda v: ""background-color: pink; color: yellow"", axis=""columns"",\n...         level=styler.columns.nlevels-1\n...     )  \n>>> df.columns = pd.MultiIndex.from_product([[""A"", ""B""], [""X"", ""Y""]])\n>>> df.style.pipe(highlight_last_level)', '>>> def highlight_header_missing(styler, level):\n...     def dynamic_highlight(s):\n...         return np.where(\n...             styler.data.isna().any(), ""background-color: red;"", """"\n...         )\n...     return styler.apply_index(dynamic_highlight, axis=1, level=level)\n>>> df.style.pipe(highlight_header_missing, level=1)']"
537,..\pandas\reference\api\pandas.TimedeltaIndex.components.html,pandas.TimedeltaIndex.components,"property TimedeltaIndex.components[source]# Return a DataFrame of the individual resolution components of the Timedeltas. The components (days, hours, minutes seconds, milliseconds, microseconds, nanoseconds) are returned as columns in a DataFrame.",Returns: DataFrame,"["">>> tdelta_idx = pd.to_timedelta(['1 day 3 min 2 us 42 ns'])\n>>> tdelta_idx\nTimedeltaIndex(['1 days 00:03:00.000002042'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.components\n   days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\n0     1      0        3        0             0             2           42""]"
538,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_on_offset.html,pandas.tseries.offsets.MonthEnd.is_on_offset,MonthEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
539,..\pandas\reference\api\pandas.api.types.is_numeric_dtype.html,pandas.api.types.is_numeric_dtype,pandas.api.types.is_numeric_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of a numeric dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of a numeric dtype.,"["">>> from pandas.api.types import is_numeric_dtype\n>>> is_numeric_dtype(str)\nFalse\n>>> is_numeric_dtype(int)\nTrue\n>>> is_numeric_dtype(float)\nTrue\n>>> is_numeric_dtype(np.uint64)\nTrue\n>>> is_numeric_dtype(np.datetime64)\nFalse\n>>> is_numeric_dtype(np.timedelta64)\nFalse\n>>> is_numeric_dtype(np.array(['a', 'b']))\nFalse\n>>> is_numeric_dtype(pd.Series([1, 2]))\nTrue\n>>> is_numeric_dtype(pd.Index([1, 2.]))\nTrue\n>>> is_numeric_dtype(np.array([], dtype=np.timedelta64))\nFalse""]"
540,..\pandas\reference\api\pandas.Series.dt.to_pydatetime.html,pandas.Series.dt.to_pydatetime,"Series.dt.to_pydatetime()[source]# Return the data as an array of datetime.datetime objects. Deprecated since version 2.1.0: The current behavior of dt.to_pydatetime is deprecated. In a future version this will return a Series containing python datetime objects instead of a ndarray. Timezone information is retained if present. Warning Python’s datetime uses microsecond resolution, which is lower than pandas (nanosecond). The values are truncated.",Returns: numpy.ndarrayObject dtype array containing native Python datetime objects.,"["">>> s = pd.Series(pd.date_range('20180310', periods=2))\n>>> s\n0   2018-03-10\n1   2018-03-11\ndtype: datetime64[ns]"", '>>> s.dt.to_pydatetime()\narray([datetime.datetime(2018, 3, 10, 0, 0),\n       datetime.datetime(2018, 3, 11, 0, 0)], dtype=object)', "">>> s = pd.Series(pd.date_range('20180310', periods=2, freq='ns'))\n>>> s\n0   2018-03-10 00:00:00.000000000\n1   2018-03-10 00:00:00.000000001\ndtype: datetime64[ns]"", '>>> s.dt.to_pydatetime()\narray([datetime.datetime(2018, 3, 10, 0, 0),\n       datetime.datetime(2018, 3, 10, 0, 0)], dtype=object)']"
541,..\pandas\reference\api\pandas.DataFrame.where.html,pandas.DataFrame.where,"DataFrame.where(cond, other=nan, *, inplace=False, axis=None, level=None)[source]# Replace values where the condition is False. Notes The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with False. The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2). For further details and examples see the where documentation in indexing. The dtype of the object takes precedence. The fill value is casted to the object’s dtype, if this can be done losslessly.","Parameters: condbool Series/DataFrame, array-like, or callableWhere cond is True, keep the original value. Where False, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn’t check it). otherscalar, Series/DataFrame, or callableEntries where cond is False are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn’t check it). If not specified, entries will be filled with the corresponding NULL value (np.nan for numpy dtypes, pd.NA for extension dtypes). inplacebool, default FalseWhether to perform the operation in place on the data. axisint, default NoneAlignment axis if needed. For Series this parameter is unused and defaults to 0. levelint, default NoneAlignment level if needed. Returns: Same type as caller or None if inplace=True.","['>>> s = pd.Series(range(5))\n>>> s.where(s > 0)\n0    NaN\n1    1.0\n2    2.0\n3    3.0\n4    4.0\ndtype: float64\n>>> s.mask(s > 0)\n0    0.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', '>>> s = pd.Series(range(5))\n>>> t = pd.Series([True, False])\n>>> s.where(t, 99)\n0     0\n1    99\n2    99\n3    99\n4    99\ndtype: int64\n>>> s.mask(t, 99)\n0    99\n1     1\n2    99\n3    99\n4    99\ndtype: int64', '>>> s.where(s > 1, 10)\n0    10\n1    10\n2    2\n3    3\n4    4\ndtype: int64\n>>> s.mask(s > 1, 10)\n0     0\n1     1\n2    10\n3    10\n4    10\ndtype: int64', "">>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n>>> df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n>>> m = df % 3 == 0\n>>> df.where(m, -df)\n   A  B\n0  0 -1\n1 -2  3\n2 -4 -5\n3  6 -7\n4 -8  9\n>>> df.where(m, -df) == np.where(m, df, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n>>> df.where(m, -df) == df.mask(~m, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True""]"
542,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.html,pandas.tseries.offsets.CustomBusinessHour,"class pandas.tseries.offsets.CustomBusinessHour# DateOffset subclass representing possibly n custom business days. In CustomBusinessHour we can use custom weekmask, holidays, and calendar. Examples In the example below the default parameters give the next business hour. We can also change the start and the end of business hours. You can divide your business day hours into several parts. Business days can be specified by weekmask parameter. To convert the returned datetime object to its string representation the function strftime() is used in the next example. Using NumPy business day calendar you can define custom holidays. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. calendar end freqstr Return a string representing the frequency. holidays kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos next_bday Used for moving to next business day. normalize offset Alias for self._offset. rule_code start weekmask","Parameters: nint, default 1The number of hours represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekmaskstr, Default ‘Mon Tue Wed Thu Fri’Weekmask of valid business days, passed to numpy.busdaycalendar. holidayslistList/array of dates to exclude from the set of valid business days, passed to numpy.busdaycalendar. calendarnp.busdaycalendarCalendar to integrate. startstr, time, or list of str/time, default “09:00”Start time of your custom business hour in 24h format. endstr, time, or list of str/time, default: “17:00”End time of your custom business hour in 24h format. offsettimedelta, default timedelta(0)Time offset to apply.","["">>> ts = pd.Timestamp(2022, 8, 5, 16)\n>>> ts + pd.offsets.CustomBusinessHour()\nTimestamp('2022-08-08 09:00:00')"", '>>> ts = pd.Timestamp(2022, 8, 5, 16)\n>>> ts + pd.offsets.CustomBusinessHour(start=""11:00"")\nTimestamp(\'2022-08-08 11:00:00\')', "">>> from datetime import time as dt_time\n>>> ts = pd.Timestamp(2022, 8, 5, 16)\n>>> ts + pd.offsets.CustomBusinessHour(end=dt_time(19, 0))\nTimestamp('2022-08-05 17:00:00')"", "">>> ts = pd.Timestamp(2022, 8, 5, 22)\n>>> ts + pd.offsets.CustomBusinessHour(end=dt_time(19, 0))\nTimestamp('2022-08-08 10:00:00')"", '>>> import datetime as dt\n>>> freq = pd.offsets.CustomBusinessHour(start=[""06:00"", ""10:00"", ""15:00""],\n...                                      end=[""08:00"", ""12:00"", ""17:00""])\n>>> pd.date_range(dt.datetime(2022, 12, 9), dt.datetime(2022, 12, 13), freq=freq)\nDatetimeIndex([\'2022-12-09 06:00:00\', \'2022-12-09 07:00:00\',\n               \'2022-12-09 10:00:00\', \'2022-12-09 11:00:00\',\n               \'2022-12-09 15:00:00\', \'2022-12-09 16:00:00\',\n               \'2022-12-12 06:00:00\', \'2022-12-12 07:00:00\',\n               \'2022-12-12 10:00:00\', \'2022-12-12 11:00:00\',\n               \'2022-12-12 15:00:00\', \'2022-12-12 16:00:00\'],\n               dtype=\'datetime64[ns]\', freq=\'cbh\')', '>>> import datetime as dt\n>>> freq = pd.offsets.CustomBusinessHour(weekmask=""Mon Wed Fri"",\n...                                      start=""10:00"", end=""13:00"")\n>>> pd.date_range(dt.datetime(2022, 12, 10), dt.datetime(2022, 12, 18),\n...               freq=freq).strftime(\'%a %d %b %Y %H:%M\')\nIndex([\'Mon 12 Dec 2022 10:00\', \'Mon 12 Dec 2022 11:00\',\n       \'Mon 12 Dec 2022 12:00\', \'Wed 14 Dec 2022 10:00\',\n       \'Wed 14 Dec 2022 11:00\', \'Wed 14 Dec 2022 12:00\',\n       \'Fri 16 Dec 2022 10:00\', \'Fri 16 Dec 2022 11:00\',\n       \'Fri 16 Dec 2022 12:00\'],\n       dtype=\'object\')', '>>> import datetime as dt\n>>> bdc = np.busdaycalendar(holidays=[\'2022-12-12\', \'2022-12-14\'])\n>>> freq = pd.offsets.CustomBusinessHour(calendar=bdc, start=""10:00"", end=""13:00"")\n>>> pd.date_range(dt.datetime(2022, 12, 10), dt.datetime(2022, 12, 18), freq=freq)\nDatetimeIndex([\'2022-12-13 10:00:00\', \'2022-12-13 11:00:00\',\n               \'2022-12-13 12:00:00\', \'2022-12-15 10:00:00\',\n               \'2022-12-15 11:00:00\', \'2022-12-15 12:00:00\',\n               \'2022-12-16 10:00:00\', \'2022-12-16 11:00:00\',\n               \'2022-12-16 12:00:00\'],\n               dtype=\'datetime64[ns]\', freq=\'cbh\')']"
543,..\pandas\reference\api\pandas.core.window.expanding.Expanding.max.html,pandas.core.window.expanding.Expanding.max,"Expanding.max(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding maximum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([3, 2, 1, 4], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().max()\na    3.0\nb    3.0\nc    3.0\nd    4.0\ndtype: float64""]"
544,..\pandas\reference\api\pandas.io.formats.style.Styler.relabel_index.html,pandas.io.formats.style.Styler.relabel_index,"Styler.relabel_index(labels, axis=0, level=None)[source]# Relabel the index, or column header, keys to display a set of specified values. Added in version 1.5.0. Notes As part of Styler, this method allows the display of an index to be completely user-specified without affecting the underlying DataFrame data, index, or column headers. This means that the flexibility of indexing is maintained whilst the final display is customisable. Since Styler is designed to be progressively constructed with method chaining, this method is adapted to react to the currently specified hidden elements. This is useful because it means one does not have to specify all the new labels if the majority of an index, or column headers, have already been hidden. The following produce equivalent display (note the length of labels in each case). # relabel first, then hide df = pd.DataFrame({""col"": [""a"", ""b"", ""c""]}) df.style.relabel_index([""A"", ""B"", ""C""]).hide([0,1]) # hide first, then relabel df = pd.DataFrame({""col"": [""a"", ""b"", ""c""]}) df.style.hide([0,1]).relabel_index([""C""]) This method should be used, rather than Styler.format_index(), in one of the following cases (see examples): A specified set of labels are required which are not a function of the underlying index keys. The function of the underlying index keys requires a counter variable, such as those available upon enumeration.","Parameters: labelslist-like or IndexNew labels to display. Must have same length as the underlying values not hidden. axis{“index”, 0, “columns”, 1}Apply to the index or columns. levelint, str, list, optionalThe level(s) over which to apply the new labels. If None will apply to all levels of an Index or MultiIndex which are not hidden. Returns: Styler","['>>> df = pd.DataFrame({""col"": [""a"", ""b"", ""c""]})\n>>> df.style.relabel_index([""A"", ""B"", ""C""])  \n     col\nA      a\nB      b\nC      c', '>>> df.style.hide([0,1]).relabel_index([""C""])  \n     col\nC      c', '>>> midx = pd.MultiIndex.from_product([[0, 1], [0, 1], [0, 1]])\n>>> df = pd.DataFrame({""col"": list(range(8))}, index=midx)\n>>> styler = df.style  \n          col\n0  0  0     0\n      1     1\n   1  0     2\n      1     3\n1  0  0     4\n      1     5\n   1  0     6\n      1     7\n>>> styler.hide((midx.get_level_values(0)==0)|(midx.get_level_values(1)==0))\n...  \n>>> styler.hide(level=[0,1])  \n>>> styler.relabel_index([""binary6"", ""binary7""])  \n          col\nbinary6     6\nbinary7     7', '>>> styler = df.loc[[(1,1,0), (1,1,1)]].style\n>>> styler.hide(level=[0,1]).relabel_index([""binary6"", ""binary7""])\n...  \n          col\nbinary6     6\nbinary7     7', '>>> df = pd.DataFrame({""samples"": np.random.rand(10)})\n>>> styler = df.loc[np.random.randint(0,10,3)].style\n>>> styler.relabel_index([f""sample{i+1} ({{}})"" for i in range(3)])\n...  \n                 samples\nsample1 (5)     0.315811\nsample2 (0)     0.495941\nsample3 (2)     0.067946']"
545,..\pandas\reference\api\pandas.DataFrame.xs.html,pandas.DataFrame.xs,"DataFrame.xs(key, axis=0, level=None, drop_level=True)[source]# Return cross-section from the Series/DataFrame. This method takes a key argument to select data at a particular level of a MultiIndex. Notes xs can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see MultiIndex Slicers.","Parameters: keylabel or tuple of labelLabel contained in the index, or partially in a MultiIndex. axis{0 or ‘index’, 1 or ‘columns’}, default 0Axis to retrieve cross-section on. levelobject, defaults to first n levels (n=1 or len(key))In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position. drop_levelbool, default TrueIf False, returns object with same levels as self. Returns: Series or DataFrameCross-section from the original Series or DataFrame corresponding to the selected index levels.","["">>> d = {'num_legs': [4, 4, 2, 2],\n...      'num_wings': [0, 0, 2, 2],\n...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n>>> df = pd.DataFrame(data=d)\n>>> df = df.set_index(['class', 'animal', 'locomotion'])\n>>> df\n                           num_legs  num_wings\nclass  animal  locomotion\nmammal cat     walks              4          0\n       dog     walks              4          0\n       bat     flies              2          2\nbird   penguin walks              2          2"", "">>> df.xs('mammal')\n                   num_legs  num_wings\nanimal locomotion\ncat    walks              4          0\ndog    walks              4          0\nbat    flies              2          2"", "">>> df.xs(('mammal', 'dog', 'walks'))\nnum_legs     4\nnum_wings    0\nName: (mammal, dog, walks), dtype: int64"", "">>> df.xs('cat', level=1)\n                   num_legs  num_wings\nclass  locomotion\nmammal walks              4          0"", "">>> df.xs(('bird', 'walks'),\n...       level=[0, 'locomotion'])\n         num_legs  num_wings\nanimal\npenguin         2          2"", "">>> df.xs('num_wings', axis=1)\nclass   animal   locomotion\nmammal  cat      walks         0\n        dog      walks         0\n        bat      flies         2\nbird    penguin  walks         2\nName: num_wings, dtype: int64""]"
546,..\pandas\reference\api\pandas.TimedeltaIndex.days.html,pandas.TimedeltaIndex.days,property TimedeltaIndex.days[source]# Number of days for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='d'))\n>>> ser\n0   1 days\n1   2 days\n2   3 days\ndtype: timedelta64[ns]\n>>> ser.dt.days\n0    1\n1    2\n2    3\ndtype: int64"", '>>> tdelta_idx = pd.to_timedelta([""0 days"", ""10 days"", ""20 days""])\n>>> tdelta_idx\nTimedeltaIndex([\'0 days\', \'10 days\', \'20 days\'],\n                dtype=\'timedelta64[ns]\', freq=None)\n>>> tdelta_idx.days\nIndex([0, 10, 20], dtype=\'int64\')']"
547,..\pandas\reference\api\pandas.Series.dt.to_pytimedelta.html,pandas.Series.dt.to_pytimedelta,Series.dt.to_pytimedelta()[source]# Return an array of native datetime.timedelta objects. Python’s standard datetime library uses a different representation timedelta’s. This method converts a Series of pandas Timedeltas to datetime.timedelta format with the same length as the original Series.,Returns: numpy.ndarrayArray of 1D containing data with datetime.timedelta type.,"['>>> s = pd.Series(pd.to_timedelta(np.arange(5), unit=""d""))\n>>> s\n0   0 days\n1   1 days\n2   2 days\n3   3 days\n4   4 days\ndtype: timedelta64[ns]', '>>> s.dt.to_pytimedelta()\narray([datetime.timedelta(0), datetime.timedelta(days=1),\ndatetime.timedelta(days=2), datetime.timedelta(days=3),\ndatetime.timedelta(days=4)], dtype=object)']"
548,..\pandas\reference\api\pandas.api.types.is_object_dtype.html,pandas.api.types.is_object_dtype,pandas.api.types.is_object_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of the object dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of the object dtype.,"['>>> from pandas.api.types import is_object_dtype\n>>> is_object_dtype(object)\nTrue\n>>> is_object_dtype(int)\nFalse\n>>> is_object_dtype(np.array([], dtype=object))\nTrue\n>>> is_object_dtype(np.array([], dtype=int))\nFalse\n>>> is_object_dtype([1, 2, 3])\nFalse']"
549,..\pandas\reference\api\pandas.core.window.expanding.Expanding.mean.html,pandas.core.window.expanding.Expanding.mean,"Expanding.mean(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding mean. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().mean()\na    1.0\nb    1.5\nc    2.0\nd    2.5\ndtype: float64""]"
550,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_quarter_end.html,pandas.tseries.offsets.MonthEnd.is_quarter_end,MonthEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
551,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_anchored.html,pandas.tseries.offsets.CustomBusinessHour.is_anchored,CustomBusinessHour.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
552,..\pandas\reference\api\pandas.io.formats.style.Styler.set_caption.html,pandas.io.formats.style.Styler.set_caption,Styler.set_caption(caption)[source]# Set the text added to a <caption> HTML element.,"Parameters: captionstr, tuple, listFor HTML output either the string input is used or the first element of the tuple. For LaTeX the string input provides a caption and the additional tuple input allows for full captions and short captions, in that order. Returns: Styler","['>>> df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]})\n>>> df.style.set_caption(""test"")']"
553,..\pandas\reference\api\pandas.io.formats.style.Styler.set_properties.html,pandas.io.formats.style.Styler.set_properties,"Styler.set_properties(subset=None, **kwargs)[source]# Set defined CSS-properties to each <td> HTML element for the given subset. Notes This is a convenience methods which wraps the Styler.map() calling a function returning the CSS-properties independently of the data.","Parameters: subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. **kwargsdictA dictionary of property, value pairs to be set for each cell. Returns: Styler","['>>> df = pd.DataFrame(np.random.randn(10, 4))\n>>> df.style.set_properties(color=""white"", align=""right"")  \n>>> df.style.set_properties(**{\'background-color\': \'yellow\'})']"
554,..\pandas\reference\api\pandas.core.window.expanding.Expanding.median.html,pandas.core.window.expanding.Expanding.median,"Expanding.median(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding median. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().median()\na    1.0\nb    1.5\nc    2.0\nd    2.5\ndtype: float64""]"
555,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_quarter_start.html,pandas.tseries.offsets.MonthEnd.is_quarter_start,MonthEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
556,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_month_end.html,pandas.tseries.offsets.CustomBusinessHour.is_month_end,CustomBusinessHour.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
557,..\pandas\reference\api\pandas.DataFrame.__add__.html,pandas.DataFrame.__add__,"DataFrame.__add__(other)[source]# Get Addition of DataFrame and other, column-wise. Equivalent to DataFrame.add(other).","Parameters: otherscalar, sequence, Series, dict or DataFrameObject to be added to the DataFrame. Returns: DataFrameThe result of adding other to DataFrame.","["">>> df = pd.DataFrame({'height': [1.5, 2.6], 'weight': [500, 800]},\n...                   index=['elk', 'moose'])\n>>> df\n       height  weight\nelk       1.5     500\nmoose     2.6     800"", "">>> df[['height', 'weight']] + 1.5\n       height  weight\nelk       3.0   501.5\nmoose     4.1   801.5"", "">>> df[['height', 'weight']] + [0.5, 1.5]\n       height  weight\nelk       2.0   501.5\nmoose     3.1   801.5"", "">>> df[['height', 'weight']] + {'height': 0.5, 'weight': 1.5}\n       height  weight\nelk       2.0   501.5\nmoose     3.1   801.5"", "">>> s1 = pd.Series([0.5, 1.5], index=['weight', 'height'])\n>>> df[['height', 'weight']] + s1\n       height  weight\nelk       3.0   500.5\nmoose     4.1   800.5"", "">>> s2 = pd.Series([0.5, 1.5], index=['elk', 'moose'])\n>>> df[['height', 'weight']] + s2\n       elk  height  moose  weight\nelk    NaN     NaN    NaN     NaN\nmoose  NaN     NaN    NaN     NaN"", "">>> df[['height', 'weight']].add(s2, axis='index')\n       height  weight\nelk       2.0   500.5\nmoose     4.1   801.5"", "">>> other = pd.DataFrame({'height': [0.2, 0.4, 0.6]},\n...                      index=['elk', 'moose', 'deer'])\n>>> df[['height', 'weight']] + other\n       height  weight\ndeer      NaN     NaN\nelk       1.7     NaN\nmoose     3.0     NaN""]"
558,..\pandas\reference\api\pandas.Series.dt.tz.html,pandas.Series.dt.tz,Series.dt.tz[source]# Return the timezone.,"Returns: datetime.tzinfo, pytz.tzinfo.BaseTZInfo, dateutil.tz.tz.tzfile, or NoneReturns None when the array is tz-naive.","['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.tz\ndatetime.timezone.utc', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.tz\ndatetime.timezone.utc']"
559,..\pandas\reference\api\pandas.api.types.is_period_dtype.html,pandas.api.types.is_period_dtype,"pandas.api.types.is_period_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of the Period dtype. Deprecated since version 2.2.0: Use isinstance(dtype, pd.Period) instead.",Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of the Period dtype.,"['>>> from pandas.core.dtypes.common import is_period_dtype\n>>> is_period_dtype(object)\nFalse\n>>> is_period_dtype(pd.PeriodDtype(freq=""D""))\nTrue\n>>> is_period_dtype([1, 2, 3])\nFalse\n>>> is_period_dtype(pd.Period(""2017-01-01""))\nFalse\n>>> is_period_dtype(pd.PeriodIndex([], freq=""Y""))\nTrue']"
560,..\pandas\reference\api\pandas.TimedeltaIndex.floor.html,pandas.TimedeltaIndex.floor,"TimedeltaIndex.floor(*args, **kwargs)[source]# Perform floor operation on the data to the specified freq. Notes If the timestamps have a timezone, flooring will take place relative to the local (“wall”) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to floor the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.floor('h')\nDatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.floor(""h"")\n0   2018-01-01 11:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 12:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 03:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.floor(""2h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n             dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.floor(""2h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
561,..\pandas\reference\api\pandas.io.formats.style.Styler.set_sticky.html,pandas.io.formats.style.Styler.set_sticky,"Styler.set_sticky(axis=0, pixel_size=None, levels=None)[source]# Add CSS to permanently display the index or column headers in a scrolling frame. Notes This method uses the CSS ‘position: sticky;’ property to display. It is designed to work with visible axes, therefore both: styler.set_sticky(axis=”index”).hide(axis=”index”) styler.set_sticky(axis=”columns”).hide(axis=”columns”) may produce strange behaviour due to CSS controls with missing elements.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0Whether to make the index or column headers sticky. pixel_sizeint, optionalRequired to configure the width of index cells or the height of column header cells when sticking a MultiIndex (or with a named Index). Defaults to 75 and 25 respectively. levelsint, str, list, optionalIf axis is a MultiIndex the specific levels to stick. If None will stick all levels. Returns: Styler","['>>> df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]})\n>>> df.style.set_sticky(axis=""index"")']"
562,..\pandas\reference\api\pandas.core.window.expanding.Expanding.min.html,pandas.core.window.expanding.Expanding.min,"Expanding.min(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding minimum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([2, 3, 4, 1], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().min()\na    2.0\nb    2.0\nc    2.0\nd    1.0\ndtype: float64""]"
563,..\pandas\reference\api\pandas.Series.dt.tz_convert.html,pandas.Series.dt.tz_convert,"Series.dt.tz_convert(*args, **kwargs)[source]# Convert tz-aware Datetime Array/Index from one time zone to another.","Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or NoneTime zone for time. Corresponding timestamps would be converted to this time zone of the Datetime Array/Index. A tz of None will convert to UTC and remove the timezone information. Returns: Array or Index Raises: TypeErrorIf Datetime Array/Index is tz-naive.","["">>> dti = pd.date_range(start='2014-08-01 09:00',\n...                     freq='h', periods=3, tz='Europe/Berlin')"", "">>> dti\nDatetimeIndex(['2014-08-01 09:00:00+02:00',\n               '2014-08-01 10:00:00+02:00',\n               '2014-08-01 11:00:00+02:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='h')"", "">>> dti.tz_convert('US/Central')\nDatetimeIndex(['2014-08-01 02:00:00-05:00',\n               '2014-08-01 03:00:00-05:00',\n               '2014-08-01 04:00:00-05:00'],\n              dtype='datetime64[ns, US/Central]', freq='h')"", "">>> dti = pd.date_range(start='2014-08-01 09:00', freq='h',\n...                     periods=3, tz='Europe/Berlin')"", "">>> dti\nDatetimeIndex(['2014-08-01 09:00:00+02:00',\n               '2014-08-01 10:00:00+02:00',\n               '2014-08-01 11:00:00+02:00'],\n                dtype='datetime64[ns, Europe/Berlin]', freq='h')"", "">>> dti.tz_convert(None)\nDatetimeIndex(['2014-08-01 07:00:00',\n               '2014-08-01 08:00:00',\n               '2014-08-01 09:00:00'],\n                dtype='datetime64[ns]', freq='h')""]"
564,..\pandas\reference\api\pandas.DataFrame.__dataframe__.html,pandas.DataFrame.__dataframe__,"DataFrame.__dataframe__(nan_as_null=False, allow_copy=True)[source]# Return the dataframe interchange object implementing the interchange protocol. Notes Details on the interchange protocol: https://data-apis.org/dataframe-protocol/latest/index.html","Parameters: nan_as_nullbool, default Falsenan_as_null is DEPRECATED and has no effect. Please avoid using it; it will be removed in a future release. allow_copybool, default TrueWhether to allow memory copying when exporting. If set to False it would cause non-zero-copy exports to fail. Returns: DataFrame interchange objectThe object which consuming library can use to ingress the dataframe.","["">>> df_not_necessarily_pandas = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n>>> interchange_object = df_not_necessarily_pandas.__dataframe__()\n>>> interchange_object.column_names()\nIndex(['A', 'B'], dtype='object')\n>>> df_pandas = (pd.api.interchange.from_dataframe\n...              (interchange_object.select_columns_by_name(['A'])))\n>>> df_pandas\n     A\n0    1\n1    2""]"
565,..\pandas\reference\api\pandas.api.types.is_re.html,pandas.api.types.is_re,pandas.api.types.is_re(obj)[source]# Check if the object is a regex pattern instance.,Parameters: objThe object to check Returns: boolWhether obj is a regex pattern.,"['>>> from pandas.api.types import is_re\n>>> import re\n>>> is_re(re.compile("".*""))\nTrue\n>>> is_re(""foo"")\nFalse']"
566,..\pandas\reference\api\pandas.TimedeltaIndex.html,pandas.TimedeltaIndex,"class pandas.TimedeltaIndex(data=None, unit=<no_default>, freq=<no_default>, closed=<no_default>, dtype=None, copy=False, name=None)[source]# Immutable Index of timedelta64 data. Represented internally as int64, and scalars returned Timedelta objects. Attributes days Number of days for each element. seconds Number of seconds (>= 0 and less than 1 day) for each element. microseconds Number of microseconds (>= 0 and less than 1 second) for each element. nanoseconds Number of nanoseconds (>= 0 and less than 1 microsecond) for each element. components Return a DataFrame of the individual resolution components of the Timedeltas. inferred_freq Tries to return a string representing a frequency generated by infer_freq. Methods to_pytimedelta(*args, **kwargs) Return an ndarray of datetime.timedelta objects. to_series([index, name]) Create a Series with both index and values equal to the index keys. round(*args, **kwargs) Perform round operation on the data to the specified freq. floor(*args, **kwargs) Perform floor operation on the data to the specified freq. ceil(*args, **kwargs) Perform ceil operation on the data to the specified freq. to_frame([index, name]) Create a DataFrame with a column containing the Index. mean(*[, skipna, axis]) Return the mean value of the Array. Notes To learn more about the frequency strings, please see this link.","Parameters: dataarray-like (1-dimensional), optionalOptional timedelta-like data to construct index with. unit{‘D’, ‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, ‘ns’}, optionalThe unit of data. Deprecated since version 2.2.0: Use pd.to_timedelta instead. freqstr or pandas offset object, optionalOne of pandas date offset strings or corresponding objects. The string 'infer' can be passed in order to set the frequency of the index as the inferred frequency upon creation. dtypenumpy.dtype or str, default NoneValid numpy dtypes are timedelta64[ns], timedelta64[us], timedelta64[ms], and timedelta64[s]. copyboolMake a copy of input array. nameobjectName to be stored in the index.","["">>> pd.TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'])\nTimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],\n               dtype='timedelta64[ns]', freq=None)"", "">>> pd.TimedeltaIndex(np.arange(5) * 24 * 3600 * 1e9, freq='infer')\nTimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],\n               dtype='timedelta64[ns]', freq='D')""]"
567,..\pandas\reference\api\pandas.io.formats.style.Styler.set_table_attributes.html,pandas.io.formats.style.Styler.set_table_attributes,Styler.set_table_attributes(attributes)[source]# Set the table attributes added to the <table> HTML element. These are items in addition to automatic (by default) id attribute.,Parameters: attributesstr Returns: Styler,"['>>> df = pd.DataFrame(np.random.randn(10, 4))\n>>> df.style.set_table_attributes(\'class=""pure-table""\')  \n# ... <table class=""pure-table""> ...']"
568,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_year_end.html,pandas.tseries.offsets.MonthEnd.is_year_end,MonthEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
569,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_month_start.html,pandas.tseries.offsets.CustomBusinessHour.is_month_start,CustomBusinessHour.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
570,..\pandas\reference\api\pandas.core.window.expanding.Expanding.quantile.html,pandas.core.window.expanding.Expanding.quantile,"Expanding.quantile(q, interpolation='linear', numeric_only=False)[source]# Calculate the expanding quantile.","Parameters: quantilefloatQuantile to compute. 0 <= quantile <= 1. Deprecated since version 2.1.0: This will be renamed to ‘q’ in a future version. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j: linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j. lower: i. higher: j. nearest: i or j whichever is nearest. midpoint: (i + j) / 2. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([1, 2, 3, 4, 5, 6], index=['a', 'b', 'c', 'd', 'e', 'f'])\n>>> ser.expanding(min_periods=4).quantile(.25)\na     NaN\nb     NaN\nc     NaN\nd    1.75\ne    2.00\nf    2.25\ndtype: float64""]"
571,..\pandas\reference\api\pandas.Series.dt.tz_localize.html,pandas.Series.dt.tz_localize,"Series.dt.tz_localize(*args, **kwargs)[source]# Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index. This method takes a time zone (tz) naive Datetime Array/Index object and makes this time zone aware. It does not move the time to another time zone. This method can also be used to do the inverse – to create a time zone unaware object from an aware object. To that end, pass tz=None.","Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or NoneTime zone to convert timestamps to. Passing None will remove the time zone information preserving local time. ambiguous‘infer’, ‘NaT’, bool array, default ‘raise’When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: Same type as selfArray/Index converted to the specified time zone. Raises: TypeErrorIf the Datetime Array/Index is tz-aware and tz is not None.","["">>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)\n>>> tz_naive\nDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n               '2018-03-03 09:00:00'],\n              dtype='datetime64[ns]', freq='D')"", "">>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')\n>>> tz_aware\nDatetimeIndex(['2018-03-01 09:00:00-05:00',\n               '2018-03-02 09:00:00-05:00',\n               '2018-03-03 09:00:00-05:00'],\n              dtype='datetime64[ns, US/Eastern]', freq=None)"", "">>> tz_aware.tz_localize(None)\nDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n               '2018-03-03 09:00:00'],\n              dtype='datetime64[ns]', freq=None)"", "">>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',\n...                               '2018-10-28 02:00:00',\n...                               '2018-10-28 02:30:00',\n...                               '2018-10-28 02:00:00',\n...                               '2018-10-28 02:30:00',\n...                               '2018-10-28 03:00:00',\n...                               '2018-10-28 03:30:00']))\n>>> s.dt.tz_localize('CET', ambiguous='infer')\n0   2018-10-28 01:30:00+02:00\n1   2018-10-28 02:00:00+02:00\n2   2018-10-28 02:30:00+02:00\n3   2018-10-28 02:00:00+01:00\n4   2018-10-28 02:30:00+01:00\n5   2018-10-28 03:00:00+01:00\n6   2018-10-28 03:30:00+01:00\ndtype: datetime64[ns, CET]"", "">>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',\n...                               '2018-10-28 02:36:00',\n...                               '2018-10-28 03:46:00']))\n>>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))\n0   2018-10-28 01:20:00+02:00\n1   2018-10-28 02:36:00+02:00\n2   2018-10-28 03:46:00+01:00\ndtype: datetime64[ns, CET]"", "">>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',\n...                               '2015-03-29 03:30:00']))\n>>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n0   2015-03-29 03:00:00+02:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]"", "">>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n0   2015-03-29 01:59:59.999999999+01:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]"", "">>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1h'))\n0   2015-03-29 03:30:00+02:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]""]"
572,..\pandas\reference\api\pandas.DataFrame.__iter__.html,pandas.DataFrame.__iter__,DataFrame.__iter__()[source]# Iterate over info axis.,Returns: iteratorInfo axis as iterator.,"["">>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n>>> for x in df:\n...     print(x)\nA\nB""]"
573,..\pandas\reference\api\pandas.TimedeltaIndex.inferred_freq.html,pandas.TimedeltaIndex.inferred_freq,TimedeltaIndex.inferred_freq[source]# Tries to return a string representing a frequency generated by infer_freq. Returns None if it can’t autodetect the frequency.,No parameters found,"['>>> idx = pd.DatetimeIndex([""2018-01-01"", ""2018-01-03"", ""2018-01-05""])\n>>> idx.inferred_freq\n\'2D\'', '>>> tdelta_idx = pd.to_timedelta([""0 days"", ""10 days"", ""20 days""])\n>>> tdelta_idx\nTimedeltaIndex([\'0 days\', \'10 days\', \'20 days\'],\n               dtype=\'timedelta64[ns]\', freq=None)\n>>> tdelta_idx.inferred_freq\n\'10D\'']"
574,..\pandas\reference\api\pandas.api.types.is_re_compilable.html,pandas.api.types.is_re_compilable,pandas.api.types.is_re_compilable(obj)[source]# Check if the object can be compiled into a regex pattern instance.,Parameters: objThe object to check Returns: boolWhether obj can be compiled as a regex pattern.,"['>>> from pandas.api.types import is_re_compilable\n>>> is_re_compilable("".*"")\nTrue\n>>> is_re_compilable(1)\nFalse']"
575,..\pandas\reference\api\pandas.io.formats.style.Styler.set_table_styles.html,pandas.io.formats.style.Styler.set_table_styles,"Styler.set_table_styles(table_styles=None, axis=0, overwrite=True, css_class_names=None)[source]# Set the table styles included within the <style> HTML element. This function can be used to style the entire table, columns, rows or specific HTML selectors. Notes The default CSS classes dict, whose values can be replaced is as follows: css_class_names = {""row_heading"": ""row_heading"",                    ""col_heading"": ""col_heading"",                    ""index_name"": ""index_name"",                    ""col"": ""col"",                    ""row"": ""row"",                    ""col_trim"": ""col_trim"",                    ""row_trim"": ""row_trim"",                    ""level"": ""level"",                    ""data"": ""data"",                    ""blank"": ""blank"",                    ""foot"": ""foot""}","Parameters: table_styleslist or dictIf supplying a list, each individual table_style should be a dictionary with selector and props keys. selector should be a CSS selector that the style will be applied to (automatically prefixed by the table’s UUID) and props should be a list of tuples with (attribute, value). If supplying a dict, the dict keys should correspond to column names or index values, depending upon the specified axis argument. These will be mapped to row or col CSS selectors. MultiIndex values as dict keys should be in their respective tuple form. The dict values should be a list as specified in the form with CSS selectors and props that will be applied to the specified row or column. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'). Only used if table_styles is dict. overwritebool, default TrueStyles are replaced if True, or extended if False. CSS rules are preserved so most recent styles set will dominate if selectors intersect. css_class_namesdict, optionalA dict of strings used to replace the default CSS classes described below. Added in version 1.4.0. Returns: Styler","["">>> df = pd.DataFrame(np.random.randn(10, 4),\n...                   columns=['A', 'B', 'C', 'D'])\n>>> df.style.set_table_styles(\n...     [{'selector': 'tr:hover',\n...       'props': [('background-color', 'yellow')]}]\n... )"", "">>> df.style.set_table_styles(\n...     [{'selector': 'tr:hover',\n...       'props': 'background-color: yellow; font-size: 1em;'}]\n... )"", "">>> df.style.set_table_styles({\n...     'A': [{'selector': '',\n...            'props': [('color', 'red')]}],\n...     'B': [{'selector': 'td',\n...            'props': 'color: blue;'}]\n... }, overwrite=False)"", "">>> df.style.set_table_styles({\n...     0: [{'selector': 'td:hover',\n...          'props': [('font-size', '25px')]}]\n... }, axis=1, overwrite=False)""]"
576,..\pandas\reference\api\pandas.core.window.expanding.Expanding.rank.html,pandas.core.window.expanding.Expanding.rank,"Expanding.rank(method='average', ascending=True, pct=False, numeric_only=False)[source]# Calculate the expanding rank. Added in version 1.4.0.","Parameters: method{‘average’, ‘min’, ‘max’}, default ‘average’How to rank the group of records that have the same value (i.e. ties): average: average rank of the group min: lowest rank in the group max: highest rank in the group ascendingbool, default TrueWhether or not the elements should be ranked in ascending order. pctbool, default FalseWhether or not to display the returned rankings in percentile form. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([1, 4, 2, 3, 5, 3])\n>>> s.expanding().rank()\n0    1.0\n1    2.0\n2    2.0\n3    3.0\n4    5.0\n5    3.5\ndtype: float64', '>>> s.expanding().rank(method=""max"")\n0    1.0\n1    2.0\n2    2.0\n3    3.0\n4    5.0\n5    4.0\ndtype: float64', '>>> s.expanding().rank(method=""min"")\n0    1.0\n1    2.0\n2    2.0\n3    3.0\n4    5.0\n5    3.0\ndtype: float64']"
577,..\pandas\reference\api\pandas.Series.dt.unit.html,pandas.Series.dt.unit,Series.dt.unit[source]#,No parameters found,[]
578,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.is_year_start.html,pandas.tseries.offsets.MonthEnd.is_year_start,MonthEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
579,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_on_offset.html,pandas.tseries.offsets.CustomBusinessHour.is_on_offset,CustomBusinessHour.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
580,..\pandas\reference\api\pandas.DatetimeIndex.as_unit.html,pandas.DatetimeIndex.as_unit,"DatetimeIndex.as_unit(*args, **kwargs)[source]# Convert to a dtype with the given unit resolution.","Parameters: unit{‘s’, ‘ms’, ‘us’, ‘ns’} Returns: same type as self","["">>> idx = pd.DatetimeIndex(['2020-01-02 01:02:03.004005006'])\n>>> idx\nDatetimeIndex(['2020-01-02 01:02:03.004005006'],\n              dtype='datetime64[ns]', freq=None)\n>>> idx.as_unit('s')\nDatetimeIndex(['2020-01-02 01:02:03'], dtype='datetime64[s]', freq=None)"", "">>> tdelta_idx = pd.to_timedelta(['1 day 3 min 2 us 42 ns'])\n>>> tdelta_idx\nTimedeltaIndex(['1 days 00:03:00.000002042'],\n                dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.as_unit('s')\nTimedeltaIndex(['1 days 00:03:00'], dtype='timedelta64[s]', freq=None)""]"
581,..\pandas\reference\api\pandas.TimedeltaIndex.mean.html,pandas.TimedeltaIndex.mean,"TimedeltaIndex.mean(*, skipna=True, axis=0)[source]# Return the mean value of the Array. Notes mean is only defined for Datetime and Timedelta dtypes, not for Period.","Parameters: skipnabool, default TrueWhether to ignore any NaT elements. axisint, optional, default 0 Returns: scalarTimestamp or Timedelta.","["">>> idx = pd.date_range('2001-01-01 00:00', periods=3)\n>>> idx\nDatetimeIndex(['2001-01-01', '2001-01-02', '2001-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.mean()\nTimestamp('2001-01-02 00:00:00')"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='D')\n>>> tdelta_idx\nTimedeltaIndex(['1 days', '2 days', '3 days'],\n                dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.mean()\nTimedelta('2 days 00:00:00')""]"
582,..\pandas\reference\api\pandas.DatetimeIndex.ceil.html,pandas.DatetimeIndex.ceil,"DatetimeIndex.ceil(*args, **kwargs)[source]# Perform ceil operation on the data to the specified freq. Notes If the timestamps have a timezone, ceiling will take place relative to the local (“wall”) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to ceil the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.ceil('h')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 13:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.ceil(""h"")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 13:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 01:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.ceil(""h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.ceil(""h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
583,..\pandas\reference\api\pandas.api.types.is_scalar.html,pandas.api.types.is_scalar,pandas.api.types.is_scalar(val)# Return True if given object is scalar.,Parameters: valobjectThis includes: numpy array scalar (e.g. np.int64) Python builtin numerics Python builtin byte arrays and strings None datetime.datetime datetime.timedelta Period decimal.Decimal Interval DateOffset Fraction Number. Returns: boolReturn True if given object is scalar.,"['>>> import datetime\n>>> dt = datetime.datetime(2018, 10, 3)\n>>> pd.api.types.is_scalar(dt)\nTrue', '>>> pd.api.types.is_scalar([2, 3])\nFalse', '>>> pd.api.types.is_scalar({0: 1, 2: 3})\nFalse', '>>> pd.api.types.is_scalar((0, 2))\nFalse', '>>> from fractions import Fraction\n>>> pd.api.types.is_scalar(Fraction(3, 5))\nTrue']"
584,..\pandas\reference\api\pandas.Series.dt.weekday.html,pandas.Series.dt.weekday,"Series.dt.weekday[source]# The day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.",Returns: Series or IndexContaining integers indicating the day number.,"["">>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int32""]"
585,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.kwds.html,pandas.tseries.offsets.MonthEnd.kwds,MonthEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
586,..\pandas\reference\api\pandas.core.window.expanding.Expanding.sem.html,pandas.core.window.expanding.Expanding.sem,"Expanding.sem(ddof=1, numeric_only=False)[source]# Calculate the expanding standard error of mean. Notes A minimum of one period is required for the calculation.","Parameters: ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([0, 1, 2, 3])', '>>> s.expanding().sem()\n0         NaN\n1    0.707107\n2    0.707107\n3    0.745356\ndtype: float64']"
587,..\pandas\reference\api\pandas.io.formats.style.Styler.set_td_classes.html,pandas.io.formats.style.Styler.set_td_classes,Styler.set_td_classes(classes)[source]# Set the class attribute of <td> HTML elements. Notes Can be used in combination with Styler.set_table_styles to define an internal CSS solution without reference to external CSS files.,"Parameters: classesDataFrameDataFrame containing strings that will be translated to CSS classes, mapped by identical column and index key values that must exist on the underlying Styler data. None, NaN values, and empty strings will be ignored and not affect the rendered HTML. Returns: Styler","['>>> df = pd.DataFrame(data=[[1, 2, 3], [4, 5, 6]], columns=[""A"", ""B"", ""C""])\n>>> classes = pd.DataFrame([\n...     [""min-val red"", """", ""blue""],\n...     [""red"", None, ""blue max-val""]\n... ], index=df.index, columns=df.columns)\n>>> df.style.set_td_classes(classes)', '>>> df = pd.DataFrame([[1,2],[3,4]], index=[""a"", ""b""],\n...     columns=[[""level0"", ""level0""], [""level1a"", ""level1b""]])\n>>> classes = pd.DataFrame([""min-val""], index=[""a""],\n...     columns=[[""level0""],[""level1a""]])\n>>> df.style.set_td_classes(classes)', '>>> from pandas.io.formats.style import Styler\n>>> df = pd.DataFrame([[1]])\n>>> css = pd.DataFrame([[""other-class""]])\n>>> s = Styler(df, uuid=""_"", cell_ids=False).set_td_classes(css)\n>>> s.hide(axis=0).to_html()  \n\'<style type=""text/css""></style>\'\n\'<table id=""T__"">\'\n\'  <thead>\'\n\'    <tr><th class=""col_heading level0 col0"" >0</th></tr>\'\n\'  </thead>\'\n\'  <tbody>\'\n\'    <tr><td class=""data row0 col0 other-class"" >1</td></tr>\'\n\'  </tbody>\'\n\'</table>\'']"
588,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_quarter_end.html,pandas.tseries.offsets.CustomBusinessHour.is_quarter_end,CustomBusinessHour.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
589,..\pandas\reference\api\pandas.TimedeltaIndex.microseconds.html,pandas.TimedeltaIndex.microseconds,property TimedeltaIndex.microseconds[source]# Number of microseconds (>= 0 and less than 1 second) for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='us'))\n>>> ser\n0   0 days 00:00:00.000001\n1   0 days 00:00:00.000002\n2   0 days 00:00:00.000003\ndtype: timedelta64[ns]\n>>> ser.dt.microseconds\n0    1\n1    2\n2    3\ndtype: int32"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='us')\n>>> tdelta_idx\nTimedeltaIndex(['0 days 00:00:00.000001', '0 days 00:00:00.000002',\n                '0 days 00:00:00.000003'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.microseconds\nIndex([1, 2, 3], dtype='int32')""]"
590,..\pandas\reference\api\pandas.api.types.is_signed_integer_dtype.html,pandas.api.types.is_signed_integer_dtype,"pandas.api.types.is_signed_integer_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of a signed integer dtype. Unlike in is_any_int_dtype, timedelta64 instances will return False. The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as integer by this function.",Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of a signed integer dtype and not an instance of timedelta64.,"["">>> from pandas.core.dtypes.common import is_signed_integer_dtype\n>>> is_signed_integer_dtype(str)\nFalse\n>>> is_signed_integer_dtype(int)\nTrue\n>>> is_signed_integer_dtype(float)\nFalse\n>>> is_signed_integer_dtype(np.uint64)  # unsigned\nFalse\n>>> is_signed_integer_dtype('int8')\nTrue\n>>> is_signed_integer_dtype('Int8')\nTrue\n>>> is_signed_integer_dtype(pd.Int8Dtype)\nTrue\n>>> is_signed_integer_dtype(np.datetime64)\nFalse\n>>> is_signed_integer_dtype(np.timedelta64)\nFalse\n>>> is_signed_integer_dtype(np.array(['a', 'b']))\nFalse\n>>> is_signed_integer_dtype(pd.Series([1, 2]))\nTrue\n>>> is_signed_integer_dtype(np.array([], dtype=np.timedelta64))\nFalse\n>>> is_signed_integer_dtype(pd.Index([1, 2.]))  # float\nFalse\n>>> is_signed_integer_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned\nFalse""]"
591,..\pandas\reference\api\pandas.DatetimeIndex.date.html,pandas.DatetimeIndex.date,"property DatetimeIndex.date[source]# Returns numpy array of python datetime.date objects. Namely, the date part of Timestamps without time and timezone information.",No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.date\n0    2020-01-01\n1    2020-02-01\ndtype: object', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.date\narray([datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)], dtype=object)']"
592,..\pandas\reference\api\pandas.Series.dt.year.html,pandas.Series.dt.year,Series.dt.year[source]# The year of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""YE"")\n... )\n>>> datetime_series\n0   2000-12-31\n1   2001-12-31\n2   2002-12-31\ndtype: datetime64[ns]\n>>> datetime_series.dt.year\n0    2000\n1    2001\n2    2002\ndtype: int32']"
593,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.n.html,pandas.tseries.offsets.MonthEnd.n,MonthEnd.n#,No parameters found,[]
594,..\pandas\reference\api\pandas.core.window.expanding.Expanding.skew.html,pandas.core.window.expanding.Expanding.skew,Expanding.skew(numeric_only=False)[source]# Calculate the expanding unbiased skewness. Notes A minimum of three periods is required for the rolling calculation.,"Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([-1, 0, 2, -1, 2], index=['a', 'b', 'c', 'd', 'e'])\n>>> ser.expanding().skew()\na         NaN\nb         NaN\nc    0.935220\nd    1.414214\ne    0.315356\ndtype: float64""]"
595,..\pandas\reference\api\pandas.io.formats.style.Styler.set_tooltips.html,pandas.io.formats.style.Styler.set_tooltips,"Styler.set_tooltips(ttips, props=None, css_class=None)[source]# Set the DataFrame of strings on Styler generating :hover tooltips. These string based tooltips are only applicable to <td> HTML elements, and cannot be used for column or index headers. Added in version 1.3.0. Notes Tooltips are created by adding <span class=”pd-t”></span> to each data cell and then manipulating the table level CSS to attach pseudo hover and pseudo after selectors to produce the required the results. The default properties for the tooltip CSS class are: visibility: hidden position: absolute z-index: 1 background-color: black color: white transform: translate(-20px, -20px) The property ‘visibility: hidden;’ is a key prerequisite to the hover functionality, and should always be included in any manual properties specification, using the props argument. Tooltips are not designed to be efficient, and can add large amounts of additional HTML for larger tables, since they also require that cell_ids is forced to True.","Parameters: ttipsDataFrameDataFrame containing strings that will be translated to tooltips, mapped by identical column and index values that must exist on the underlying Styler data. None, NaN values, and empty strings will be ignored and not affect the rendered HTML. propslist-like or str, optionalList of (attr, value) tuples or a valid CSS string. If None adopts the internal default values described in notes. css_classstr, optionalName of the tooltip class used in CSS, should conform to HTML standards. Only useful if integrating tooltips with external CSS. If None uses the internal default value ‘pd-t’. Returns: Styler","['>>> df = pd.DataFrame(data=[[0, 1], [2, 3]])\n>>> ttips = pd.DataFrame(\n...    data=[[""Min"", """"], [np.nan, ""Max""]], columns=df.columns, index=df.index\n... )\n>>> s = df.style.set_tooltips(ttips).to_html()', "">>> df.style.set_tooltips(ttips, css_class='tt-add', props=[\n...     ('visibility', 'hidden'),\n...     ('position', 'absolute'),\n...     ('z-index', 1)])  \n>>> df.style.set_tooltips(ttips, css_class='tt-add',\n...     props='visibility:hidden; position:absolute; z-index:1;')\n...""]"
596,..\pandas\reference\api\pandas.DatetimeIndex.day.html,pandas.DatetimeIndex.day,property DatetimeIndex.day[source]# The day of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""D"")\n... )\n>>> datetime_series\n0   2000-01-01\n1   2000-01-02\n2   2000-01-03\ndtype: datetime64[ns]\n>>> datetime_series.dt.day\n0    1\n1    2\n2    3\ndtype: int32']"
597,..\pandas\reference\api\pandas.Series.dtype.html,pandas.Series.dtype,property Series.dtype[source]# Return the dtype object of the underlying data.,No parameters found,"["">>> s = pd.Series([1, 2, 3])\n>>> s.dtype\ndtype('int64')""]"
598,..\pandas\reference\api\pandas.api.types.is_sparse.html,pandas.api.types.is_sparse,"pandas.api.types.is_sparse(arr)[source]# Check whether an array-like is a 1-D pandas sparse array. Deprecated since version 2.1.0: Use isinstance(dtype, pd.SparseDtype) instead. Check that the one-dimensional array-like is a pandas sparse array. Returns True if it is a pandas sparse array, not another type of sparse array.",Parameters: arrarray-likeArray-like to check. Returns: boolWhether or not the array-like is a pandas sparse array.,"['>>> from pandas.api.types import is_sparse\n>>> is_sparse(pd.arrays.SparseArray([0, 0, 1, 0]))\nTrue\n>>> is_sparse(pd.Series(pd.arrays.SparseArray([0, 0, 1, 0])))\nTrue', '>>> is_sparse(np.array([0, 0, 1, 0]))\nFalse\n>>> is_sparse(pd.Series([0, 1, 0, 0]))\nFalse', '>>> from scipy.sparse import bsr_matrix\n>>> is_sparse(bsr_matrix([0, 1, 0, 0]))\nFalse']"
599,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.name.html,pandas.tseries.offsets.MonthEnd.name,MonthEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
600,..\pandas\reference\api\pandas.TimedeltaIndex.nanoseconds.html,pandas.TimedeltaIndex.nanoseconds,property TimedeltaIndex.nanoseconds[source]# Number of nanoseconds (>= 0 and less than 1 microsecond) for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='ns'))\n>>> ser\n0   0 days 00:00:00.000000001\n1   0 days 00:00:00.000000002\n2   0 days 00:00:00.000000003\ndtype: timedelta64[ns]\n>>> ser.dt.nanoseconds\n0    1\n1    2\n2    3\ndtype: int32"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='ns')\n>>> tdelta_idx\nTimedeltaIndex(['0 days 00:00:00.000000001', '0 days 00:00:00.000000002',\n                '0 days 00:00:00.000000003'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.nanoseconds\nIndex([1, 2, 3], dtype='int32')""]"
601,..\pandas\reference\api\pandas.core.window.expanding.Expanding.std.html,pandas.core.window.expanding.Expanding.std,"Expanding.std(ddof=1, numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding standard deviation. Notes The default ddof of 1 used in Series.std() is different than the default ddof of 0 in numpy.std(). A minimum of one period is required for the rolling calculation.","Parameters: ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.4.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])', '>>> s.expanding(3).std()\n0         NaN\n1         NaN\n2    0.577350\n3    0.957427\n4    0.894427\n5    0.836660\n6    0.786796\ndtype: float64']"
602,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_quarter_start.html,pandas.tseries.offsets.CustomBusinessHour.is_quarter_start,CustomBusinessHour.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
603,..\pandas\reference\api\pandas.io.formats.style.Styler.set_uuid.html,pandas.io.formats.style.Styler.set_uuid,"Styler.set_uuid(uuid)[source]# Set the uuid applied to id attributes of HTML elements. Notes Almost all HTML elements within the table, and including the <table> element are assigned id attributes. The format is T_uuid_<extra> where <extra> is typically a more specific identifier, such as row1_col2.",Parameters: uuidstr Returns: Styler,"["">>> df = pd.DataFrame([[1, 2], [3, 4]], index=['A', 'B'], columns=['c1', 'c2'])"", '>>> print((df).style.to_html())', '>>> df.style.set_uuid(""T_20a7d_level0_col0"")\n... .set_caption(""Test"")']"
604,..\pandas\reference\api\pandas.DatetimeIndex.dayofweek.html,pandas.DatetimeIndex.dayofweek,"property DatetimeIndex.dayofweek[source]# The day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.",Returns: Series or IndexContaining integers indicating the day number.,"["">>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int32""]"
605,..\pandas\reference\api\pandas.Series.dtypes.html,pandas.Series.dtypes,property Series.dtypes[source]# Return the dtype object of the underlying data.,No parameters found,"["">>> s = pd.Series([1, 2, 3])\n>>> s.dtypes\ndtype('int64')""]"
606,..\pandas\reference\api\pandas.TimedeltaIndex.round.html,pandas.TimedeltaIndex.round,"TimedeltaIndex.round(*args, **kwargs)[source]# Perform round operation on the data to the specified freq. Notes If the timestamps have a timezone, rounding will take place relative to the local (“wall”) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to round the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.round('h')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.round(""h"")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 12:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 03:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.floor(""2h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.floor(""2h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
607,..\pandas\reference\api\pandas.DatetimeIndex.dayofyear.html,pandas.DatetimeIndex.dayofyear,property DatetimeIndex.dayofyear[source]# The ordinal day of the year.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.dayofyear\n0    1\n1   32\ndtype: int32', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.dayofyear\nIndex([1, 32], dtype=\'int32\')']"
608,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.nanos.html,pandas.tseries.offsets.MonthEnd.nanos,MonthEnd.nanos#,No parameters found,[]
609,..\pandas\reference\api\pandas.io.formats.style.Styler.template_html.html,pandas.io.formats.style.Styler.template_html,Styler.template_html = <Template 'html.tpl'>#,No parameters found,[]
610,..\pandas\reference\api\pandas.api.types.is_string_dtype.html,pandas.api.types.is_string_dtype,"pandas.api.types.is_string_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of the string dtype. If an array is passed with an object dtype, the elements must be inferred as strings.",Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of the string dtype.,"["">>> from pandas.api.types import is_string_dtype\n>>> is_string_dtype(str)\nTrue\n>>> is_string_dtype(object)\nTrue\n>>> is_string_dtype(int)\nFalse\n>>> is_string_dtype(np.array(['a', 'b']))\nTrue\n>>> is_string_dtype(pd.Series([1, 2]))\nFalse\n>>> is_string_dtype(pd.Series([1, 2], dtype=object))\nFalse""]"
611,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_year_end.html,pandas.tseries.offsets.CustomBusinessHour.is_year_end,CustomBusinessHour.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
612,..\pandas\reference\api\pandas.core.window.expanding.Expanding.sum.html,pandas.core.window.expanding.Expanding.sum,"Expanding.sum(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding sum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> ser = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n>>> ser.expanding().sum()\na     1.0\nb     3.0\nc     6.0\nd    10.0\ndtype: float64""]"
613,..\pandas\reference\api\pandas.Series.duplicated.html,pandas.Series.duplicated,"Series.duplicated(keep='first')[source]# Indicate duplicate Series values. Duplicated values are indicated as True values in the resulting Series. Either all duplicates, all except the first or all except the last occurrence of duplicates can be indicated.","Parameters: keep{‘first’, ‘last’, False}, default ‘first’Method to handle dropping duplicates: ‘first’ : Mark duplicates as True except for the first occurrence. ‘last’ : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True. Returns: Series[bool]Series indicating whether each value has occurred in the preceding values.","["">>> animals = pd.Series(['llama', 'cow', 'llama', 'beetle', 'llama'])\n>>> animals.duplicated()\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool"", "">>> animals.duplicated(keep='first')\n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool"", "">>> animals.duplicated(keep='last')\n0     True\n1    False\n2     True\n3    False\n4    False\ndtype: bool"", '>>> animals.duplicated(keep=False)\n0     True\n1    False\n2     True\n3    False\n4     True\ndtype: bool']"
614,..\pandas\reference\api\pandas.TimedeltaIndex.seconds.html,pandas.TimedeltaIndex.seconds,property TimedeltaIndex.seconds[source]# Number of seconds (>= 0 and less than 1 day) for each element.,No parameters found,"["">>> ser = pd.Series(pd.to_timedelta([1, 2, 3], unit='s'))\n>>> ser\n0   0 days 00:00:01\n1   0 days 00:00:02\n2   0 days 00:00:03\ndtype: timedelta64[ns]\n>>> ser.dt.seconds\n0    1\n1    2\n2    3\ndtype: int32"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='s')\n>>> tdelta_idx\nTimedeltaIndex(['0 days 00:00:01', '0 days 00:00:02', '0 days 00:00:03'],\n               dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.seconds\nIndex([1, 2, 3], dtype='int32')""]"
615,..\pandas\reference\api\pandas.io.formats.style.Styler.template_html_style.html,pandas.io.formats.style.Styler.template_html_style,Styler.template_html_style = <Template 'html_style.tpl'>#,No parameters found,[]
616,..\pandas\reference\api\pandas.Series.empty.html,pandas.Series.empty,"property Series.empty[source]# Indicator whether Series/DataFrame is empty. True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0. Notes If Series/DataFrame contains only NaNs, it is still not considered empty. See the example below.","Returns: boolIf Series/DataFrame is empty, return True, if not return False.","["">>> df_empty = pd.DataFrame({'A' : []})\n>>> df_empty\nEmpty DataFrame\nColumns: [A]\nIndex: []\n>>> df_empty.empty\nTrue"", "">>> df = pd.DataFrame({'A' : [np.nan]})\n>>> df\n    A\n0 NaN\n>>> df.empty\nFalse\n>>> df.dropna().empty\nTrue"", "">>> ser_empty = pd.Series({'A' : []})\n>>> ser_empty\nA    []\ndtype: object\n>>> ser_empty.empty\nFalse\n>>> ser_empty = pd.Series()\n>>> ser_empty.empty\nTrue""]"
617,..\pandas\reference\api\pandas.DatetimeIndex.day_name.html,pandas.DatetimeIndex.day_name,"DatetimeIndex.day_name(*args, **kwargs)[source]# Return the day names with specified locale.","Parameters: localestr, optionalLocale determining the language in which to return the day name. Default is English locale ('en_US.utf8'). Use the command locale -a on your terminal on Unix systems to find your locale language code. Returns: Series or IndexSeries or Index of day names.","["">>> s = pd.Series(pd.date_range(start='2018-01-01', freq='D', periods=3))\n>>> s\n0   2018-01-01\n1   2018-01-02\n2   2018-01-03\ndtype: datetime64[ns]\n>>> s.dt.day_name()\n0       Monday\n1      Tuesday\n2    Wednesday\ndtype: object"", "">>> idx = pd.date_range(start='2018-01-01', freq='D', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.day_name()\nIndex(['Monday', 'Tuesday', 'Wednesday'], dtype='object')"", "">>> idx = pd.date_range(start='2018-01-01', freq='D', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.day_name(locale='pt_BR.utf8') \nIndex(['Segunda', 'Terça', 'Quarta'], dtype='object')""]"
618,..\pandas\reference\api\pandas.api.types.is_timedelta64_dtype.html,pandas.api.types.is_timedelta64_dtype,pandas.api.types.is_timedelta64_dtype(arr_or_dtype)[source]# Check whether an array-like or dtype is of the timedelta64 dtype.,Parameters: arr_or_dtypearray-like or dtypeThe array-like or dtype to check. Returns: booleanWhether or not the array-like or dtype is of the timedelta64 dtype.,"['>>> from pandas.core.dtypes.common import is_timedelta64_dtype\n>>> is_timedelta64_dtype(object)\nFalse\n>>> is_timedelta64_dtype(np.timedelta64)\nTrue\n>>> is_timedelta64_dtype([1, 2, 3])\nFalse\n>>> is_timedelta64_dtype(pd.Series([], dtype=""timedelta64[ns]""))\nTrue\n>>> is_timedelta64_dtype(\'0 days\')\nFalse']"
619,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.normalize.html,pandas.tseries.offsets.MonthEnd.normalize,MonthEnd.normalize#,No parameters found,[]
620,..\pandas\reference\api\pandas.io.formats.style.Styler.template_html_table.html,pandas.io.formats.style.Styler.template_html_table,Styler.template_html_table = <Template 'html_table.tpl'>#,No parameters found,[]
621,..\pandas\reference\api\pandas.core.window.expanding.Expanding.var.html,pandas.core.window.expanding.Expanding.var,"Expanding.var(ddof=1, numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the expanding variance. Notes The default ddof of 1 used in Series.var() is different than the default ddof of 0 in numpy.var(). A minimum of one period is required for the rolling calculation.","Parameters: ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.4.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])', '>>> s.expanding(3).var()\n0         NaN\n1         NaN\n2    0.333333\n3    0.916667\n4    0.800000\n5    0.700000\n6    0.619048\ndtype: float64']"
622,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.is_year_start.html,pandas.tseries.offsets.CustomBusinessHour.is_year_start,CustomBusinessHour.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
623,..\pandas\reference\api\pandas.TimedeltaIndex.to_frame.html,pandas.TimedeltaIndex.to_frame,"TimedeltaIndex.to_frame(index=True, name=<no_default>)[source]# Create a DataFrame with a column containing the Index.","Parameters: indexbool, default TrueSet the index of the returned DataFrame as the original Index. nameobject, defaults to index.nameThe passed name should substitute for the index name (if it has one). Returns: DataFrameDataFrame containing the original Index data.","["">>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow"", '>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow', "">>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow""]"
624,..\pandas\reference\api\pandas.Series.eq.html,pandas.Series.eq,"Series.eq(other, level=None, fill_value=None, axis=0)[source]# Return Equal to of series and other, element-wise (binary operator eq). Equivalent to series == other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.eq(b, fill_value=0)\na     True\nb    False\nc    False\nd    False\ne    False\ndtype: bool""]"
625,..\pandas\reference\api\pandas.DatetimeIndex.day_of_week.html,pandas.DatetimeIndex.day_of_week,"property DatetimeIndex.day_of_week[source]# The day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.",Returns: Series or IndexContaining integers indicating the day number.,"["">>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int32""]"
626,..\pandas\reference\api\pandas.io.formats.style.Styler.template_latex.html,pandas.io.formats.style.Styler.template_latex,Styler.template_latex = <Template 'latex.tpl'>#,No parameters found,[]
627,..\pandas\reference\api\pandas.api.types.is_timedelta64_ns_dtype.html,pandas.api.types.is_timedelta64_ns_dtype,"pandas.api.types.is_timedelta64_ns_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of the timedelta64[ns] dtype. This is a very specific dtype, so generic ones like np.timedelta64 will return False if passed into this function.",Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of the timedelta64[ns] dtype.,"["">>> from pandas.core.dtypes.common import is_timedelta64_ns_dtype\n>>> is_timedelta64_ns_dtype(np.dtype('m8[ns]'))\nTrue\n>>> is_timedelta64_ns_dtype(np.dtype('m8[ps]'))  # Wrong frequency\nFalse\n>>> is_timedelta64_ns_dtype(np.array([1, 2], dtype='m8[ns]'))\nTrue\n>>> is_timedelta64_ns_dtype(np.array([1, 2], dtype=np.timedelta64))\nFalse""]"
628,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.kwds.html,pandas.tseries.offsets.CustomBusinessHour.kwds,CustomBusinessHour.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
629,..\pandas\reference\api\pandas.tseries.offsets.MonthEnd.rule_code.html,pandas.tseries.offsets.MonthEnd.rule_code,MonthEnd.rule_code#,No parameters found,[]
630,..\pandas\reference\api\pandas.Series.equals.html,pandas.Series.equals,"Series.equals(other)[source]# Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The row/column index do not need to have the same type, as long as the values are considered equal. Corresponding columns and index must be of the same dtype.","Parameters: otherSeries or DataFrameThe other Series or DataFrame to be compared with the first. Returns: boolTrue if all elements are the same in both objects, False otherwise.","['>>> df = pd.DataFrame({1: [10], 2: [20]})\n>>> df\n    1   2\n0  10  20', '>>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n>>> exactly_equal\n    1   2\n0  10  20\n>>> df.equals(exactly_equal)\nTrue', '>>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n>>> different_column_type\n   1.0  2.0\n0   10   20\n>>> df.equals(different_column_type)\nTrue', '>>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n>>> different_data_type\n      1     2\n0  10.0  20.0\n>>> df.equals(different_data_type)\nFalse']"
631,..\pandas\reference\api\pandas.TimedeltaIndex.to_pytimedelta.html,pandas.TimedeltaIndex.to_pytimedelta,"TimedeltaIndex.to_pytimedelta(*args, **kwargs)[source]# Return an ndarray of datetime.timedelta objects.",Returns: numpy.ndarray,"["">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='D')\n>>> tdelta_idx\nTimedeltaIndex(['1 days', '2 days', '3 days'],\n                dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.to_pytimedelta()\narray([datetime.timedelta(days=1), datetime.timedelta(days=2),\n       datetime.timedelta(days=3)], dtype=object)""]"
632,..\pandas\reference\api\pandas.DatetimeIndex.day_of_year.html,pandas.DatetimeIndex.day_of_year,property DatetimeIndex.day_of_year[source]# The ordinal day of the year.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.dayofyear\n0    1\n1   32\ndtype: int32', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.dayofyear\nIndex([1, 32], dtype=\'int32\')']"
633,..\pandas\reference\api\pandas.io.formats.style.Styler.template_string.html,pandas.io.formats.style.Styler.template_string,Styler.template_string = <Template 'string.tpl'>#,No parameters found,[]
634,..\pandas\reference\api\pandas.api.types.is_unsigned_integer_dtype.html,pandas.api.types.is_unsigned_integer_dtype,pandas.api.types.is_unsigned_integer_dtype(arr_or_dtype)[source]# Check whether the provided array or dtype is of an unsigned integer dtype. The nullable Integer dtypes (e.g. pandas.UInt64Dtype) are also considered as integer by this function.,Parameters: arr_or_dtypearray-like or dtypeThe array or dtype to check. Returns: booleanWhether or not the array or dtype is of an unsigned integer dtype.,"["">>> from pandas.api.types import is_unsigned_integer_dtype\n>>> is_unsigned_integer_dtype(str)\nFalse\n>>> is_unsigned_integer_dtype(int)  # signed\nFalse\n>>> is_unsigned_integer_dtype(float)\nFalse\n>>> is_unsigned_integer_dtype(np.uint64)\nTrue\n>>> is_unsigned_integer_dtype('uint8')\nTrue\n>>> is_unsigned_integer_dtype('UInt8')\nTrue\n>>> is_unsigned_integer_dtype(pd.UInt8Dtype)\nTrue\n>>> is_unsigned_integer_dtype(np.array(['a', 'b']))\nFalse\n>>> is_unsigned_integer_dtype(pd.Series([1, 2]))  # signed\nFalse\n>>> is_unsigned_integer_dtype(pd.Index([1, 2.]))  # float\nFalse\n>>> is_unsigned_integer_dtype(np.array([1, 2], dtype=np.uint32))\nTrue""]"
635,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.n.html,pandas.tseries.offsets.CustomBusinessHour.n,CustomBusinessHour.n#,No parameters found,[]
636,..\pandas\reference\api\pandas.TimedeltaIndex.to_series.html,pandas.TimedeltaIndex.to_series,"TimedeltaIndex.to_series(index=None, name=None)[source]# Create a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.","Parameters: indexIndex, optionalIndex of resulting Series. If None, defaults to original index. namestr, optionalName of resulting Series. If None, defaults to name of original index. Returns: SeriesThe dtype will be based on the type of the Index values.","["">>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')"", '>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object', '>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object', "">>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object""]"
637,..\pandas\reference\api\pandas.api.types.pandas_dtype.html,pandas.api.types.pandas_dtype,pandas.api.types.pandas_dtype(dtype)[source]# Convert input into a pandas only dtype object or a numpy dtype object.,Parameters: dtypeobject to be converted Returns: np.dtype or a pandas dtype Raises: TypeError if not a dtype,"["">>> pd.api.types.pandas_dtype(int)\ndtype('int64')""]"
638,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.name.html,pandas.tseries.offsets.CustomBusinessHour.name,CustomBusinessHour.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
639,..\pandas\reference\api\pandas.DatetimeIndex.floor.html,pandas.DatetimeIndex.floor,"DatetimeIndex.floor(*args, **kwargs)[source]# Perform floor operation on the data to the specified freq. Notes If the timestamps have a timezone, flooring will take place relative to the local (“wall”) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to floor the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.floor('h')\nDatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.floor(""h"")\n0   2018-01-01 11:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 12:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 03:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.floor(""2h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n             dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.floor(""2h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
640,..\pandas\reference\api\pandas.io.formats.style.Styler.text_gradient.html,pandas.io.formats.style.Styler.text_gradient,"Styler.text_gradient(cmap='PuBu', low=0, high=0, axis=0, subset=None, vmin=None, vmax=None, gmap=None)[source]# Color the text in a gradient style. The text color is determined according to the data in each column, row or frame, or by a given gradient map. Requires matplotlib. Notes When using low and high the range of the gradient, given by the data if gmap is not given or by gmap, is extended at the low end effectively by map.min - low * map.range and at the high end by map.max + high * map.range before the colors are normalized and determined. If combining with vmin and vmax the map.min, map.max and map.range are replaced by values according to the values derived from vmin and vmax. This method will preselect numeric columns and ignore non-numeric columns unless a gmap is supplied in which case no preselection occurs.","Parameters: cmapstr or colormapMatplotlib colormap. lowfloatCompress the color range at the low end. This is a multiple of the data range to extend below the minimum; good values usually in [0, 1], defaults to 0. highfloatCompress the color range at the high end. This is a multiple of the data range to extend above the maximum; good values usually in [0, 1], defaults to 0. axis{0, 1, “index”, “columns”, None}, default 0Apply to each column (axis=0 or 'index'), to each row (axis=1 or 'columns'), or to the entire DataFrame at once with axis=None. subsetlabel, array-like, IndexSlice, optionalA valid 2d input to DataFrame.loc[<subset>], or, in the case of a 1d input or single key, to DataFrame.loc[:, <subset>] where the columns are prioritised, to limit data to before applying the function. vminfloat, optionalMinimum data value that corresponds to colormap minimum value. If not specified the minimum value of the data (or gmap) will be used. vmaxfloat, optionalMaximum data value that corresponds to colormap maximum value. If not specified the maximum value of the data (or gmap) will be used. gmaparray-like, optionalGradient map for determining the text colors. If not supplied will use the underlying data from rows, columns or frame. If given as an ndarray or list-like must be an identical shape to the underlying data considering axis and subset. If given as DataFrame or Series must have same index and column labels considering axis and subset. If supplied, vmin and vmax should be given relative to this gradient map. Added in version 1.3.0. Returns: Styler","['>>> df = pd.DataFrame(columns=[""City"", ""Temp (c)"", ""Rain (mm)"", ""Wind (m/s)""],\n...                   data=[[""Stockholm"", 21.6, 5.0, 3.2],\n...                         [""Oslo"", 22.4, 13.3, 3.1],\n...                         [""Copenhagen"", 24.5, 0.0, 6.7]])', '>>> df.style.text_gradient(axis=0)', '>>> df.style.text_gradient(axis=None)', '>>> df.style.text_gradient(axis=None, low=0.75, high=1.0)', '>>> df.style.text_gradient(axis=None, vmin=6.7, vmax=21.6)', "">>> df.style.text_gradient(axis=0, gmap=df['Temp (c)'], cmap='YlOrRd')\n..."", "">>> gmap = np.array([[1,2,3], [2,3,4], [3,4,5]])\n>>> df.style.text_gradient(axis=None, gmap=gmap,\n...     cmap='YlOrRd', subset=['Temp (c)', 'Rain (mm)', 'Wind (m/s)']\n... )""]"
641,..\pandas\reference\api\pandas.Series.ewm.html,pandas.Series.ewm,"Series.ewm(com=None, span=None, halflife=None, alpha=None, min_periods=0, adjust=True, ignore_na=False, axis=<no_default>, times=None, method='single')[source]# Provide exponentially weighted (EW) calculations. Exactly one of com, span, halflife, or alpha must be provided if times is not provided. If times is provided, halflife and one of com, span or alpha may be provided. Notes See Windowing Operations for further usage details and examples.","Parameters: comfloat, optionalSpecify decay in terms of center of mass \(\alpha = 1 / (1 + com)\), for \(com \geq 0\). spanfloat, optionalSpecify decay in terms of span \(\alpha = 2 / (span + 1)\), for \(span \geq 1\). halflifefloat, str, timedelta, optionalSpecify decay in terms of half-life \(\alpha = 1 - \exp\left(-\ln(2) / halflife\right)\), for \(halflife > 0\). If times is specified, a timedelta convertible unit over which an observation decays to half its value. Only applicable to mean(), and halflife value will not apply to the other functions. alphafloat, optionalSpecify smoothing factor \(\alpha\) directly \(0 < \alpha \leq 1\). min_periodsint, default 0Minimum number of observations in window required to have a value; otherwise, result is np.nan. adjustbool, default TrueDivide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average). When adjust=True (default), the EW function is calculated using weights \(w_i = (1 - \alpha)^i\). For example, the EW moving average of the series [\(x_0, x_1, ..., x_t\)] would be: \[y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ... + (1 - \alpha)^t x_0}{1 + (1 - \alpha) + (1 - \alpha)^2 + ... + (1 - \alpha)^t}\] When adjust=False, the exponentially weighted function is calculated recursively: \[\begin{split}\begin{split}     y_0 &= x_0\\     y_t &= (1 - \alpha) y_{t-1} + \alpha x_t, \end{split}\end{split}\] ignore_nabool, default FalseIgnore missing values when calculating weights. When ignore_na=False (default), weights are based on absolute positions. For example, the weights of \(x_0\) and \(x_2\) used in calculating the final weighted average of [\(x_0\), None, \(x_2\)] are \((1-\alpha)^2\) and \(1\) if adjust=True, and \((1-\alpha)^2\) and \(\alpha\) if adjust=False. When ignore_na=True, weights are based on relative positions. For example, the weights of \(x_0\) and \(x_2\) used in calculating the final weighted average of [\(x_0\), None, \(x_2\)] are \(1-\alpha\) and \(1\) if adjust=True, and \(1-\alpha\) and \(\alpha\) if adjust=False. axis{0, 1}, default 0If 0 or 'index', calculate across the rows. If 1 or 'columns', calculate across the columns. For Series this parameter is unused and defaults to 0. timesnp.ndarray, Series, default NoneOnly applicable to mean(). Times corresponding to the observations. Must be monotonically increasing and datetime64[ns] dtype. If 1-D array like, a sequence with the same shape as the observations. methodstr {‘single’, ‘table’}, default ‘single’ Added in version 1.4.0. Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Only applicable to mean() Returns: pandas.api.typing.ExponentialMovingWindow","["">>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0"", '>>> df.ewm(com=0.5).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213\n>>> df.ewm(alpha=2 / 3).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213', '>>> df.ewm(com=0.5, adjust=True).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213\n>>> df.ewm(com=0.5, adjust=False).mean()\n          B\n0  0.000000\n1  0.666667\n2  1.555556\n3  1.555556\n4  3.650794', '>>> df.ewm(com=0.5, ignore_na=True).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.225000\n>>> df.ewm(com=0.5, ignore_na=False).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213', "">>> times = ['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15', '2020-01-17']\n>>> df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean()\n          B\n0  0.000000\n1  0.585786\n2  1.523889\n3  1.523889\n4  3.233686""]"
642,..\pandas\reference\api\pandas.tseries.offsets.Nano.copy.html,pandas.tseries.offsets.Nano.copy,Nano.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
643,..\pandas\reference\api\pandas.api.types.union_categoricals.html,pandas.api.types.union_categoricals,"pandas.api.types.union_categoricals(to_union, sort_categories=False, ignore_order=False)[source]# Combine list-like of Categorical-like, unioning categories. All categories must have the same dtype. Notes To learn more about categories, see link","Parameters: to_unionlist-likeCategorical, CategoricalIndex, or Series with dtype=’category’. sort_categoriesbool, default FalseIf true, resulting categories will be lexsorted, otherwise they will be ordered as they appear in the data. ignore_orderbool, default FalseIf true, the ordered attribute of the Categoricals will be ignored. Results in an unordered categorical. Returns: Categorical Raises: TypeError all inputs do not have the same dtype all inputs do not have the same ordered property all inputs are ordered and their categories are not identical sort_categories=True and Categoricals are ordered ValueErrorEmpty list of categoricals passed","['>>> a = pd.Categorical([""b"", ""c""])\n>>> b = pd.Categorical([""a"", ""b""])\n>>> pd.api.types.union_categoricals([a, b])\n[\'b\', \'c\', \'a\', \'b\']\nCategories (3, object): [\'b\', \'c\', \'a\']', "">>> pd.api.types.union_categoricals([a, b], sort_categories=True)\n['b', 'c', 'a', 'b']\nCategories (3, object): ['a', 'b', 'c']"", '>>> a = pd.Categorical([""a"", ""b""], ordered=True)\n>>> b = pd.Categorical([""a"", ""b"", ""a""], ordered=True)\n>>> pd.api.types.union_categoricals([a, b])\n[\'a\', \'b\', \'a\', \'b\', \'a\']\nCategories (2, object): [\'a\' < \'b\']', '>>> a = pd.Categorical([""a"", ""b""], ordered=True)\n>>> b = pd.Categorical([""a"", ""b"", ""c""], ordered=True)\n>>> pd.api.types.union_categoricals([a, b])\nTraceback (most recent call last):\n    ...\nTypeError: to union ordered Categoricals, all categories must be the same', '>>> a = pd.Categorical([""a"", ""b"", ""c""], ordered=True)\n>>> b = pd.Categorical([""c"", ""b"", ""a""], ordered=True)\n>>> pd.api.types.union_categoricals([a, b], ignore_order=True)\n[\'a\', \'b\', \'c\', \'c\', \'b\', \'a\']\nCategories (3, object): [\'a\', \'b\', \'c\']', '>>> a = pd.Series([""b"", ""c""], dtype=\'category\')\n>>> b = pd.Series([""a"", ""b""], dtype=\'category\')\n>>> pd.api.types.union_categoricals([a, b])\n[\'b\', \'c\', \'a\', \'b\']\nCategories (3, object): [\'b\', \'c\', \'a\']']"
644,..\pandas\reference\api\pandas.timedelta_range.html,pandas.timedelta_range,"pandas.timedelta_range(start=None, end=None, periods=None, freq=None, name=None, closed=None, *, unit=None)[source]# Return a fixed frequency TimedeltaIndex with day as the default. Notes Of the four parameters start, end, periods, and freq, exactly three must be specified. If freq is omitted, the resulting TimedeltaIndex will have periods linearly spaced elements between start and end (closed on both sides). To learn more about the frequency strings, please see this link.","Parameters: startstr or timedelta-like, default NoneLeft bound for generating timedeltas. endstr or timedelta-like, default NoneRight bound for generating timedeltas. periodsint, default NoneNumber of periods to generate. freqstr, Timedelta, datetime.timedelta, or DateOffset, default ‘D’Frequency strings can have multiples, e.g. ‘5h’. namestr, default NoneName of the resulting TimedeltaIndex. closedstr, default NoneMake the interval closed with respect to the given frequency to the ‘left’, ‘right’, or both sides (None). unitstr, default NoneSpecify the desired resolution of the result. Added in version 2.0.0. Returns: TimedeltaIndex","["">>> pd.timedelta_range(start='1 day', periods=4)\nTimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],\n               dtype='timedelta64[ns]', freq='D')"", "">>> pd.timedelta_range(start='1 day', periods=4, closed='right')\nTimedeltaIndex(['2 days', '3 days', '4 days'],\n               dtype='timedelta64[ns]', freq='D')"", "">>> pd.timedelta_range(start='1 day', end='2 days', freq='6h')\nTimedeltaIndex(['1 days 00:00:00', '1 days 06:00:00', '1 days 12:00:00',\n                '1 days 18:00:00', '2 days 00:00:00'],\n               dtype='timedelta64[ns]', freq='6h')"", "">>> pd.timedelta_range(start='1 day', end='5 days', periods=4)\nTimedeltaIndex(['1 days 00:00:00', '2 days 08:00:00', '3 days 16:00:00',\n                '5 days 00:00:00'],\n               dtype='timedelta64[ns]', freq=None)"", '>>> pd.timedelta_range(""1 Day"", periods=3, freq=""100000D"", unit=""s"")\nTimedeltaIndex([\'1 days\', \'100001 days\', \'200001 days\'],\n               dtype=\'timedelta64[s]\', freq=\'100000D\')']"
645,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.nanos.html,pandas.tseries.offsets.CustomBusinessHour.nanos,CustomBusinessHour.nanos#,No parameters found,[]
646,..\pandas\reference\api\pandas.core.window.rolling.Rolling.aggregate.html,pandas.core.window.rolling.Rolling.aggregate,"Rolling.aggregate(func, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a Series/Dataframe or when passed to Series/Dataframe.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","['>>> df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6], ""C"": [7, 8, 9]})\n>>> df\n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9', '>>> df.rolling(2).sum()\n     A     B     C\n0  NaN   NaN   NaN\n1  3.0   9.0  15.0\n2  5.0  11.0  17.0', '>>> df.rolling(2).agg({""A"": ""sum"", ""B"": ""min""})\n     A    B\n0  NaN  NaN\n1  3.0  4.0\n2  5.0  5.0']"
647,..\pandas\reference\api\pandas.array.html,pandas.array,"pandas.array(data, dtype=None, copy=True)[source]# Create an array. Notes Omitting the dtype argument means pandas will attempt to infer the best array type from the values in the data. As new array types are added by pandas and 3rd party libraries, the “best” array type may change. We recommend specifying dtype to ensure that the correct array type for the data is returned the returned array type doesn’t change as new extension types are added by pandas and third-party libraries Additionally, if the underlying memory representation of the returned array matters, we recommend specifying the dtype as a concrete object rather than a string alias or allowing it to be inferred. For example, a future version of pandas or a 3rd-party library may include a dedicated ExtensionArray for string data. In this event, the following would no longer return a arrays.NumpyExtensionArray backed by a NumPy array. This would instead return the new ExtensionArray dedicated for string data. If you really need the new array to be backed by a  NumPy array, specify that in the dtype. Finally, Pandas has arrays that mostly overlap with NumPy arrays.DatetimeArray arrays.TimedeltaArray When data with a datetime64[ns] or timedelta64[ns] dtype is passed, pandas will always return a DatetimeArray or TimedeltaArray rather than a NumpyExtensionArray. This is for symmetry with the case of timezone-aware data, which NumPy does not natively support.","Parameters: dataSequence of objectsThe scalars inside data should be instances of the scalar type for dtype. It’s expected that data represents a 1-dimensional array of data. When data is an Index or Series, the underlying array will be extracted from data. dtypestr, np.dtype, or ExtensionDtype, optionalThe dtype to use for the array. This may be a NumPy dtype or an extension type registered with pandas using pandas.api.extensions.register_extension_dtype(). If not specified, there are two possibilities: When data is a Series, Index, or ExtensionArray, the dtype will be taken from the data. Otherwise, pandas will attempt to infer the dtype from the data. Note that when data is a NumPy array, data.dtype is not used for inferring the array type. This is because NumPy cannot represent all the types of data that can be held in extension arrays. Currently, pandas will infer an extension dtype for sequences of Scalar Type Array Type pandas.Interval pandas.arrays.IntervalArray pandas.Period pandas.arrays.PeriodArray datetime.datetime pandas.arrays.DatetimeArray datetime.timedelta pandas.arrays.TimedeltaArray int pandas.arrays.IntegerArray float pandas.arrays.FloatingArray str pandas.arrays.StringArray or pandas.arrays.ArrowStringArray bool pandas.arrays.BooleanArray The ExtensionArray created when the scalar type is str is determined by pd.options.mode.string_storage if the dtype is not explicitly given. For all other cases, NumPy’s usual inference rules will be used. copybool, default TrueWhether to copy the data, even if not necessary. Depending on the type of data, creating the new array may require copying data, even if copy=False. Returns: ExtensionArrayThe newly created array. Raises: ValueErrorWhen data is not 1-dimensional.","["">>> pd.array(['a', 'b'], dtype=str)\n<NumpyExtensionArray>\n['a', 'b']\nLength: 2, dtype: str32"", '>>> pd.array([\'a\', \'b\'], dtype=np.dtype(""<U1""))\n<NumpyExtensionArray>\n[\'a\', \'b\']\nLength: 2, dtype: str32', "">>> pd.array(['2015', '2016'], dtype='datetime64[ns]')\n<DatetimeArray>\n['2015-01-01 00:00:00', '2016-01-01 00:00:00']\nLength: 2, dtype: datetime64[ns]"", '>>> pd.array([""1h"", ""2h""], dtype=\'timedelta64[ns]\')\n<TimedeltaArray>\n[\'0 days 01:00:00\', \'0 days 02:00:00\']\nLength: 2, dtype: timedelta64[ns]', '>>> pd.array([1, 2])\n<IntegerArray>\n[1, 2]\nLength: 2, dtype: Int64', '>>> pd.array([1, 2, np.nan])\n<IntegerArray>\n[1, 2, <NA>]\nLength: 3, dtype: Int64', '>>> pd.array([1.1, 2.2])\n<FloatingArray>\n[1.1, 2.2]\nLength: 2, dtype: Float64', '>>> pd.array([""a"", None, ""c""])\n<StringArray>\n[\'a\', <NA>, \'c\']\nLength: 3, dtype: string', '>>> with pd.option_context(""string_storage"", ""pyarrow""):\n...     arr = pd.array([""a"", None, ""c""])\n...\n>>> arr\n<ArrowStringArray>\n[\'a\', <NA>, \'c\']\nLength: 3, dtype: string', '>>> pd.array([pd.Period(\'2000\', freq=""D""), pd.Period(""2000"", freq=""D"")])\n<PeriodArray>\n[\'2000-01-01\', \'2000-01-01\']\nLength: 2, dtype: period[D]', "">>> pd.array(['a', 'b', 'a'], dtype='category')\n['a', 'b', 'a']\nCategories (2, object): ['a', 'b']"", "">>> pd.array(['a', 'b', 'a'],\n...          dtype=pd.CategoricalDtype(['a', 'b', 'c'], ordered=True))\n['a', 'b', 'a']\nCategories (3, object): ['a' < 'b' < 'c']"", '>>> pd.array([1 + 1j, 3 + 2j])\n<NumpyExtensionArray>\n[(1+1j), (3+2j)]\nLength: 2, dtype: complex128', '>>> pd.array([1, 2], dtype=np.dtype(""int32""))\n<NumpyExtensionArray>\n[1, 2]\nLength: 2, dtype: int32', "">>> pd.array(1)\nTraceback (most recent call last):\n  ...\nValueError: Cannot pass scalar '1' to 'pandas.array'.""]"
648,..\pandas\reference\api\pandas.Series.expanding.html,pandas.Series.expanding,"Series.expanding(min_periods=1, axis=<no_default>, method='single')[source]# Provide expanding window calculations. Notes See Windowing Operations for further usage details and examples.","Parameters: min_periodsint, default 1Minimum number of observations in window required to have a value; otherwise, result is np.nan. axisint or str, default 0If 0 or 'index', roll across the rows. If 1 or 'columns', roll across the columns. For Series this parameter is unused and defaults to 0. methodstr {‘single’, ‘table’}, default ‘single’Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Added in version 1.3.0. Returns: pandas.api.typing.Expanding","['>>> df = pd.DataFrame({""B"": [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0', '>>> df.expanding(1).sum()\n     B\n0  0.0\n1  1.0\n2  3.0\n3  3.0\n4  7.0\n>>> df.expanding(3).sum()\n     B\n0  NaN\n1  NaN\n2  3.0\n3  3.0\n4  7.0']"
649,..\pandas\reference\api\pandas.io.formats.style.Styler.to_excel.html,pandas.io.formats.style.Styler.to_excel,"Styler.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options=None)[source]# Write Styler to an Excel sheet. To write a single Styler to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name. With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased. Notes For compatibility with to_csv(), to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible to write further data without rewriting the whole workbook.","Parameters: excel_writerpath-like, file-like, or ExcelWriter objectFile path or existing ExcelWriter. sheet_namestr, default ‘Sheet1’Name of sheet which will contain DataFrame. na_repstr, default ‘’Missing data representation. float_formatstr, optionalFormat string for floating point numbers. For example float_format=""%.2f"" will format 0.1234 to 0.12. columnssequence or list of str, optionalColumns to write. headerbool or list of str, default TrueWrite out the column names. If a list of string is given it is assumed to be aliases for the column names. indexbool, default TrueWrite row names (index). index_labelstr or sequence, optionalColumn label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrowint, default 0Upper left cell row to dump data frame. startcolint, default 0Upper left cell column to dump data frame. enginestr, optionalWrite engine to use, ‘openpyxl’ or ‘xlsxwriter’. You can also set this via the options io.excel.xlsx.writer or io.excel.xlsm.writer. merge_cellsbool, default TrueWrite MultiIndex and Hierarchical Rows as merged cells. inf_repstr, default ‘inf’Representation for infinity (there is no native representation for infinity in Excel). freeze_panestuple of int (length 2), optionalSpecifies the one-based bottommost row and rightmost column that is to be frozen. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Added in version 1.5.0. engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.","['>>> df1 = pd.DataFrame([[\'a\', \'b\'], [\'c\', \'d\']],\n...                    index=[\'row 1\', \'row 2\'],\n...                    columns=[\'col 1\', \'col 2\'])\n>>> df1.to_excel(""output.xlsx"")', '>>> df1.to_excel(""output.xlsx"",\n...              sheet_name=\'Sheet_name_1\')', "">>> df2 = df1.copy()\n>>> with pd.ExcelWriter('output.xlsx') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n...     df2.to_excel(writer, sheet_name='Sheet_name_2')"", "">>> with pd.ExcelWriter('output.xlsx',\n...                     mode='a') as writer:  \n...     df1.to_excel(writer, sheet_name='Sheet_name_3')"", "">>> df1.to_excel('output1.xlsx', engine='xlsxwriter')""]"
650,..\pandas\reference\api\pandas.tseries.offsets.Nano.delta.html,pandas.tseries.offsets.Nano.delta,Nano.delta#,No parameters found,[]
651,..\pandas\reference\api\pandas.Timestamp.asm8.html,pandas.Timestamp.asm8,Timestamp.asm8# Return numpy datetime64 format in nanoseconds.,No parameters found,"["">>> ts = pd.Timestamp(2020, 3, 14, 15)\n>>> ts.asm8\nnumpy.datetime64('2020-03-14T15:00:00.000000')""]"
652,..\pandas\reference\api\pandas.core.window.rolling.Rolling.apply.html,pandas.core.window.rolling.Rolling.apply,"Rolling.apply(func, raw=False, engine=None, engine_kwargs=None, args=None, kwargs=None)[source]# Calculate the rolling custom aggregation function.","Parameters: funcfunctionMust produce a single value from an ndarray input if raw=True or a single value from a Series if raw=False. Can also accept a Numba JIT function with engine='numba' specified. rawbool, default False False : passes each row or column as a Series to the function. True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. enginestr, default None 'cython' : Runs rolling apply through C-extensions from cython. 'numba' : Runs rolling apply through JIT compiled code from numba. Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply rolling aggregation. argstuple, default NonePositional arguments to be passed into func. kwargsdict, default NoneKeyword arguments to be passed into func. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 6, 5, 4])\n>>> ser.rolling(2).apply(lambda s: s.sum() - s.min())\n0    NaN\n1    6.0\n2    6.0\n3    5.0\ndtype: float64']"
653,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.normalize.html,pandas.tseries.offsets.CustomBusinessHour.normalize,CustomBusinessHour.normalize#,No parameters found,[]
654,..\pandas\reference\api\pandas.DatetimeIndex.freq.html,pandas.DatetimeIndex.freq,property DatetimeIndex.freq[source]#,No parameters found,[]
655,..\pandas\reference\api\pandas.arrays.ArrowExtensionArray.html,pandas.arrays.ArrowExtensionArray,class pandas.arrays.ArrowExtensionArray(values)[source]# Pandas ExtensionArray backed by a PyArrow ChunkedArray. Warning ArrowExtensionArray is considered experimental. The implementation and parts of the API may change without warning. Attributes None Methods None Returns: ArrowExtensionArray Notes Most methods are implemented using pyarrow compute functions. Some methods may either raise an exception or raise a PerformanceWarning if an associated compute function is not available based on the installed version of PyArrow. Please install the latest version of PyArrow to enable the best functionality and avoid potential bugs in prior versions of PyArrow.,Parameters: valuespyarrow.Array or pyarrow.ChunkedArray,"['>>> pd.array([1, 1, None], dtype=""int64[pyarrow]"")\n<ArrowExtensionArray>\n[1, 1, <NA>]\nLength: 3, dtype: int64[pyarrow]']"
656,..\pandas\reference\api\pandas.arrays.ArrowStringArray.html,pandas.arrays.ArrowStringArray,class pandas.arrays.ArrowStringArray(values)[source]# Extension array for string data in a pyarrow.ChunkedArray. Warning ArrowStringArray is considered experimental. The implementation and parts of the API may change without warning. Attributes None Methods None Notes ArrowStringArray returns a BooleanArray for comparison methods.,Parameters: valuespyarrow.Array or pyarrow.ChunkedArrayThe array of data.,"['>>> pd.array([\'This is\', \'some text\', None, \'data.\'], dtype=""string[pyarrow]"")\n<ArrowStringArray>\n[\'This is\', \'some text\', <NA>, \'data.\']\nLength: 4, dtype: string']"
657,..\pandas\reference\api\pandas.Series.explode.html,pandas.Series.explode,"Series.explode(ignore_index=False)[source]# Transform each element of a list-like to a row. Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of elements in the output will be non-deterministic when exploding sets. Reference the user guide for more examples.","Parameters: ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, …, n - 1. Returns: SeriesExploded lists to rows; index will be duplicated for these rows.","["">>> s = pd.Series([[1, 2, 3], 'foo', [], [3, 4]])\n>>> s\n0    [1, 2, 3]\n1          foo\n2           []\n3       [3, 4]\ndtype: object"", '>>> s.explode()\n0      1\n0      2\n0      3\n1    foo\n2    NaN\n3      3\n3      4\ndtype: object']"
658,..\pandas\reference\api\pandas.io.formats.style.Styler.to_html.html,pandas.io.formats.style.Styler.to_html,"Styler.to_html(buf=None, *, table_uuid=None, table_attributes=None, sparse_index=None, sparse_columns=None, bold_headers=False, caption=None, max_rows=None, max_columns=None, encoding=None, doctype_html=False, exclude_styles=False, **kwargs)[source]# Write Styler to a file, buffer or string in HTML-CSS format. Added in version 1.3.0.","Parameters: bufstr, path object, file-like object, optionalString, path object (implementing os.PathLike[str]), or file-like object implementing a string write() function. If None, the result is returned as a string. table_uuidstr, optionalId attribute assigned to the <table> HTML element in the format: <table id=""T_<table_uuid>"" ..> If not given uses Styler’s initially assigned value. table_attributesstr, optionalAttributes to assign within the <table> HTML element in the format: <table .. <table_attributes> > If not given defaults to Styler’s preexisting value. sparse_indexbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.index value. Added in version 1.4.0. sparse_columnsbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each column. Defaults to pandas.options.styler.sparse.columns value. Added in version 1.4.0. bold_headersbool, optionalAdds “font-weight: bold;” as a CSS property to table style header cells. Added in version 1.4.0. captionstr, optionalSet, or overwrite, the caption on Styler before rendering. Added in version 1.4.0. max_rowsint, optionalThe maximum number of rows that will be rendered. Defaults to pandas.options.styler.render.max_rows/max_columns. Added in version 1.4.0. max_columnsint, optionalThe maximum number of columns that will be rendered. Defaults to pandas.options.styler.render.max_columns, which is None. Rows and columns may be reduced if the number of total elements is large. This value is set to pandas.options.styler.render.max_elements, which is 262144 (18 bit browser rendering). Added in version 1.4.0. encodingstr, optionalCharacter encoding setting for file output (and meta tags if available). Defaults to pandas.options.styler.render.encoding value of “utf-8”. doctype_htmlbool, default FalseWhether to output a fully structured HTML file including all HTML elements, or just the core <style> and <table> elements. exclude_stylesbool, default FalseWhether to include the <style> element and all associated element class and id identifiers, or solely the <table> element without styling identifiers. **kwargsAny additional keyword arguments are passed through to the jinja2 self.template.render process. This is useful when you need to provide additional variables for a custom template. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","['>>> df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]})\n>>> print(df.style.to_html())  \n<style type=""text/css"">\n</style>\n<table id=""T_1e78e"">\n  <thead>\n    <tr>\n      <th class=""blank level0"" >&nbsp;</th>\n      <th id=""T_1e78e_level0_col0"" class=""col_heading level0 col0"" >A</th>\n      <th id=""T_1e78e_level0_col1"" class=""col_heading level0 col1"" >B</th>\n    </tr>\n...']"
659,..\pandas\reference\api\pandas.DatetimeIndex.freqstr.html,pandas.DatetimeIndex.freqstr,"property DatetimeIndex.freqstr[source]# Return the frequency object as a string if it’s set, otherwise None.",No parameters found,"['>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00""], freq=""D"")\n>>> idx.freqstr\n\'D\'', '>>> idx = pd.DatetimeIndex([""2018-01-01"", ""2018-01-03"", ""2018-01-05""],\n...                        freq=""infer"")\n>>> idx.freqstr\n\'2D\'', '>>> idx = pd.PeriodIndex([""2023-1"", ""2023-2"", ""2023-3""], freq=""M"")\n>>> idx.freqstr\n\'M\'']"
660,..\pandas\reference\api\pandas.core.window.rolling.Rolling.corr.html,pandas.core.window.rolling.Rolling.corr,"Rolling.corr(other=None, pairwise=None, ddof=1, numeric_only=False)[source]# Calculate the rolling correlation. Notes This function uses Pearson’s definition of correlation (https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). When other is not specified, the output will be self correlation (e.g. all 1’s), except for DataFrame inputs with pairwise set to True. Function will return NaN for correlations of equal valued sequences; this is the result of a 0/0 division error. When pairwise is set to False, only matching columns between self and other will be used. When pairwise is set to True, the output will be a MultiIndex DataFrame with the original index on the first level, and the other DataFrame columns on the second level. In the case of missing elements, only complete pairwise observations will be used.","Parameters: otherSeries or DataFrame, optionalIf not supplied then will default to self and produce pairwise output. pairwisebool, default NoneIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used. ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> v1 = [3, 3, 3, 5, 8]\n>>> v2 = [3, 4, 4, 4, 8]\n>>> np.corrcoef(v1[:-1], v2[:-1])\narray([[1.        , 0.33333333],\n       [0.33333333, 1.        ]])\n>>> np.corrcoef(v1[1:], v2[1:])\narray([[1.       , 0.9169493],\n       [0.9169493, 1.       ]])\n>>> s1 = pd.Series(v1)\n>>> s2 = pd.Series(v2)\n>>> s1.rolling(4).corr(s2)\n0         NaN\n1         NaN\n2         NaN\n3    0.333333\n4    0.916949\ndtype: float64', "">>> matrix = np.array([[51., 35.],\n...                    [49., 30.],\n...                    [47., 32.],\n...                    [46., 31.],\n...                    [50., 36.]])\n>>> np.corrcoef(matrix[:-1, 0], matrix[:-1, 1])\narray([[1.       , 0.6263001],\n       [0.6263001, 1.       ]])\n>>> np.corrcoef(matrix[1:, 0], matrix[1:, 1])\narray([[1.        , 0.55536811],\n       [0.55536811, 1.        ]])\n>>> df = pd.DataFrame(matrix, columns=['X', 'Y'])\n>>> df\n      X     Y\n0  51.0  35.0\n1  49.0  30.0\n2  47.0  32.0\n3  46.0  31.0\n4  50.0  36.0\n>>> df.rolling(4).corr(pairwise=True)\n            X         Y\n0 X       NaN       NaN\n  Y       NaN       NaN\n1 X       NaN       NaN\n  Y       NaN       NaN\n2 X       NaN       NaN\n  Y       NaN       NaN\n3 X  1.000000  0.626300\n  Y  0.626300  1.000000\n4 X  1.000000  0.555368\n  Y  0.555368  1.000000""]"
661,..\pandas\reference\api\pandas.arrays.BooleanArray.html,pandas.arrays.BooleanArray,"class pandas.arrays.BooleanArray(values, mask, copy=False)[source]# Array of boolean (True/False) data with missing values. This is a pandas Extension array for boolean data, under the hood represented by 2 numpy arrays: a boolean array with the data and a boolean array with the mask (True indicating missing). BooleanArray implements Kleene logic (sometimes called three-value logic) for logical operations. See Kleene logical operations for more. To construct an BooleanArray from generic array-like input, use pandas.array() specifying dtype=""boolean"" (see examples below). Warning BooleanArray is considered experimental. The implementation and parts of the API may change without warning. Attributes None Methods None Returns: BooleanArray","Parameters: valuesnumpy.ndarrayA 1-d boolean-dtype array with the data. masknumpy.ndarrayA 1-d boolean-dtype array indicating missing values (True indicates missing). copybool, default FalseWhether to copy the values and mask arrays.","['>>> pd.array([True, False, None], dtype=""boolean"")\n<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean']"
662,..\pandas\reference\api\pandas.tseries.offsets.Nano.freqstr.html,pandas.tseries.offsets.Nano.freqstr,Nano.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
663,..\pandas\reference\api\pandas.Timestamp.astimezone.html,pandas.Timestamp.astimezone,Timestamp.astimezone(tz)# Convert timezone-aware Timestamp to another time zone.,"Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile or NoneTime zone for time which Timestamp will be converted to. None will remove timezone holding UTC time. Returns: convertedTimestamp Raises: TypeErrorIf Timestamp is tz-naive.","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651', tz='UTC')\n>>> ts\nTimestamp('2020-03-14 15:32:52.192548651+0000', tz='UTC')"", "">>> ts.tz_convert(tz='Asia/Tokyo')\nTimestamp('2020-03-15 00:32:52.192548651+0900', tz='Asia/Tokyo')"", "">>> ts.astimezone(tz='Asia/Tokyo')\nTimestamp('2020-03-15 00:32:52.192548651+0900', tz='Asia/Tokyo')"", "">>> pd.NaT.tz_convert(tz='Asia/Tokyo')\nNaT""]"
664,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.rule_code.html,pandas.tseries.offsets.CustomBusinessHour.rule_code,CustomBusinessHour.rule_code#,No parameters found,[]
665,..\pandas\reference\api\pandas.Series.factorize.html,pandas.Series.factorize,"Series.factorize(sort=False, use_na_sentinel=True)[source]# Encode the object as an enumerated type or categorical variable. This method is useful for obtaining a numeric representation of an array when all that matters is identifying distinct values. factorize is available as both a top-level function pandas.factorize(), and as a method Series.factorize() and Index.factorize(). Notes Reference the user guide for more examples.","Parameters: sortbool, default FalseSort uniques and shuffle codes to maintain the relationship. use_na_sentinelbool, default TrueIf True, the sentinel -1 will be used for NaN values. If False, NaN values will be encoded as non-negative integers and will not drop the NaN from the uniques of the values. Added in version 1.5.0. Returns: codesndarrayAn integer ndarray that’s an indexer into uniques. uniques.take(codes) will have the same values as values. uniquesndarray, Index, or CategoricalThe unique valid values. When values is Categorical, uniques is a Categorical. When values is some other pandas object, an Index is returned. Otherwise, a 1-D ndarray is returned. Note Even if there’s a missing value in values, uniques will not contain an entry for it.","['>>> codes, uniques = pd.factorize(np.array([\'b\', \'b\', \'a\', \'c\', \'b\'], dtype=""O""))\n>>> codes\narray([0, 0, 1, 2, 0])\n>>> uniques\narray([\'b\', \'a\', \'c\'], dtype=object)', '>>> codes, uniques = pd.factorize(np.array([\'b\', \'b\', \'a\', \'c\', \'b\'], dtype=""O""),\n...                               sort=True)\n>>> codes\narray([1, 1, 0, 2, 1])\n>>> uniques\narray([\'a\', \'b\', \'c\'], dtype=object)', '>>> codes, uniques = pd.factorize(np.array([\'b\', None, \'a\', \'c\', \'b\'], dtype=""O""))\n>>> codes\narray([ 0, -1,  1,  2,  0])\n>>> uniques\narray([\'b\', \'a\', \'c\'], dtype=object)', "">>> cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])\n>>> codes, uniques = pd.factorize(cat)\n>>> codes\narray([0, 0, 1])\n>>> uniques\n['a', 'c']\nCategories (3, object): ['a', 'b', 'c']"", "">>> cat = pd.Series(['a', 'a', 'c'])\n>>> codes, uniques = pd.factorize(cat)\n>>> codes\narray([0, 0, 1])\n>>> uniques\nIndex(['a', 'c'], dtype='object')"", '>>> values = np.array([1, 2, 1, np.nan])\n>>> codes, uniques = pd.factorize(values)  # default: use_na_sentinel=True\n>>> codes\narray([ 0,  1,  0, -1])\n>>> uniques\narray([1., 2.])', '>>> codes, uniques = pd.factorize(values, use_na_sentinel=False)\n>>> codes\narray([0, 1, 0, 2])\n>>> uniques\narray([ 1.,  2., nan])']"
666,..\pandas\reference\api\pandas.io.formats.style.Styler.to_latex.html,pandas.io.formats.style.Styler.to_latex,"Styler.to_latex(buf=None, *, column_format=None, position=None, position_float=None, hrules=None, clines=None, label=None, caption=None, sparse_index=None, sparse_columns=None, multirow_align=None, multicol_align=None, siunitx=False, environment=None, encoding=None, convert_css=False)[source]# Write Styler to a file, buffer or string in LaTeX format. Added in version 1.3.0. Notes Latex Packages For the following features we recommend the following LaTeX inclusions: Feature Inclusion sparse columns none: included within default {tabular} environment sparse rows \usepackage{multirow} hrules \usepackage{booktabs} colors \usepackage[table]{xcolor} siunitx \usepackage{siunitx} bold (with siunitx) \usepackage{etoolbox} \robustify\bfseries \sisetup{detect-all = true}  (within {document}) italic (with siunitx) \usepackage{etoolbox} \robustify\itshape \sisetup{detect-all = true}  (within {document}) environment \usepackage{longtable} if arg is “longtable” | or any other relevant environment package hyperlinks \usepackage{hyperref} Cell Styles LaTeX styling can only be rendered if the accompanying styling functions have been constructed with appropriate LaTeX commands. All styling functionality is built around the concept of a CSS (<attribute>, <value>) pair (see Table Visualization), and this should be replaced by a LaTeX (<command>, <options>) approach. Each cell will be styled individually using nested LaTeX commands with their accompanied options. For example the following code will highlight and bold a cell in HTML-CSS: The equivalent using LaTeX only commands is the following: Internally these structured LaTeX (<command>, <options>) pairs are translated to the display_value with the default structure: \<command><options> <display_value>. Where there are multiple commands the latter is nested recursively, so that the above example highlighted cell is rendered as \cellcolor{red} \bfseries 4. Occasionally this format does not suit the applied command, or combination of LaTeX packages that is in use, so additional flags can be added to the <options>, within the tuple, to result in different positions of required braces (the default being the same as --nowrap): Tuple Format Output Structure (<command>,<options>) \<command><options> <display_value> (<command>,<options> --nowrap) \<command><options> <display_value> (<command>,<options> --rwrap) \<command><options>{<display_value>} (<command>,<options> --wrap) {\<command><options> <display_value>} (<command>,<options> --lwrap) {\<command><options>} <display_value> (<command>,<options> --dwrap) {\<command><options>}{<display_value>} For example the textbf command for font-weight should always be used with –rwrap so ('textbf', '--rwrap') will render a working cell, wrapped with braces, as \textbf{<display_value>}. A more comprehensive example is as follows: Table Styles Internally Styler uses its table_styles object to parse the column_format, position, position_float, and label input arguments. These arguments are added to table styles in the format: set_table_styles([     {""selector"": ""column_format"", ""props"": f"":{column_format};""},     {""selector"": ""position"", ""props"": f"":{position};""},     {""selector"": ""position_float"", ""props"": f"":{position_float};""},     {""selector"": ""label"", ""props"": f"":{{{label.replace(':','§')}}};""} ], overwrite=False) Exception is made for the hrules argument which, in fact, controls all three commands: toprule, bottomrule and midrule simultaneously. Instead of setting hrules to True, it is also possible to set each individual rule definition, by manually setting the table_styles, for example below we set a regular toprule, set an hline for bottomrule and exclude the midrule: set_table_styles([     {'selector': 'toprule', 'props': ':toprule;'},     {'selector': 'bottomrule', 'props': ':hline;'}, ], overwrite=False) If other commands are added to table styles they will be detected, and positioned immediately above the ‘\begin{tabular}’ command. For example to add odd and even row coloring, from the {colortbl} package, in format \rowcolors{1}{pink}{red}, use: set_table_styles([     {'selector': 'rowcolors', 'props': ':{1}{pink}{red};'} ], overwrite=False) A more comprehensive example using these arguments is as follows: Formatting To format values Styler.format() should be used prior to calling Styler.to_latex, as well as other methods such as Styler.hide() for example: CSS Conversion This method can convert a Styler constructured with HTML-CSS to LaTeX using the following limited conversions. CSS Attribute CSS value LaTeX Command LaTeX Options font-weight bold bolder bfseries bfseries font-style italic oblique itshape slshape background-color red #fe01ea #f0e rgb(128,255,0) rgba(128,0,0,0.5) rgb(25%,255,50%) cellcolor {red}–lwrap [HTML]{FE01EA}–lwrap [HTML]{FF00EE}–lwrap [rgb]{0.5,1,0}–lwrap [rgb]{0.5,0,0}–lwrap [rgb]{0.25,1,0.5}–lwrap color red #fe01ea #f0e rgb(128,255,0) rgba(128,0,0,0.5) rgb(25%,255,50%) color {red} [HTML]{FE01EA} [HTML]{FF00EE} [rgb]{0.5,1,0} [rgb]{0.5,0,0} [rgb]{0.25,1,0.5} It is also possible to add user-defined LaTeX only styles to a HTML-CSS Styler using the --latex flag, and to add LaTeX parsing options that the converter will detect within a CSS-comment.","Parameters: bufstr, path object, file-like object, or None, default NoneString, path object (implementing os.PathLike[str]), or file-like object implementing a string write() function. If None, the result is returned as a string. column_formatstr, optionalThe LaTeX column specification placed in location: \begin{tabular}{<column_format>} Defaults to ‘l’ for index and non-numeric data columns, and, for numeric data columns, to ‘r’ by default, or ‘S’ if siunitx is True. positionstr, optionalThe LaTeX positional argument (e.g. ‘h!’) for tables, placed in location: \\begin{table}[<position>]. position_float{“centering”, “raggedleft”, “raggedright”}, optionalThe LaTeX float command placed in location: \begin{table}[<position>] \<position_float> Cannot be used if environment is “longtable”. hrulesboolSet to True to add \toprule, \midrule and \bottomrule from the {booktabs} LaTeX package. Defaults to pandas.options.styler.latex.hrules, which is False. Changed in version 1.4.0. clinesstr, optionalUse to control adding \cline commands for the index labels separation. Possible values are: None: no cline commands are added (default). “all;data”: a cline is added for every index value extending the width of the table, including data entries. “all;index”: as above with lines extending only the width of the index entries. “skip-last;data”: a cline is added for each index value except the last level (which is never sparsified), extending the widtn of the table. “skip-last;index”: as above with lines extending only the width of the index entries. Added in version 1.4.0. labelstr, optionalThe LaTeX label included as: \label{<label>}. This is used with \ref{<label>} in the main .tex file. captionstr, tuple, optionalIf string, the LaTeX table caption included as: \caption{<caption>}. If tuple, i.e (“full caption”, “short caption”), the caption included as: \caption[<caption[1]>]{<caption[0]>}. sparse_indexbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.index, which is True. sparse_columnsbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each column. Defaults to pandas.options.styler.sparse.columns, which is True. multirow_align{“c”, “t”, “b”, “naive”}, optionalIf sparsifying hierarchical MultiIndexes whether to align text centrally, at the top or bottom using the multirow package. If not given defaults to pandas.options.styler.latex.multirow_align, which is “c”. If “naive” is given renders without multirow. Changed in version 1.4.0. multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}, optionalIf sparsifying hierarchical MultiIndex columns whether to align text at the left, centrally, or at the right. If not given defaults to pandas.options.styler.latex.multicol_align, which is “r”. If a naive option is given renders without multicol. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. “|r” will draw a rule on the left side of right aligned merged cells. Changed in version 1.4.0. siunitxbool, default FalseSet to True to structure LaTeX compatible with the {siunitx} package. environmentstr, optionalIf given, the environment that will replace ‘table’ in \\begin{table}. If ‘longtable’ is specified then a more suitable template is rendered. If not given defaults to pandas.options.styler.latex.environment, which is None. Added in version 1.4.0. encodingstr, optionalCharacter encoding setting. Defaults to pandas.options.styler.render.encoding, which is “utf-8”. convert_cssbool, default FalseConvert simple cell-styles from CSS to LaTeX format. Any CSS not found in conversion table is dropped. A style can be forced by adding option –latex. See notes. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","["">>> df = pd.DataFrame([[1,2], [3,4]])\n>>> s = df.style.highlight_max(axis=None,\n...                            props='background-color:red; font-weight:bold;')\n>>> s.to_html()"", "">>> s = df.style.highlight_max(axis=None,\n...                            props='cellcolor:{red}; bfseries: ;')\n>>> s.to_latex()"", '>>> df = pd.DataFrame([[1, 2.2, ""dogs""], [3, 4.4, ""cats""], [2, 6.6, ""cows""]],\n...                   index=[""ix1"", ""ix2"", ""ix3""],\n...                   columns=[""Integers"", ""Floats"", ""Strings""])\n>>> s = df.style.highlight_max(\n...     props=\'cellcolor:[HTML]{FFFF00}; color:{red};\'\n...           \'textit:--rwrap; textbf:--rwrap;\'\n... )\n>>> s.to_latex()', '>>> df.columns = pd.MultiIndex.from_tuples([\n...     (""Numeric"", ""Integers""),\n...     (""Numeric"", ""Floats""),\n...     (""Non-Numeric"", ""Strings"")\n... ])\n>>> df.index = pd.MultiIndex.from_tuples([\n...     (""L0"", ""ix1""), (""L0"", ""ix2""), (""L1"", ""ix3"")\n... ])\n>>> s = df.style.highlight_max(\n...     props=\'cellcolor:[HTML]{FFFF00}; color:{red}; itshape:; bfseries:;\'\n... )\n>>> s.to_latex(\n...     column_format=""rrrrr"", position=""h"", position_float=""centering"",\n...     hrules=True, label=""table:5"", caption=""Styled LaTeX Table"",\n...     multirow_align=""t"", multicol_align=""r""\n... )', '>>> s.clear()\n>>> s.table_styles = []\n>>> s.caption = None\n>>> s.format({\n...    (""Numeric"", ""Integers""): \'\\${}\',\n...    (""Numeric"", ""Floats""): \'{:.3f}\',\n...    (""Non-Numeric"", ""Strings""): str.upper\n... })  \n                Numeric      Non-Numeric\n          Integers   Floats    Strings\nL0    ix1       $1   2.200      DOGS\n      ix2       $3   4.400      CATS\nL1    ix3       $2   6.600      COWS', '>>> s.to_latex()  \n\\begin{tabular}{llrrl}\n{} & {} & \\multicolumn{2}{r}{Numeric} & {Non-Numeric} \\\\\n{} & {} & {Integers} & {Floats} & {Strings} \\\\\n\\multirow[c]{2}{*}{L0} & ix1 & \\\\$1 & 2.200 & DOGS \\\\\n & ix2 & \\$3 & 4.400 & CATS \\\\\nL1 & ix3 & \\$2 & 6.600 & COWS \\\\\n\\end{tabular}', '>>> df = pd.DataFrame([[1]])\n>>> df.style.set_properties(\n...     **{""font-weight"": ""bold /* --dwrap */"", ""Huge"": ""--latex--rwrap""}\n... ).to_latex(convert_css=True)  \n\\begin{tabular}{lr}\n{} & {0} \\\\\n0 & {\\bfseries}{\\Huge{1}} \\\\\n\\end{tabular}', '>>> cidx = pd.MultiIndex.from_arrays([\n...     [""Equity"", ""Equity"", ""Equity"", ""Equity"",\n...      ""Stats"", ""Stats"", ""Stats"", ""Stats"", ""Rating""],\n...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer"", """", """", """", """", """"],\n...     [""BP"", ""Shell"", ""H&M"", ""Unilever"",\n...      ""Std Dev"", ""Variance"", ""52w High"", ""52w Low"", """"]\n... ])\n>>> iidx = pd.MultiIndex.from_arrays([\n...     [""Equity"", ""Equity"", ""Equity"", ""Equity""],\n...     [""Energy"", ""Energy"", ""Consumer"", ""Consumer""],\n...     [""BP"", ""Shell"", ""H&M"", ""Unilever""]\n... ])\n>>> styler = pd.DataFrame([\n...     [1, 0.8, 0.66, 0.72, 32.1678, 32.1678**2, 335.12, 240.89, ""Buy""],\n...     [0.8, 1.0, 0.69, 0.79, 1.876, 1.876**2, 14.12, 19.78, ""Hold""],\n...     [0.66, 0.69, 1.0, 0.86, 7, 7**2, 210.9, 140.6, ""Buy""],\n...     [0.72, 0.79, 0.86, 1.0, 213.76, 213.76**2, 2807, 3678, ""Sell""],\n... ], columns=cidx, index=iidx).style', '>>> (styler.format(subset=""Equity"", precision=2)\n...       .format(subset=""Stats"", precision=1, thousands="","")\n...       .format(subset=""Rating"", formatter=str.upper)\n...       .format_index(escape=""latex"", axis=1)\n...       .format_index(escape=""latex"", axis=0)\n...       .hide(level=0, axis=0))', '>>> def rating_color(v):\n...     if v == ""Buy"": color = ""#33ff85""\n...     elif v == ""Sell"": color = ""#ff5933""\n...     else: color = ""#ffdd33""\n...     return f""color: {color}; font-weight: bold;""\n>>> (styler.background_gradient(cmap=""inferno"", subset=""Equity"", vmin=0, vmax=1)\n...       .map(rating_color, subset=""Rating""))', '>>> styler.map_index(\n...     lambda v: ""rotatebox:{45}--rwrap--latex;"", level=2, axis=1\n... )', '>>> styler.to_latex(\n...     caption=""Selected stock correlation and simple statistics."",\n...     clines=""skip-last;data"",\n...     convert_css=True,\n...     position_float=""centering"",\n...     multicol_align=""|c|"",\n...     hrules=True,\n... )  \n\\begin{table}\n\\centering\n\\caption{Selected stock correlation and simple statistics.}\n\\begin{tabular}{llrrrrrrrrl}\n\\toprule\n &  & \\multicolumn{4}{|c|}{Equity} & \\multicolumn{4}{|c|}{Stats} & Rating \\\\\n &  & \\multicolumn{2}{|c|}{Energy} & \\multicolumn{2}{|c|}{Consumer} &\n\\multicolumn{4}{|c|}{} &  \\\\\n &  & \\rotatebox{45}{BP} & \\rotatebox{45}{Shell} & \\rotatebox{45}{H\\&M} &\n\\rotatebox{45}{Unilever} & \\rotatebox{45}{Std Dev} & \\rotatebox{45}{Variance} &\n\\rotatebox{45}{52w High} & \\rotatebox{45}{52w Low} & \\rotatebox{45}{} \\\\\n\\midrule\n\\multirow[c]{2}{*}{Energy} & BP & {\\cellcolor[HTML]{FCFFA4}}\n\\color[HTML]{000000} 1.00 & {\\cellcolor[HTML]{FCA50A}} \\color[HTML]{000000}\n0.80 & {\\cellcolor[HTML]{EB6628}} \\color[HTML]{F1F1F1} 0.66 &\n{\\cellcolor[HTML]{F68013}} \\color[HTML]{F1F1F1} 0.72 & 32.2 & 1,034.8 & 335.1\n& 240.9 & \\color[HTML]{33FF85} \\bfseries BUY \\\\\n & Shell & {\\cellcolor[HTML]{FCA50A}} \\color[HTML]{000000} 0.80 &\n{\\cellcolor[HTML]{FCFFA4}} \\color[HTML]{000000} 1.00 &\n{\\cellcolor[HTML]{F1731D}} \\color[HTML]{F1F1F1} 0.69 &\n{\\cellcolor[HTML]{FCA108}} \\color[HTML]{000000} 0.79 & 1.9 & 3.5 & 14.1 &\n19.8 & \\color[HTML]{FFDD33} \\bfseries HOLD \\\\\n\\cline{1-11}\n\\multirow[c]{2}{*}{Consumer} & H\\&M & {\\cellcolor[HTML]{EB6628}}\n\\color[HTML]{F1F1F1} 0.66 & {\\cellcolor[HTML]{F1731D}} \\color[HTML]{F1F1F1}\n0.69 & {\\cellcolor[HTML]{FCFFA4}} \\color[HTML]{000000} 1.00 &\n{\\cellcolor[HTML]{FAC42A}} \\color[HTML]{000000} 0.86 & 7.0 & 49.0 & 210.9 &\n140.6 & \\color[HTML]{33FF85} \\bfseries BUY \\\\\n & Unilever & {\\cellcolor[HTML]{F68013}} \\color[HTML]{F1F1F1} 0.72 &\n{\\cellcolor[HTML]{FCA108}} \\color[HTML]{000000} 0.79 &\n{\\cellcolor[HTML]{FAC42A}} \\color[HTML]{000000} 0.86 &\n{\\cellcolor[HTML]{FCFFA4}} \\color[HTML]{000000} 1.00 & 213.8 & 45,693.3 &\n2,807.0 & 3,678.0 & \\color[HTML]{FF5933} \\bfseries SELL \\\\\n\\cline{1-11}\n\\bottomrule\n\\end{tabular}\n\\end{table}']"
667,..\pandas\reference\api\pandas.DatetimeIndex.hour.html,pandas.DatetimeIndex.hour,property DatetimeIndex.hour[source]# The hours of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""h"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 01:00:00\n2   2000-01-01 02:00:00\ndtype: datetime64[ns]\n>>> datetime_series.dt.hour\n0    0\n1    1\n2    2\ndtype: int32']"
668,..\pandas\reference\api\pandas.Timestamp.as_unit.html,pandas.Timestamp.as_unit,"Timestamp.as_unit(unit, round_ok=True)# Convert the underlying int64 representaton to the given unit.","Parameters: unit{“ns”, “us”, “ms”, “s”} round_okbool, default TrueIf False and the conversion requires rounding, raise. Returns: Timestamp","["">>> ts = pd.Timestamp('2023-01-01 00:00:00.01')\n>>> ts\nTimestamp('2023-01-01 00:00:00.010000')\n>>> ts.unit\n'ms'\n>>> ts = ts.as_unit('s')\n>>> ts\nTimestamp('2023-01-01 00:00:00')\n>>> ts.unit\n's'""]"
669,..\pandas\reference\api\pandas.io.formats.style.Styler.to_string.html,pandas.io.formats.style.Styler.to_string,"Styler.to_string(buf=None, *, encoding=None, sparse_index=None, sparse_columns=None, max_rows=None, max_columns=None, delimiter=' ')[source]# Write Styler to a file, buffer or string in text format. Added in version 1.5.0.","Parameters: bufstr, path object, file-like object, optionalString, path object (implementing os.PathLike[str]), or file-like object implementing a string write() function. If None, the result is returned as a string. encodingstr, optionalCharacter encoding setting for file output (and meta tags if available). Defaults to pandas.options.styler.render.encoding value of “utf-8”. sparse_indexbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. Defaults to pandas.options.styler.sparse.index value. sparse_columnsbool, optionalWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each column. Defaults to pandas.options.styler.sparse.columns value. max_rowsint, optionalThe maximum number of rows that will be rendered. Defaults to pandas.options.styler.render.max_rows, which is None. max_columnsint, optionalThe maximum number of columns that will be rendered. Defaults to pandas.options.styler.render.max_columns, which is None. Rows and columns may be reduced if the number of total elements is large. This value is set to pandas.options.styler.render.max_elements, which is 262144 (18 bit browser rendering). delimiterstr, default single spaceThe separator between data elements. Returns: str or NoneIf buf is None, returns the result as a string. Otherwise returns None.","["">>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n>>> df.style.to_string()\n' A B\\n0 1 3\\n1 2 4\\n'""]"
670,..\pandas\reference\api\pandas.arrays.DatetimeArray.html,pandas.arrays.DatetimeArray,"class pandas.arrays.DatetimeArray(values, dtype=None, freq=<no_default>, copy=False)[source]# Pandas ExtensionArray for tz-naive or tz-aware datetime data. Warning DatetimeArray is currently experimental, and its API may change without warning. In particular, DatetimeArray.dtype is expected to change to always be an instance of an ExtensionDtype subclass. Attributes None Methods None","Parameters: valuesSeries, Index, DatetimeArray, ndarrayThe datetime data. For DatetimeArray values (or a Series or Index boxing one), dtype and freq will be extracted from values. dtypenumpy.dtype or DatetimeTZDtypeNote that the only NumPy dtype allowed is ‘datetime64[ns]’. freqstr or Offset, optionalThe frequency. copybool, default FalseWhether to copy the underlying array of values.","["">>> pd.arrays.DatetimeArray._from_sequence(\n...    pd.DatetimeIndex(['2023-01-01', '2023-01-02'], freq='D'))\n<DatetimeArray>\n['2023-01-01 00:00:00', '2023-01-02 00:00:00']\nLength: 2, dtype: datetime64[ns]""]"
671,..\pandas\reference\api\pandas.Series.ffill.html,pandas.Series.ffill,"Series.ffill(*, axis=None, inplace=False, limit=None, limit_area=None, downcast=<no_default>)[source]# Fill NA/NaN values by propagating the last valid observation to next valid.","Parameters: axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrameAxis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplacebool, default FalseIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area{None, ‘inside’, ‘outside’}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). Added in version 2.2.0. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.2.0. Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.","['>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n...                    [3, 4, np.nan, 1],\n...                    [np.nan, np.nan, np.nan, np.nan],\n...                    [np.nan, 3, np.nan, 4]],\n...                   columns=list(""ABCD""))\n>>> df\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  NaN  NaN NaN  NaN\n3  NaN  3.0 NaN  4.0', '>>> df.ffill()\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  3.0  4.0 NaN  1.0\n3  3.0  3.0 NaN  4.0', '>>> ser = pd.Series([1, np.nan, 2, 3])\n>>> ser.ffill()\n0   1.0\n1   1.0\n2   2.0\n3   3.0\ndtype: float64']"
672,..\pandas\reference\api\pandas.tseries.offsets.Nano.html,pandas.tseries.offsets.Nano,class pandas.tseries.offsets.Nano# Offset n nanoseconds. Examples You can use the parameter n to represent a shift of n nanoseconds. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of nanoseconds represented.","["">>> from pandas.tseries.offsets import Nano\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Nano(n=1000)\nTimestamp('2022-12-09 15:00:00.000001')"", "">>> ts - Nano(n=1000)\nTimestamp('2022-12-09 14:59:59.999999')"", "">>> ts + Nano(n=-1000)\nTimestamp('2022-12-09 14:59:59.999999')""]"
673,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.start.html,pandas.tseries.offsets.CustomBusinessHour.start,CustomBusinessHour.start#,No parameters found,[]
674,..\pandas\reference\api\pandas.core.window.rolling.Rolling.count.html,pandas.core.window.rolling.Rolling.count,Rolling.count(numeric_only=False)[source]# Calculate the rolling count of non NaN observations.,"Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([2, 3, np.nan, 10])\n>>> s.rolling(2).count()\n0    NaN\n1    2.0\n2    1.0\n3    1.0\ndtype: float64\n>>> s.rolling(3).count()\n0    NaN\n1    NaN\n2    2.0\n3    2.0\ndtype: float64\n>>> s.rolling(4).count()\n0    NaN\n1    NaN\n2    NaN\n3    3.0\ndtype: float64']"
675,..\pandas\reference\api\pandas.DatetimeIndex.html,pandas.DatetimeIndex,"class pandas.DatetimeIndex(data=None, freq=<no_default>, tz=<no_default>, normalize=<no_default>, closed=<no_default>, ambiguous='raise', dayfirst=False, yearfirst=False, dtype=None, copy=False, name=None)[source]# Immutable ndarray-like of datetime64 data. Represented internally as int64, and which can be boxed to Timestamp objects that are subclasses of datetime and carry metadata. Changed in version 2.0.0: The various numeric date/time attributes (day, month, year etc.) now have dtype int32. Previously they had dtype int64. Attributes year The year of the datetime. month The month as January=1, December=12. day The day of the datetime. hour The hours of the datetime. minute The minutes of the datetime. second The seconds of the datetime. microsecond The microseconds of the datetime. nanosecond The nanoseconds of the datetime. date Returns numpy array of python datetime.date objects. time Returns numpy array of datetime.time objects. timetz Returns numpy array of datetime.time objects with timezones. dayofyear The ordinal day of the year. day_of_year The ordinal day of the year. dayofweek The day of the week with Monday=0, Sunday=6. day_of_week The day of the week with Monday=0, Sunday=6. weekday The day of the week with Monday=0, Sunday=6. quarter The quarter of the date. tz Return the timezone. freqstr Return the frequency object as a string if it's set, otherwise None. is_month_start Indicates whether the date is the first day of the month. is_month_end Indicates whether the date is the last day of the month. is_quarter_start Indicator for whether the date is the first day of a quarter. is_quarter_end Indicator for whether the date is the last day of a quarter. is_year_start Indicate whether the date is the first day of a year. is_year_end Indicate whether the date is the last day of the year. is_leap_year Boolean indicator if the date belongs to a leap year. inferred_freq Tries to return a string representing a frequency generated by infer_freq. freq Methods normalize(*args, **kwargs) Convert times to midnight. strftime(date_format) Convert to Index using specified date_format. snap([freq]) Snap time stamps to nearest occurring frequency. tz_convert(tz) Convert tz-aware Datetime Array/Index from one time zone to another. tz_localize(tz[, ambiguous, nonexistent]) Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index. round(*args, **kwargs) Perform round operation on the data to the specified freq. floor(*args, **kwargs) Perform floor operation on the data to the specified freq. ceil(*args, **kwargs) Perform ceil operation on the data to the specified freq. to_period(*args, **kwargs) Cast to PeriodArray/PeriodIndex at a particular frequency. to_pydatetime(*args, **kwargs) Return an ndarray of datetime.datetime objects. to_series([index, name]) Create a Series with both index and values equal to the index keys. to_frame([index, name]) Create a DataFrame with a column containing the Index. month_name(*args, **kwargs) Return the month names with specified locale. day_name(*args, **kwargs) Return the day names with specified locale. mean(*[, skipna, axis]) Return the mean value of the Array. std(*args, **kwargs) Return sample standard deviation over requested axis. Notes To learn more about the frequency strings, please see this link.","Parameters: dataarray-like (1-dimensional)Datetime-like data to construct index with. freqstr or pandas offset object, optionalOne of pandas date offset strings or corresponding objects. The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation. tzpytz.timezone or dateutil.tz.tzfile or datetime.tzinfo or strSet the Timezone of the data. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. Deprecated since version 2.1.0. closed{‘left’, ‘right’}, optionalSet whether to include start and end that are on the boundary. The default includes boundary points on either end. Deprecated since version 2.1.0. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. dayfirstbool, default FalseIf True, parse dates in data with the day first order. yearfirstbool, default FalseIf True parse dates in data with the year first order. dtypenumpy.dtype or DatetimeTZDtype or str, default NoneNote that the only NumPy dtype allowed is datetime64[ns]. copybool, default FalseMake a copy of input ndarray. namelabel, default NoneName to be stored in the index.","['>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> idx\nDatetimeIndex([\'2020-01-01 10:00:00+00:00\', \'2020-02-01 11:00:00+00:00\'],\ndtype=\'datetime64[ns, UTC]\', freq=None)']"
676,..\pandas\reference\api\pandas.Timestamp.ceil.html,pandas.Timestamp.ceil,"Timestamp.ceil(freq, ambiguous='raise', nonexistent='raise')# Return a new Timestamp ceiled to this resolution. Notes If the Timestamp has a timezone, ceiling will take place relative to the local (“wall”) time and re-localized to the same timezone. When ceiling near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstrFrequency string indicating the ceiling resolution. ambiguousbool or {‘raise’, ‘NaT’}, default ‘raise’The behavior is as follows: bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates). ‘NaT’ will return NaT for an ambiguous time. ‘raise’ will raise an AmbiguousTimeError for an ambiguous time. nonexistent{‘raise’, ‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta}, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time. ‘shift_backward’ will shift the nonexistent time backward to the closest existing time. ‘NaT’ will return NaT where there are nonexistent times. timedelta objects will shift nonexistent times by the timedelta. ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Raises: ValueError if the freq cannot be converted.","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')"", "">>> ts.ceil(freq='h') # hour\nTimestamp('2020-03-14 16:00:00')"", "">>> ts.ceil(freq='min') # minute\nTimestamp('2020-03-14 15:33:00')"", "">>> ts.ceil(freq='s') # seconds\nTimestamp('2020-03-14 15:32:53')"", "">>> ts.ceil(freq='us') # microseconds\nTimestamp('2020-03-14 15:32:52.192549')"", "">>> ts.ceil(freq='5min')\nTimestamp('2020-03-14 15:35:00')"", "">>> ts.ceil(freq='1h30min')\nTimestamp('2020-03-14 16:30:00')"", '>>> pd.NaT.ceil()\nNaT', '>>> ts_tz = pd.Timestamp(""2021-10-31 01:30:00"").tz_localize(""Europe/Amsterdam"")', '>>> ts_tz.ceil(""h"", ambiguous=False)\nTimestamp(\'2021-10-31 02:00:00+0100\', tz=\'Europe/Amsterdam\')', '>>> ts_tz.ceil(""h"", ambiguous=True)\nTimestamp(\'2021-10-31 02:00:00+0200\', tz=\'Europe/Amsterdam\')']"
677,..\pandas\reference\api\pandas.io.formats.style.Styler.use.html,pandas.io.formats.style.Styler.use,Styler.use(styles)[source]# Set the styles on the current Styler. Possibly uses styles from Styler.export.,"Parameters: stylesdict(str, Any) List of attributes to add to Styler. Dict keys should contain only: “apply”: list of styler functions, typically added with apply or map. “table_attributes”: HTML attributes, typically added with set_table_attributes. “table_styles”: CSS selectors and properties, typically added with set_table_styles. “hide_index”:  whether the index is hidden, typically added with hide_index, or a boolean list for hidden levels. “hide_columns”: whether column headers are hidden, typically added with hide_columns, or a boolean list for hidden levels. “hide_index_names”: whether index names are hidden. “hide_column_names”: whether column header names are hidden. “css”: the css class names used. Returns: Styler","['>>> styler = pd.DataFrame([[1, 2], [3, 4]]).style\n>>> styler2 = pd.DataFrame([[9, 9, 9]]).style\n>>> styler.hide(axis=0).highlight_max(axis=1)  \n>>> export = styler.export()\n>>> styler2.use(export)']"
678,..\pandas\reference\api\pandas.Timestamp.combine.html,pandas.Timestamp.combine,"classmethod Timestamp.combine(date, time)# Combine date, time into datetime with same date and time fields.",No parameters found,"["">>> from datetime import date, time\n>>> pd.Timestamp.combine(date(2020, 3, 14), time(15, 30, 15))\nTimestamp('2020-03-14 15:30:15')""]"
679,..\pandas\reference\api\pandas.Series.fillna.html,pandas.Series.fillna,"Series.fillna(value=None, *, method=None, axis=None, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values using the specified method.","Parameters: valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame).  Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. method{‘backfill’, ‘bfill’, ‘ffill’, None}, default NoneMethod to use for filling holes in reindexed Series: ffill: propagate last valid observation forward to next valid. backfill / bfill: use next valid observation to fill gap. Deprecated since version 2.1.0: Use ffill or bfill instead. axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrameAxis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplacebool, default FalseIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.2.0. Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.","['>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n...                    [3, 4, np.nan, 1],\n...                    [np.nan, np.nan, np.nan, np.nan],\n...                    [np.nan, 3, np.nan, 4]],\n...                   columns=list(""ABCD""))\n>>> df\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  NaN  NaN NaN  NaN\n3  NaN  3.0 NaN  4.0', '>>> df.fillna(0)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  0.0\n3  0.0  3.0  0.0  4.0', '>>> values = {""A"": 0, ""B"": 1, ""C"": 2, ""D"": 3}\n>>> df.fillna(value=values)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  2.0  1.0\n2  0.0  1.0  2.0  3.0\n3  0.0  3.0  2.0  4.0', '>>> df.fillna(value=values, limit=1)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  NaN  1.0\n2  NaN  1.0  NaN  3.0\n3  NaN  3.0  NaN  4.0', '>>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(""ABCE""))\n>>> df.fillna(df2)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  NaN\n3  0.0  3.0  0.0  4.0']"
680,..\pandas\reference\api\pandas.arrays.FloatingArray.html,pandas.arrays.FloatingArray,"class pandas.arrays.FloatingArray(values, mask, copy=False)[source]# Array of floating (optional missing) values. Warning FloatingArray is currently experimental, and its API or internal implementation may change without warning. Especially the behaviour regarding NaN (distinct from NA missing values) is subject to change. We represent a FloatingArray with 2 numpy arrays: data: contains a numpy float array of the appropriate dtype mask: a boolean array holding a mask on the data, True is missing To construct an FloatingArray from generic array-like input, use pandas.array() with one of the float dtypes (see examples). See Nullable integer data type for more. Attributes None Methods None Returns: FloatingArray","Parameters: valuesnumpy.ndarrayA 1-d float-dtype array. masknumpy.ndarrayA 1-d boolean-dtype array indicating missing values. copybool, default FalseWhether to copy the values and mask.","['>>> pd.array([0.1, None, 0.3], dtype=pd.Float32Dtype())\n<FloatingArray>\n[0.1, <NA>, 0.3]\nLength: 3, dtype: Float32', '>>> pd.array([0.1, None, 0.3], dtype=""Float32"")\n<FloatingArray>\n[0.1, <NA>, 0.3]\nLength: 3, dtype: Float32']"
681,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_anchored.html,pandas.tseries.offsets.Nano.is_anchored,Nano.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
682,..\pandas\reference\api\pandas.core.window.rolling.Rolling.cov.html,pandas.core.window.rolling.Rolling.cov,"Rolling.cov(other=None, pairwise=None, ddof=1, numeric_only=False)[source]# Calculate the rolling sample covariance.","Parameters: otherSeries or DataFrame, optionalIf not supplied then will default to self and produce pairwise output. pairwisebool, default NoneIf False then only matching columns between self and other will be used and the output will be a DataFrame. If True then all pairwise combinations will be calculated and the output will be a MultiIndexed DataFrame in the case of DataFrame inputs. In the case of missing elements, only complete pairwise observations will be used. ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser1 = pd.Series([1, 2, 3, 4])\n>>> ser2 = pd.Series([1, 4, 5, 8])\n>>> ser1.rolling(2).cov(ser2)\n0    NaN\n1    1.5\n2    0.5\n3    1.5\ndtype: float64']"
683,..\pandas\reference\api\pandas.DatetimeIndex.indexer_at_time.html,pandas.DatetimeIndex.indexer_at_time,"DatetimeIndex.indexer_at_time(time, asof=False)[source]# Return index locations of values at particular time of day.","Parameters: timedatetime.time or strTime passed in either as object (datetime.time) or as string in appropriate format (“%H:%M”, “%H%M”, “%I:%M%p”, “%I%M%p”, “%H:%M:%S”, “%H%M%S”, “%I:%M:%S%p”, “%I%M%S%p”). Returns: np.ndarray[np.intp]","['>>> idx = pd.DatetimeIndex([""1/1/2020 10:00"", ""2/1/2020 11:00"",\n...                         ""3/1/2020 10:00""])\n>>> idx.indexer_at_time(""10:00"")\narray([0, 2])']"
684,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessHour.weekmask.html,pandas.tseries.offsets.CustomBusinessHour.weekmask,CustomBusinessHour.weekmask#,No parameters found,[]
685,..\pandas\reference\api\pandas.Timestamp.ctime.html,pandas.Timestamp.ctime,Timestamp.ctime()# Return ctime() style string.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00.00')\n>>> ts\nTimestamp('2023-01-01 10:00:00')\n>>> ts.ctime()\n'Sun Jan  1 10:00:00 2023'""]"
686,..\pandas\reference\api\pandas.io.json.build_table_schema.html,pandas.io.json.build_table_schema,"pandas.io.json.build_table_schema(data, index=True, primary_key=None, version=True)[source]# Create a Table schema from data. Notes See Table Schema for conversion types. Timedeltas as converted to ISO8601 duration format with 9 decimal places after the seconds field for nanosecond precision. Categoricals are converted to the any dtype, and use the enum field constraint to list the allowed values. The ordered attribute is included in an ordered field.","Parameters: dataSeries, DataFrame indexbool, default TrueWhether to include data.index in the schema. primary_keybool or None, default TrueColumn names to designate as the primary key. The default None will set ‘primaryKey’ to the index level or levels if the index is unique. versionbool, default TrueWhether to include a field pandas_version with the version of pandas that last revised the table schema. This version can be different from the installed pandas version. Returns: dict","["">>> from pandas.io.json._table_schema import build_table_schema\n>>> df = pd.DataFrame(\n...     {'A': [1, 2, 3],\n...      'B': ['a', 'b', 'c'],\n...      'C': pd.date_range('2016-01-01', freq='d', periods=3),\n...     }, index=pd.Index(range(3), name='idx'))\n>>> build_table_schema(df)\n{'fields': [{'name': 'idx', 'type': 'integer'}, {'name': 'A', 'type': 'integer'}, {'name': 'B', 'type': 'string'}, {'name': 'C', 'type': 'datetime'}], 'primaryKey': ['idx'], 'pandas_version': '1.4.0'}""]"
687,..\pandas\reference\api\pandas.Series.filter.html,pandas.Series.filter,"Series.filter(items=None, like=None, regex=None, axis=None)[source]# Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with [].","Parameters: itemslist-likeKeep labels from axis which are in items. likestrKeep labels from axis for which “like in label == True”. regexstr (regular expression)Keep labels from axis for which re.search(regex, label) == True. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘columns’ for DataFrame. For Series this parameter is unused and defaults to None. Returns: same type as input object","["">>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6"", "">>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6"", "">>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6"", "">>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6""]"
688,..\pandas\reference\api\pandas.Timestamp.date.html,pandas.Timestamp.date,"Timestamp.date()# Return date object with same year, month and day.",No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00.00')\n>>> ts\nTimestamp('2023-01-01 10:00:00')\n>>> ts.date()\ndatetime.date(2023, 1, 1)""]"
689,..\pandas\reference\api\pandas.DatetimeIndex.indexer_between_time.html,pandas.DatetimeIndex.indexer_between_time,"DatetimeIndex.indexer_between_time(start_time, end_time, include_start=True, include_end=True)[source]# Return index locations of values between particular times of day.","Parameters: start_time, end_timedatetime.time, strTime passed either as object (datetime.time) or as string in appropriate format (“%H:%M”, “%H%M”, “%I:%M%p”, “%I%M%p”, “%H:%M:%S”, “%H%M%S”, “%I:%M:%S%p”,”%I%M%S%p”). include_startbool, default True include_endbool, default True Returns: np.ndarray[np.intp]","['>>> idx = pd.date_range(""2023-01-01"", periods=4, freq=""h"")\n>>> idx\nDatetimeIndex([\'2023-01-01 00:00:00\', \'2023-01-01 01:00:00\',\n                   \'2023-01-01 02:00:00\', \'2023-01-01 03:00:00\'],\n                  dtype=\'datetime64[ns]\', freq=\'h\')\n>>> idx.indexer_between_time(""00:00"", ""2:00"", include_end=False)\narray([0, 1])']"
690,..\pandas\reference\api\pandas.arrays.IntegerArray.html,pandas.arrays.IntegerArray,"class pandas.arrays.IntegerArray(values, mask, copy=False)[source]# Array of integer (optional missing) values. Uses pandas.NA as the missing value. Warning IntegerArray is currently experimental, and its API or internal implementation may change without warning. We represent an IntegerArray with 2 numpy arrays: data: contains a numpy integer array of the appropriate dtype mask: a boolean array holding a mask on the data, True is missing To construct an IntegerArray from generic array-like input, use pandas.array() with one of the integer dtypes (see examples). See Nullable integer data type for more. Attributes None Methods None Returns: IntegerArray","Parameters: valuesnumpy.ndarrayA 1-d integer-dtype array. masknumpy.ndarrayA 1-d boolean-dtype array indicating missing values. copybool, default FalseWhether to copy the values and mask.","['>>> int_array = pd.array([1, None, 3], dtype=pd.Int32Dtype())\n>>> int_array\n<IntegerArray>\n[1, <NA>, 3]\nLength: 3, dtype: Int32', "">>> pd.array([1, None, 3], dtype='Int32')\n<IntegerArray>\n[1, <NA>, 3]\nLength: 3, dtype: Int32"", "">>> pd.array([1, None, 3], dtype='UInt16')\n<IntegerArray>\n[1, <NA>, 3]\nLength: 3, dtype: UInt16""]"
691,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.calendar.html,pandas.tseries.offsets.CustomBusinessMonthBegin.calendar,CustomBusinessMonthBegin.calendar#,No parameters found,[]
692,..\pandas\reference\api\pandas.core.window.rolling.Rolling.kurt.html,pandas.core.window.rolling.Rolling.kurt,Rolling.kurt(numeric_only=False)[source]# Calculate the rolling Fisher’s definition of kurtosis without bias. Notes A minimum of four periods is required for the calculation.,"Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> arr = [1, 2, 3, 4, 999]\n>>> import scipy.stats\n>>> print(f""{scipy.stats.kurtosis(arr[:-1], bias=False):.6f}"")\n-1.200000\n>>> print(f""{scipy.stats.kurtosis(arr[1:], bias=False):.6f}"")\n3.999946\n>>> s = pd.Series(arr)\n>>> s.rolling(4).kurt()\n0         NaN\n1         NaN\n2         NaN\n3   -1.200000\n4    3.999946\ndtype: float64']"
693,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_month_end.html,pandas.tseries.offsets.Nano.is_month_end,Nano.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
694,..\pandas\reference\api\pandas.io.stata.StataReader.data_label.html,pandas.io.stata.StataReader.data_label,property StataReader.data_label[source]# Return data label of Stata file.,No parameters found,"['>>> df = pd.DataFrame([(1,)], columns=[""variable""])\n>>> time_stamp = pd.Timestamp(2000, 2, 29, 14, 21)\n>>> data_label = ""This is a data file.""\n>>> path = ""/My_path/filename.dta""\n>>> df.to_stata(path, time_stamp=time_stamp,    \n...             data_label=data_label,  \n...             version=None)  \n>>> with pd.io.stata.StataReader(path) as reader:  \n...     print(reader.data_label)  \nThis is a data file.']"
695,..\pandas\reference\api\pandas.Series.first.html,pandas.Series.first,"Series.first(offset)[source]# Select initial periods of time series data based on a date offset. Deprecated since version 2.1: first() is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function can select the first few rows based on a date offset.","Parameters: offsetstr, DateOffset or dateutil.relativedeltaThe offset length of the data that will be selected. For instance, ‘1ME’ will display all the rows having their index within the first month. Returns: Series or DataFrameA subset of the caller. Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n            A\n2018-04-09  1\n2018-04-11  2\n2018-04-13  3\n2018-04-15  4"", "">>> ts.first('3D')\n            A\n2018-04-09  1\n2018-04-11  2""]"
696,..\pandas\reference\api\pandas.Timestamp.day.html,pandas.Timestamp.day,Timestamp.day#,No parameters found,[]
697,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_month_start.html,pandas.tseries.offsets.Nano.is_month_start,Nano.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
698,..\pandas\reference\api\pandas.Timestamp.dayofweek.html,pandas.Timestamp.dayofweek,Timestamp.dayofweek# Return day of the week.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.day_of_week\n5']"
699,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.copy.html,pandas.tseries.offsets.CustomBusinessMonthBegin.copy,CustomBusinessMonthBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
700,..\pandas\reference\api\pandas.DatetimeIndex.inferred_freq.html,pandas.DatetimeIndex.inferred_freq,DatetimeIndex.inferred_freq[source]# Tries to return a string representing a frequency generated by infer_freq. Returns None if it can’t autodetect the frequency.,No parameters found,"['>>> idx = pd.DatetimeIndex([""2018-01-01"", ""2018-01-03"", ""2018-01-05""])\n>>> idx.inferred_freq\n\'2D\'', '>>> tdelta_idx = pd.to_timedelta([""0 days"", ""10 days"", ""20 days""])\n>>> tdelta_idx\nTimedeltaIndex([\'0 days\', \'10 days\', \'20 days\'],\n               dtype=\'timedelta64[ns]\', freq=None)\n>>> tdelta_idx.inferred_freq\n\'10D\'']"
701,..\pandas\reference\api\pandas.core.window.rolling.Rolling.max.html,pandas.core.window.rolling.Rolling.max,"Rolling.max(numeric_only=False, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Calculate the rolling maximum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 2, 3, 4])\n>>> ser.rolling(2).max()\n0    NaN\n1    2.0\n2    3.0\n3    4.0\ndtype: float64']"
702,..\pandas\reference\api\pandas.Series.first_valid_index.html,pandas.Series.first_valid_index,"Series.first_valid_index()[source]# Return index for first non-NA value or None, if no non-NA value is found.",Returns: type of index,"['>>> s = pd.Series([None, 3, 4])\n>>> s.first_valid_index()\n1\n>>> s.last_valid_index()\n2', '>>> s = pd.Series([None, None])\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', '>>> s = pd.Series()\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', "">>> df = pd.DataFrame({'A': [None, None, 2], 'B': [None, 3, 4]})\n>>> df\n     A      B\n0  NaN    NaN\n1  NaN    3.0\n2  2.0    4.0\n>>> df.first_valid_index()\n1\n>>> df.last_valid_index()\n2"", "">>> df = pd.DataFrame({'A': [None, None, None], 'B': [None, None, None]})\n>>> df\n     A      B\n0  None   None\n1  None   None\n2  None   None\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone"", '>>> df = pd.DataFrame()\n>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone']"
703,..\pandas\reference\api\pandas.io.stata.StataReader.value_labels.html,pandas.io.stata.StataReader.value_labels,StataReader.value_labels()[source]# Return a nested dict associating each variable name to its value and label.,Returns: dict,"['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[""col_1"", ""col_2""])\n>>> time_stamp = pd.Timestamp(2000, 2, 29, 14, 21)\n>>> path = ""/My_path/filename.dta""\n>>> value_labels = {""col_1"": {3: ""x""}}\n>>> df.to_stata(path, time_stamp=time_stamp,  \n...             value_labels=value_labels, version=None)  \n>>> with pd.io.stata.StataReader(path) as reader:  \n...     print(reader.value_labels())  \n{\'col_1\': {3: \'x\'}}\n>>> pd.read_stata(path)  \n    index col_1 col_2\n0       0    1    2\n1       1    x    4']"
704,..\pandas\reference\api\pandas.arrays.IntervalArray.closed.html,pandas.arrays.IntervalArray.closed,"property IntervalArray.closed[source]# String describing the inclusive side the intervals. Either left, right, both or neither.",No parameters found,"["">>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.closed\n'right'"", "">>> interv_idx = pd.interval_range(start=0, end=2)\n>>> interv_idx\nIntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')\n>>> interv_idx.closed\n'right'""]"
705,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_on_offset.html,pandas.tseries.offsets.Nano.is_on_offset,Nano.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
706,..\pandas\reference\api\pandas.arrays.IntervalArray.contains.html,pandas.arrays.IntervalArray.contains,IntervalArray.contains(other)[source]# Check elementwise if the Intervals contain the value. Return a boolean mask whether the value is contained in the Intervals of the IntervalArray.,Parameters: otherscalarThe value to check whether it is contained in the Intervals. Returns: boolean array,"['>>> intervals = pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 3), (2, 4)])\n>>> intervals\n<IntervalArray>\n[(0, 1], (1, 3], (2, 4]]\nLength: 3, dtype: interval[int64, right]', '>>> intervals.contains(0.5)\narray([ True, False, False])']"
707,..\pandas\reference\api\pandas.Series.flags.html,pandas.Series.flags,"property Series.flags[source]# Get the properties associated with this pandas object. The available flags are Flags.allows_duplicate_labels Notes “Flags” differ from “metadata”. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.",No parameters found,"['>>> df = pd.DataFrame({""A"": [1, 2]})\n>>> df.flags\n<Flags(allows_duplicate_labels=True)>', '>>> df.flags.allows_duplicate_labels\nTrue\n>>> df.flags.allows_duplicate_labels = False', '>>> df.flags[""allows_duplicate_labels""]\nFalse\n>>> df.flags[""allows_duplicate_labels""] = True']"
708,..\pandas\reference\api\pandas.DatetimeIndex.is_leap_year.html,pandas.DatetimeIndex.is_leap_year,"property DatetimeIndex.is_leap_year[source]# Boolean indicator if the date belongs to a leap year. A leap year is a year, which has 366 days (instead of 365) including 29th of February as an intercalary day. Leap years are years which are multiples of four with the exception of years divisible by 100 but not by 400.",Returns: Series or ndarrayBooleans indicating if dates belong to a leap year.,"['>>> idx = pd.date_range(""2012-01-01"", ""2015-01-01"", freq=""YE"")\n>>> idx\nDatetimeIndex([\'2012-12-31\', \'2013-12-31\', \'2014-12-31\'],\n              dtype=\'datetime64[ns]\', freq=\'YE-DEC\')\n>>> idx.is_leap_year\narray([ True, False, False])', '>>> dates_series = pd.Series(idx)\n>>> dates_series\n0   2012-12-31\n1   2013-12-31\n2   2014-12-31\ndtype: datetime64[ns]\n>>> dates_series.dt.is_leap_year\n0     True\n1    False\n2    False\ndtype: bool']"
709,..\pandas\reference\api\pandas.io.stata.StataReader.variable_labels.html,pandas.io.stata.StataReader.variable_labels,StataReader.variable_labels()[source]# Return a dict associating each variable name with corresponding label.,Returns: dict,"['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[""col_1"", ""col_2""])\n>>> time_stamp = pd.Timestamp(2000, 2, 29, 14, 21)\n>>> path = ""/My_path/filename.dta""\n>>> variable_labels = {""col_1"": ""This is an example""}\n>>> df.to_stata(path, time_stamp=time_stamp,  \n...             variable_labels=variable_labels, version=None)  \n>>> with pd.io.stata.StataReader(path) as reader:  \n...     print(reader.variable_labels())  \n{\'index\': \'\', \'col_1\': \'This is an example\', \'col_2\': \'\'}\n>>> pd.read_stata(path)  \n    index col_1 col_2\n0       0    1    2\n1       1    3    4']"
710,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.freqstr.html,pandas.tseries.offsets.CustomBusinessMonthBegin.freqstr,CustomBusinessMonthBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
711,..\pandas\reference\api\pandas.Timestamp.dayofyear.html,pandas.Timestamp.dayofyear,Timestamp.dayofyear# Return the day of the year.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.day_of_year\n74']"
712,..\pandas\reference\api\pandas.arrays.IntervalArray.from_arrays.html,pandas.arrays.IntervalArray.from_arrays,"classmethod IntervalArray.from_arrays(left, right, closed='right', copy=False, dtype=None)[source]# Construct from two arrays defining the left and right bounds. Notes Each element of left must be less than or equal to the right element at the same position. If an element is missing, it must be missing in both left and right. A TypeError is raised when using an unsupported type for left or right. At the moment, ‘category’, ‘object’, and ‘string’ subtypes are not supported.","Parameters: leftarray-like (1-dimensional)Left bounds for each interval. rightarray-like (1-dimensional)Right bounds for each interval. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. copybool, default FalseCopy the data. dtypedtype, optionalIf None, dtype will be inferred. Returns: IntervalArray Raises: ValueErrorWhen a value is missing in only one of left or right. When a value in left is greater than the corresponding value in right.","['>>> pd.arrays.IntervalArray.from_arrays([0, 1, 2], [1, 2, 3])\n<IntervalArray>\n[(0, 1], (1, 2], (2, 3]]\nLength: 3, dtype: interval[int64, right]']"
713,..\pandas\reference\api\pandas.core.window.rolling.Rolling.mean.html,pandas.core.window.rolling.Rolling.mean,"Rolling.mean(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the rolling mean. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s.rolling(2).mean()\n0    NaN\n1    1.5\n2    2.5\n3    3.5\ndtype: float64', '>>> s.rolling(3).mean()\n0    NaN\n1    NaN\n2    2.0\n3    3.0\ndtype: float64']"
714,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_quarter_end.html,pandas.tseries.offsets.Nano.is_quarter_end,Nano.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
715,..\pandas\reference\api\pandas.Series.floordiv.html,pandas.Series.floordiv,"Series.floordiv(other, level=None, fill_value=None, axis=0)[source]# Return Integer division of series and other, element-wise (binary operator floordiv). Equivalent to series // other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.floordiv(b, fill_value=0)\na    1.0\nb    inf\nc    inf\nd    0.0\ne    NaN\ndtype: float64""]"
716,..\pandas\reference\api\pandas.DatetimeIndex.is_month_end.html,pandas.DatetimeIndex.is_month_end,property DatetimeIndex.is_month_end[source]# Indicates whether the date is the last day of the month.,"Returns: Series or arrayFor Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.","['>>> s = pd.Series(pd.date_range(""2018-02-27"", periods=3))\n>>> s\n0   2018-02-27\n1   2018-02-28\n2   2018-03-01\ndtype: datetime64[ns]\n>>> s.dt.is_month_start\n0    False\n1    False\n2    True\ndtype: bool\n>>> s.dt.is_month_end\n0    False\n1    True\n2    False\ndtype: bool', '>>> idx = pd.date_range(""2018-02-27"", periods=3)\n>>> idx.is_month_start\narray([False, False, True])\n>>> idx.is_month_end\narray([False, True, False])']"
717,..\pandas\reference\api\pandas.arrays.IntervalArray.from_breaks.html,pandas.arrays.IntervalArray.from_breaks,"classmethod IntervalArray.from_breaks(breaks, closed='right', copy=False, dtype=None)[source]# Construct an IntervalArray from an array of splits.","Parameters: breaksarray-like (1-dimensional)Left and right bounds for each interval. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. copybool, default FalseCopy the data. dtypedtype or None, default NoneIf None, dtype will be inferred. Returns: IntervalArray","['>>> pd.arrays.IntervalArray.from_breaks([0, 1, 2, 3])\n<IntervalArray>\n[(0, 1], (1, 2], (2, 3]]\nLength: 3, dtype: interval[int64, right]']"
718,..\pandas\reference\api\pandas.Series.ge.html,pandas.Series.ge,"Series.ge(other, level=None, fill_value=None, axis=0)[source]# Return Greater than or equal to of series and other, element-wise (binary operator ge). Equivalent to series >= other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ne    1.0\ndtype: float64\n>>> b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n>>> b\na    0.0\nb    1.0\nc    2.0\nd    NaN\nf    1.0\ndtype: float64\n>>> a.ge(b, fill_value=0)\na     True\nb     True\nc    False\nd    False\ne     True\nf    False\ndtype: bool""]"
719,..\pandas\reference\api\pandas.core.window.rolling.Rolling.median.html,pandas.core.window.rolling.Rolling.median,"Rolling.median(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the rolling median. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([0, 1, 2, 3, 4])\n>>> s.rolling(3).median()\n0    NaN\n1    NaN\n2    1.0\n3    2.0\n4    3.0\ndtype: float64']"
720,..\pandas\reference\api\pandas.Timestamp.daysinmonth.html,pandas.Timestamp.daysinmonth,Timestamp.daysinmonth# Return the number of days in the month.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.days_in_month\n31']"
721,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.holidays.html,pandas.tseries.offsets.CustomBusinessMonthBegin.holidays,CustomBusinessMonthBegin.holidays#,No parameters found,[]
722,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_quarter_start.html,pandas.tseries.offsets.Nano.is_quarter_start,Nano.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
723,..\pandas\reference\api\pandas.io.stata.StataWriter.write_file.html,pandas.io.stata.StataWriter.write_file,StataWriter.write_file()[source]# Export DataFrame object to Stata dta format.,No parameters found,"['>>> df = pd.DataFrame({""fully_labelled"": [1, 2, 3, 3, 1],\n...                    ""partially_labelled"": [1.0, 2.0, np.nan, 9.0, np.nan],\n...                    ""Y"": [7, 7, 9, 8, 10],\n...                    ""Z"": pd.Categorical([""j"", ""k"", ""l"", ""k"", ""j""]),\n...                    })\n>>> path = ""/My_path/filename.dta""\n>>> labels = {""fully_labelled"": {1: ""one"", 2: ""two"", 3: ""three""},\n...           ""partially_labelled"": {1.0: ""one"", 2.0: ""two""},\n...           }\n>>> writer = pd.io.stata.StataWriter(path,\n...                                  df,\n...                                  value_labels=labels)  \n>>> writer.write_file()  \n>>> df = pd.read_stata(path)  \n>>> df  \n    index fully_labelled  partially_labeled  Y  Z\n0       0            one                one  7  j\n1       1            two                two  7  k\n2       2          three                NaN  9  l\n3       3          three                9.0  8  k\n4       4            one                NaN 10  j']"
724,..\pandas\reference\api\pandas.DatetimeIndex.is_month_start.html,pandas.DatetimeIndex.is_month_start,property DatetimeIndex.is_month_start[source]# Indicates whether the date is the first day of the month.,"Returns: Series or arrayFor Series, returns a Series with boolean values. For DatetimeIndex, returns a boolean array.","['>>> s = pd.Series(pd.date_range(""2018-02-27"", periods=3))\n>>> s\n0   2018-02-27\n1   2018-02-28\n2   2018-03-01\ndtype: datetime64[ns]\n>>> s.dt.is_month_start\n0    False\n1    False\n2    True\ndtype: bool\n>>> s.dt.is_month_end\n0    False\n1    True\n2    False\ndtype: bool', '>>> idx = pd.date_range(""2018-02-27"", periods=3)\n>>> idx.is_month_start\narray([False, False, True])\n>>> idx.is_month_end\narray([False, True, False])']"
725,..\pandas\reference\api\pandas.arrays.IntervalArray.from_tuples.html,pandas.arrays.IntervalArray.from_tuples,"classmethod IntervalArray.from_tuples(data, closed='right', copy=False, dtype=None)[source]# Construct an IntervalArray from an array-like of tuples.","Parameters: dataarray-like (1-dimensional)Array of tuples. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. copybool, default FalseBy-default copy the data, this is compat only and ignored. dtypedtype or None, default NoneIf None, dtype will be inferred. Returns: IntervalArray","['>>> pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 2)])\n<IntervalArray>\n[(0, 1], (1, 2]]\nLength: 2, dtype: interval[int64, right]']"
726,..\pandas\reference\api\pandas.Series.get.html,pandas.Series.get,"Series.get(key, default=None)[source]# Get item from object for given key (ex: DataFrame column). Returns default value if not found.",Parameters: keyobject Returns: same type as items contained in object,"['>>> df = pd.DataFrame(\n...     [\n...         [24.3, 75.7, ""high""],\n...         [31, 87.8, ""high""],\n...         [22, 71.6, ""medium""],\n...         [35, 95, ""medium""],\n...     ],\n...     columns=[""temp_celsius"", ""temp_fahrenheit"", ""windspeed""],\n...     index=pd.date_range(start=""2014-02-12"", end=""2014-02-15"", freq=""D""),\n... )', '>>> df\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium', '>>> df.get([""temp_celsius"", ""windspeed""])\n            temp_celsius windspeed\n2014-02-12          24.3      high\n2014-02-13          31.0      high\n2014-02-14          22.0    medium\n2014-02-15          35.0    medium', "">>> ser = df['windspeed']\n>>> ser.get('2014-02-13')\n'high'"", '>>> df.get([""temp_celsius"", ""temp_kelvin""], default=""default_value"")\n\'default_value\'', "">>> ser.get('2014-02-10', '[unknown]')\n'[unknown]'""]"
727,..\pandas\reference\api\pandas.core.window.rolling.Rolling.min.html,pandas.core.window.rolling.Rolling.min,"Rolling.min(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the rolling minimum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([4, 3, 5, 2, 6])\n>>> s.rolling(3).min()\n0    NaN\n1    NaN\n2    3.0\n3    2.0\n4    2.0\ndtype: float64']"
728,..\pandas\reference\api\pandas.Timestamp.days_in_month.html,pandas.Timestamp.days_in_month,Timestamp.days_in_month# Return the number of days in the month.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.days_in_month\n31']"
729,..\pandas\reference\api\pandas.Series.groupby.html,pandas.Series.groupby,"Series.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, observed=<no_default>, dropna=True)[source]# Group Series using a mapper or by a Series of columns. A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. Notes See the user guide for more detailed usage and examples, including splitting an object into groups, iterating through groups, selecting a group, aggregation, and more.","Parameters: bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby. If by is a function, it’s called on each value of the object’s index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series’ values are first aligned; see .align() method). If a list or ndarray of length equal to the selected axis is passed (see the groupby user guide), the values are used as-is to determine the groups. A label or list of labels may be passed to group by the columns in self. Notice that a tuple is interpreted as a (single) key. axis{0 or ‘index’, 1 or ‘columns’}, default 0Split along rows (0) or columns (1). For Series this parameter is unused and defaults to 0. Deprecated since version 2.1.0: Will be removed and behave like axis=0 in a future version. For axis=1, do frame.T.groupby(...) instead. levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular level or levels. Do not specify both by and level. as_indexbool, default TrueReturn object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively “SQL-style” grouped output. This argument has no effect on filtrations (see the filtrations in the user guide), such as head(), tail(), nth() and in transformations (see the transformations in the user guide). sortbool, default TrueSort group keys. Get better performance by turning this off. Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group. If False, the groups will appear in the same order as they did in the original DataFrame. This argument has no effect on filtrations (see the filtrations in the user guide), such as head(), tail(), nth() and in transformations (see the transformations in the user guide). Changed in version 2.0.0: Specifying sort=False with an ordered categorical grouper will no longer sort the values. group_keysbool, default TrueWhen calling apply and the by argument produces a like-indexed (i.e. a transform) result, add group keys to index to identify pieces. By default group keys are not included when the result’s index (and column) labels match the inputs, and are included otherwise. Changed in version 1.5.0: Warns that group_keys will no longer be ignored when the result from apply is a like-indexed Series or DataFrame. Specify group_keys explicitly to include the group keys or not. Changed in version 2.0.0: group_keys now defaults to True. observedbool, default FalseThis only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. Deprecated since version 2.1.0: The default value will change to True in a future version of pandas. dropnabool, default TrueIf True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups. Returns: pandas.api.typing.SeriesGroupByReturns a groupby object that contains information about the groups.","['>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=[\'Falcon\', \'Falcon\', \'Parrot\', \'Parrot\'],\n...                 name=""Max Speed"")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([""a"", ""b"", ""a"", ""b""]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64', '>>> arrays = [[\'Falcon\', \'Falcon\', \'Parrot\', \'Parrot\'],\n...           [\'Captive\', \'Wild\', \'Captive\', \'Wild\']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=(\'Animal\', \'Type\'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=""Max Speed"")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=""Type"").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64', '>>> ser = pd.Series([1, 2, 3, 3], index=[""a"", \'a\', \'b\', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64', '>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64', '>>> arrays = [\'Falcon\', \'Falcon\', \'Parrot\', \'Parrot\']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=""Max Speed"")\n>>> ser.groupby([""a"", ""b"", ""a"", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64', '>>> ser.groupby([""a"", ""b"", ""a"", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64']"
730,..\pandas\reference\api\pandas.core.window.rolling.Rolling.quantile.html,pandas.core.window.rolling.Rolling.quantile,"Rolling.quantile(q, interpolation='linear', numeric_only=False)[source]# Calculate the rolling quantile.","Parameters: quantilefloatQuantile to compute. 0 <= quantile <= 1. Deprecated since version 2.1.0: This will be renamed to ‘q’ in a future version. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j: linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j. lower: i. higher: j. nearest: i or j whichever is nearest. midpoint: (i + j) / 2. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","["">>> s = pd.Series([1, 2, 3, 4])\n>>> s.rolling(2).quantile(.4, interpolation='lower')\n0    NaN\n1    1.0\n2    2.0\n3    3.0\ndtype: float64"", "">>> s.rolling(2).quantile(.4, interpolation='midpoint')\n0    NaN\n1    1.5\n2    2.5\n3    3.5\ndtype: float64""]"
731,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_year_end.html,pandas.tseries.offsets.Nano.is_year_end,Nano.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
732,..\pandas\reference\api\pandas.DatetimeIndex.is_quarter_end.html,pandas.DatetimeIndex.is_quarter_end,property DatetimeIndex.is_quarter_end[source]# Indicator for whether the date is the last day of a quarter.,Returns: is_quarter_endSeries or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> df = pd.DataFrame({\'dates\': pd.date_range(""2017-03-30"",\n...                    periods=4)})\n>>> df.assign(quarter=df.dates.dt.quarter,\n...           is_quarter_end=df.dates.dt.is_quarter_end)\n       dates  quarter    is_quarter_end\n0 2017-03-30        1             False\n1 2017-03-31        1              True\n2 2017-04-01        2             False\n3 2017-04-02        2             False', "">>> idx = pd.date_range('2017-03-30', periods=4)\n>>> idx\nDatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n              dtype='datetime64[ns]', freq='D')"", '>>> idx.is_quarter_end\narray([False,  True, False, False])']"
733,..\pandas\reference\api\pandas.isna.html,pandas.isna,"pandas.isna(obj)[source]# Detect missing values for an array-like object. This function takes a scalar or array-like object and indicates whether values are missing (NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).","Parameters: objscalar or array-likeObject to check for null or missing values. Returns: bool or array-like of boolFor scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is missing.","["">>> pd.isna('dog')\nFalse"", '>>> pd.isna(pd.NA)\nTrue', '>>> pd.isna(np.nan)\nTrue', '>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n>>> array\narray([[ 1., nan,  3.],\n       [ 4.,  5., nan]])\n>>> pd.isna(array)\narray([[False,  True, False],\n       [False, False,  True]])', '>>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,\n...                           ""2017-07-08""])\n>>> index\nDatetimeIndex([\'2017-07-05\', \'2017-07-06\', \'NaT\', \'2017-07-08\'],\n              dtype=\'datetime64[ns]\', freq=None)\n>>> pd.isna(index)\narray([False, False,  True, False])', "">>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])\n>>> df\n     0     1    2\n0  ant   bee  cat\n1  dog  None  fly\n>>> pd.isna(df)\n       0      1      2\n0  False  False  False\n1  False   True  False"", '>>> pd.isna(df[1])\n0    False\n1     True\nName: 1, dtype: bool']"
734,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.html,pandas.tseries.offsets.CustomBusinessMonthBegin,class pandas.tseries.offsets.CustomBusinessMonthBegin# DateOffset subclass representing custom business month(s). Increments between beginning of month dates. Examples In the example below we use the default parameters. Custom business month start can be specified by weekmask parameter. To convert the returned datetime object to its string representation the function strftime() is used in the next example. Using NumPy business day calendar you can define custom holidays. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. calendar cbday_roll Define default roll function to be called in apply method. freqstr Return a string representing the frequency. holidays kwds Return a dict of extra parameters for the offset. m_offset month_roll Define default roll function to be called in apply method. n name Return a string representing the base frequency. nanos normalize offset Alias for self._offset. rule_code weekmask,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start dates to midnight before generating date range. weekmaskstr, Default ‘Mon Tue Wed Thu Fri’Weekmask of valid business days, passed to numpy.busdaycalendar. holidayslistList/array of dates to exclude from the set of valid business days, passed to numpy.busdaycalendar. calendarnp.busdaycalendarCalendar to integrate. offsettimedelta, default timedelta(0)Time offset to apply.","["">>> ts = pd.Timestamp(2022, 8, 5)\n>>> ts + pd.offsets.CustomBusinessMonthBegin()\nTimestamp('2022-09-01 00:00:00')"", '>>> import datetime as dt\n>>> freq = pd.offsets.CustomBusinessMonthBegin(weekmask=""Wed Thu"")\n>>> pd.date_range(dt.datetime(2022, 7, 10), dt.datetime(2022, 12, 18),\n...               freq=freq).strftime(\'%a %d %b %Y %H:%M\')\nIndex([\'Wed 03 Aug 2022 00:00\', \'Thu 01 Sep 2022 00:00\',\n       \'Wed 05 Oct 2022 00:00\', \'Wed 02 Nov 2022 00:00\',\n       \'Thu 01 Dec 2022 00:00\'],\n       dtype=\'object\')', "">>> import datetime as dt\n>>> bdc = np.busdaycalendar(holidays=['2022-08-01', '2022-09-30',\n...                                   '2022-10-31', '2022-11-01'])\n>>> freq = pd.offsets.CustomBusinessMonthBegin(calendar=bdc)\n>>> pd.date_range(dt.datetime(2022, 7, 10), dt.datetime(2022, 11, 10), freq=freq)\nDatetimeIndex(['2022-08-02', '2022-09-01', '2022-10-03', '2022-11-02'],\n               dtype='datetime64[ns]', freq='CBMS')""]"
735,..\pandas\reference\api\pandas.arrays.IntervalArray.html,pandas.arrays.IntervalArray,"class pandas.arrays.IntervalArray(data, closed=None, dtype=None, copy=False, verify_integrity=True)[source]# Pandas array for interval data that are closed on the same side. Attributes left Return the left endpoints of each Interval in the IntervalArray as an Index. right Return the right endpoints of each Interval in the IntervalArray as an Index. closed String describing the inclusive side the intervals. mid Return the midpoint of each Interval in the IntervalArray as an Index. length Return an Index with entries denoting the length of each Interval. is_empty Indicates if an interval is empty, meaning it contains no points. is_non_overlapping_monotonic Return a boolean whether the IntervalArray is non-overlapping and monotonic. Methods from_arrays(left, right[, closed, copy, dtype]) Construct from two arrays defining the left and right bounds. from_tuples(data[, closed, copy, dtype]) Construct an IntervalArray from an array-like of tuples. from_breaks(breaks[, closed, copy, dtype]) Construct an IntervalArray from an array of splits. contains(other) Check elementwise if the Intervals contain the value. overlaps(other) Check elementwise if an Interval overlaps the values in the IntervalArray. set_closed(closed) Return an identical IntervalArray closed on the specified side. to_tuples([na_tuple]) Return an ndarray (if self is IntervalArray) or Index (if self is IntervalIndex) of tuples of the form (left, right). Notes See the user guide for more.","Parameters: dataarray-like (1-dimensional)Array-like (ndarray, DateTimeArray, TimeDeltaArray) containing Interval objects from which to build the IntervalArray. closed{‘left’, ‘right’, ‘both’, ‘neither’}, default ‘right’Whether the intervals are closed on the left-side, right-side, both or neither. dtypedtype or None, default NoneIf None, dtype will be inferred. copybool, default FalseCopy the input data. verify_integritybool, default TrueVerify that the IntervalArray is valid.","['>>> pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]']"
736,..\pandas\reference\api\pandas.Timestamp.day_name.html,pandas.Timestamp.day_name,Timestamp.day_name(locale=None)# Return the day name of the Timestamp with specified locale.,"Parameters: localestr, default None (English locale)Locale determining the language in which to return the day name. Returns: str","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.day_name()\n'Saturday'"", '>>> pd.NaT.day_name()\nnan']"
737,..\pandas\reference\api\pandas.Series.gt.html,pandas.Series.gt,"Series.gt(other, level=None, fill_value=None, axis=0)[source]# Return Greater than of series and other, element-wise (binary operator gt). Equivalent to series > other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ne    1.0\ndtype: float64\n>>> b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n>>> b\na    0.0\nb    1.0\nc    2.0\nd    NaN\nf    1.0\ndtype: float64\n>>> a.gt(b, fill_value=0)\na     True\nb    False\nc    False\nd    False\ne     True\nf    False\ndtype: bool""]"
738,..\pandas\reference\api\pandas.core.window.rolling.Rolling.rank.html,pandas.core.window.rolling.Rolling.rank,"Rolling.rank(method='average', ascending=True, pct=False, numeric_only=False)[source]# Calculate the rolling rank. Added in version 1.4.0.","Parameters: method{‘average’, ‘min’, ‘max’}, default ‘average’How to rank the group of records that have the same value (i.e. ties): average: average rank of the group min: lowest rank in the group max: highest rank in the group ascendingbool, default TrueWhether or not the elements should be ranked in ascending order. pctbool, default FalseWhether or not to display the returned rankings in percentile form. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([1, 4, 2, 3, 5, 3])\n>>> s.rolling(3).rank()\n0    NaN\n1    NaN\n2    2.0\n3    2.0\n4    3.0\n5    1.5\ndtype: float64', '>>> s.rolling(3).rank(method=""max"")\n0    NaN\n1    NaN\n2    2.0\n3    2.0\n4    3.0\n5    2.0\ndtype: float64', '>>> s.rolling(3).rank(method=""min"")\n0    NaN\n1    NaN\n2    2.0\n3    2.0\n4    3.0\n5    1.0\ndtype: float64']"
739,..\pandas\reference\api\pandas.tseries.offsets.Nano.is_year_start.html,pandas.tseries.offsets.Nano.is_year_start,Nano.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
740,..\pandas\reference\api\pandas.DatetimeIndex.is_quarter_start.html,pandas.DatetimeIndex.is_quarter_start,property DatetimeIndex.is_quarter_start[source]# Indicator for whether the date is the first day of a quarter.,Returns: is_quarter_startSeries or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> df = pd.DataFrame({\'dates\': pd.date_range(""2017-03-30"",\n...                   periods=4)})\n>>> df.assign(quarter=df.dates.dt.quarter,\n...           is_quarter_start=df.dates.dt.is_quarter_start)\n       dates  quarter  is_quarter_start\n0 2017-03-30        1             False\n1 2017-03-31        1             False\n2 2017-04-01        2              True\n3 2017-04-02        2             False', "">>> idx = pd.date_range('2017-03-30', periods=4)\n>>> idx\nDatetimeIndex(['2017-03-30', '2017-03-31', '2017-04-01', '2017-04-02'],\n              dtype='datetime64[ns]', freq='D')"", '>>> idx.is_quarter_start\narray([False, False,  True, False])']"
741,..\pandas\reference\api\pandas.isnull.html,pandas.isnull,"pandas.isnull(obj)[source]# Detect missing values for an array-like object. This function takes a scalar or array-like object and indicates whether values are missing (NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).","Parameters: objscalar or array-likeObject to check for null or missing values. Returns: bool or array-like of boolFor scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is missing.","["">>> pd.isna('dog')\nFalse"", '>>> pd.isna(pd.NA)\nTrue', '>>> pd.isna(np.nan)\nTrue', '>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n>>> array\narray([[ 1., nan,  3.],\n       [ 4.,  5., nan]])\n>>> pd.isna(array)\narray([[False,  True, False],\n       [False, False,  True]])', '>>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,\n...                           ""2017-07-08""])\n>>> index\nDatetimeIndex([\'2017-07-05\', \'2017-07-06\', \'NaT\', \'2017-07-08\'],\n              dtype=\'datetime64[ns]\', freq=None)\n>>> pd.isna(index)\narray([False, False,  True, False])', "">>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])\n>>> df\n     0     1    2\n0  ant   bee  cat\n1  dog  None  fly\n>>> pd.isna(df)\n       0      1      2\n0  False  False  False\n1  False   True  False"", '>>> pd.isna(df[1])\n0    False\n1     True\nName: 1, dtype: bool']"
742,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_anchored.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_anchored,CustomBusinessMonthBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
743,..\pandas\reference\api\pandas.Timestamp.day_of_week.html,pandas.Timestamp.day_of_week,Timestamp.day_of_week# Return day of the week.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.day_of_week\n5']"
744,..\pandas\reference\api\pandas.arrays.IntervalArray.is_empty.html,pandas.arrays.IntervalArray.is_empty,"IntervalArray.is_empty# Indicates if an interval is empty, meaning it contains no points.","Returns: bool or ndarrayA boolean indicating if a scalar Interval is empty, or a boolean ndarray positionally indicating if an Interval in an IntervalArray or IntervalIndex is empty.","["">>> pd.Interval(0, 1, closed='right').is_empty\nFalse"", "">>> pd.Interval(0, 0, closed='right').is_empty\nTrue\n>>> pd.Interval(0, 0, closed='left').is_empty\nTrue\n>>> pd.Interval(0, 0, closed='neither').is_empty\nTrue"", "">>> pd.Interval(0, 0, closed='both').is_empty\nFalse"", "">>> ivs = [pd.Interval(0, 0, closed='neither'),\n...        pd.Interval(1, 2, closed='neither')]\n>>> pd.arrays.IntervalArray(ivs).is_empty\narray([ True, False])"", "">>> ivs = [pd.Interval(0, 0, closed='neither'), np.nan]\n>>> pd.IntervalIndex(ivs).is_empty\narray([ True, False])""]"
745,..\pandas\reference\api\pandas.Series.hasnans.html,pandas.Series.hasnans,property Series.hasnans[source]# Return True if there are any NaNs. Enables various performance speedups.,Returns: bool,"['>>> s = pd.Series([1, 2, 3, None])\n>>> s\n0    1.0\n1    2.0\n2    3.0\n3    NaN\ndtype: float64\n>>> s.hasnans\nTrue']"
746,..\pandas\reference\api\pandas.core.window.rolling.Rolling.sem.html,pandas.core.window.rolling.Rolling.sem,"Rolling.sem(ddof=1, numeric_only=False)[source]# Calculate the rolling standard error of mean. Notes A minimum of one period is required for the calculation.","Parameters: ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([0, 1, 2, 3])\n>>> s.rolling(2, min_periods=1).sem()\n0         NaN\n1    0.707107\n2    0.707107\n3    0.707107\ndtype: float64']"
747,..\pandas\reference\api\pandas.tseries.offsets.Nano.kwds.html,pandas.tseries.offsets.Nano.kwds,Nano.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
748,..\pandas\reference\api\pandas.json_normalize.html,pandas.json_normalize,"pandas.json_normalize(data, record_path=None, meta=None, meta_prefix=None, record_prefix=None, errors='raise', sep='.', max_level=None)[source]# Normalize semi-structured JSON data into a flat table.","Parameters: datadict or list of dictsUnserialized JSON objects. record_pathstr or list of str, default NonePath in each object to list of records. If not passed, data will be assumed to be an array of records. metalist of paths (str or list of str), default NoneFields to use as metadata for each record in resulting table. meta_prefixstr, default NoneIf True, prefix records with dotted (?) path, e.g. foo.bar.field if meta is [‘foo’, ‘bar’]. record_prefixstr, default NoneIf True, prefix records with dotted (?) path, e.g. foo.bar.field if path to records is [‘foo’, ‘bar’]. errors{‘raise’, ‘ignore’}, default ‘raise’Configures error handling. ‘ignore’ : will ignore KeyError if keys listed in meta are not always present. ‘raise’ : will raise KeyError if keys listed in meta are not always present. sepstr, default ‘.’Nested records will generate names separated by sep. e.g., for sep=’.’, {‘foo’: {‘bar’: 0}} -> foo.bar. max_levelint, default NoneMax number of levels(depth of dict) to normalize. if None, normalizes all levels. Returns: frameDataFrame Normalize semi-structured JSON data into a flat table.","['>>> data = [\n...     {""id"": 1, ""name"": {""first"": ""Coleen"", ""last"": ""Volk""}},\n...     {""name"": {""given"": ""Mark"", ""family"": ""Regner""}},\n...     {""id"": 2, ""name"": ""Faye Raker""},\n... ]\n>>> pd.json_normalize(data)\n    id name.first name.last name.given name.family        name\n0  1.0     Coleen      Volk        NaN         NaN         NaN\n1  NaN        NaN       NaN       Mark      Regner         NaN\n2  2.0        NaN       NaN        NaN         NaN  Faye Raker', '>>> data = [\n...     {\n...         ""id"": 1,\n...         ""name"": ""Cole Volk"",\n...         ""fitness"": {""height"": 130, ""weight"": 60},\n...     },\n...     {""name"": ""Mark Reg"", ""fitness"": {""height"": 130, ""weight"": 60}},\n...     {\n...         ""id"": 2,\n...         ""name"": ""Faye Raker"",\n...         ""fitness"": {""height"": 130, ""weight"": 60},\n...     },\n... ]\n>>> pd.json_normalize(data, max_level=0)\n    id        name                        fitness\n0  1.0   Cole Volk  {\'height\': 130, \'weight\': 60}\n1  NaN    Mark Reg  {\'height\': 130, \'weight\': 60}\n2  2.0  Faye Raker  {\'height\': 130, \'weight\': 60}', '>>> data = [\n...     {\n...         ""id"": 1,\n...         ""name"": ""Cole Volk"",\n...         ""fitness"": {""height"": 130, ""weight"": 60},\n...     },\n...     {""name"": ""Mark Reg"", ""fitness"": {""height"": 130, ""weight"": 60}},\n...     {\n...         ""id"": 2,\n...         ""name"": ""Faye Raker"",\n...         ""fitness"": {""height"": 130, ""weight"": 60},\n...     },\n... ]\n>>> pd.json_normalize(data, max_level=1)\n    id        name  fitness.height  fitness.weight\n0  1.0   Cole Volk             130              60\n1  NaN    Mark Reg             130              60\n2  2.0  Faye Raker             130              60', '>>> data = [\n...     {\n...         ""state"": ""Florida"",\n...         ""shortname"": ""FL"",\n...         ""info"": {""governor"": ""Rick Scott""},\n...         ""counties"": [\n...             {""name"": ""Dade"", ""population"": 12345},\n...             {""name"": ""Broward"", ""population"": 40000},\n...             {""name"": ""Palm Beach"", ""population"": 60000},\n...         ],\n...     },\n...     {\n...         ""state"": ""Ohio"",\n...         ""shortname"": ""OH"",\n...         ""info"": {""governor"": ""John Kasich""},\n...         ""counties"": [\n...             {""name"": ""Summit"", ""population"": 1234},\n...             {""name"": ""Cuyahoga"", ""population"": 1337},\n...         ],\n...     },\n... ]\n>>> result = pd.json_normalize(\n...     data, ""counties"", [""state"", ""shortname"", [""info"", ""governor""]]\n... )\n>>> result\n         name  population    state shortname info.governor\n0        Dade       12345   Florida    FL    Rick Scott\n1     Broward       40000   Florida    FL    Rick Scott\n2  Palm Beach       60000   Florida    FL    Rick Scott\n3      Summit        1234   Ohio       OH    John Kasich\n4    Cuyahoga        1337   Ohio       OH    John Kasich', '>>> data = {""A"": [1, 2]}\n>>> pd.json_normalize(data, ""A"", record_prefix=""Prefix."")\n    Prefix.0\n0          1\n1          2']"
749,..\pandas\reference\api\pandas.core.window.rolling.Rolling.skew.html,pandas.core.window.rolling.Rolling.skew,Rolling.skew(numeric_only=False)[source]# Calculate the rolling unbiased skewness. Notes A minimum of three periods is required for the rolling calculation.,"Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([1, 5, 2, 7, 15, 6])\n>>> ser.rolling(3).skew().round(6)\n0         NaN\n1         NaN\n2    1.293343\n3   -0.585583\n4    0.670284\n5    1.652317\ndtype: float64']"
750,..\pandas\reference\api\pandas.Series.head.html,pandas.Series.head,"Series.head(n=5)[source]# Return the first n rows. This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of n, this function returns all rows except the last |n| rows, equivalent to df[:n]. If n is larger than the number of rows, this function returns all rows.","Parameters: nint, default 5Number of rows to select. Returns: same type as callerThe first n rows of the caller object.","["">>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n>>> df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra"", '>>> df.head()\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey', '>>> df.head(3)\n      animal\n0  alligator\n1        bee\n2     falcon', '>>> df.head(-3)\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot']"
751,..\pandas\reference\api\pandas.DatetimeIndex.is_year_end.html,pandas.DatetimeIndex.is_year_end,property DatetimeIndex.is_year_end[source]# Indicate whether the date is the last day of the year.,Returns: Series or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> dates = pd.Series(pd.date_range(""2017-12-30"", periods=3))\n>>> dates\n0   2017-12-30\n1   2017-12-31\n2   2018-01-01\ndtype: datetime64[ns]', '>>> dates.dt.is_year_end\n0    False\n1     True\n2    False\ndtype: bool', '>>> idx = pd.date_range(""2017-12-30"", periods=3)\n>>> idx\nDatetimeIndex([\'2017-12-30\', \'2017-12-31\', \'2018-01-01\'],\n              dtype=\'datetime64[ns]\', freq=\'D\')', '>>> idx.is_year_end\narray([False,  True, False])']"
752,..\pandas\reference\api\pandas.arrays.IntervalArray.is_non_overlapping_monotonic.html,pandas.arrays.IntervalArray.is_non_overlapping_monotonic,"property IntervalArray.is_non_overlapping_monotonic[source]# Return a boolean whether the IntervalArray is non-overlapping and monotonic. Non-overlapping means (no Intervals share points), and monotonic means either monotonic increasing or monotonic decreasing.",No parameters found,"['>>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.is_non_overlapping_monotonic\nTrue', '>>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1),\n...                                       pd.Interval(-1, 0.1)])\n>>> interv_arr\n<IntervalArray>\n[(0.0, 1.0], (-1.0, 0.1]]\nLength: 2, dtype: interval[float64, right]\n>>> interv_arr.is_non_overlapping_monotonic\nFalse', "">>> interv_idx = pd.interval_range(start=0, end=2)\n>>> interv_idx\nIntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')\n>>> interv_idx.is_non_overlapping_monotonic\nTrue"", "">>> interv_idx = pd.interval_range(start=0, end=2, closed='both')\n>>> interv_idx\nIntervalIndex([[0, 1], [1, 2]], dtype='interval[int64, both]')\n>>> interv_idx.is_non_overlapping_monotonic\nFalse""]"
753,..\pandas\reference\api\pandas.Timestamp.day_of_year.html,pandas.Timestamp.day_of_year,Timestamp.day_of_year# Return the day of the year.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.day_of_year\n74']"
754,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_end.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_end,CustomBusinessMonthBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
755,..\pandas\reference\api\pandas.tseries.offsets.Nano.n.html,pandas.tseries.offsets.Nano.n,Nano.n#,No parameters found,[]
756,..\pandas\reference\api\pandas.lreshape.html,pandas.lreshape,"pandas.lreshape(data, groups, dropna=True)[source]# Reshape wide-format data to long. Generalized inverse of DataFrame.pivot. Accepts a dictionary, groups, in which each key is a new column name and each value is a list of old column names that will be “melted” under the new column name as part of the reshape.","Parameters: dataDataFrameThe wide-format DataFrame. groupsdict{new_name : list_of_columns}. dropnabool, default TrueDo not include columns whose entries are all NaN. Returns: DataFrameReshaped DataFrame.","["">>> data = pd.DataFrame({'hr1': [514, 573], 'hr2': [545, 526],\n...                      'team': ['Red Sox', 'Yankees'],\n...                      'year1': [2007, 2007], 'year2': [2008, 2008]})\n>>> data\n   hr1  hr2     team  year1  year2\n0  514  545  Red Sox   2007   2008\n1  573  526  Yankees   2007   2008"", "">>> pd.lreshape(data, {'year': ['year1', 'year2'], 'hr': ['hr1', 'hr2']})\n      team  year   hr\n0  Red Sox  2007  514\n1  Yankees  2007  573\n2  Red Sox  2008  545\n3  Yankees  2008  526""]"
757,..\pandas\reference\api\pandas.core.window.rolling.Rolling.std.html,pandas.core.window.rolling.Rolling.std,"Rolling.std(ddof=1, numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the rolling standard deviation. Notes The default ddof of 1 used in Series.std() is different than the default ddof of 0 in numpy.std(). A minimum of one period is required for the rolling calculation.","Parameters: ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.4.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])\n>>> s.rolling(3).std()\n0         NaN\n1         NaN\n2    0.577350\n3    1.000000\n4    1.000000\n5    1.154701\n6    0.000000\ndtype: float64']"
758,..\pandas\reference\api\pandas.DatetimeIndex.is_year_start.html,pandas.DatetimeIndex.is_year_start,property DatetimeIndex.is_year_start[source]# Indicate whether the date is the first day of a year.,Returns: Series or DatetimeIndexThe same type as the original data with boolean values. Series will have the same name and index. DatetimeIndex will have the same name.,"['>>> dates = pd.Series(pd.date_range(""2017-12-30"", periods=3))\n>>> dates\n0   2017-12-30\n1   2017-12-31\n2   2018-01-01\ndtype: datetime64[ns]', '>>> dates.dt.is_year_start\n0    False\n1    False\n2    True\ndtype: bool', '>>> idx = pd.date_range(""2017-12-30"", periods=3)\n>>> idx\nDatetimeIndex([\'2017-12-30\', \'2017-12-31\', \'2018-01-01\'],\n              dtype=\'datetime64[ns]\', freq=\'D\')', '>>> idx.is_year_start\narray([False, False,  True])']"
759,..\pandas\reference\api\pandas.arrays.IntervalArray.left.html,pandas.arrays.IntervalArray.left,property IntervalArray.left[source]# Return the left endpoints of each Interval in the IntervalArray as an Index.,No parameters found,"["">>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(2, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (2, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.left\nIndex([0, 2], dtype='int64')""]"
760,..\pandas\reference\api\pandas.core.window.rolling.Rolling.sum.html,pandas.core.window.rolling.Rolling.sum,"Rolling.sum(numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the rolling sum. Notes See Numba engine and Numba (JIT compilation) for extended documentation and performance considerations for the Numba engine.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.3.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.3.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64', '>>> s.rolling(3).sum()\n0     NaN\n1     NaN\n2     6.0\n3     9.0\n4    12.0\ndtype: float64', '>>> s.rolling(3, center=True).sum()\n0     NaN\n1     6.0\n2     9.0\n3    12.0\n4     NaN\ndtype: float64', '>>> df = pd.DataFrame({""A"": s, ""B"": s ** 2})\n>>> df\n   A   B\n0  1   1\n1  2   4\n2  3   9\n3  4  16\n4  5  25', '>>> df.rolling(3).sum()\n      A     B\n0   NaN   NaN\n1   NaN   NaN\n2   6.0  14.0\n3   9.0  29.0\n4  12.0  50.0']"
761,..\pandas\reference\api\pandas.Timestamp.dst.html,pandas.Timestamp.dst,Timestamp.dst()# Return the daylight saving time (DST) adjustment.,No parameters found,"["">>> ts = pd.Timestamp('2000-06-01 00:00:00', tz='Europe/Brussels')\n>>> ts\nTimestamp('2000-06-01 00:00:00+0200', tz='Europe/Brussels')\n>>> ts.dst()\ndatetime.timedelta(seconds=3600)""]"
762,..\pandas\reference\api\pandas.tseries.offsets.Nano.name.html,pandas.tseries.offsets.Nano.name,Nano.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
763,..\pandas\reference\api\pandas.melt.html,pandas.melt,"pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]# Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’. Notes Reference the user guide for more examples.","Parameters: id_varsscalar, tuple, list, or ndarray, optionalColumn(s) to use as identifier variables. value_varsscalar, tuple, list, or ndarray, optionalColumn(s) to unpivot. If not specified, uses all columns that are not set as id_vars. var_namescalar, default NoneName to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’. value_namescalar, default ‘value’Name to use for the ‘value’ column, can’t be an existing column label. col_levelscalar, optionalIf columns are a MultiIndex then use this level to melt. ignore_indexbool, default TrueIf True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary. Returns: DataFrameUnpivoted DataFrame.","["">>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n>>> df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6"", "">>> pd.melt(df, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5"", "">>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6"", "">>> pd.melt(df, id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5"", "">>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6"", "">>> df.columns = [list('ABC'), list('DEF')]\n>>> df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6"", "">>> pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5"", "">>> pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5""]"
764,..\pandas\reference\api\pandas.DatetimeIndex.mean.html,pandas.DatetimeIndex.mean,"DatetimeIndex.mean(*, skipna=True, axis=0)[source]# Return the mean value of the Array. Notes mean is only defined for Datetime and Timedelta dtypes, not for Period.","Parameters: skipnabool, default TrueWhether to ignore any NaT elements. axisint, optional, default 0 Returns: scalarTimestamp or Timedelta.","["">>> idx = pd.date_range('2001-01-01 00:00', periods=3)\n>>> idx\nDatetimeIndex(['2001-01-01', '2001-01-02', '2001-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.mean()\nTimestamp('2001-01-02 00:00:00')"", "">>> tdelta_idx = pd.to_timedelta([1, 2, 3], unit='D')\n>>> tdelta_idx\nTimedeltaIndex(['1 days', '2 days', '3 days'],\n                dtype='timedelta64[ns]', freq=None)\n>>> tdelta_idx.mean()\nTimedelta('2 days 00:00:00')""]"
765,..\pandas\reference\api\pandas.Series.hist.html,pandas.Series.hist,"Series.hist(by=None, ax=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None, figsize=None, bins=10, backend=None, legend=False, **kwargs)[source]# Draw histogram of the input series using matplotlib.","Parameters: byobject, optionalIf passed, then used to form histograms for separate groups. axmatplotlib axis objectIf not passed, uses gca(). gridbool, default TrueWhether to show axis grid lines. xlabelsizeint, default NoneIf specified changes the x-axis label size. xrotfloat, default NoneRotation of x axis labels. ylabelsizeint, default NoneIf specified changes the y-axis label size. yrotfloat, default NoneRotation of y axis labels. figsizetuple, default NoneFigure size in inches by default. binsint or sequence, default 10Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. legendbool, default FalseWhether to show the legend. **kwargsTo be passed to the actual plotting function. Returns: matplotlib.AxesSubplotA histogram plot.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> hist = ser.hist()"", "">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> hist = ser.groupby(level=0).hist()""]"
766,..\pandas\reference\api\pandas.arrays.IntervalArray.length.html,pandas.arrays.IntervalArray.length,property IntervalArray.length[source]# Return an Index with entries denoting the length of each Interval.,No parameters found,"["">>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.length\nIndex([1, 4], dtype='int64')""]"
767,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_start.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_month_start,CustomBusinessMonthBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
768,..\pandas\reference\api\pandas.core.window.rolling.Rolling.var.html,pandas.core.window.rolling.Rolling.var,"Rolling.var(ddof=1, numeric_only=False, engine=None, engine_kwargs=None)[source]# Calculate the rolling variance. Notes The default ddof of 1 used in Series.var() is different than the default ddof of 0 in numpy.var(). A minimum of one period is required for the rolling calculation.","Parameters: ddofint, default 1Delta Degrees of Freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} Added in version 1.4.0. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> s = pd.Series([5, 5, 6, 7, 5, 5, 5])\n>>> s.rolling(3).var()\n0         NaN\n1         NaN\n2    0.333333\n3    1.000000\n4    1.000000\n5    1.333333\n6    0.000000\ndtype: float64']"
769,..\pandas\reference\api\pandas.merge.html,pandas.merge,"pandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=None, indicator=False, validate=None)[source]# Merge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed. Warning If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.","Parameters: leftDataFrame or named Series rightDataFrame or named SeriesObject to merge with. how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’Type of merge to be performed. left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. cross: creates the cartesian product from both frames, preserves the order of the left keys. onlabel or listColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels. right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as left_index. sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword). suffixeslist-like, default is (“_x”, “_y”)A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. copybool, default TrueIf False, avoid copy if possible. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True indicatorbool or str, default FalseIf True, adds a column to the output DataFrame called “_merge” with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of “left_only” for observations whose merge key only appears in the left DataFrame, “right_only” for observations whose merge key only appears in the right DataFrame, and “both” if the observation’s merge key is found in both DataFrames. validatestr, optionalIf specified, checks if merge is of specified type. “one_to_one” or “1:1”: check if merge keys are unique in both left and right datasets. “one_to_many” or “1:m”: check if merge keys are unique in left dataset. “many_to_one” or “m:1”: check if merge keys are unique in right dataset. “many_to_many” or “m:m”: allowed, but does not result in checks. Returns: DataFrameA DataFrame of the two merged objects.","["">>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8"", "">>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  bar        2  bar        6\n3  baz        3  baz        7\n4  foo        5  foo        5\n5  foo        5  foo        8"", "">>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  bar           2  bar            6\n3  baz           3  baz            7\n4  foo           5  foo            5\n5  foo           5  foo            8"", "">>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')"", "">>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4"", "">>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3"", "">>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN"", "">>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8"", "">>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8""]"
770,..\pandas\reference\api\pandas.DatetimeIndex.microsecond.html,pandas.DatetimeIndex.microsecond,property DatetimeIndex.microsecond[source]# The microseconds of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""us"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00.000000\n1   2000-01-01 00:00:00.000001\n2   2000-01-01 00:00:00.000002\ndtype: datetime64[ns]\n>>> datetime_series.dt.microsecond\n0       0\n1       1\n2       2\ndtype: int32']"
771,..\pandas\reference\api\pandas.arrays.IntervalArray.mid.html,pandas.arrays.IntervalArray.mid,property IntervalArray.mid[source]# Return the midpoint of each Interval in the IntervalArray as an Index.,No parameters found,"["">>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(1, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (1, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.mid\nIndex([0.5, 3.0], dtype='float64')""]"
772,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_on_offset.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_on_offset,CustomBusinessMonthBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
772,..\pandas\reference\api\pandas.Timestamp.floor.html,pandas.Timestamp.floor,"Timestamp.floor(freq, ambiguous='raise', nonexistent='raise')# Return a new Timestamp floored to this resolution. Notes If the Timestamp has a timezone, flooring will take place relative to the local (“wall”) time and re-localized to the same timezone. When flooring near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstrFrequency string indicating the flooring resolution. ambiguousbool or {‘raise’, ‘NaT’}, default ‘raise’The behavior is as follows: bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates). ‘NaT’ will return NaT for an ambiguous time. ‘raise’ will raise an AmbiguousTimeError for an ambiguous time. nonexistent{‘raise’, ‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta}, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time. ‘shift_backward’ will shift the nonexistent time backward to the closest existing time. ‘NaT’ will return NaT where there are nonexistent times. timedelta objects will shift nonexistent times by the timedelta. ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Raises: ValueError if the freq cannot be converted.","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')"", "">>> ts.floor(freq='h') # hour\nTimestamp('2020-03-14 15:00:00')"", "">>> ts.floor(freq='min') # minute\nTimestamp('2020-03-14 15:32:00')"", "">>> ts.floor(freq='s') # seconds\nTimestamp('2020-03-14 15:32:52')"", "">>> ts.floor(freq='ns') # nanoseconds\nTimestamp('2020-03-14 15:32:52.192548651')"", "">>> ts.floor(freq='5min')\nTimestamp('2020-03-14 15:30:00')"", "">>> ts.floor(freq='1h30min')\nTimestamp('2020-03-14 15:00:00')"", '>>> pd.NaT.floor()\nNaT', '>>> ts_tz = pd.Timestamp(""2021-10-31 03:30:00"").tz_localize(""Europe/Amsterdam"")', '>>> ts_tz.floor(""2h"", ambiguous=False)\nTimestamp(\'2021-10-31 02:00:00+0100\', tz=\'Europe/Amsterdam\')', '>>> ts_tz.floor(""2h"", ambiguous=True)\nTimestamp(\'2021-10-31 02:00:00+0200\', tz=\'Europe/Amsterdam\')']"
774,..\pandas\reference\api\pandas.tseries.offsets.Nano.nanos.html,pandas.tseries.offsets.Nano.nanos,Nano.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
775,..\pandas\reference\api\pandas.Series.html,pandas.Series,"class pandas.Series(data=None, index=None, dtype=None, name=None, copy=None, fastpath=<no_default>)[source]# One-dimensional ndarray with axis labels (including time series). Labels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data (currently represented as NaN). Operations between Series (+, -, /, *, **) align values based on their associated index values– they need not be the same length. The result index will be the sorted union of the two indexes. Notes Please reference the User Guide for more information. Examples Constructing Series from a dictionary with an Index specified The keys of the dictionary match with the Index values, hence the Index values have no effect. Note that the Index is first build with the keys from the dictionary. After this the Series is reindexed with the given Index values, hence we get all NaN as a result. Constructing Series from a list with copy=False. Due to input data type the Series has a copy of the original data even though copy=False, so the data is unchanged. Constructing Series from a 1d ndarray with copy=False. Due to input data type the Series has a view on the original data, so the data is changed as well. Attributes T Return the transpose, which is by definition self. array The ExtensionArray of the data backing this Series or Index. at Access a single value for a row/column label pair. attrs Dictionary of global attributes of this dataset. axes Return a list of the row axis labels. dtype Return the dtype object of the underlying data. dtypes Return the dtype object of the underlying data. empty Indicator whether Series/DataFrame is empty. flags Get the properties associated with this pandas object. hasnans Return True if there are any NaNs. iat Access a single value for a row/column pair by integer position. iloc (DEPRECATED) Purely integer-location based indexing for selection by position. index The index (axis labels) of the Series. is_monotonic_decreasing Return boolean if values in the object are monotonically decreasing. is_monotonic_increasing Return boolean if values in the object are monotonically increasing. is_unique Return boolean if values in the object are unique. loc Access a group of rows and columns by label(s) or a boolean array. name Return the name of the Series. nbytes Return the number of bytes in the underlying data. ndim Number of dimensions of the underlying data, by definition 1. shape Return a tuple of the shape of the underlying data. size Return the number of elements in the underlying data. values Return Series as ndarray or ndarray-like depending on the dtype.","Parameters: dataarray-like, Iterable, dict, or scalar valueContains data stored in Series. If data is a dict, argument order is maintained. indexarray-like or Index (1d)Values must be hashable and have the same length as data. Non-unique index values are allowed. Will default to RangeIndex (0, 1, 2, …, n) if not provided. If data is dict-like and index is None, then the keys in the data are used as the index. If the index is not None, the resulting Series is reindexed with the index values. dtypestr, numpy.dtype, or ExtensionDtype, optionalData type for the output Series. If not specified, this will be inferred from data. See the user guide for more usages. nameHashable, default NoneThe name to give to the Series. copybool, default FalseCopy input data. Only affects Series or 1d ndarray input. See examples.","["">>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n>>> ser\na   1\nb   2\nc   3\ndtype: int64"", "">>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n>>> ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64"", '>>> r = [1, 2]\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\n[1, 2]\n>>> ser\n0    999\n1      2\ndtype: int64', '>>> r = np.array([1, 2])\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\narray([999,   2])\n>>> ser\n0    999\n1      2\ndtype: int64']"
776,..\pandas\reference\api\pandas.merge_asof.html,pandas.merge_asof,"pandas.merge_asof(left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=('_x', '_y'), tolerance=None, allow_exact_matches=True, direction='backward')[source]# Perform a merge by key distance. This is similar to a left-join except that we match on nearest key rather than equal keys. Both DataFrames must be sorted by the key. For each row in the left DataFrame: A “backward” search selects the last row in the right DataFrame whose ‘on’ key is less than or equal to the left’s key. A “forward” search selects the first row in the right DataFrame whose ‘on’ key is greater than or equal to the left’s key. A “nearest” search selects the row in the right DataFrame whose ‘on’ key is closest in absolute distance to the left’s key. Optionally match on equivalent keys with ‘by’ before searching with ‘on’.","Parameters: leftDataFrame or named Series rightDataFrame or named Series onlabelField name to join on. Must be found in both DataFrames. The data MUST be ordered. Furthermore this must be a numeric column, such as datetimelike, integer, or float. On or left_on/right_on must be given. left_onlabelField name to join on in left DataFrame. right_onlabelField name to join on in right DataFrame. left_indexboolUse the index of the left DataFrame as the join key. right_indexboolUse the index of the right DataFrame as the join key. bycolumn name or list of column namesMatch on these columns before performing merge operation. left_bycolumn nameField names to match on in the left DataFrame. right_bycolumn nameField names to match on in the right DataFrame. suffixes2-length sequence (tuple, list, …)Suffix to apply to overlapping column names in the left and right side, respectively. toleranceint or Timedelta, optional, default NoneSelect asof tolerance within this range; must be compatible with the merge index. allow_exact_matchesbool, default True If True, allow matching with the same ‘on’ value (i.e. less-than-or-equal-to / greater-than-or-equal-to) If False, don’t match the same ‘on’ value (i.e., strictly less-than / strictly greater-than). direction‘backward’ (default), ‘forward’, or ‘nearest’Whether to search for prior, subsequent, or closest matches. Returns: DataFrame","['>>> left = pd.DataFrame({""a"": [1, 5, 10], ""left_val"": [""a"", ""b"", ""c""]})\n>>> left\n    a left_val\n0   1        a\n1   5        b\n2  10        c', '>>> right = pd.DataFrame({""a"": [1, 2, 3, 6, 7], ""right_val"": [1, 2, 3, 6, 7]})\n>>> right\n   a  right_val\n0  1          1\n1  2          2\n2  3          3\n3  6          6\n4  7          7', '>>> pd.merge_asof(left, right, on=""a"")\n    a left_val  right_val\n0   1        a          1\n1   5        b          3\n2  10        c          7', '>>> pd.merge_asof(left, right, on=""a"", allow_exact_matches=False)\n    a left_val  right_val\n0   1        a        NaN\n1   5        b        3.0\n2  10        c        7.0', '>>> pd.merge_asof(left, right, on=""a"", direction=""forward"")\n    a left_val  right_val\n0   1        a        1.0\n1   5        b        6.0\n2  10        c        NaN', '>>> pd.merge_asof(left, right, on=""a"", direction=""nearest"")\n    a left_val  right_val\n0   1        a          1\n1   5        b          6\n2  10        c          7', '>>> left = pd.DataFrame({""left_val"": [""a"", ""b"", ""c""]}, index=[1, 5, 10])\n>>> left\n   left_val\n1         a\n5         b\n10        c', '>>> right = pd.DataFrame({""right_val"": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])\n>>> right\n   right_val\n1          1\n2          2\n3          3\n6          6\n7          7', '>>> pd.merge_asof(left, right, left_index=True, right_index=True)\n   left_val  right_val\n1         a          1\n5         b          3\n10        c          7', '>>> quotes = pd.DataFrame(\n...     {\n...         ""time"": [\n...             pd.Timestamp(""2016-05-25 13:30:00.023""),\n...             pd.Timestamp(""2016-05-25 13:30:00.023""),\n...             pd.Timestamp(""2016-05-25 13:30:00.030""),\n...             pd.Timestamp(""2016-05-25 13:30:00.041""),\n...             pd.Timestamp(""2016-05-25 13:30:00.048""),\n...             pd.Timestamp(""2016-05-25 13:30:00.049""),\n...             pd.Timestamp(""2016-05-25 13:30:00.072""),\n...             pd.Timestamp(""2016-05-25 13:30:00.075"")\n...         ],\n...         ""ticker"": [\n...                ""GOOG"",\n...                ""MSFT"",\n...                ""MSFT"",\n...                ""MSFT"",\n...                ""GOOG"",\n...                ""AAPL"",\n...                ""GOOG"",\n...                ""MSFT""\n...            ],\n...            ""bid"": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n...            ""ask"": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]\n...     }\n... )\n>>> quotes\n                     time ticker     bid     ask\n0 2016-05-25 13:30:00.023   GOOG  720.50  720.93\n1 2016-05-25 13:30:00.023   MSFT   51.95   51.96\n2 2016-05-25 13:30:00.030   MSFT   51.97   51.98\n3 2016-05-25 13:30:00.041   MSFT   51.99   52.00\n4 2016-05-25 13:30:00.048   GOOG  720.50  720.93\n5 2016-05-25 13:30:00.049   AAPL   97.99   98.01\n6 2016-05-25 13:30:00.072   GOOG  720.50  720.88\n7 2016-05-25 13:30:00.075   MSFT   52.01   52.03', '>>> trades = pd.DataFrame(\n...        {\n...            ""time"": [\n...                pd.Timestamp(""2016-05-25 13:30:00.023""),\n...                pd.Timestamp(""2016-05-25 13:30:00.038""),\n...                pd.Timestamp(""2016-05-25 13:30:00.048""),\n...                pd.Timestamp(""2016-05-25 13:30:00.048""),\n...                pd.Timestamp(""2016-05-25 13:30:00.048"")\n...            ],\n...            ""ticker"": [""MSFT"", ""MSFT"", ""GOOG"", ""GOOG"", ""AAPL""],\n...            ""price"": [51.95, 51.95, 720.77, 720.92, 98.0],\n...            ""quantity"": [75, 155, 100, 100, 100]\n...        }\n...    )\n>>> trades\n                     time ticker   price  quantity\n0 2016-05-25 13:30:00.023   MSFT   51.95        75\n1 2016-05-25 13:30:00.038   MSFT   51.95       155\n2 2016-05-25 13:30:00.048   GOOG  720.77       100\n3 2016-05-25 13:30:00.048   GOOG  720.92       100\n4 2016-05-25 13:30:00.048   AAPL   98.00       100', '>>> pd.merge_asof(trades, quotes, on=""time"", by=""ticker"")\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN', '>>> pd.merge_asof(\n...     trades, quotes, on=""time"", by=""ticker"", tolerance=pd.Timedelta(""2ms"")\n... )\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96\n1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN\n2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93\n3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN', '>>> pd.merge_asof(\n...     trades,\n...     quotes,\n...     on=""time"",\n...     by=""ticker"",\n...     tolerance=pd.Timedelta(""10ms""),\n...     allow_exact_matches=False\n... )\n                     time ticker   price  quantity     bid     ask\n0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN\n1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98\n2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN\n3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN\n4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN']"
777,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_end.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_end,CustomBusinessMonthBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
778,..\pandas\reference\api\pandas.DatetimeIndex.minute.html,pandas.DatetimeIndex.minute,property DatetimeIndex.minute[source]# The minutes of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""min"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 00:01:00\n2   2000-01-01 00:02:00\ndtype: datetime64[ns]\n>>> datetime_series.dt.minute\n0    0\n1    1\n2    2\ndtype: int32']"
779,..\pandas\reference\api\pandas.arrays.IntervalArray.overlaps.html,pandas.arrays.IntervalArray.overlaps,"IntervalArray.overlaps(other)[source]# Check elementwise if an Interval overlaps the values in the IntervalArray. Two intervals overlap if they share a common point, including closed endpoints. Intervals that only have an open endpoint in common do not overlap.",Parameters: otherIntervalArrayInterval to check against for an overlap. Returns: ndarrayBoolean array positionally indicating where an overlap occurs.,"['>>> data = [(0, 1), (1, 3), (2, 4)]\n>>> intervals = pd.arrays.IntervalArray.from_tuples(data)\n>>> intervals\n<IntervalArray>\n[(0, 1], (1, 3], (2, 4]]\nLength: 3, dtype: interval[int64, right]', '>>> intervals.overlaps(pd.Interval(0.5, 1.5))\narray([ True,  True, False])', "">>> intervals.overlaps(pd.Interval(1, 3, closed='left'))\narray([ True,  True, True])"", "">>> intervals.overlaps(pd.Interval(1, 2, closed='right'))\narray([False,  True, False])""]"
780,..\pandas\reference\api\pandas.Series.iat.html,pandas.Series.iat,"property Series.iat[source]# Access a single value for a row/column pair by integer position. Similar to iloc, in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.",Raises: IndexErrorWhen integer position is out of bounds.,"["">>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n0   0   2   3\n1   0   4   1\n2  10  20  30"", '>>> df.iat[1, 2]\n1', '>>> df.iat[1, 2] = 10\n>>> df.iat[1, 2]\n10', '>>> df.loc[0].iat[1]\n2']"
781,..\pandas\reference\api\pandas.Timestamp.fold.html,pandas.Timestamp.fold,Timestamp.fold#,No parameters found,[]
782,..\pandas\reference\api\pandas.tseries.offsets.Nano.normalize.html,pandas.tseries.offsets.Nano.normalize,Nano.normalize#,No parameters found,[]
783,..\pandas\reference\api\pandas.merge_ordered.html,pandas.merge_ordered,"pandas.merge_ordered(left, right, on=None, left_on=None, right_on=None, left_by=None, right_by=None, fill_method=None, suffixes=('_x', '_y'), how='outer')[source]# Perform a merge for ordered data with optional filling/interpolation. Designed for ordered data like time series data. Optionally perform group-wise merge (see examples).","Parameters: leftDataFrame or named Series rightDataFrame or named Series onlabel or listField names to join on. Must be found in both DataFrames. left_onlabel or list, or array-likeField names to join on in left DataFrame. Can be a vector or list of vectors of the length of the DataFrame to use a particular vector as the join key instead of columns. right_onlabel or list, or array-likeField names to join on in right DataFrame or vector/list of vectors per left_on docs. left_bycolumn name or list of column namesGroup left DataFrame by group columns and merge piece by piece with right DataFrame. Must be None if either left or right are a Series. right_bycolumn name or list of column namesGroup right DataFrame by group columns and merge piece by piece with left DataFrame. Must be None if either left or right are a Series. fill_method{‘ffill’, None}, default NoneInterpolation method for data. suffixeslist-like, default is (“_x”, “_y”)A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. how{‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘outer’ left: use only keys from left frame (SQL: left outer join) right: use only keys from right frame (SQL: right outer join) outer: use union of keys from both frames (SQL: full outer join) inner: use intersection of keys from both frames (SQL: inner join). Returns: DataFrameThe merged DataFrame output type will be the same as ‘left’, if it is a subclass of DataFrame.","['>>> from pandas import merge_ordered\n>>> df1 = pd.DataFrame(\n...     {\n...         ""key"": [""a"", ""c"", ""e"", ""a"", ""c"", ""e""],\n...         ""lvalue"": [1, 2, 3, 1, 2, 3],\n...         ""group"": [""a"", ""a"", ""a"", ""b"", ""b"", ""b""]\n...     }\n... )\n>>> df1\n  key  lvalue group\n0   a       1     a\n1   c       2     a\n2   e       3     a\n3   a       1     b\n4   c       2     b\n5   e       3     b', '>>> df2 = pd.DataFrame({""key"": [""b"", ""c"", ""d""], ""rvalue"": [1, 2, 3]})\n>>> df2\n  key  rvalue\n0   b       1\n1   c       2\n2   d       3', '>>> merge_ordered(df1, df2, fill_method=""ffill"", left_by=""group"")\n  key  lvalue group  rvalue\n0   a       1     a     NaN\n1   b       1     a     1.0\n2   c       2     a     2.0\n3   d       2     a     3.0\n4   e       3     a     3.0\n5   a       1     b     NaN\n6   b       1     b     1.0\n7   c       2     b     2.0\n8   d       2     b     3.0\n9   e       3     b     3.0']"
784,..\pandas\reference\api\pandas.core.window.rolling.Window.mean.html,pandas.core.window.rolling.Window.mean,"Window.mean(numeric_only=False, **kwargs)[source]# Calculate the rolling weighted window mean.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. **kwargsKeyword arguments to configure the SciPy weighted window type. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([0, 1, 5, 2, 8])', "">>> type(ser.rolling(2, win_type='gaussian'))\n<class 'pandas.core.window.rolling.Window'>"", "">>> ser.rolling(2, win_type='gaussian').mean(std=3)\n0    NaN\n1    0.5\n2    3.0\n3    3.5\n4    5.0\ndtype: float64""]"
785,..\pandas\reference\api\pandas.Timestamp.fromordinal.html,pandas.Timestamp.fromordinal,"classmethod Timestamp.fromordinal(ordinal, tz=None)# Construct a timestamp from a a proleptic Gregorian ordinal. Notes By definition there cannot be any tz info on the ordinal itself.","Parameters: ordinalintDate corresponding to a proleptic Gregorian ordinal. tzstr, pytz.timezone, dateutil.tz.tzfile or NoneTime zone for the Timestamp.","["">>> pd.Timestamp.fromordinal(737425)\nTimestamp('2020-01-01 00:00:00')""]"
786,..\pandas\reference\api\pandas.DatetimeIndex.month.html,pandas.DatetimeIndex.month,"property DatetimeIndex.month[source]# The month as January=1, December=12.",No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""ME"")\n... )\n>>> datetime_series\n0   2000-01-31\n1   2000-02-29\n2   2000-03-31\ndtype: datetime64[ns]\n>>> datetime_series.dt.month\n0    1\n1    2\n2    3\ndtype: int32']"
787,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_start.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_quarter_start,CustomBusinessMonthBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
788,..\pandas\reference\api\pandas.arrays.IntervalArray.right.html,pandas.arrays.IntervalArray.right,property IntervalArray.right[source]# Return the right endpoints of each Interval in the IntervalArray as an Index.,No parameters found,"["">>> interv_arr = pd.arrays.IntervalArray([pd.Interval(0, 1), pd.Interval(2, 5)])\n>>> interv_arr\n<IntervalArray>\n[(0, 1], (2, 5]]\nLength: 2, dtype: interval[int64, right]\n>>> interv_arr.right\nIndex([1, 5], dtype='int64')""]"
789,..\pandas\reference\api\pandas.MultiIndex.append.html,pandas.MultiIndex.append,MultiIndex.append(other)[source]# Append a collection of Index options together.,Parameters: otherIndex or list/tuple of indices Returns: IndexThe combined index.,"["">>> mi = pd.MultiIndex.from_arrays([['a'], ['b']])\n>>> mi\nMultiIndex([('a', 'b')],\n           )\n>>> mi.append(mi)\nMultiIndex([('a', 'b'), ('a', 'b')],\n           )""]"
790,..\pandas\reference\api\pandas.tseries.offsets.Nano.rule_code.html,pandas.tseries.offsets.Nano.rule_code,Nano.rule_code#,No parameters found,[]
791,..\pandas\reference\api\pandas.Series.idxmax.html,pandas.Series.idxmax,"Series.idxmax(axis=0, skipna=True, *args, **kwargs)[source]# Return the row label of the maximum value. If multiple values equal the maximum, the first row label with that value is returned. Notes This method is the Series version of ndarray.argmax. This method returns the label of the maximum, while ndarray.argmax returns the position. To get the position, use series.values.argmax().","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values. If the entire Series is NA, the result will be NA. *args, **kwargsAdditional arguments and keywords have no effect but might be accepted for compatibility with NumPy. Returns: IndexLabel of the maximum value. Raises: ValueErrorIf the Series is empty.","["">>> s = pd.Series(data=[1, None, 4, 3, 4],\n...               index=['A', 'B', 'C', 'D', 'E'])\n>>> s\nA    1.0\nB    NaN\nC    4.0\nD    3.0\nE    4.0\ndtype: float64"", "">>> s.idxmax()\n'C'"", '>>> s.idxmax(skipna=False)\nnan']"
792,..\pandas\reference\api\pandas.core.window.rolling.Window.std.html,pandas.core.window.rolling.Window.std,"Window.std(ddof=1, numeric_only=False, **kwargs)[source]# Calculate the rolling weighted window standard deviation.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. **kwargsKeyword arguments to configure the SciPy weighted window type. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([0, 1, 5, 2, 8])', "">>> type(ser.rolling(2, win_type='gaussian'))\n<class 'pandas.core.window.rolling.Window'>"", "">>> ser.rolling(2, win_type='gaussian').std(std=3)\n0         NaN\n1    0.707107\n2    2.828427\n3    2.121320\n4    4.242641\ndtype: float64""]"
793,..\pandas\reference\api\pandas.Timestamp.fromtimestamp.html,pandas.Timestamp.fromtimestamp,"classmethod Timestamp.fromtimestamp(ts)# Transform timestamp[, tz] to tz’s local time from POSIX timestamp.",No parameters found,"["">>> pd.Timestamp.fromtimestamp(1584199972)  \nTimestamp('2020-03-14 15:32:52')""]"
794,..\pandas\reference\api\pandas.DatetimeIndex.month_name.html,pandas.DatetimeIndex.month_name,"DatetimeIndex.month_name(*args, **kwargs)[source]# Return the month names with specified locale.","Parameters: localestr, optionalLocale determining the language in which to return the month name. Default is English locale ('en_US.utf8'). Use the command locale -a on your terminal on Unix systems to find your locale language code. Returns: Series or IndexSeries or Index of month names.","["">>> s = pd.Series(pd.date_range(start='2018-01', freq='ME', periods=3))\n>>> s\n0   2018-01-31\n1   2018-02-28\n2   2018-03-31\ndtype: datetime64[ns]\n>>> s.dt.month_name()\n0     January\n1    February\n2       March\ndtype: object"", "">>> idx = pd.date_range(start='2018-01', freq='ME', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n              dtype='datetime64[ns]', freq='ME')\n>>> idx.month_name()\nIndex(['January', 'February', 'March'], dtype='object')"", "">>> idx = pd.date_range(start='2018-01', freq='ME', periods=3)\n>>> idx\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31'],\n              dtype='datetime64[ns]', freq='ME')\n>>> idx.month_name(locale='pt_BR.utf8')  \nIndex(['Janeiro', 'Fevereiro', 'Março'], dtype='object')""]"
795,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_end.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_end,CustomBusinessMonthBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
796,..\pandas\reference\api\pandas.arrays.IntervalArray.set_closed.html,pandas.arrays.IntervalArray.set_closed,IntervalArray.set_closed(closed)[source]# Return an identical IntervalArray closed on the specified side.,"Parameters: closed{‘left’, ‘right’, ‘both’, ‘neither’}Whether the intervals are closed on the left-side, right-side, both or neither. Returns: IntervalArray","["">>> index = pd.arrays.IntervalArray.from_breaks(range(4))\n>>> index\n<IntervalArray>\n[(0, 1], (1, 2], (2, 3]]\nLength: 3, dtype: interval[int64, right]\n>>> index.set_closed('both')\n<IntervalArray>\n[[0, 1], [1, 2], [2, 3]]\nLength: 3, dtype: interval[int64, both]""]"
797,..\pandas\reference\api\pandas.MultiIndex.codes.html,pandas.MultiIndex.codes,property MultiIndex.codes[source]#,No parameters found,[]
798,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.copy.html,pandas.tseries.offsets.QuarterBegin.copy,QuarterBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
799,..\pandas\reference\api\pandas.Series.idxmin.html,pandas.Series.idxmin,"Series.idxmin(axis=0, skipna=True, *args, **kwargs)[source]# Return the row label of the minimum value. If multiple values equal the minimum, the first row label with that value is returned. Notes This method is the Series version of ndarray.argmin. This method returns the label of the minimum, while ndarray.argmin returns the position. To get the position, use series.values.argmin().","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values. If the entire Series is NA, the result will be NA. *args, **kwargsAdditional arguments and keywords have no effect but might be accepted for compatibility with NumPy. Returns: IndexLabel of the minimum value. Raises: ValueErrorIf the Series is empty.","["">>> s = pd.Series(data=[1, None, 4, 1],\n...               index=['A', 'B', 'C', 'D'])\n>>> s\nA    1.0\nB    NaN\nC    4.0\nD    1.0\ndtype: float64"", "">>> s.idxmin()\n'A'"", '>>> s.idxmin(skipna=False)\nnan']"
800,..\pandas\reference\api\pandas.core.window.rolling.Window.sum.html,pandas.core.window.rolling.Window.sum,"Window.sum(numeric_only=False, **kwargs)[source]# Calculate the rolling weighted window sum.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. **kwargsKeyword arguments to configure the SciPy weighted window type. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([0, 1, 5, 2, 8])', "">>> type(ser.rolling(2, win_type='gaussian'))\n<class 'pandas.core.window.rolling.Window'>"", "">>> ser.rolling(2, win_type='gaussian').sum(std=3)\n0         NaN\n1    0.986207\n2    5.917243\n3    6.903450\n4    9.862071\ndtype: float64""]"
801,..\pandas\reference\api\pandas.Timestamp.hour.html,pandas.Timestamp.hour,Timestamp.hour#,No parameters found,[]
802,..\pandas\reference\api\pandas.DatetimeIndex.nanosecond.html,pandas.DatetimeIndex.nanosecond,property DatetimeIndex.nanosecond[source]# The nanoseconds of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""ns"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00.000000000\n1   2000-01-01 00:00:00.000000001\n2   2000-01-01 00:00:00.000000002\ndtype: datetime64[ns]\n>>> datetime_series.dt.nanosecond\n0       0\n1       1\n2       2\ndtype: int32']"
803,..\pandas\reference\api\pandas.MultiIndex.copy.html,pandas.MultiIndex.copy,"MultiIndex.copy(names=None, deep=False, name=None)[source]# Make a copy of this object. Names, dtype, levels and codes can be passed and will be set on new copy. Notes In most cases, there should be no functional difference from using deep, but if deep is passed it will attempt to deepcopy. This could be potentially expensive on large MultiIndex objects.","Parameters: namessequence, optional deepbool, default False nameLabelKept for compatibility with 1-dimensional Index. Should not be used. Returns: MultiIndex","["">>> mi = pd.MultiIndex.from_arrays([['a'], ['b'], ['c']])\n>>> mi\nMultiIndex([('a', 'b', 'c')],\n           )\n>>> mi.copy()\nMultiIndex([('a', 'b', 'c')],\n           )""]"
804,..\pandas\reference\api\pandas.core.window.rolling.Window.var.html,pandas.core.window.rolling.Window.var,"Window.var(ddof=1, numeric_only=False, **kwargs)[source]# Calculate the rolling weighted window variance.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Added in version 1.5.0. **kwargsKeyword arguments to configure the SciPy weighted window type. Returns: Series or DataFrameReturn type is the same as the original object with np.float64 dtype.","['>>> ser = pd.Series([0, 1, 5, 2, 8])', "">>> type(ser.rolling(2, win_type='gaussian'))\n<class 'pandas.core.window.rolling.Window'>"", "">>> ser.rolling(2, win_type='gaussian').var(std=3)\n0     NaN\n1     0.5\n2     8.0\n3     4.5\n4    18.0\ndtype: float64""]"
805,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.freqstr.html,pandas.tseries.offsets.QuarterBegin.freqstr,QuarterBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
806,..\pandas\reference\api\pandas.Timestamp.html,pandas.Timestamp,"class pandas.Timestamp(ts_input=<object object>, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, tzinfo=None, *, nanosecond=None, tz=None, unit=None, fold=None)# Pandas replacement for python datetime.datetime object. Timestamp is the pandas equivalent of python’s Datetime and is interchangeable with it in most cases. It’s the type used for the entries that make up a DatetimeIndex, and other timeseries oriented data structures in pandas. Notes There are essentially three calling conventions for the constructor. The primary form accepts four parameters. They can be passed by position or keyword. The other two forms mimic the parameters from datetime.datetime. They can be passed by either position or keyword, but not both mixed together. Examples Using the primary calling convention: This converts a datetime-like string This converts a float representing a Unix epoch in units of seconds This converts an int representing a Unix-epoch in units of seconds and for a particular timezone Using the other two forms that mimic the API for datetime.datetime: Attributes asm8 Return numpy datetime64 format in nanoseconds. day day_of_week Return day of the week. day_of_year Return the day of the year. dayofweek Return day of the week. dayofyear Return the day of the year. days_in_month Return the number of days in the month. daysinmonth Return the number of days in the month. fold hour is_leap_year Return True if year is a leap year. is_month_end Check if the date is the last day of the month. is_month_start Check if the date is the first day of the month. is_quarter_end Check if date is last day of the quarter. is_quarter_start Check if the date is the first day of the quarter. is_year_end Return True if date is last day of the year. is_year_start Return True if date is first day of the year. max microsecond min minute month nanosecond quarter Return the quarter of the year. resolution second tz Alias for tzinfo. tzinfo unit The abbreviation associated with self._creso. value week Return the week number of the year. weekofyear Return the week number of the year. year","Parameters: ts_inputdatetime-like, str, int, floatValue to be converted to Timestamp. year, month, dayint hour, minute, second, microsecondint, optional, default 0 tzinfodatetime.tzinfo, optional, default None nanosecondint, optional, default 0 tzstr, pytz.timezone, dateutil.tz.tzfile or NoneTime zone for time which Timestamp will have. unitstrUnit used for conversion if ts_input is of type int or float. The valid values are ‘D’, ‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, and ‘ns’. For example, ‘s’ means seconds and ‘ms’ means milliseconds. For float inputs, the result will be stored in nanoseconds, and the unit attribute will be set as 'ns'. fold{0, 1}, default None, keyword-onlyDue to daylight saving time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds  to the first (0) or the second time (1) the wall clock hits the ambiguous time.","["">>> pd.Timestamp('2017-01-01T12')\nTimestamp('2017-01-01 12:00:00')"", "">>> pd.Timestamp(1513393355.5, unit='s')\nTimestamp('2017-12-16 03:02:35.500000')"", "">>> pd.Timestamp(1513393355, unit='s', tz='US/Pacific')\nTimestamp('2017-12-15 19:02:35-0800', tz='US/Pacific')"", "">>> pd.Timestamp(2017, 1, 1, 12)\nTimestamp('2017-01-01 12:00:00')"", "">>> pd.Timestamp(year=2017, month=1, day=1, hour=12)\nTimestamp('2017-01-01 12:00:00')""]"
807,..\pandas\reference\api\pandas.Series.iloc.html,pandas.Series.iloc,"property Series.iloc[source]# Purely integer-location based indexing for selection by position. Deprecated since version 2.2.0: Returning a tuple from a callable is deprecated. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. Allowed inputs are: An integer, e.g. 5. A list or array of integers, e.g. [4, 3, 0]. A slice object with ints, e.g. 1:7. A boolean array. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don’t have a reference to the calling object, but would like to base your selection on some value. A tuple of row and column indexes. The tuple elements consist of one of the above inputs, e.g. (0, 1). .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics). See more at Selection by Position.",No parameters found,"["">>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000}]\n>>> df = pd.DataFrame(mydict)\n>>> df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000"", "">>> type(df.iloc[0])\n<class 'pandas.core.series.Series'>\n>>> df.iloc[0]\na    1\nb    2\nc    3\nd    4\nName: 0, dtype: int64"", "">>> df.iloc[[0]]\n   a  b  c  d\n0  1  2  3  4\n>>> type(df.iloc[[0]])\n<class 'pandas.core.frame.DataFrame'>"", '>>> df.iloc[[0, 1]]\n     a    b    c    d\n0    1    2    3    4\n1  100  200  300  400', '>>> df.iloc[:3]\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000', '>>> df.iloc[[True, False, True]]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000', '>>> df.iloc[lambda x: x.index % 2 == 0]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000', '>>> df.iloc[0, 1]\n2', '>>> df.iloc[[0, 2], [1, 3]]\n      b     d\n0     2     4\n2  2000  4000', '>>> df.iloc[1:3, 0:3]\n      a     b     c\n1   100   200   300\n2  1000  2000  3000', '>>> df.iloc[:, [True, False, True, False]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000', '>>> df.iloc[:, lambda df: [0, 2]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000']"
808,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_start.html,pandas.tseries.offsets.CustomBusinessMonthBegin.is_year_start,CustomBusinessMonthBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
809,..\pandas\reference\api\pandas.DatetimeIndex.normalize.html,pandas.DatetimeIndex.normalize,"DatetimeIndex.normalize(*args, **kwargs)[source]# Convert times to midnight. The time component of the date-time is converted to midnight i.e. 00:00:00. This is useful in cases, when the time does not matter. Length is unaltered. The timezones are unaffected. This method is available on Series with datetime values under the .dt accessor, and directly on Datetime Array/Index.","Returns: DatetimeArray, DatetimeIndex or SeriesThe same type as the original data. Series will have the same name and index. DatetimeIndex will have the same name.","["">>> idx = pd.date_range(start='2014-08-01 10:00', freq='h',\n...                     periods=3, tz='Asia/Calcutta')\n>>> idx\nDatetimeIndex(['2014-08-01 10:00:00+05:30',\n               '2014-08-01 11:00:00+05:30',\n               '2014-08-01 12:00:00+05:30'],\n                dtype='datetime64[ns, Asia/Calcutta]', freq='h')\n>>> idx.normalize()\nDatetimeIndex(['2014-08-01 00:00:00+05:30',\n               '2014-08-01 00:00:00+05:30',\n               '2014-08-01 00:00:00+05:30'],\n               dtype='datetime64[ns, Asia/Calcutta]', freq=None)""]"
810,..\pandas\reference\api\pandas.MultiIndex.drop.html,pandas.MultiIndex.drop,"MultiIndex.drop(codes, level=None, errors='raise')[source]# Make a new pandas.MultiIndex with the passed list of codes deleted.","Parameters: codesarray-likeMust be a list of tuples when level is not specified. levelint or level name, default None errorsstr, default ‘raise’ Returns: MultiIndex","['>>> idx = pd.MultiIndex.from_product([(0, 1, 2), (\'green\', \'purple\')],\n...                                  names=[""number"", ""color""])\n>>> idx\nMultiIndex([(0,  \'green\'),\n            (0, \'purple\'),\n            (1,  \'green\'),\n            (1, \'purple\'),\n            (2,  \'green\'),\n            (2, \'purple\')],\n           names=[\'number\', \'color\'])\n>>> idx.drop([(1, \'green\'), (2, \'purple\')])\nMultiIndex([(0,  \'green\'),\n            (0, \'purple\'),\n            (1, \'purple\'),\n            (2,  \'green\')],\n           names=[\'number\', \'color\'])', "">>> idx.drop('green', level='color')\nMultiIndex([(0, 'purple'),\n            (1, 'purple'),\n            (2, 'purple')],\n           names=['number', 'color'])"", "">>> idx.drop([1, 2], level=0)\nMultiIndex([(0,  'green'),\n            (0, 'purple')],\n           names=['number', 'color'])""]"
811,..\pandas\reference\api\pandas.arrays.IntervalArray.to_tuples.html,pandas.arrays.IntervalArray.to_tuples,"IntervalArray.to_tuples(na_tuple=True)[source]# Return an ndarray (if self is IntervalArray) or Index (if self is IntervalIndex) of tuples of the form (left, right).","Parameters: na_tuplebool, default TrueIf True, return NA as a tuple (nan, nan). If False, just return NA as nan. Returns: tuples: ndarray (if self is IntervalArray) or Index (if self is IntervalIndex)","['>>> idx = pd.arrays.IntervalArray.from_tuples([(0, 1), (1, 2)])\n>>> idx\n<IntervalArray>\n[(0, 1], (1, 2]]\nLength: 2, dtype: interval[int64, right]\n>>> idx.to_tuples()\narray([(0, 1), (1, 2)], dtype=object)', "">>> idx = pd.interval_range(start=0, end=2)\n>>> idx\nIntervalIndex([(0, 1], (1, 2]], dtype='interval[int64, right]')\n>>> idx.to_tuples()\nIndex([(0, 1), (1, 2)], dtype='object')""]"
812,..\pandas\reference\api\pandas.crosstab.html,pandas.crosstab,"pandas.crosstab(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins=False, margins_name='All', dropna=True, normalize=False)[source]# Compute a simple cross tabulation of two (or more) factors. By default, computes a frequency table of the factors unless an array of values and an aggregation function are passed. Notes Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified. Any input passed containing Categorical data will have all of its categories included in the cross-tabulation, even if the actual data does not contain any instances of a particular category. In the event that there aren’t overlapping indexes an empty DataFrame will be returned. Reference the user guide for more examples.","Parameters: indexarray-like, Series, or list of arrays/SeriesValues to group by in the rows. columnsarray-like, Series, or list of arrays/SeriesValues to group by in the columns. valuesarray-like, optionalArray of values to aggregate according to the factors. Requires aggfunc be specified. rownamessequence, default NoneIf passed, must match number of row arrays passed. colnamessequence, default NoneIf passed, must match number of column arrays passed. aggfuncfunction, optionalIf specified, requires values be specified as well. marginsbool, default FalseAdd row/column margins (subtotals). margins_namestr, default ‘All’Name of the row/column that will contain the totals when margins is True. dropnabool, default TrueDo not include columns whose entries are all NaN. normalizebool, {‘all’, ‘index’, ‘columns’}, or {0,1}, default FalseNormalize by dividing all values by the sum of values. If passed ‘all’ or True, will normalize over all values. If passed ‘index’ will normalize over each row. If passed ‘columns’ will normalize over each column. If margins is True, will also normalize margin values. Returns: DataFrameCross tabulation of the data.","['>>> a = np.array([""foo"", ""foo"", ""foo"", ""foo"", ""bar"", ""bar"",\n...               ""bar"", ""bar"", ""foo"", ""foo"", ""foo""], dtype=object)\n>>> b = np.array([""one"", ""one"", ""one"", ""two"", ""one"", ""one"",\n...               ""one"", ""two"", ""two"", ""two"", ""one""], dtype=object)\n>>> c = np.array([""dull"", ""dull"", ""shiny"", ""dull"", ""dull"", ""shiny"",\n...               ""shiny"", ""dull"", ""shiny"", ""shiny"", ""shiny""],\n...              dtype=object)\n>>> pd.crosstab(a, [b, c], rownames=[\'a\'], colnames=[\'b\', \'c\'])\nb   one        two\nc   dull shiny dull shiny\na\nbar    1     2    1     0\nfoo    2     2    1     2', "">>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n>>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n>>> pd.crosstab(foo, bar)\ncol_0  d  e\nrow_0\na      1  0\nb      0  1\n>>> pd.crosstab(foo, bar, dropna=False)\ncol_0  d  e  f\nrow_0\na      1  0  0\nb      0  1  0\nc      0  0  0""]"
813,..\pandas\reference\api\pandas.arrays.NumpyExtensionArray.html,pandas.arrays.NumpyExtensionArray,"class pandas.arrays.NumpyExtensionArray(values, copy=False)[source]# A pandas ExtensionArray for NumPy data. This is mostly for internal compatibility, and is not especially useful on its own. Attributes None Methods None","Parameters: valuesndarrayThe NumPy ndarray to wrap. Must be 1-dimensional. copybool, default FalseWhether to copy values.","['>>> pd.arrays.NumpyExtensionArray(np.array([0, 1, 2, 3]))\n<NumpyExtensionArray>\n[0, 1, 2, 3]\nLength: 4, dtype: int64']"
814,..\pandas\reference\api\pandas.DatetimeIndex.quarter.html,pandas.DatetimeIndex.quarter,property DatetimeIndex.quarter[source]# The quarter of the date.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""4/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-04-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.quarter\n0    1\n1    2\ndtype: int32', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.quarter\nIndex([1, 1], dtype=\'int32\')']"
815,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.kwds.html,pandas.tseries.offsets.CustomBusinessMonthBegin.kwds,CustomBusinessMonthBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
816,..\pandas\reference\api\pandas.Timestamp.isocalendar.html,pandas.Timestamp.isocalendar,"Timestamp.isocalendar()# Return a named tuple containing ISO year, week number, and weekday.",No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00')\n>>> ts\nTimestamp('2023-01-01 10:00:00')\n>>> ts.isocalendar()\ndatetime.IsoCalendarDate(year=2022, week=52, weekday=7)""]"
817,..\pandas\reference\api\pandas.MultiIndex.droplevel.html,pandas.MultiIndex.droplevel,"MultiIndex.droplevel(level=0)[source]# Return index with requested level(s) removed. If resulting index has only 1 level left, the result will be of Index type, not MultiIndex. The original index is not modified inplace.","Parameters: levelint, str, or list-like, default 0If a string is given, must be the name of a level If list-like, elements must be names or indexes of levels. Returns: Index or MultiIndex","["">>> mi = pd.MultiIndex.from_arrays(\n... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n>>> mi\nMultiIndex([(1, 3, 5),\n            (2, 4, 6)],\n           names=['x', 'y', 'z'])"", "">>> mi.droplevel()\nMultiIndex([(3, 5),\n            (4, 6)],\n           names=['y', 'z'])"", "">>> mi.droplevel(2)\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])"", "">>> mi.droplevel('z')\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])"", "">>> mi.droplevel(['x', 'y'])\nIndex([5, 6], dtype='int64', name='z')""]"
818,..\pandas\reference\api\pandas.Series.index.html,pandas.Series.index,"Series.index# The index (axis labels) of the Series. The index of a Series is used to label and identify each element of the underlying data. The index can be thought of as an immutable ordered set (technically a multi-set, as it may contain duplicate labels), and is used to index and align data in pandas. Notes For more information on pandas indexing, see the indexing user guide.",Returns: IndexThe index labels of the Series.,"["">>> cities = ['Kolkata', 'Chicago', 'Toronto', 'Lisbon']\n>>> populations = [14.85, 2.71, 2.93, 0.51]\n>>> city_series = pd.Series(populations, index=cities)\n>>> city_series.index\nIndex(['Kolkata', 'Chicago', 'Toronto', 'Lisbon'], dtype='object')"", "">>> city_series.index = ['KOL', 'CHI', 'TOR', 'LIS']\n>>> city_series.index\nIndex(['KOL', 'CHI', 'TOR', 'LIS'], dtype='object')""]"
819,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.html,pandas.tseries.offsets.QuarterBegin,"class pandas.tseries.offsets.QuarterBegin# DateOffset increments between Quarter start dates. startingMonth = 1 corresponds to dates like 1/01/2007, 4/01/2007, … startingMonth = 2 corresponds to dates like 2/01/2007, 5/01/2007, … startingMonth = 3 corresponds to dates like 3/01/2007, 6/01/2007, … Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code startingMonth","Parameters: nint, default 1The number of quarters represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. startingMonthint, default 3A specific integer for the month of the year from which we start quarters.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.QuarterBegin()\nTimestamp('2022-03-01 00:00:00')""]"
820,..\pandas\reference\api\pandas.cut.html,pandas.cut,"pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise', ordered=True)[source]# Bin values into discrete intervals. Use cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins. Notes Any NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object. Reference the user guide for more examples.","Parameters: xarray-likeThe input array to be binned. Must be 1-dimensional. binsint, sequence of scalars, or IntervalIndexThe criteria to bin by. int : Defines the number of equal-width bins in the range of x. The range of x is extended by .1% on each side to include the minimum and maximum values of x. sequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of x is done. IntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for bins must be non-overlapping. rightbool, default TrueIndicates whether bins includes the rightmost edge or not. If right == True (the default), then the bins [1, 2, 3, 4] indicate (1,2], (2,3], (3,4]. This argument is ignored when bins is an IntervalIndex. labelsarray or False, default NoneSpecifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when bins is an IntervalIndex. If True, raises an error. When ordered=False, labels must be provided. retbinsbool, default FalseWhether to return the bins or not. Useful when bins is provided as a scalar. precisionint, default 3The precision at which to store and display the bins labels. include_lowestbool, default FalseWhether the first interval should be left-inclusive or not. duplicates{default ‘raise’, ‘drop’}, optionalIf bin edges are not unique, raise ValueError or drop non-uniques. orderedbool, default TrueWhether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided). Returns: outCategorical, Series, or ndarrayAn array-like object representing the respective bin for each value of x. The type depends on the value of labels. None (default) : returns a Series for Series x or a Categorical for all other inputs. The values stored within are Interval dtype. sequence of scalars : returns a Series for Series x or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is. False : returns an ndarray of integers. binsnumpy.ndarray or IntervalIndex.The computed or specified bins. Only returned when retbins=True. For scalar or sequence bins, this is an ndarray with the computed bins. If set duplicates=drop, bins will drop non-unique bin. For an IntervalIndex bins, this is equal to bins.","['>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\n... \n[(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\nCategories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...', '>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\n... \n([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\nCategories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...\narray([0.994, 3.   , 5.   , 7.   ]))', '>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),\n...        3, labels=[""bad"", ""medium"", ""good""])\n[\'bad\', \'good\', \'medium\', \'medium\', \'good\', \'bad\']\nCategories (3, object): [\'bad\' < \'medium\' < \'good\']', '>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3,\n...        labels=[""B"", ""A"", ""B""], ordered=False)\n[\'B\', \'B\', \'A\', \'A\', \'B\', \'B\']\nCategories (2, object): [\'A\', \'B\']', '>>> pd.cut([0, 1, 1, 2], bins=4, labels=False)\narray([0, 1, 1, 3])', "">>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\n...               index=['a', 'b', 'c', 'd', 'e'])\n>>> pd.cut(s, 3)\n... \na    (1.992, 4.667]\nb    (1.992, 4.667]\nc    (4.667, 7.333]\nd     (7.333, 10.0]\ne     (7.333, 10.0]\ndtype: category\nCategories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ..."", "">>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\n...               index=['a', 'b', 'c', 'd', 'e'])\n>>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)\n... \n(a    1.0\n b    2.0\n c    3.0\n d    4.0\n e    NaN\n dtype: float64,\n array([ 0,  2,  4,  6,  8, 10]))"", "">>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,\n...        right=False, duplicates='drop')\n... \n(a    1.0\n b    2.0\n c    3.0\n d    3.0\n e    NaN\n dtype: float64,\n array([ 0,  2,  4,  6, 10]))"", '>>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\n>>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)\n[NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]]\nCategories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]]']"
821,..\pandas\reference\api\pandas.DatetimeIndex.round.html,pandas.DatetimeIndex.round,"DatetimeIndex.round(*args, **kwargs)[source]# Perform round operation on the data to the specified freq. Notes If the timestamps have a timezone, rounding will take place relative to the local (“wall”) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstr or OffsetThe frequency level to round the index to. Must be a fixed frequency like ‘S’ (second) not ‘ME’ (month end). See frequency aliases for a list of possible freq values. ambiguous‘infer’, bool-ndarray, ‘NaT’, default ‘raise’Only relevant for DatetimeIndex: ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward’, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: DatetimeIndex, TimedeltaIndex, or SeriesIndex of the same type for a DatetimeIndex or TimedeltaIndex, or a Series with the same index for a Series. Raises: ValueError if the freq cannot be converted.","["">>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')\n>>> rng\nDatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:01:00'],\n              dtype='datetime64[ns]', freq='min')\n>>> rng.round('h')\nDatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',\n               '2018-01-01 12:00:00'],\n              dtype='datetime64[ns]', freq=None)"", '>>> pd.Series(rng).dt.round(""h"")\n0   2018-01-01 12:00:00\n1   2018-01-01 12:00:00\n2   2018-01-01 12:00:00\ndtype: datetime64[ns]', '>>> rng_tz = pd.DatetimeIndex([""2021-10-31 03:30:00""], tz=""Europe/Amsterdam"")', '>>> rng_tz.floor(""2h"", ambiguous=False)\nDatetimeIndex([\'2021-10-31 02:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)', '>>> rng_tz.floor(""2h"", ambiguous=True)\nDatetimeIndex([\'2021-10-31 02:00:00+02:00\'],\n              dtype=\'datetime64[ns, Europe/Amsterdam]\', freq=None)']"
822,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.m_offset.html,pandas.tseries.offsets.CustomBusinessMonthBegin.m_offset,CustomBusinessMonthBegin.m_offset#,No parameters found,[]
823,..\pandas\reference\api\pandas.Timestamp.isoformat.html,pandas.Timestamp.isoformat,"Timestamp.isoformat(sep='T', timespec='auto')# Return the time formatted according to ISO 8601. The full format looks like ‘YYYY-MM-DD HH:MM:SS.mmmmmmnnn’. By default, the fractional part is omitted if self.microsecond == 0 and self.nanosecond == 0. If self.tzinfo is not None, the UTC offset is also attached, giving giving a full format of ‘YYYY-MM-DD HH:MM:SS.mmmmmmnnn+HH:MM’.","Parameters: sepstr, default ‘T’String used as the separator between the date and time. timespecstr, default ‘auto’Specifies the number of additional terms of the time to include. The valid values are ‘auto’, ‘hours’, ‘minutes’, ‘seconds’, ‘milliseconds’, ‘microseconds’, and ‘nanoseconds’. Returns: str","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.isoformat()\n'2020-03-14T15:32:52.192548651'\n>>> ts.isoformat(timespec='microseconds')\n'2020-03-14T15:32:52.192548'""]"
824,..\pandas\reference\api\pandas.MultiIndex.dtypes.html,pandas.MultiIndex.dtypes,MultiIndex.dtypes[source]# Return the dtypes as a Series for the underlying MultiIndex.,No parameters found,"["">>> idx = pd.MultiIndex.from_product([(0, 1, 2), ('green', 'purple')],\n...                                  names=['number', 'color'])\n>>> idx\nMultiIndex([(0,  'green'),\n            (0, 'purple'),\n            (1,  'green'),\n            (1, 'purple'),\n            (2,  'green'),\n            (2, 'purple')],\n           names=['number', 'color'])\n>>> idx.dtypes\nnumber     int64\ncolor     object\ndtype: object""]"
825,..\pandas\reference\api\pandas.arrays.PeriodArray.html,pandas.arrays.PeriodArray,"class pandas.arrays.PeriodArray(values, dtype=None, freq=None, copy=False)[source]# Pandas ExtensionArray for storing Period data. Users should use array() to create new instances. Attributes None Methods None Notes There are two components to a PeriodArray ordinals : integer ndarray freq : pd.tseries.offsets.Offset The values are physically stored as a 1-D ndarray of integers. These are called “ordinals” and represent some kind of offset from a base. The freq indicates the span covered by each element of the array. All elements in the PeriodArray have the same freq.","Parameters: valuesUnion[PeriodArray, Series[period], ndarray[int], PeriodIndex]The data to store. These should be arrays that can be directly converted to ordinals without inference or copy (PeriodArray, ndarray[int64]), or a box around such an array (Series[period], PeriodIndex). dtypePeriodDtype, optionalA PeriodDtype instance from which to extract a freq. If both freq and dtype are specified, then the frequencies must match. freqstr or DateOffsetThe freq to use for the array. Mostly applicable when values is an ndarray of integers, when freq is required. When values is a PeriodArray (or box around), it’s checked that values.freq matches freq. copybool, default FalseWhether to copy the ordinals before storing.","["">>> pd.arrays.PeriodArray(pd.PeriodIndex(['2023-01-01',\n...                                       '2023-01-02'], freq='D'))\n<PeriodArray>\n['2023-01-01', '2023-01-02']\nLength: 2, dtype: period[D]""]"
826,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_anchored.html,pandas.tseries.offsets.QuarterBegin.is_anchored,QuarterBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
827,..\pandas\reference\api\pandas.Series.infer_objects.html,pandas.Series.infer_objects,"Series.infer_objects(copy=None)[source]# Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.","Parameters: copybool, default TrueWhether to make a copy for non-object or non-inferable columns or Series. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: same type as input object","['>>> df = pd.DataFrame({""A"": [""a"", 1, 2, 3]})\n>>> df = df.iloc[1:]\n>>> df\n   A\n1  1\n2  2\n3  3', '>>> df.dtypes\nA    object\ndtype: object', '>>> df.infer_objects().dtypes\nA    int64\ndtype: object']"
828,..\pandas\reference\api\pandas.DatetimeIndex.second.html,pandas.DatetimeIndex.second,property DatetimeIndex.second[source]# The seconds of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""s"")\n... )\n>>> datetime_series\n0   2000-01-01 00:00:00\n1   2000-01-01 00:00:01\n2   2000-01-01 00:00:02\ndtype: datetime64[ns]\n>>> datetime_series.dt.second\n0    0\n1    1\n2    2\ndtype: int32']"
829,..\pandas\reference\api\pandas.Timestamp.isoweekday.html,pandas.Timestamp.isoweekday,Timestamp.isoweekday()# Return the day of the week represented by the date. Monday == 1 … Sunday == 7.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00')\n>>> ts\nTimestamp('2023-01-01 10:00:00')\n>>> ts.isoweekday()\n7""]"
830,..\pandas\reference\api\pandas.DataFrame.abs.html,pandas.DataFrame.abs,"DataFrame.abs()[source]# Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Notes For complex inputs, 1.2 + 1j, the absolute value is \(\sqrt{ a^2 + b^2 }\).",Returns: absSeries/DataFrame containing the absolute value of each element.,"['>>> s = pd.Series([-1.10, 2, -3.33, 4])\n>>> s.abs()\n0    1.10\n1    2.00\n2    3.33\n3    4.00\ndtype: float64', '>>> s = pd.Series([1.2 + 1j])\n>>> s.abs()\n0    1.56205\ndtype: float64', "">>> s = pd.Series([pd.Timedelta('1 days')])\n>>> s.abs()\n0   1 days\ndtype: timedelta64[ns]"", "">>> df = pd.DataFrame({\n...     'a': [4, 5, 6, 7],\n...     'b': [10, 20, 30, 40],\n...     'c': [100, 50, -30, -50]\n... })\n>>> df\n     a    b    c\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n>>> df.loc[(df.c - 43).abs().argsort()]\n     a    b    c\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50""]"
831,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.n.html,pandas.tseries.offsets.CustomBusinessMonthBegin.n,CustomBusinessMonthBegin.n#,No parameters found,[]
832,..\pandas\reference\api\pandas.MultiIndex.from_arrays.html,pandas.MultiIndex.from_arrays,"classmethod MultiIndex.from_arrays(arrays, sortorder=None, names=<no_default>)[source]# Convert arrays to MultiIndex.","Parameters: arrayslist / sequence of array-likesEach array-like gives one level’s value for each data point. len(arrays) is the number of levels. sortorderint or NoneLevel of sortedness (must be lexicographically sorted by that level). nameslist / sequence of str, optionalNames for the levels in the index. Returns: MultiIndex","["">>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n>>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\nMultiIndex([(1,  'red'),\n            (1, 'blue'),\n            (2,  'red'),\n            (2, 'blue')],\n           names=['number', 'color'])""]"
833,..\pandas\reference\api\pandas.Series.interpolate.html,pandas.Series.interpolate,"Series.interpolate(method='linear', *, axis=0, limit=None, inplace=False, limit_direction=None, limit_area=None, downcast=<no_default>, **kwargs)[source]# Fill NaN values using an interpolation method. Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex. Notes The ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’ and ‘akima’ methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation.","Parameters: methodstr, default ‘linear’Interpolation technique to use. One of: ‘linear’: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. ‘time’: Works on daily and higher resolution data to interpolate given length of interval. ‘index’, ‘values’: use the actual numerical values of the index. ‘pad’: Fill in NaNs using existing values. ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘barycentric’, ‘polynomial’: Passed to scipy.interpolate.interp1d, whereas ‘spline’ is passed to scipy.interpolate.UnivariateSpline. These methods use the numerical values of the index.  Both ‘polynomial’ and ‘spline’ require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5). Note that, slinear method in Pandas refers to the Scipy first order spline instead of Pandas first order spline. ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’, ‘akima’, ‘cubicspline’: Wrappers around the SciPy interpolation methods of similar names. See Notes. ‘from_derivatives’: Refers to scipy.interpolate.BPoly.from_derivatives. axis{{0 or ‘index’, 1 or ‘columns’, None}}, default NoneAxis to interpolate along. For Series this parameter is unused and defaults to 0. limitint, optionalMaximum number of consecutive NaNs to fill. Must be greater than 0. inplacebool, default FalseUpdate the data in place if possible. limit_direction{{‘forward’, ‘backward’, ‘both’}}, OptionalConsecutive NaNs will be filled in this direction. If limit is specified: If ‘method’ is ‘pad’ or ‘ffill’, ‘limit_direction’ must be ‘forward’. If ‘method’ is ‘backfill’ or ‘bfill’, ‘limit_direction’ must be ‘backwards’. If ‘limit’ is not specified: If ‘method’ is ‘backfill’ or ‘bfill’, the default is ‘backward’ else the default is ‘forward’ raises ValueError if limit_direction is ‘forward’ or ‘both’ andmethod is ‘backfill’ or ‘bfill’. raises ValueError if limit_direction is ‘backward’ or ‘both’ andmethod is ‘pad’ or ‘ffill’. limit_area{{None, ‘inside’, ‘outside’}}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). downcastoptional, ‘infer’ or None, defaults to NoneDowncast dtypes if possible. Deprecated since version 2.1.0. ``**kwargs``optionalKeyword arguments to pass on to the interpolating function. Returns: Series or DataFrame or NoneReturns the same object type as the caller, interpolated at some or all NaN values or None if inplace=True.","['>>> s = pd.Series([0, 1, np.nan, 3])\n>>> s\n0    0.0\n1    1.0\n2    NaN\n3    3.0\ndtype: float64\n>>> s.interpolate()\n0    0.0\n1    1.0\n2    2.0\n3    3.0\ndtype: float64', "">>> s = pd.Series([0, 2, np.nan, 8])\n>>> s.interpolate(method='polynomial', order=2)\n0    0.000000\n1    2.000000\n2    4.666667\n3    8.000000\ndtype: float64"", "">>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n...                    (np.nan, 2.0, np.nan, np.nan),\n...                    (2.0, 3.0, np.nan, 9.0),\n...                    (np.nan, 4.0, -4.0, 16.0)],\n...                   columns=list('abcd'))\n>>> df\n     a    b    c     d\n0  0.0  NaN -1.0   1.0\n1  NaN  2.0  NaN   NaN\n2  2.0  3.0  NaN   9.0\n3  NaN  4.0 -4.0  16.0\n>>> df.interpolate(method='linear', limit_direction='forward', axis=0)\n     a    b    c     d\n0  0.0  NaN -1.0   1.0\n1  1.0  2.0 -2.0   5.0\n2  2.0  3.0 -3.0   9.0\n3  2.0  4.0 -4.0  16.0"", "">>> df['d'].interpolate(method='polynomial', order=2)\n0     1.0\n1     4.0\n2     9.0\n3    16.0\nName: d, dtype: float64""]"
834,..\pandas\reference\api\pandas.DataFrame.add.html,pandas.DataFrame.add,"DataFrame.add(other, axis='columns', level=None, fill_value=None)[source]# Get Addition of dataframe and other, element-wise (binary operator add). Equivalent to dataframe + other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, radd. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
835,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.name.html,pandas.tseries.offsets.CustomBusinessMonthBegin.name,CustomBusinessMonthBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
836,..\pandas\reference\api\pandas.arrays.SparseArray.html,pandas.arrays.SparseArray,"class pandas.arrays.SparseArray(data, sparse_index=None, fill_value=None, kind='integer', dtype=None, copy=False)[source]# An ExtensionArray for storing sparse data. Attributes None Methods None","Parameters: dataarray-like or scalarA dense array of values to store in the SparseArray. This may contain fill_value. sparse_indexSparseIndex, optional fill_valuescalar, optionalElements in data that are fill_value are not stored in the SparseArray. For memory savings, this should be the most common value in data. By default, fill_value depends on the dtype of data: data.dtype na_value float np.nan int 0 bool False datetime64 pd.NaT timedelta64 pd.NaT The fill value is potentially specified in three ways. In order of precedence, these are The fill_value argument dtype.fill_value if fill_value is None and dtype is a SparseDtype data.dtype.fill_value if fill_value is None and dtype is not a SparseDtype and data is a SparseArray. kindstrCan be ‘integer’ or ‘block’, default is ‘integer’. The type of storage for sparse locations. ‘block’: Stores a block and block_length for each contiguous span of sparse values. This is best when sparse data tends to be clumped together, with large regions of fill-value values between sparse values. ‘integer’: uses an integer to store the location of each sparse value. dtypenp.dtype or SparseDtype, optionalThe dtype to use for the SparseArray. For numpy dtypes, this determines the dtype of self.sp_values. For SparseDtype, this determines self.sp_values and self.fill_value. copybool, default FalseWhether to explicitly copy the incoming data array.","['>>> from pandas.arrays import SparseArray\n>>> arr = SparseArray([0, 0, 1, 2])\n>>> arr\n[0, 0, 1, 2]\nFill: 0\nIntIndex\nIndices: array([2, 3], dtype=int32)']"
837,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_month_end.html,pandas.tseries.offsets.QuarterBegin.is_month_end,QuarterBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
838,..\pandas\reference\api\pandas.Timestamp.is_leap_year.html,pandas.Timestamp.is_leap_year,Timestamp.is_leap_year# Return True if year is a leap year.,Returns: bool,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_leap_year\nTrue']"
839,..\pandas\reference\api\pandas.DatetimeIndex.snap.html,pandas.DatetimeIndex.snap,DatetimeIndex.snap(freq='S')[source]# Snap time stamps to nearest occurring frequency.,Returns: DatetimeIndex,"["">>> idx = pd.DatetimeIndex(['2023-01-01', '2023-01-02',\n...                        '2023-02-01', '2023-02-02'])\n>>> idx\nDatetimeIndex(['2023-01-01', '2023-01-02', '2023-02-01', '2023-02-02'],\ndtype='datetime64[ns]', freq=None)\n>>> idx.snap('MS')\nDatetimeIndex(['2023-01-01', '2023-01-01', '2023-02-01', '2023-02-01'],\ndtype='datetime64[ns]', freq=None)""]"
840,..\pandas\reference\api\pandas.MultiIndex.from_frame.html,pandas.MultiIndex.from_frame,"classmethod MultiIndex.from_frame(df, sortorder=None, names=None)[source]# Make a MultiIndex from a DataFrame.","Parameters: dfDataFrameDataFrame to be converted to MultiIndex. sortorderint, optionalLevel of sortedness (must be lexicographically sorted by that level). nameslist-like, optionalIf no names are provided, use the column names, or tuple of column names if the columns is a MultiIndex. If a sequence, overwrite names with the given sequence. Returns: MultiIndexThe MultiIndex representation of the given DataFrame.","["">>> df = pd.DataFrame([['HI', 'Temp'], ['HI', 'Precip'],\n...                    ['NJ', 'Temp'], ['NJ', 'Precip']],\n...                   columns=['a', 'b'])\n>>> df\n      a       b\n0    HI    Temp\n1    HI  Precip\n2    NJ    Temp\n3    NJ  Precip"", "">>> pd.MultiIndex.from_frame(df)\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['a', 'b'])"", "">>> pd.MultiIndex.from_frame(df, names=['state', 'observation'])\nMultiIndex([('HI',   'Temp'),\n            ('HI', 'Precip'),\n            ('NJ',   'Temp'),\n            ('NJ', 'Precip')],\n           names=['state', 'observation'])""]"
841,..\pandas\reference\api\pandas.Series.isin.html,pandas.Series.isin,Series.isin(values)[source]# Whether elements in Series are contained in values. Return a boolean Series showing whether each element in the Series matches an element in the passed sequence of values exactly.,"Parameters: valuesset or list-likeThe sequence of values to test. Passing in a single string will raise a TypeError. Instead, turn a single string into a list of one element. Returns: SeriesSeries of booleans indicating if each element is in values. Raises: TypeError If values is a string","["">>> s = pd.Series(['llama', 'cow', 'llama', 'beetle', 'llama',\n...                'hippo'], name='animal')\n>>> s.isin(['cow', 'llama'])\n0     True\n1     True\n2     True\n3    False\n4     True\n5    False\nName: animal, dtype: bool"", "">>> ~s.isin(['cow', 'llama'])\n0    False\n1    False\n2    False\n3     True\n4    False\n5     True\nName: animal, dtype: bool"", "">>> s.isin(['llama'])\n0     True\n1    False\n2     True\n3    False\n4     True\n5    False\nName: animal, dtype: bool"", "">>> pd.Series([1]).isin(['1'])\n0    False\ndtype: bool\n>>> pd.Series([1.1]).isin(['1.1'])\n0    False\ndtype: bool""]"
842,..\pandas\reference\api\pandas.DataFrame.add_prefix.html,pandas.DataFrame.add_prefix,"DataFrame.add_prefix(prefix, axis=None)[source]# Prefix labels with string prefix. For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed.","Parameters: prefixstrThe string to add before each label. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneAxis to add prefix on Added in version 2.0.0. Returns: Series or DataFrameNew Series or DataFrame with updated labels.","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', "">>> s.add_prefix('item_')\nitem_0    1\nitem_1    2\nitem_2    3\nitem_3    4\ndtype: int64"", "">>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6"", "">>> df.add_prefix('col_')\n     col_A  col_B\n0       1       3\n1       2       4\n2       3       5\n3       4       6""]"
843,..\pandas\reference\api\pandas.arrays.StringArray.html,pandas.arrays.StringArray,"class pandas.arrays.StringArray(values, copy=False)[source]# Extension array for string data. Warning StringArray is considered experimental. The implementation and parts of the API may change without warning. Attributes None Methods None Notes StringArray returns a BooleanArray for comparison methods.","Parameters: valuesarray-likeThe array of data. Warning Currently, this expects an object-dtype ndarray where the elements are Python strings or nan-likes (None, np.nan, NA). This may change without warning in the future. Use pandas.array() with dtype=""string"" for a stable way of creating a StringArray from any sequence. Changed in version 1.5.0: StringArray now accepts array-likes containing nan-likes(None, np.nan) for the values parameter in addition to strings and pandas.NA copybool, default FalseWhether to copy the array of data.","['>>> pd.array([\'This is\', \'some text\', None, \'data.\'], dtype=""string"")\n<StringArray>\n[\'This is\', \'some text\', <NA>, \'data.\']\nLength: 4, dtype: string', '>>> pd.array([\'1\', 1], dtype=""object"")\n<NumpyExtensionArray>\n[\'1\', 1]\nLength: 2, dtype: object\n>>> pd.array([\'1\', 1], dtype=""string"")\n<StringArray>\n[\'1\', \'1\']\nLength: 2, dtype: string', '>>> pd.array([""a"", None, ""c""], dtype=""string"") == ""a""\n<BooleanArray>\n[True, <NA>, False]\nLength: 3, dtype: boolean']"
844,..\pandas\reference\api\pandas.Timestamp.is_month_end.html,pandas.Timestamp.is_month_end,Timestamp.is_month_end# Check if the date is the last day of the month.,Returns: boolTrue if the date is the last day of the month.,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_month_end\nFalse', '>>> ts = pd.Timestamp(2020, 12, 31)\n>>> ts.is_month_end\nTrue']"
845,..\pandas\reference\api\pandas.DataFrame.add_suffix.html,pandas.DataFrame.add_suffix,"DataFrame.add_suffix(suffix, axis=None)[source]# Suffix labels with string suffix. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.","Parameters: suffixstrThe string to add after each label. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneAxis to add suffix on Added in version 2.0.0. Returns: Series or DataFrameNew Series or DataFrame with updated labels.","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', "">>> s.add_suffix('_item')\n0_item    1\n1_item    2\n2_item    3\n3_item    4\ndtype: int64"", "">>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6"", "">>> df.add_suffix('_col')\n     A_col  B_col\n0       1       3\n1       2       4\n2       3       5\n3       4       6""]"
846,..\pandas\reference\api\pandas.Series.isna.html,pandas.Series.isna,"Series.isna()[source]# Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).",Returns: SeriesMask of bool values for each element in Series that indicates whether an element is an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool']"
847,..\pandas\reference\api\pandas.arrays.TimedeltaArray.html,pandas.arrays.TimedeltaArray,"class pandas.arrays.TimedeltaArray(values, dtype=None, freq=<no_default>, copy=False)[source]# Pandas ExtensionArray for timedelta data. Warning TimedeltaArray is currently experimental, and its API may change without warning. In particular, TimedeltaArray.dtype is expected to change to be an instance of an ExtensionDtype subclass. Attributes None Methods None","Parameters: valuesarray-likeThe timedelta data. dtypenumpy.dtypeCurrently, only numpy.dtype(""timedelta64[ns]"") is accepted. freqOffset, optional copybool, default FalseWhether to copy the underlying array of data.","["">>> pd.arrays.TimedeltaArray._from_sequence(pd.TimedeltaIndex(['1h', '2h']))\n<TimedeltaArray>\n['0 days 01:00:00', '0 days 02:00:00']\nLength: 2, dtype: timedelta64[ns]""]"
848,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_month_start.html,pandas.tseries.offsets.QuarterBegin.is_month_start,QuarterBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
849,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.nanos.html,pandas.tseries.offsets.CustomBusinessMonthBegin.nanos,CustomBusinessMonthBegin.nanos#,No parameters found,[]
850,..\pandas\reference\api\pandas.DatetimeIndex.std.html,pandas.DatetimeIndex.std,"DatetimeIndex.std(*args, **kwargs)[source]# Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using ddof.","Parameters: axisint, optionalAxis for the function to be applied on. For pandas.Series this parameter is unused and defaults to None. ddofint, default 1Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Returns: Timedelta","["">>> idx = pd.date_range('2001-01-01 00:00', periods=3)\n>>> idx\nDatetimeIndex(['2001-01-01', '2001-01-02', '2001-01-03'],\n              dtype='datetime64[ns]', freq='D')\n>>> idx.std()\nTimedelta('1 days 00:00:00')""]"
851,..\pandas\reference\api\pandas.MultiIndex.from_product.html,pandas.MultiIndex.from_product,"classmethod MultiIndex.from_product(iterables, sortorder=None, names=<no_default>)[source]# Make a MultiIndex from the cartesian product of multiple iterables.","Parameters: iterableslist / sequence of iterablesEach iterable has unique labels for each level of the index. sortorderint or NoneLevel of sortedness (must be lexicographically sorted by that level). nameslist / sequence of str, optionalNames for the levels in the index. If not explicitly provided, names will be inferred from the elements of iterables if an element has a name attribute. Returns: MultiIndex","["">>> numbers = [0, 1, 2]\n>>> colors = ['green', 'purple']\n>>> pd.MultiIndex.from_product([numbers, colors],\n...                            names=['number', 'color'])\nMultiIndex([(0,  'green'),\n            (0, 'purple'),\n            (1,  'green'),\n            (1, 'purple'),\n            (2,  'green'),\n            (2, 'purple')],\n           names=['number', 'color'])""]"
852,..\pandas\reference\api\pandas.DatetimeIndex.strftime.html,pandas.DatetimeIndex.strftime,"DatetimeIndex.strftime(date_format)[source]# Convert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc. Formats supported by the C strftime API but not by the python string format doc (such as “%R”, “%r”) are not officially supported and should be preferably replaced with their supported equivalents (such as “%H:%M”, “%I:%M:%S %p”). Note that PeriodIndex support additional directives, detailed in Period.strftime.",Parameters: date_formatstrDate format string (e.g. “%Y-%m-%d”). Returns: ndarray[object]NumPy ndarray of formatted strings.,"['>>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),\n...                     periods=3, freq=\'s\')\n>>> rng.strftime(\'%B %d, %Y, %r\')\nIndex([\'March 10, 2018, 09:00:00 AM\', \'March 10, 2018, 09:00:01 AM\',\n       \'March 10, 2018, 09:00:02 AM\'],\n      dtype=\'object\')']"
853,..\pandas\reference\api\pandas.ArrowDtype.html,pandas.ArrowDtype,"class pandas.ArrowDtype(pyarrow_dtype)[source]# An ExtensionDtype for PyArrow data types. Warning ArrowDtype is considered experimental. The implementation and parts of the API may change without warning. While most dtype arguments can accept the “string” constructor, e.g. ""int64[pyarrow]"", ArrowDtype is useful if the data type contains parameters like pyarrow.timestamp. Attributes pyarrow_dtype Methods None Returns: ArrowDtype",Parameters: pyarrow_dtypepa.DataTypeAn instance of a pyarrow.DataType.,"['>>> import pyarrow as pa\n>>> pd.ArrowDtype(pa.int64())\nint64[pyarrow]', '>>> pd.ArrowDtype(pa.timestamp(""s"", tz=""America/New_York""))\ntimestamp[s, tz=America/New_York][pyarrow]\n>>> pd.ArrowDtype(pa.list_(pa.int64()))\nlist<item: int64>[pyarrow]']"
854,..\pandas\reference\api\pandas.DataFrame.agg.html,pandas.DataFrame.agg,"DataFrame.agg(func=None, axis=0, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. axis{0 or ‘index’, 1 or ‘columns’}, default 0If 0 or ‘index’: apply function to each column. If 1 or ‘columns’: apply function to each row. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","["">>> df = pd.DataFrame([[1, 2, 3],\n...                    [4, 5, 6],\n...                    [7, 8, 9],\n...                    [np.nan, np.nan, np.nan]],\n...                   columns=['A', 'B', 'C'])"", "">>> df.agg(['sum', 'min'])\n        A     B     C\nsum  12.0  15.0  18.0\nmin   1.0   2.0   3.0"", "">>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n        A    B\nsum  12.0  NaN\nmin   1.0  2.0\nmax   NaN  8.0"", "">>> df.agg(x=('A', 'max'), y=('B', 'min'), z=('C', 'mean'))\n     A    B    C\nx  7.0  NaN  NaN\ny  NaN  2.0  NaN\nz  NaN  NaN  6.0"", '>>> df.agg(""mean"", axis=""columns"")\n0    2.0\n1    5.0\n2    8.0\n3    NaN\ndtype: float64']"
855,..\pandas\reference\api\pandas.MultiIndex.from_tuples.html,pandas.MultiIndex.from_tuples,"classmethod MultiIndex.from_tuples(tuples, sortorder=None, names=None)[source]# Convert list of tuples to MultiIndex.","Parameters: tupleslist / sequence of tuple-likesEach tuple is the index of one row/column. sortorderint or NoneLevel of sortedness (must be lexicographically sorted by that level). nameslist / sequence of str, optionalNames for the levels in the index. Returns: MultiIndex","["">>> tuples = [(1, 'red'), (1, 'blue'),\n...           (2, 'red'), (2, 'blue')]\n>>> pd.MultiIndex.from_tuples(tuples, names=('number', 'color'))\nMultiIndex([(1,  'red'),\n            (1, 'blue'),\n            (2,  'red'),\n            (2, 'blue')],\n           names=['number', 'color'])""]"
856,..\pandas\reference\api\pandas.Timestamp.is_month_start.html,pandas.Timestamp.is_month_start,Timestamp.is_month_start# Check if the date is the first day of the month.,Returns: boolTrue if the date is the first day of the month.,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_month_start\nFalse', '>>> ts = pd.Timestamp(2020, 1, 1)\n>>> ts.is_month_start\nTrue']"
857,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.normalize.html,pandas.tseries.offsets.CustomBusinessMonthBegin.normalize,CustomBusinessMonthBegin.normalize#,No parameters found,[]
858,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_on_offset.html,pandas.tseries.offsets.QuarterBegin.is_on_offset,QuarterBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
859,..\pandas\reference\api\pandas.Series.isnull.html,pandas.Series.isnull,"Series.isnull()[source]# Series.isnull is an alias for Series.isna. Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).",Returns: SeriesMask of bool values for each element in Series that indicates whether an element is an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool']"
860,..\pandas\reference\api\pandas.DatetimeIndex.time.html,pandas.DatetimeIndex.time,property DatetimeIndex.time[source]# Returns numpy array of datetime.time objects. The time part of the Timestamps.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.time\n0    10:00:00\n1    11:00:00\ndtype: object', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.time\narray([datetime.time(10, 0), datetime.time(11, 0)], dtype=object)']"
861,..\pandas\reference\api\pandas.bdate_range.html,pandas.bdate_range,"pandas.bdate_range(start=None, end=None, periods=None, freq='B', tz=None, normalize=True, name=None, weekmask=None, holidays=None, inclusive='both', **kwargs)[source]# Return a fixed frequency DatetimeIndex with business day as the default. Notes Of the four parameters: start, end, periods, and freq, exactly three must be specified.  Specifying freq is a requirement for bdate_range.  Use date_range if specifying freq is not desired. To learn more about the frequency strings, please see this link.","Parameters: startstr or datetime-like, default NoneLeft bound for generating dates. endstr or datetime-like, default NoneRight bound for generating dates. periodsint, default NoneNumber of periods to generate. freqstr, Timedelta, datetime.timedelta, or DateOffset, default ‘B’Frequency strings can have multiples, e.g. ‘5h’. The default is business daily (‘B’). tzstr or NoneTime zone name for returning localized DatetimeIndex, for example Asia/Beijing. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. namestr, default NoneName of the resulting DatetimeIndex. weekmaskstr or None, default NoneWeekmask of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed.  The default value None is equivalent to ‘Mon Tue Wed Thu Fri’. holidayslist-like or None, default NoneDates to exclude from the set of valid business days, passed to numpy.busdaycalendar, only used when custom frequency strings are passed. inclusive{“both”, “neither”, “left”, “right”}, default “both”Include boundaries; Whether to set each bound as closed or open. Added in version 1.4.0. **kwargsFor compatibility. Has no effect on the result. Returns: DatetimeIndex","["">>> pd.bdate_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n           '2018-01-05', '2018-01-08'],\n          dtype='datetime64[ns]', freq='B')""]"
862,..\pandas\reference\api\pandas.DataFrame.aggregate.html,pandas.DataFrame.aggregate,"DataFrame.aggregate(func=None, axis=0, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. axis{0 or ‘index’, 1 or ‘columns’}, default 0If 0 or ‘index’: apply function to each column. If 1 or ‘columns’: apply function to each row. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","["">>> df = pd.DataFrame([[1, 2, 3],\n...                    [4, 5, 6],\n...                    [7, 8, 9],\n...                    [np.nan, np.nan, np.nan]],\n...                   columns=['A', 'B', 'C'])"", "">>> df.agg(['sum', 'min'])\n        A     B     C\nsum  12.0  15.0  18.0\nmin   1.0   2.0   3.0"", "">>> df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n        A    B\nsum  12.0  NaN\nmin   1.0  2.0\nmax   NaN  8.0"", "">>> df.agg(x=('A', 'max'), y=('B', 'min'), z=('C', 'mean'))\n     A    B    C\nx  7.0  NaN  NaN\ny  NaN  2.0  NaN\nz  NaN  NaN  6.0"", '>>> df.agg(""mean"", axis=""columns"")\n0    2.0\n1    5.0\n2    8.0\n3    NaN\ndtype: float64']"
863,..\pandas\reference\api\pandas.Timestamp.is_quarter_end.html,pandas.Timestamp.is_quarter_end,Timestamp.is_quarter_end# Check if date is last day of the quarter.,Returns: boolTrue if date is last day of the quarter.,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_quarter_end\nFalse', '>>> ts = pd.Timestamp(2020, 3, 31)\n>>> ts.is_quarter_end\nTrue']"
864,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.rule_code.html,pandas.tseries.offsets.CustomBusinessMonthBegin.rule_code,CustomBusinessMonthBegin.rule_code#,No parameters found,[]
865,..\pandas\reference\api\pandas.Series.is_monotonic_decreasing.html,pandas.Series.is_monotonic_decreasing,property Series.is_monotonic_decreasing[source]# Return boolean if values in the object are monotonically decreasing.,Returns: bool,"['>>> s = pd.Series([3, 2, 2, 1])\n>>> s.is_monotonic_decreasing\nTrue', '>>> s = pd.Series([1, 2, 3])\n>>> s.is_monotonic_decreasing\nFalse']"
866,..\pandas\reference\api\pandas.MultiIndex.get_indexer.html,pandas.MultiIndex.get_indexer,"MultiIndex.get_indexer(target, method=None, limit=None, tolerance=None)[source]# Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index. Notes Returns -1 for unmatched values, for further explanation see the example below.","Parameters: targetIndex method{None, ‘pad’/’ffill’, ‘backfill’/’bfill’, ‘nearest’}, optional default: exact matches only. pad / ffill: find the PREVIOUS index value if no exact match. backfill / bfill: use NEXT index value if no exact match nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value. limitint, optionalMaximum number of consecutive labels in target to match for inexact matches. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: np.ndarray[np.intp]Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.","["">>> index = pd.Index(['c', 'a', 'b'])\n>>> index.get_indexer(['a', 'b', 'x'])\narray([ 1,  2, -1])""]"
867,..\pandas\reference\api\pandas.DatetimeIndex.timetz.html,pandas.DatetimeIndex.timetz,property DatetimeIndex.timetz[source]# Returns numpy array of datetime.time objects with timezones. The time part of the Timestamps.,No parameters found,"['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.timetz\n0    10:00:00+00:00\n1    11:00:00+00:00\ndtype: object', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.timetz\narray([datetime.time(10, 0, tzinfo=datetime.timezone.utc),\ndatetime.time(11, 0, tzinfo=datetime.timezone.utc)], dtype=object)']"
868,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_quarter_end.html,pandas.tseries.offsets.QuarterBegin.is_quarter_end,QuarterBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
869,..\pandas\reference\api\pandas.BooleanDtype.html,pandas.BooleanDtype,class pandas.BooleanDtype[source]# Extension dtype for boolean data. Warning BooleanDtype is considered experimental. The implementation and parts of the API may change without warning. Attributes None Methods None,No parameters found,['>>> pd.BooleanDtype()\nBooleanDtype']
870,..\pandas\reference\api\pandas.DataFrame.align.html,pandas.DataFrame.align,"DataFrame.align(other, join='outer', axis=None, level=None, copy=None, fill_value=None, method=<no_default>, limit=<no_default>, fill_axis=<no_default>, broadcast_axis=<no_default>)[source]# Align two objects on their axes with the specified join method. Join method is specified for each axis Index.","Parameters: otherDataFrame or Series join{‘outer’, ‘inner’, ‘left’, ‘right’}, default ‘outer’Type of alignment to be performed. left: use only keys from left frame, preserve key order. right: use only keys from right frame, preserve key order. outer: use union of keys from both frames, sort keys lexicographically. inner: use intersection of keys from both frames, preserve the order of the left keys. axisallowed axis of the other object, default NoneAlign on index (0), columns (1), or both (None). levelint or level name, default NoneBroadcast across a level, matching Index values on the passed MultiIndex level. copybool, default TrueAlways returns new objects. If copy=False and no reindexing is required then original objects are returned. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any “compatible” value. method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default NoneMethod to use for filling holes in reindexed Series: pad / ffill: propagate last valid observation forward to next valid. backfill / bfill: use NEXT valid observation to fill gap. Deprecated since version 2.1. limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. Deprecated since version 2.1. fill_axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrame, default 0Filling axis, method and limit. Deprecated since version 2.1. broadcast_axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrame, default NoneBroadcast values along this axis, if aligning two objects of different dimensions. Deprecated since version 2.1. Returns: tuple of (Series/DataFrame, type of other)Aligned objects.","['>>> df = pd.DataFrame(\n...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[""D"", ""B"", ""E"", ""A""], index=[1, 2]\n... )\n>>> other = pd.DataFrame(\n...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n...     columns=[""A"", ""B"", ""C"", ""D""],\n...     index=[2, 3, 4],\n... )\n>>> df\n   D  B  E  A\n1  1  2  3  4\n2  6  7  8  9\n>>> other\n    A    B    C    D\n2   10   20   30   40\n3   60   70   80   90\n4  600  700  800  900', '>>> left, right = df.align(other, join=""outer"", axis=1)\n>>> left\n   A  B   C  D  E\n1  4  2 NaN  1  3\n2  9  7 NaN  6  8\n>>> right\n    A    B    C    D   E\n2   10   20   30   40 NaN\n3   60   70   80   90 NaN\n4  600  700  800  900 NaN', '>>> left, right = df.align(other, join=""outer"", axis=0)\n>>> left\n    D    B    E    A\n1  1.0  2.0  3.0  4.0\n2  6.0  7.0  8.0  9.0\n3  NaN  NaN  NaN  NaN\n4  NaN  NaN  NaN  NaN\n>>> right\n    A      B      C      D\n1    NaN    NaN    NaN    NaN\n2   10.0   20.0   30.0   40.0\n3   60.0   70.0   80.0   90.0\n4  600.0  700.0  800.0  900.0', '>>> left, right = df.align(other, join=""outer"", axis=None)\n>>> left\n     A    B   C    D    E\n1  4.0  2.0 NaN  1.0  3.0\n2  9.0  7.0 NaN  6.0  8.0\n3  NaN  NaN NaN  NaN  NaN\n4  NaN  NaN NaN  NaN  NaN\n>>> right\n       A      B      C      D   E\n1    NaN    NaN    NaN    NaN NaN\n2   10.0   20.0   30.0   40.0 NaN\n3   60.0   70.0   80.0   90.0 NaN\n4  600.0  700.0  800.0  900.0 NaN']"
871,..\pandas\reference\api\pandas.Timestamp.is_quarter_start.html,pandas.Timestamp.is_quarter_start,Timestamp.is_quarter_start# Check if the date is the first day of the quarter.,Returns: boolTrue if date is first day of the quarter.,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_quarter_start\nFalse', '>>> ts = pd.Timestamp(2020, 4, 1)\n>>> ts.is_quarter_start\nTrue']"
872,..\pandas\reference\api\pandas.DataFrame.all.html,pandas.DataFrame.all,"DataFrame.all(axis=0, bool_only=False, skipna=True, **kwargs)[source]# Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty).","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Indicate which axis or axes should be reduced. For Series this parameter is unused and defaults to 0. 0 / ‘index’ : reduce the index, return a Series whose index is the original column labels. 1 / ‘columns’ : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series. skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargsany, default NoneAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: Series or DataFrameIf level is specified, then, DataFrame is returned; otherwise, Series is returned.","['>>> pd.Series([True, True]).all()\nTrue\n>>> pd.Series([True, False]).all()\nFalse\n>>> pd.Series([], dtype=""float64"").all()\nTrue\n>>> pd.Series([np.nan]).all()\nTrue\n>>> pd.Series([np.nan]).all(skipna=False)\nTrue', "">>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n>>> df\n   col1   col2\n0  True   True\n1  True  False"", '>>> df.all()\ncol1     True\ncol2    False\ndtype: bool', "">>> df.all(axis='columns')\n0     True\n1    False\ndtype: bool"", '>>> df.all(axis=None)\nFalse']"
873,..\pandas\reference\api\pandas.MultiIndex.get_level_values.html,pandas.MultiIndex.get_level_values,"MultiIndex.get_level_values(level)[source]# Return vector of label values for requested level. Length of returned vector is equal to the length of the index. Notes If the level contains missing values, the result may be casted to float with missing values specified as NaN. This is because the level is converted to a regular Index.","Parameters: levelint or strlevel is either the integer position of the level in the MultiIndex, or the name of the level. Returns: IndexValues is a level of this MultiIndex converted to a single Index (or subclass thereof).","["">>> mi = pd.MultiIndex.from_arrays((list('abc'), list('def')))\n>>> mi.names = ['level_1', 'level_2']"", "">>> mi.get_level_values(0)\nIndex(['a', 'b', 'c'], dtype='object', name='level_1')\n>>> mi.get_level_values('level_2')\nIndex(['d', 'e', 'f'], dtype='object', name='level_2')"", "">>> pd.MultiIndex.from_arrays([[1, None, 2], [3, 4, 5]]).dtypes\nlevel_0    int64\nlevel_1    int64\ndtype: object\n>>> pd.MultiIndex.from_arrays([[1, None, 2], [3, 4, 5]]).get_level_values(0)\nIndex([1.0, nan, 2.0], dtype='float64')""]"
874,..\pandas\reference\api\pandas.Categorical.categories.html,pandas.Categorical.categories,property Categorical.categories[source]# The categories of this categorical. Setting assigns new values to each category (effectively a rename of each individual category). The assigned value has to be a list-like object. All items must be unique and the number of items in the new categories must be the same as the number of items in the old categories.,Raises: ValueErrorIf the new categories do not validate as categories or if the number of new categories is unequal the number of old categories,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.categories\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], categories=['b', 'c', 'd'])\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.categories\nIndex(['b', 'c', 'd'], dtype='object')"", "">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.categories\nIndex(['a', 'b'], dtype='object')"", "">>> ci = pd.CategoricalIndex(['a', 'c', 'b', 'a', 'c', 'b'])\n>>> ci.categories\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> ci = pd.CategoricalIndex(['a', 'c'], categories=['c', 'b', 'a'])\n>>> ci.categories\nIndex(['c', 'b', 'a'], dtype='object')""]"
875,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_quarter_start.html,pandas.tseries.offsets.QuarterBegin.is_quarter_start,QuarterBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
876,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthBegin.weekmask.html,pandas.tseries.offsets.CustomBusinessMonthBegin.weekmask,CustomBusinessMonthBegin.weekmask#,No parameters found,[]
877,..\pandas\reference\api\pandas.DatetimeIndex.to_frame.html,pandas.DatetimeIndex.to_frame,"DatetimeIndex.to_frame(index=True, name=<no_default>)[source]# Create a DataFrame with a column containing the Index.","Parameters: indexbool, default TrueSet the index of the returned DataFrame as the original Index. nameobject, defaults to index.nameThe passed name should substitute for the index name (if it has one). Returns: DataFrameDataFrame containing the original Index data.","["">>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow"", '>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow', "">>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow""]"
878,..\pandas\reference\api\pandas.Timestamp.is_year_end.html,pandas.Timestamp.is_year_end,Timestamp.is_year_end# Return True if date is last day of the year.,Returns: bool,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_year_end\nFalse', '>>> ts = pd.Timestamp(2020, 12, 31)\n>>> ts.is_year_end\nTrue']"
879,..\pandas\reference\api\pandas.Series.is_monotonic_increasing.html,pandas.Series.is_monotonic_increasing,property Series.is_monotonic_increasing[source]# Return boolean if values in the object are monotonically increasing.,Returns: bool,"['>>> s = pd.Series([1, 2, 2])\n>>> s.is_monotonic_increasing\nTrue', '>>> s = pd.Series([3, 2, 1])\n>>> s.is_monotonic_increasing\nFalse']"
880,..\pandas\reference\api\pandas.DataFrame.any.html,pandas.DataFrame.any,"DataFrame.any(*, axis=0, bool_only=False, skipna=True, **kwargs)[source]# Return whether any element is True, potentially over an axis. Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Indicate which axis or axes should be reduced. For Series this parameter is unused and defaults to 0. 0 / ‘index’ : reduce the index, return a Series whose index is the original column labels. 1 / ‘columns’ : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series. skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargsany, default NoneAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: Series or DataFrameIf level is specified, then, DataFrame is returned; otherwise, Series is returned.","['>>> pd.Series([False, False]).any()\nFalse\n>>> pd.Series([True, False]).any()\nTrue\n>>> pd.Series([], dtype=""float64"").any()\nFalse\n>>> pd.Series([np.nan]).any()\nFalse\n>>> pd.Series([np.nan]).any(skipna=False)\nTrue', '>>> df = pd.DataFrame({""A"": [1, 2], ""B"": [0, 2], ""C"": [0, 0]})\n>>> df\n   A  B  C\n0  1  0  0\n1  2  2  0', '>>> df.any()\nA     True\nB     True\nC    False\ndtype: bool', '>>> df = pd.DataFrame({""A"": [True, False], ""B"": [1, 2]})\n>>> df\n       A  B\n0   True  1\n1  False  2', "">>> df.any(axis='columns')\n0    True\n1    True\ndtype: bool"", '>>> df = pd.DataFrame({""A"": [True, False], ""B"": [1, 0]})\n>>> df\n       A  B\n0   True  1\n1  False  0', "">>> df.any(axis='columns')\n0    True\n1    False\ndtype: bool"", '>>> df.any(axis=None)\nTrue', '>>> pd.DataFrame([]).any()\nSeries([], dtype: bool)']"
881,..\pandas\reference\api\pandas.MultiIndex.get_loc.html,pandas.MultiIndex.get_loc,"MultiIndex.get_loc(key)[source]# Get location for a label or a tuple of labels. The location is returned as an integer/slice or boolean mask. Notes The key cannot be a slice, list of same-level labels, a boolean mask, or a sequence of such. If you want to use those, use MultiIndex.get_locs() instead.","Parameters: keylabel or tuple of labels (one for each level) Returns: int, slice object or boolean maskIf the key is past the lexsort depth, the return may be a boolean mask array, otherwise it is always a slice or int.","["">>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])"", "">>> mi.get_loc('b')\nslice(1, 3, None)"", "">>> mi.get_loc(('b', 'e'))\n1""]"
882,..\pandas\reference\api\pandas.Categorical.codes.html,pandas.Categorical.codes,"property Categorical.codes[source]# The category codes of this categorical index. Codes are an array of integers which are the positions of the actual values in the categories array. There is no setter, use the other categorical methods and the normal item setter to change values in the categorical.",Returns: ndarray[int]A non-writable view of the codes array.,"["">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.codes\narray([0, 1], dtype=int8)"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'])\n>>> ci.codes\narray([0, 1, 2, 0, 1, 2], dtype=int8)"", "">>> ci = pd.CategoricalIndex(['a', 'c'], categories=['c', 'b', 'a'])\n>>> ci.codes\narray([2, 0], dtype=int8)""]"
883,..\pandas\reference\api\pandas.DatetimeIndex.to_period.html,pandas.DatetimeIndex.to_period,"DatetimeIndex.to_period(*args, **kwargs)[source]# Cast to PeriodArray/PeriodIndex at a particular frequency. Converts DatetimeArray/Index to PeriodArray/PeriodIndex.","Parameters: freqstr or Period, optionalOne of pandas’ period aliases or an Period object. Will be inferred by default. Returns: PeriodArray/PeriodIndex Raises: ValueErrorWhen converting a DatetimeArray/Index with non-regular values, so that a frequency cannot be inferred.","['>>> df = pd.DataFrame({""y"": [1, 2, 3]},\n...                   index=pd.to_datetime([""2000-03-31 00:00:00"",\n...                                         ""2000-05-31 00:00:00"",\n...                                         ""2000-08-31 00:00:00""]))\n>>> df.index.to_period(""M"")\nPeriodIndex([\'2000-03\', \'2000-05\', \'2000-08\'],\n            dtype=\'period[M]\')', '>>> idx = pd.date_range(""2017-01-01"", periods=2)\n>>> idx.to_period()\nPeriodIndex([\'2017-01-01\', \'2017-01-02\'],\n            dtype=\'period[D]\')']"
884,..\pandas\reference\api\pandas.Series.is_unique.html,pandas.Series.is_unique,property Series.is_unique[source]# Return boolean if values in the object are unique.,Returns: bool,"['>>> s = pd.Series([1, 2, 3])\n>>> s.is_unique\nTrue', '>>> s = pd.Series([1, 2, 3, 1])\n>>> s.is_unique\nFalse']"
885,..\pandas\reference\api\pandas.DataFrame.apply.html,pandas.DataFrame.apply,"DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), by_row='compat', engine='python', engine_kwargs=None, **kwargs)[source]# Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame’s index (axis=0) or the DataFrame’s columns (axis=1). By default (result_type=None), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funcfunctionFunction to apply to each column or row. axis{0 or ‘index’, 1 or ‘columns’}, default 0Axis along which the function is applied: 0 or ‘index’: apply function to each column. 1 or ‘columns’: apply function to each row. rawbool, default FalseDetermines if row or column is passed as a Series or ndarray object: False : passes each row or column as a Series to the function. True : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. result_type{‘expand’, ‘reduce’, ‘broadcast’, None}, default NoneThese only act when axis=1 (columns): ‘expand’ : list-like results will be turned into columns. ‘reduce’ : returns a Series if possible rather than expanding list-like results. This is the opposite of ‘expand’. ‘broadcast’ : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained. The default behaviour (None) depends on the return value of the applied function: list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns. argstuplePositional arguments to pass to func in addition to the array/series. by_rowFalse or “compat”, default “compat”Only has an effect when func is a listlike or dictlike of funcs and the func isn’t a string. If “compat”, will if possible first translate the func into pandas methods (e.g. Series().apply(np.sum) will be translated to Series().sum()). If that doesn’t work, will try call to apply again with by_row=True and if that fails, will call apply again with by_row=False (backward compatible). If False, the funcs will be passed the whole Series at once. Added in version 2.1.0. engine{‘python’, ‘numba’}, default ‘python’Choose between the python (default) engine or the numba engine in apply. The numba engine will attempt to JIT compile the passed function, which may result in speedups for large DataFrames. It also supports the following engine_kwargs : nopython (compile the function in nopython mode) nogil (release the GIL inside the JIT compiled function) parallel (try to apply the function in parallel over the DataFrame) Note: Due to limitations within numba/how pandas interfaces with numba, you should only use this if raw=True Note: The numba compiler only supports a subset of valid Python/numpy operations. Please read more about the supported python features and supported numpy features in numba to learn what you can or cannot use in the passed function. Added in version 2.2.0. engine_kwargsdictPass keyword arguments to the engine. This is currently only used by the numba engine, see the documentation for the engine argument for more information. **kwargsAdditional keyword arguments to pass as keywords arguments to func. Returns: Series or DataFrameResult of applying func along the given axis of the DataFrame.","["">>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n>>> df\n   A  B\n0  4  9\n1  4  9\n2  4  9"", '>>> df.apply(np.sqrt)\n     A    B\n0  2.0  3.0\n1  2.0  3.0\n2  2.0  3.0', '>>> df.apply(np.sum, axis=0)\nA    12\nB    27\ndtype: int64', '>>> df.apply(np.sum, axis=1)\n0    13\n1    13\n2    13\ndtype: int64', '>>> df.apply(lambda x: [1, 2], axis=1)\n0    [1, 2]\n1    [1, 2]\n2    [1, 2]\ndtype: object', "">>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n   0  1\n0  1  2\n1  1  2\n2  1  2"", "">>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n   foo  bar\n0    1    2\n1    1    2\n2    1    2"", "">>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n   A  B\n0  1  2\n1  1  2\n2  1  2""]"
886,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_year_end.html,pandas.tseries.offsets.QuarterBegin.is_year_end,QuarterBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
887,..\pandas\reference\api\pandas.MultiIndex.get_locs.html,pandas.MultiIndex.get_locs,MultiIndex.get_locs(seq)[source]# Get location for a sequence of labels.,"Parameters: seqlabel, slice, list, mask or a sequence of suchYou should use one of the above for each level. If a level should not be used, set it to slice(None). Returns: numpy.ndarrayNumPy array of integers suitable for passing to iloc.","["">>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])"", "">>> mi.get_locs('b')  \narray([1, 2], dtype=int64)"", "">>> mi.get_locs([slice(None), ['e', 'f']])  \narray([1, 2], dtype=int64)"", "">>> mi.get_locs([[True, False, True], slice('e', 'f')])  \narray([2], dtype=int64)""]"
888,..\pandas\reference\api\pandas.Timestamp.is_year_start.html,pandas.Timestamp.is_year_start,Timestamp.is_year_start# Return True if date is first day of the year.,Returns: bool,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.is_year_start\nFalse', '>>> ts = pd.Timestamp(2020, 1, 1)\n>>> ts.is_year_start\nTrue']"
889,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.calendar.html,pandas.tseries.offsets.CustomBusinessMonthEnd.calendar,CustomBusinessMonthEnd.calendar#,No parameters found,[]
890,..\pandas\reference\api\pandas.Categorical.dtype.html,pandas.Categorical.dtype,property Categorical.dtype[source]# The CategoricalDtype for this instance.,No parameters found,"["">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat\n['a', 'b']\nCategories (2, object): ['a' < 'b']\n>>> cat.dtype\nCategoricalDtype(categories=['a', 'b'], ordered=True, categories_dtype=object)""]"
891,..\pandas\reference\api\pandas.DatetimeIndex.to_pydatetime.html,pandas.DatetimeIndex.to_pydatetime,"DatetimeIndex.to_pydatetime(*args, **kwargs)[source]# Return an ndarray of datetime.datetime objects.",Returns: numpy.ndarray,"["">>> idx = pd.date_range('2018-02-27', periods=3)\n>>> idx.to_pydatetime()\narray([datetime.datetime(2018, 2, 27, 0, 0),\n       datetime.datetime(2018, 2, 28, 0, 0),\n       datetime.datetime(2018, 3, 1, 0, 0)], dtype=object)""]"
892,..\pandas\reference\api\pandas.Series.item.html,pandas.Series.item,Series.item()[source]# Return the first element of the underlying data as a Python scalar.,Returns: scalarThe first element of Series or Index. Raises: ValueErrorIf the data is not length = 1.,"['>>> s = pd.Series([1])\n>>> s.item()\n1', "">>> s = pd.Series([1], index=['a'])\n>>> s.index.item()\n'a'""]"
893,..\pandas\reference\api\pandas.DataFrame.applymap.html,pandas.DataFrame.applymap,"DataFrame.applymap(func, na_action=None, **kwargs)[source]# Apply a function to a Dataframe elementwise. Deprecated since version 2.1.0: DataFrame.applymap has been deprecated. Use DataFrame.map instead. This method applies a function that accepts and returns a scalar to every element of a DataFrame.","Parameters: funccallablePython function, returns a single value from a single value. na_action{None, ‘ignore’}, default NoneIf ‘ignore’, propagate NaN values, without passing them to func. **kwargsAdditional keyword arguments to pass as keywords arguments to func. Returns: DataFrameTransformed DataFrame.","['>>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n>>> df\n       0      1\n0  1.000  2.120\n1  3.356  4.567', '>>> df.map(lambda x: len(str(x)))\n   0  1\n0  3  4\n1  5  5']"
894,..\pandas\reference\api\pandas.Timestamp.max.html,pandas.Timestamp.max,Timestamp.max = Timestamp('2262-04-11 23:47:16.854775807')#,No parameters found,[]
895,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.is_year_start.html,pandas.tseries.offsets.QuarterBegin.is_year_start,QuarterBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
896,..\pandas\reference\api\pandas.Categorical.from_codes.html,pandas.Categorical.from_codes,"classmethod Categorical.from_codes(codes, categories=None, ordered=None, dtype=None, validate=True)[source]# Make a Categorical type from codes and categories or dtype. This constructor is useful if you already have codes and categories/dtype and so do not need the (computation intensive) factorization step, which is usually done on the constructor. If your data does not follow this convention, please use the normal constructor.","Parameters: codesarray-like of intAn integer array, where each integer points to a category in categories or dtype.categories, or else is -1 for NaN. categoriesindex-like, optionalThe categories for the categorical. Items need to be unique. If the categories are not given here, then they must be provided in dtype. orderedbool, optionalWhether or not this categorical is treated as an ordered categorical. If not given here or in dtype, the resulting categorical will be unordered. dtypeCategoricalDtype or “category”, optionalIf CategoricalDtype, cannot be used together with categories or ordered. validatebool, default TrueIf True, validate that the codes are valid for the dtype. If False, don’t validate that the codes are valid. Be careful about skipping validation, as invalid codes can lead to severe problems, such as segfaults. Added in version 2.1.0. Returns: Categorical","["">>> dtype = pd.CategoricalDtype(['a', 'b'], ordered=True)\n>>> pd.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)\n['a', 'b', 'a', 'b']\nCategories (2, object): ['a' < 'b']""]"
897,..\pandas\reference\api\pandas.DatetimeIndex.to_series.html,pandas.DatetimeIndex.to_series,"DatetimeIndex.to_series(index=None, name=None)[source]# Create a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.","Parameters: indexIndex, optionalIndex of resulting Series. If None, defaults to original index. namestr, optionalName of resulting Series. If None, defaults to name of original index. Returns: SeriesThe dtype will be based on the type of the Index values.","["">>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')"", '>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object', '>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object', "">>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object""]"
898,..\pandas\reference\api\pandas.Series.items.html,pandas.Series.items,"Series.items()[source]# Lazily iterate over (index, value) tuples. This method returns an iterable tuple (index, value). This is convenient if you want to create a lazy iterator.","Returns: iterableIterable of tuples containing the (index, value) pairs from a Series.","['>>> s = pd.Series([\'A\', \'B\', \'C\'])\n>>> for index, value in s.items():\n...     print(f""Index : {index}, Value : {value}"")\nIndex : 0, Value : A\nIndex : 1, Value : B\nIndex : 2, Value : C']"
899,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.copy.html,pandas.tseries.offsets.CustomBusinessMonthEnd.copy,CustomBusinessMonthEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
900,..\pandas\reference\api\pandas.MultiIndex.get_loc_level.html,pandas.MultiIndex.get_loc_level,"MultiIndex.get_loc_level(key, level=0, drop_level=True)[source]# Get location and sliced index for requested label(s)/level(s).","Parameters: keylabel or sequence of labels levelint/level name or list thereof, optional drop_levelbool, default TrueIf False, the resulting index will not drop any level. Returns: tupleA 2-tuple where the elements : Element 0: int, slice object or boolean array. Element 1: The resulting sliced multiindex/index. If the key contains all levels, this will be None.","["">>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')],\n...                                names=['A', 'B'])"", "">>> mi.get_loc_level('b')\n(slice(1, 3, None), Index(['e', 'f'], dtype='object', name='B'))"", "">>> mi.get_loc_level('e', level='B')\n(array([False,  True, False]), Index(['b'], dtype='object', name='A'))"", "">>> mi.get_loc_level(['b', 'e'])\n(1, None)""]"
901,..\pandas\reference\api\pandas.DataFrame.asfreq.html,pandas.DataFrame.asfreq,"DataFrame.asfreq(freq, method=None, how=None, normalize=False, fill_value=None)[source]# Convert time series to specified frequency. Returns the original data conformed to a new index with the specified frequency. If the index of this Series/DataFrame is a PeriodIndex, the new index is the result of transforming the original index with PeriodIndex.asfreq (so the original index will map one-to-one to the new index). Otherwise, the new index will be equivalent to pd.date_range(start, end, freq=freq) where start and end are, respectively, the first and last entries in the original index (see pandas.date_range()). The values corresponding to any timesteps in the new index which were not present in the original index will be null (NaN), unless a method for filling such unknowns is provided (see the method parameter below). The resample() method is more appropriate if an operation on each group of timesteps (such as an aggregate) is necessary to represent the data at the new frequency. Notes To learn more about the frequency strings, please see this link.","Parameters: freqDateOffset or strFrequency DateOffset or string. method{‘backfill’/’bfill’, ‘pad’/’ffill’}, default NoneMethod to use for filling holes in reindexed Series (note this does not fill NaNs that already were present): ‘pad’ / ‘ffill’: propagate last valid observation forward to next valid ‘backfill’ / ‘bfill’: use NEXT valid observation to fill. how{‘start’, ‘end’}, default endFor PeriodIndex only (see PeriodIndex.asfreq). normalizebool, default FalseWhether to reset output index to midnight. fill_valuescalar, optionalValue to use for missing values, applied during upsampling (note this does not fill NaNs that already were present). Returns: Series/DataFrameSeries/DataFrame object reindexed to the specified frequency.","["">>> index = pd.date_range('1/1/2000', periods=4, freq='min')\n>>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n>>> df = pd.DataFrame({'s': series})\n>>> df\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:01:00    NaN\n2000-01-01 00:02:00    2.0\n2000-01-01 00:03:00    3.0"", "">>> df.asfreq(freq='30s')\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    NaN\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    NaN\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    NaN\n2000-01-01 00:03:00    3.0"", "">>> df.asfreq(freq='30s', fill_value=9.0)\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    9.0\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    9.0\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    9.0\n2000-01-01 00:03:00    3.0"", "">>> df.asfreq(freq='30s', method='bfill')\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    NaN\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    2.0\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    3.0\n2000-01-01 00:03:00    3.0""]"
902,..\pandas\reference\api\pandas.Timestamp.microsecond.html,pandas.Timestamp.microsecond,Timestamp.microsecond#,No parameters found,[]
903,..\pandas\reference\api\pandas.DatetimeIndex.tz.html,pandas.DatetimeIndex.tz,property DatetimeIndex.tz[source]# Return the timezone.,"Returns: datetime.tzinfo, pytz.tzinfo.BaseTZInfo, dateutil.tz.tz.tzfile, or NoneReturns None when the array is tz-naive.","['>>> s = pd.Series([""1/1/2020 10:00:00+00:00"", ""2/1/2020 11:00:00+00:00""])\n>>> s = pd.to_datetime(s)\n>>> s\n0   2020-01-01 10:00:00+00:00\n1   2020-02-01 11:00:00+00:00\ndtype: datetime64[ns, UTC]\n>>> s.dt.tz\ndatetime.timezone.utc', '>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00"",\n...                         ""2/1/2020 11:00:00+00:00""])\n>>> idx.tz\ndatetime.timezone.utc']"
904,..\pandas\reference\api\pandas.MultiIndex.html,pandas.MultiIndex,"class pandas.MultiIndex(levels=None, codes=None, sortorder=None, names=None, dtype=None, copy=False, name=None, verify_integrity=True)[source]# A multi-level, or hierarchical, index object for pandas objects. Attributes names Names of levels in MultiIndex. levels Levels of the MultiIndex. nlevels Integer number of levels in this MultiIndex. levshape A tuple with the length of each level. dtypes Return the dtypes as a Series for the underlying MultiIndex. codes Methods from_arrays(arrays[, sortorder, names]) Convert arrays to MultiIndex. from_tuples(tuples[, sortorder, names]) Convert list of tuples to MultiIndex. from_product(iterables[, sortorder, names]) Make a MultiIndex from the cartesian product of multiple iterables. from_frame(df[, sortorder, names]) Make a MultiIndex from a DataFrame. set_levels(levels, *[, level, verify_integrity]) Set new levels on MultiIndex. set_codes(codes, *[, level, verify_integrity]) Set new codes on MultiIndex. to_frame([index, name, allow_duplicates]) Create a DataFrame with the levels of the MultiIndex as columns. to_flat_index() Convert a MultiIndex to an Index of Tuples containing the level values. sortlevel([level, ascending, ...]) Sort MultiIndex at the requested level. droplevel([level]) Return index with requested level(s) removed. swaplevel([i, j]) Swap level i with level j. reorder_levels(order) Rearrange levels using input order. remove_unused_levels() Create new MultiIndex from current that removes unused levels. get_level_values(level) Return vector of label values for requested level. get_indexer(target[, method, limit, tolerance]) Compute indexer and mask for new index given the current index. get_loc(key) Get location for a label or a tuple of labels. get_locs(seq) Get location for a sequence of labels. get_loc_level(key[, level, drop_level]) Get location and sliced index for requested label(s)/level(s). drop(codes[, level, errors]) Make a new pandas.MultiIndex with the passed list of codes deleted. Notes See the user guide for more.","Parameters: levelssequence of arraysThe unique labels for each level. codessequence of arraysIntegers for each level designating which label at each location. sortorderoptional intLevel of sortedness (must be lexicographically sorted by that level). namesoptional sequence of objectsNames for each of the index levels. (name is accepted for compat). copybool, default FalseCopy the meta-data. verify_integritybool, default TrueCheck that the levels/codes are consistent and valid.","["">>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n>>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\nMultiIndex([(1,  'red'),\n            (1, 'blue'),\n            (2,  'red'),\n            (2, 'blue')],\n           names=['number', 'color'])""]"
905,..\pandas\reference\api\pandas.Categorical.html,pandas.Categorical,"class pandas.Categorical(values, categories=None, ordered=None, dtype=None, fastpath=<no_default>, copy=True)[source]# Represent a categorical variable in classic R / S-plus fashion. Categoricals can only take on a limited, and usually fixed, number of possible values (categories). In contrast to statistical categorical variables, a Categorical might have an order, but numerical operations (additions, divisions, …) are not possible. All values of the Categorical are either in categories or np.nan. Assigning values outside of categories will raise a ValueError. Order is defined by the order of the categories, not lexical order of the values. Attributes categories The categories of this categorical. codes The category codes of this categorical index. ordered Whether the categories have an ordered relationship. dtype The CategoricalDtype for this instance. Methods from_codes(codes[, categories, ordered, ...]) Make a Categorical type from codes and categories or dtype. __array__([dtype, copy]) The numpy array interface. Raises: ValueErrorIf the categories do not validate. TypeErrorIf an explicit ordered=True is given but no categories and the values are not sortable. Notes See the user guide for more.","Parameters: valueslist-likeThe values of the categorical. If categories are given, values not in categories will be replaced with NaN. categoriesIndex-like (unique), optionalThe unique categories for this categorical. If not given, the categories are assumed to be the unique values of values (sorted, if possible, otherwise in the order in which they appear). orderedbool, default FalseWhether or not this categorical is treated as a ordered categorical. If True, the resulting categorical will be ordered. An ordered categorical respects, when sorted, the order of its categories attribute (which in turn is the categories argument, if provided). dtypeCategoricalDtypeAn instance of CategoricalDtype to use for this categorical.","['>>> pd.Categorical([1, 2, 3, 1, 2, 3])\n[1, 2, 3, 1, 2, 3]\nCategories (3, int64): [1, 2, 3]', "">>> pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'])\n['a', 'b', 'c', 'a', 'b', 'c']\nCategories (3, object): ['a', 'b', 'c']"", '>>> c = pd.Categorical([1, 2, 3, 1, 2, 3, np.nan])\n>>> c\n[1, 2, 3, 1, 2, 3, NaN]\nCategories (3, int64): [1, 2, 3]', '>>> c.codes\narray([ 0,  1,  2,  0,  1,  2, -1], dtype=int8)', "">>> c = pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c'], ordered=True,\n...                    categories=['c', 'b', 'a'])\n>>> c\n['a', 'b', 'c', 'a', 'b', 'c']\nCategories (3, object): ['c' < 'b' < 'a']\n>>> c.min()\n'c'""]"
906,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr.html,pandas.tseries.offsets.CustomBusinessMonthEnd.freqstr,CustomBusinessMonthEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
907,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.kwds.html,pandas.tseries.offsets.QuarterBegin.kwds,QuarterBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
908,..\pandas\reference\api\pandas.DataFrame.asof.html,pandas.DataFrame.asof,"DataFrame.asof(where, subset=None)[source]# Return the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Notes Dates are assumed to be sorted. Raises if this is not the case.","Parameters: wheredate or array-like of datesDate(s) before which the last row(s) are returned. subsetstr or array-like of str, default NoneFor DataFrame, if not None, only use these columns to check for NaNs. Returns: scalar, Series, or DataFrameThe return can be: scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like","['>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n>>> s\n10    1.0\n20    2.0\n30    NaN\n40    4.0\ndtype: float64', '>>> s.asof(20)\n2.0', '>>> s.asof([5, 20])\n5     NaN\n20    2.0\ndtype: float64', '>>> s.asof(30)\n2.0', "">>> df = pd.DataFrame({'a': [10., 20., 30., 40., 50.],\n...                    'b': [None, None, None, None, 500]},\n...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n...                                           '2018-02-27 09:02:00',\n...                                           '2018-02-27 09:03:00',\n...                                           '2018-02-27 09:04:00',\n...                                           '2018-02-27 09:05:00']))\n>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']))\n                      a   b\n2018-02-27 09:03:30 NaN NaN\n2018-02-27 09:04:30 NaN NaN"", "">>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']),\n...         subset=['a'])\n                        a   b\n2018-02-27 09:03:30  30.0 NaN\n2018-02-27 09:04:30  40.0 NaN""]"
909,..\pandas\reference\api\pandas.Timestamp.min.html,pandas.Timestamp.min,Timestamp.min = Timestamp('1677-09-21 00:12:43.145224193')#,No parameters found,[]
910,..\pandas\reference\api\pandas.Series.keys.html,pandas.Series.keys,Series.keys()[source]# Return alias for index.,Returns: IndexIndex of the Series.,"["">>> s = pd.Series([1, 2, 3], index=[0, 1, 2])\n>>> s.keys()\nIndex([0, 1, 2], dtype='int64')""]"
911,..\pandas\reference\api\pandas.DatetimeIndex.tz_convert.html,pandas.DatetimeIndex.tz_convert,DatetimeIndex.tz_convert(tz)[source]# Convert tz-aware Datetime Array/Index from one time zone to another.,"Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or NoneTime zone for time. Corresponding timestamps would be converted to this time zone of the Datetime Array/Index. A tz of None will convert to UTC and remove the timezone information. Returns: Array or Index Raises: TypeErrorIf Datetime Array/Index is tz-naive.","["">>> dti = pd.date_range(start='2014-08-01 09:00',\n...                     freq='h', periods=3, tz='Europe/Berlin')"", "">>> dti\nDatetimeIndex(['2014-08-01 09:00:00+02:00',\n               '2014-08-01 10:00:00+02:00',\n               '2014-08-01 11:00:00+02:00'],\n              dtype='datetime64[ns, Europe/Berlin]', freq='h')"", "">>> dti.tz_convert('US/Central')\nDatetimeIndex(['2014-08-01 02:00:00-05:00',\n               '2014-08-01 03:00:00-05:00',\n               '2014-08-01 04:00:00-05:00'],\n              dtype='datetime64[ns, US/Central]', freq='h')"", "">>> dti = pd.date_range(start='2014-08-01 09:00', freq='h',\n...                     periods=3, tz='Europe/Berlin')"", "">>> dti\nDatetimeIndex(['2014-08-01 09:00:00+02:00',\n               '2014-08-01 10:00:00+02:00',\n               '2014-08-01 11:00:00+02:00'],\n                dtype='datetime64[ns, Europe/Berlin]', freq='h')"", "">>> dti.tz_convert(None)\nDatetimeIndex(['2014-08-01 07:00:00',\n               '2014-08-01 08:00:00',\n               '2014-08-01 09:00:00'],\n                dtype='datetime64[ns]', freq='h')""]"
912,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.n.html,pandas.tseries.offsets.QuarterBegin.n,QuarterBegin.n#,No parameters found,[]
913,..\pandas\reference\api\pandas.MultiIndex.levels.html,pandas.MultiIndex.levels,"MultiIndex.levels[source]# Levels of the MultiIndex. Levels refer to the different hierarchical levels or layers in a MultiIndex. In a MultiIndex, each level represents a distinct dimension or category of the index. To access the levels, you can use the levels attribute of the MultiIndex, which returns a tuple of Index objects. Each Index object represents a level in the MultiIndex and contains the unique values found in that specific level. If a MultiIndex is created with levels A, B, C, and the DataFrame using it filters out all rows of the level C, MultiIndex.levels will still return A, B, C.",No parameters found,"["">>> index = pd.MultiIndex.from_product([['mammal'],\n...                                     ('goat', 'human', 'cat', 'dog')],\n...                                    names=['Category', 'Animals'])\n>>> leg_num = pd.DataFrame(data=(4, 2, 4, 4), index=index, columns=['Legs'])\n>>> leg_num\n                  Legs\nCategory Animals\nmammal   goat        4\n         human       2\n         cat         4\n         dog         4"", "">>> leg_num.index.levels\nFrozenList([['mammal'], ['cat', 'dog', 'goat', 'human']])"", '>>> large_leg_num = leg_num[leg_num.Legs > 2]\n>>> large_leg_num\n                  Legs\nCategory Animals\nmammal   goat        4\n         cat         4\n         dog         4', "">>> large_leg_num.index.levels\nFrozenList([['mammal'], ['cat', 'dog', 'goat', 'human']])""]"
914,..\pandas\reference\api\pandas.Timestamp.minute.html,pandas.Timestamp.minute,Timestamp.minute#,No parameters found,[]
915,..\pandas\reference\api\pandas.DataFrame.assign.html,pandas.DataFrame.assign,DataFrame.assign(**kwargs)[source]# Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. Notes Assigning multiple columns within the same assign is possible. Later items in ‘**kwargs’ may refer to newly created or modified columns in ‘df’; items are computed and assigned into ‘df’ in order.,"Parameters: **kwargsdict of {str: callable or Series}The column names are keywords. If the values are callable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn’t check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned. Returns: DataFrameA new DataFrame with the new columns in addition to all the existing columns.","["">>> df = pd.DataFrame({'temp_c': [17.0, 25.0]},\n...                   index=['Portland', 'Berkeley'])\n>>> df\n          temp_c\nPortland    17.0\nBerkeley    25.0"", '>>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\n          temp_c  temp_f\nPortland    17.0    62.6\nBerkeley    25.0    77.0', "">>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32)\n          temp_c  temp_f\nPortland    17.0    62.6\nBerkeley    25.0    77.0"", "">>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,\n...           temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9)\n          temp_c  temp_f  temp_k\nPortland    17.0    62.6  290.15\nBerkeley    25.0    77.0  298.15""]"
916,..\pandas\reference\api\pandas.Series.kurt.html,pandas.Series.kurt,"Series.kurt(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher’s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","["">>> s = pd.Series([1, 2, 2, 3], index=['cat', 'dog', 'dog', 'mouse'])\n>>> s\ncat    1\ndog    2\ndog    2\nmouse  3\ndtype: int64\n>>> s.kurt()\n1.5"", "">>> df = pd.DataFrame({'a': [1, 2, 2, 3], 'b': [3, 4, 4, 4]},\n...                   index=['cat', 'dog', 'dog', 'mouse'])\n>>> df\n       a   b\n  cat  1   3\n  dog  2   4\n  dog  2   4\nmouse  3   4\n>>> df.kurt()\na   1.5\nb   4.0\ndtype: float64"", '>>> df.kurt(axis=None).round(6)\n-0.988693', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [3, 4], 'd': [1, 2]},\n...                   index=['cat', 'dog'])\n>>> df.kurt(axis=1)\ncat   -6.0\ndog   -6.0\ndtype: float64""]"
917,..\pandas\reference\api\pandas.Categorical.ordered.html,pandas.Categorical.ordered,property Categorical.ordered[source]# Whether the categories have an ordered relationship.,No parameters found,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.ordered\nFalse"", "">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.ordered\nTrue"", "">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.ordered\nTrue"", "">>> cat = pd.Categorical(['a', 'b'], ordered=False)\n>>> cat.ordered\nFalse"", "">>> ci = pd.CategoricalIndex(['a', 'b'], ordered=True)\n>>> ci.ordered\nTrue"", "">>> ci = pd.CategoricalIndex(['a', 'b'], ordered=False)\n>>> ci.ordered\nFalse""]"
918,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.holidays.html,pandas.tseries.offsets.CustomBusinessMonthEnd.holidays,CustomBusinessMonthEnd.holidays#,No parameters found,[]
919,..\pandas\reference\api\pandas.DatetimeIndex.tz_localize.html,pandas.DatetimeIndex.tz_localize,"DatetimeIndex.tz_localize(tz, ambiguous='raise', nonexistent='raise')[source]# Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index. This method takes a time zone (tz) naive Datetime Array/Index object and makes this time zone aware. It does not move the time to another time zone. This method can also be used to do the inverse – to create a time zone unaware object from an aware object. To that end, pass tz=None.","Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or NoneTime zone to convert timestamps to. Passing None will remove the time zone information preserving local time. ambiguous‘infer’, ‘NaT’, bool array, default ‘raise’When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. ‘infer’ will attempt to infer fall dst-transition hours based on order bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times) ‘NaT’ will return NaT where there are ambiguous times ‘raise’ will raise an AmbiguousTimeError if there are ambiguous times. nonexistent‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time ‘shift_backward’ will shift the nonexistent time backward to the closest existing time ‘NaT’ will return NaT where there are nonexistent times timedelta objects will shift nonexistent times by the timedelta ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: Same type as selfArray/Index converted to the specified time zone. Raises: TypeErrorIf the Datetime Array/Index is tz-aware and tz is not None.","["">>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)\n>>> tz_naive\nDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n               '2018-03-03 09:00:00'],\n              dtype='datetime64[ns]', freq='D')"", "">>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')\n>>> tz_aware\nDatetimeIndex(['2018-03-01 09:00:00-05:00',\n               '2018-03-02 09:00:00-05:00',\n               '2018-03-03 09:00:00-05:00'],\n              dtype='datetime64[ns, US/Eastern]', freq=None)"", "">>> tz_aware.tz_localize(None)\nDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n               '2018-03-03 09:00:00'],\n              dtype='datetime64[ns]', freq=None)"", "">>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',\n...                               '2018-10-28 02:00:00',\n...                               '2018-10-28 02:30:00',\n...                               '2018-10-28 02:00:00',\n...                               '2018-10-28 02:30:00',\n...                               '2018-10-28 03:00:00',\n...                               '2018-10-28 03:30:00']))\n>>> s.dt.tz_localize('CET', ambiguous='infer')\n0   2018-10-28 01:30:00+02:00\n1   2018-10-28 02:00:00+02:00\n2   2018-10-28 02:30:00+02:00\n3   2018-10-28 02:00:00+01:00\n4   2018-10-28 02:30:00+01:00\n5   2018-10-28 03:00:00+01:00\n6   2018-10-28 03:30:00+01:00\ndtype: datetime64[ns, CET]"", "">>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',\n...                               '2018-10-28 02:36:00',\n...                               '2018-10-28 03:46:00']))\n>>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))\n0   2018-10-28 01:20:00+02:00\n1   2018-10-28 02:36:00+02:00\n2   2018-10-28 03:46:00+01:00\ndtype: datetime64[ns, CET]"", "">>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',\n...                               '2015-03-29 03:30:00']))\n>>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n0   2015-03-29 03:00:00+02:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]"", "">>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n0   2015-03-29 01:59:59.999999999+01:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]"", "">>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1h'))\n0   2015-03-29 03:30:00+02:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]""]"
920,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.name.html,pandas.tseries.offsets.QuarterBegin.name,QuarterBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
921,..\pandas\reference\api\pandas.Categorical.__array__.html,pandas.Categorical.__array__,"Categorical.__array__(dtype=None, copy=None)[source]# The numpy array interface.","Returns: numpy.arrayA numpy array of either the specified dtype or, if dtype==None (default), the same dtype as categorical.categories.dtype.","["">>> cat = pd.Categorical(['a', 'b'], ordered=True)"", "">>> np.asarray(cat)\narray(['a', 'b'], dtype=object)""]"
922,..\pandas\reference\api\pandas.DataFrame.astype.html,pandas.DataFrame.astype,"DataFrame.astype(dtype, copy=None, errors='raise')[source]# Cast a pandas object to a specified dtype dtype. Notes Changed in version 2.0.0: Using astype to convert from timezone-naive dtype to timezone-aware dtype will raise an exception. Use Series.dt.tz_localize() instead.","Parameters: dtypestr, data type, Series or Mapping of column name -> data typeUse a str, numpy.dtype, pandas.ExtensionDtype or Python type to cast entire pandas object to the same type. Alternatively, use a mapping, e.g. {col: dtype, …}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame’s columns to column-specific types. copybool, default TrueReturn a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects). Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True errors{‘raise’, ‘ignore’}, default ‘raise’Control raising of exceptions on invalid data for provided dtype. raise : allow exceptions to be raised ignore : suppress exceptions. On error return original object. Returns: same type as caller","["">>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object"", "">>> df.astype('int32').dtypes\ncol1    int32\ncol2    int32\ndtype: object"", "">>> df.astype({'col1': 'int32'}).dtypes\ncol1    int32\ncol2    int64\ndtype: object"", "">>> ser = pd.Series([1, 2], dtype='int32')\n>>> ser\n0    1\n1    2\ndtype: int32\n>>> ser.astype('int64')\n0    1\n1    2\ndtype: int64"", "">>> ser.astype('category')\n0    1\n1    2\ndtype: category\nCategories (2, int32): [1, 2]"", '>>> from pandas.api.types import CategoricalDtype\n>>> cat_dtype = CategoricalDtype(\n...     categories=[2, 1], ordered=True)\n>>> ser.astype(cat_dtype)\n0    1\n1    2\ndtype: category\nCategories (2, int64): [2 < 1]', "">>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n>>> ser_date\n0   2020-01-01\n1   2020-01-02\n2   2020-01-03\ndtype: datetime64[ns]""]"
923,..\pandas\reference\api\pandas.Timestamp.month.html,pandas.Timestamp.month,Timestamp.month#,No parameters found,[]
924,..\pandas\reference\api\pandas.Series.kurtosis.html,pandas.Series.kurtosis,"Series.kurtosis(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher’s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","["">>> s = pd.Series([1, 2, 2, 3], index=['cat', 'dog', 'dog', 'mouse'])\n>>> s\ncat    1\ndog    2\ndog    2\nmouse  3\ndtype: int64\n>>> s.kurt()\n1.5"", "">>> df = pd.DataFrame({'a': [1, 2, 2, 3], 'b': [3, 4, 4, 4]},\n...                   index=['cat', 'dog', 'dog', 'mouse'])\n>>> df\n       a   b\n  cat  1   3\n  dog  2   4\n  dog  2   4\nmouse  3   4\n>>> df.kurt()\na   1.5\nb   4.0\ndtype: float64"", '>>> df.kurt(axis=None).round(6)\n-0.988693', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [3, 4], 'd': [1, 2]},\n...                   index=['cat', 'dog'])\n>>> df.kurt(axis=1)\ncat   -6.0\ndog   -6.0\ndtype: float64""]"
925,..\pandas\reference\api\pandas.DatetimeIndex.weekday.html,pandas.DatetimeIndex.weekday,"property DatetimeIndex.weekday[source]# The day of the week with Monday=0, Sunday=6. Return the day of the week. It is assumed the week starts on Monday, which is denoted by 0 and ends on Sunday which is denoted by 6. This method is available on both Series with datetime values (using the dt accessor) or DatetimeIndex.",Returns: Series or IndexContaining integers indicating the day number.,"["">>> s = pd.date_range('2016-12-31', '2017-01-08', freq='D').to_series()\n>>> s.dt.dayofweek\n2016-12-31    5\n2017-01-01    6\n2017-01-02    0\n2017-01-03    1\n2017-01-04    2\n2017-01-05    3\n2017-01-06    4\n2017-01-07    5\n2017-01-08    6\nFreq: D, dtype: int32""]"
926,..\pandas\reference\api\pandas.MultiIndex.levshape.html,pandas.MultiIndex.levshape,property MultiIndex.levshape[source]# A tuple with the length of each level.,No parameters found,"["">>> mi = pd.MultiIndex.from_arrays([['a'], ['b'], ['c']])\n>>> mi\nMultiIndex([('a', 'b', 'c')],\n           )\n>>> mi.levshape\n(1, 1, 1)""]"
927,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.html,pandas.tseries.offsets.CustomBusinessMonthEnd,class pandas.tseries.offsets.CustomBusinessMonthEnd# DateOffset subclass representing custom business month(s). Increments between end of month dates. Examples In the example below we use the default parameters. Custom business month end can be specified by weekmask parameter. To convert the returned datetime object to its string representation the function strftime() is used in the next example. Using NumPy business day calendar you can define custom holidays. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. calendar cbday_roll Define default roll function to be called in apply method. freqstr Return a string representing the frequency. holidays kwds Return a dict of extra parameters for the offset. m_offset month_roll Define default roll function to be called in apply method. n name Return a string representing the base frequency. nanos normalize offset Alias for self._offset. rule_code weekmask,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize end dates to midnight before generating date range. weekmaskstr, Default ‘Mon Tue Wed Thu Fri’Weekmask of valid business days, passed to numpy.busdaycalendar. holidayslistList/array of dates to exclude from the set of valid business days, passed to numpy.busdaycalendar. calendarnp.busdaycalendarCalendar to integrate. offsettimedelta, default timedelta(0)Time offset to apply.","["">>> ts = pd.Timestamp(2022, 8, 5)\n>>> ts + pd.offsets.CustomBusinessMonthEnd()\nTimestamp('2022-08-31 00:00:00')"", '>>> import datetime as dt\n>>> freq = pd.offsets.CustomBusinessMonthEnd(weekmask=""Wed Thu"")\n>>> pd.date_range(dt.datetime(2022, 7, 10), dt.datetime(2022, 12, 18),\n...               freq=freq).strftime(\'%a %d %b %Y %H:%M\')\nIndex([\'Thu 28 Jul 2022 00:00\', \'Wed 31 Aug 2022 00:00\',\n       \'Thu 29 Sep 2022 00:00\', \'Thu 27 Oct 2022 00:00\',\n       \'Wed 30 Nov 2022 00:00\'],\n       dtype=\'object\')', "">>> import datetime as dt\n>>> bdc = np.busdaycalendar(holidays=['2022-08-01', '2022-09-30',\n...                                   '2022-10-31', '2022-11-01'])\n>>> freq = pd.offsets.CustomBusinessMonthEnd(calendar=bdc)\n>>> pd.date_range(dt.datetime(2022, 7, 10), dt.datetime(2022, 11, 10), freq=freq)\nDatetimeIndex(['2022-07-29', '2022-08-31', '2022-09-29', '2022-10-28'],\n               dtype='datetime64[ns]', freq='CBME')""]"
928,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.nanos.html,pandas.tseries.offsets.QuarterBegin.nanos,QuarterBegin.nanos#,No parameters found,[]
929,..\pandas\reference\api\pandas.CategoricalDtype.categories.html,pandas.CategoricalDtype.categories,property CategoricalDtype.categories[source]# An Index containing the unique categories allowed.,No parameters found,"["">>> cat_type = pd.CategoricalDtype(categories=['a', 'b'], ordered=True)\n>>> cat_type.categories\nIndex(['a', 'b'], dtype='object')""]"
930,..\pandas\reference\api\pandas.DataFrame.at.html,pandas.DataFrame.at,"property DataFrame.at[source]# Access a single value for a row/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series. Notes See Fast scalar value getting and setting for more details.",Raises: KeyErrorIf getting a value and ‘label’ does not exist in a DataFrame or Series. ValueErrorIf row/column label pair is not a tuple or if any label from the pair is not a scalar for DataFrame. If label is list-like (excluding NamedTuple) for Series.,"["">>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n4   0   2   3\n5   0   4   1\n6  10  20  30"", "">>> df.at[4, 'B']\n2"", "">>> df.at[4, 'B'] = 10\n>>> df.at[4, 'B']\n10"", "">>> df.loc[5].at['B']\n4""]"
931,..\pandas\reference\api\pandas.Timestamp.month_name.html,pandas.Timestamp.month_name,Timestamp.month_name(locale=None)# Return the month name of the Timestamp with specified locale.,"Parameters: localestr, default None (English locale)Locale determining the language in which to return the month name. Returns: str","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.month_name()\n'March'"", '>>> pd.NaT.month_name()\nnan']"
932,..\pandas\reference\api\pandas.DatetimeIndex.year.html,pandas.DatetimeIndex.year,property DatetimeIndex.year[source]# The year of the datetime.,No parameters found,"['>>> datetime_series = pd.Series(\n...     pd.date_range(""2000-01-01"", periods=3, freq=""YE"")\n... )\n>>> datetime_series\n0   2000-12-31\n1   2001-12-31\n2   2002-12-31\ndtype: datetime64[ns]\n>>> datetime_series.dt.year\n0    2000\n1    2001\n2    2002\ndtype: int32']"
933,..\pandas\reference\api\pandas.CategoricalDtype.html,pandas.CategoricalDtype,"class pandas.CategoricalDtype(categories=None, ordered=False)[source]# Type for categorical data with the categories and orderedness. Attributes categories An Index containing the unique categories allowed. ordered Whether the categories have an ordered relationship. Methods None Notes This class is useful for specifying the type of a Categorical independent of the values. See CategoricalDtype for more.","Parameters: categoriessequence, optionalMust be unique, and must not contain any nulls. The categories are stored in an Index, and if an index is provided the dtype of that index will be used. orderedbool or None, default FalseWhether or not this categorical is treated as a ordered categorical. None can be used to maintain the ordered value of existing categoricals when used in operations that combine categoricals, e.g. astype, and will resolve to False if there is no existing ordered to maintain.","["">>> t = pd.CategoricalDtype(categories=['b', 'a'], ordered=True)\n>>> pd.Series(['a', 'b', 'a', 'c'], dtype=t)\n0      a\n1      b\n2      a\n3    NaN\ndtype: category\nCategories (2, object): ['b' < 'a']"", "">>> pd.CategoricalDtype(pd.DatetimeIndex([])).categories.dtype\ndtype('<M8[ns]')""]"
934,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.normalize.html,pandas.tseries.offsets.QuarterBegin.normalize,QuarterBegin.normalize#,No parameters found,[]
935,..\pandas\reference\api\pandas.MultiIndex.names.html,pandas.MultiIndex.names,property MultiIndex.names[source]# Names of levels in MultiIndex.,No parameters found,"["">>> mi = pd.MultiIndex.from_arrays(\n... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n>>> mi\nMultiIndex([(1, 3, 5),\n            (2, 4, 6)],\n           names=['x', 'y', 'z'])\n>>> mi.names\nFrozenList(['x', 'y', 'z'])""]"
936,..\pandas\reference\api\pandas.Series.last.html,pandas.Series.last,"Series.last(offset)[source]# Select final periods of time series data based on a date offset. Deprecated since version 2.1: last() is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset. Notes Deprecated since version 2.1.0: Please create a mask and filter using .loc instead","Parameters: offsetstr, DateOffset, dateutil.relativedeltaThe offset length of the data that will be selected. For instance, ‘3D’ will display all the rows having their index within the last 3 days. Returns: Series or DataFrameA subset of the caller. Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n            A\n2018-04-09  1\n2018-04-11  2\n2018-04-13  3\n2018-04-15  4"", "">>> ts.last('3D')  \n            A\n2018-04-13  3\n2018-04-15  4""]"
937,..\pandas\reference\api\pandas.DataFrame.attrs.html,pandas.DataFrame.attrs,property DataFrame.attrs[source]# Dictionary of global attributes of this dataset. Warning attrs is experimental and may change without warning. Notes Many operations that create new datasets will copy attrs. Copies are always deep so that changing attrs will only affect the present dataset. pandas.concat copies attrs only if all input datasets have the same attrs.,No parameters found,"['>>> ser = pd.Series([1, 2, 3])\n>>> ser.attrs = {""A"": [10, 20, 30]}\n>>> ser.attrs\n{\'A\': [10, 20, 30]}', '>>> df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]})\n>>> df.attrs = {""A"": [10, 20, 30]}\n>>> df.attrs\n{\'A\': [10, 20, 30]}']"
938,..\pandas\reference\api\pandas.Timestamp.nanosecond.html,pandas.Timestamp.nanosecond,Timestamp.nanosecond#,No parameters found,[]
939,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_anchored.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_anchored,CustomBusinessMonthEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
940,..\pandas\reference\api\pandas.DatetimeTZDtype.html,pandas.DatetimeTZDtype,"class pandas.DatetimeTZDtype(unit='ns', tz=None)[source]# An ExtensionDtype for timezone-aware datetime data. This is not an actual numpy dtype, but a duck type. Attributes unit The precision of the datetime data. tz The timezone. Methods None Raises: ZoneInfoNotFoundErrorWhen the requested timezone cannot be found.","Parameters: unitstr, default “ns”The precision of the datetime data. Currently limited to ""ns"". tzstr, int, or datetime.tzinfoThe timezone.","["">>> from zoneinfo import ZoneInfo\n>>> pd.DatetimeTZDtype(tz=ZoneInfo('UTC'))\ndatetime64[ns, UTC]"", "">>> pd.DatetimeTZDtype(tz=ZoneInfo('Europe/Paris'))\ndatetime64[ns, Europe/Paris]""]"
941,..\pandas\reference\api\pandas.CategoricalDtype.ordered.html,pandas.CategoricalDtype.ordered,property CategoricalDtype.ordered[source]# Whether the categories have an ordered relationship.,No parameters found,"["">>> cat_type = pd.CategoricalDtype(categories=['a', 'b'], ordered=True)\n>>> cat_type.ordered\nTrue"", "">>> cat_type = pd.CategoricalDtype(categories=['a', 'b'], ordered=False)\n>>> cat_type.ordered\nFalse""]"
942,..\pandas\reference\api\pandas.DataFrame.at_time.html,pandas.DataFrame.at_time,"DataFrame.at_time(time, asof=False, axis=None)[source]# Select values at particular time of day (e.g., 9:30AM).","Parameters: timedatetime.time or strThe values to select. axis{0 or ‘index’, 1 or ‘columns’}, default 0For Series this parameter is unused and defaults to 0. Returns: Series or DataFrame Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='12h')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n                     A\n2018-04-09 00:00:00  1\n2018-04-09 12:00:00  2\n2018-04-10 00:00:00  3\n2018-04-10 12:00:00  4"", "">>> ts.at_time('12:00')\n                     A\n2018-04-09 12:00:00  2\n2018-04-10 12:00:00  4""]"
943,..\pandas\reference\api\pandas.Timestamp.normalize.html,pandas.Timestamp.normalize,"Timestamp.normalize()# Normalize Timestamp to midnight, preserving tz information.",No parameters found,"["">>> ts = pd.Timestamp(2020, 3, 14, 15, 30)\n>>> ts.normalize()\nTimestamp('2020-03-14 00:00:00')""]"
944,..\pandas\reference\api\pandas.Series.last_valid_index.html,pandas.Series.last_valid_index,"Series.last_valid_index()[source]# Return index for last non-NA value or None, if no non-NA value is found.",Returns: type of index,"['>>> s = pd.Series([None, 3, 4])\n>>> s.first_valid_index()\n1\n>>> s.last_valid_index()\n2', '>>> s = pd.Series([None, None])\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', '>>> s = pd.Series()\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', "">>> df = pd.DataFrame({'A': [None, None, 2], 'B': [None, 3, 4]})\n>>> df\n     A      B\n0  NaN    NaN\n1  NaN    3.0\n2  2.0    4.0\n>>> df.first_valid_index()\n1\n>>> df.last_valid_index()\n2"", "">>> df = pd.DataFrame({'A': [None, None, None], 'B': [None, None, None]})\n>>> df\n     A      B\n0  None   None\n1  None   None\n2  None   None\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone"", '>>> df = pd.DataFrame()\n>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone']"
945,..\pandas\reference\api\pandas.DatetimeTZDtype.tz.html,pandas.DatetimeTZDtype.tz,property DatetimeTZDtype.tz[source]# The timezone.,No parameters found,"["">>> from zoneinfo import ZoneInfo\n>>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))\n>>> dtype.tz\nzoneinfo.ZoneInfo(key='America/Los_Angeles')""]"
946,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_end.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_end,CustomBusinessMonthEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
947,..\pandas\reference\api\pandas.MultiIndex.nlevels.html,pandas.MultiIndex.nlevels,property MultiIndex.nlevels[source]# Integer number of levels in this MultiIndex.,No parameters found,"["">>> mi = pd.MultiIndex.from_arrays([['a'], ['b'], ['c']])\n>>> mi\nMultiIndex([('a', 'b', 'c')],\n           )\n>>> mi.nlevels\n3""]"
948,..\pandas\reference\api\pandas.CategoricalIndex.add_categories.html,pandas.CategoricalIndex.add_categories,"CategoricalIndex.add_categories(*args, **kwargs)[source]# Add new categories. new_categories will be included at the last/highest place in the categories and will be unused directly after this call.",Parameters: new_categoriescategory or list-like of categoryThe new categories to be included. Returns: CategoricalCategorical with new categories added. Raises: ValueErrorIf the new categories include old categories or do not validate as categories,"["">>> c = pd.Categorical(['c', 'b', 'c'])\n>>> c\n['c', 'b', 'c']\nCategories (2, object): ['b', 'c']"", "">>> c.add_categories(['d', 'a'])\n['c', 'b', 'c']\nCategories (4, object): ['b', 'c', 'd', 'a']""]"
949,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.rule_code.html,pandas.tseries.offsets.QuarterBegin.rule_code,QuarterBegin.rule_code#,No parameters found,[]
950,..\pandas\reference\api\pandas.DataFrame.axes.html,pandas.DataFrame.axes,property DataFrame.axes[source]# Return a list representing the axes of the DataFrame. It has the row axis labels and column axis labels as the only members. They are returned in that order.,No parameters found,"["">>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n>>> df.axes\n[RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'],\ndtype='object')]""]"
951,..\pandas\reference\api\pandas.Timestamp.now.html,pandas.Timestamp.now,classmethod Timestamp.now(tz=None)# Return new Timestamp object representing current time local to tz.,"Parameters: tzstr or timezone object, default NoneTimezone to localize to.","["">>> pd.Timestamp.now()  \nTimestamp('2020-11-16 22:06:16.378782')"", '>>> pd.NaT.now()\nNaT']"
952,..\pandas\reference\api\pandas.Series.le.html,pandas.Series.le,"Series.le(other, level=None, fill_value=None, axis=0)[source]# Return Less than or equal to of series and other, element-wise (binary operator le). Equivalent to series <= other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ne    1.0\ndtype: float64\n>>> b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n>>> b\na    0.0\nb    1.0\nc    2.0\nd    NaN\nf    1.0\ndtype: float64\n>>> a.le(b, fill_value=0)\na    False\nb     True\nc     True\nd    False\ne    False\nf     True\ndtype: bool""]"
953,..\pandas\reference\api\pandas.Timestamp.quarter.html,pandas.Timestamp.quarter,Timestamp.quarter# Return the quarter of the year.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.quarter\n1']"
954,..\pandas\reference\api\pandas.DataFrame.backfill.html,pandas.DataFrame.backfill,"DataFrame.backfill(*, axis=None, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values by using the next valid observation to fill the gap. Deprecated since version 2.0: Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead.",Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.,[]
955,..\pandas\reference\api\pandas.tseries.offsets.QuarterBegin.startingMonth.html,pandas.tseries.offsets.QuarterBegin.startingMonth,QuarterBegin.startingMonth#,No parameters found,[]
956,..\pandas\reference\api\pandas.DatetimeTZDtype.unit.html,pandas.DatetimeTZDtype.unit,property DatetimeTZDtype.unit[source]# The precision of the datetime data.,No parameters found,"["">>> from zoneinfo import ZoneInfo\n>>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))\n>>> dtype.unit\n'ns'""]"
957,..\pandas\reference\api\pandas.CategoricalIndex.as_ordered.html,pandas.CategoricalIndex.as_ordered,"CategoricalIndex.as_ordered(*args, **kwargs)[source]# Set the Categorical to be ordered.",Returns: CategoricalOrdered Categorical.,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.ordered\nFalse\n>>> ser = ser.cat.as_ordered()\n>>> ser.cat.ordered\nTrue"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a'])\n>>> ci.ordered\nFalse\n>>> ci = ci.as_ordered()\n>>> ci.ordered\nTrue""]"
958,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_start.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_month_start,CustomBusinessMonthEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
959,..\pandas\reference\api\pandas.MultiIndex.remove_unused_levels.html,pandas.MultiIndex.remove_unused_levels,"MultiIndex.remove_unused_levels()[source]# Create new MultiIndex from current that removes unused levels. Unused level(s) means levels that are not expressed in the labels. The resulting MultiIndex will have the same outward appearance, meaning the same .values and ordering. It will also be .equals() to the original.",Returns: MultiIndex,"["">>> mi = pd.MultiIndex.from_product([range(2), list('ab')])\n>>> mi\nMultiIndex([(0, 'a'),\n            (0, 'b'),\n            (1, 'a'),\n            (1, 'b')],\n           )"", "">>> mi[2:]\nMultiIndex([(1, 'a'),\n            (1, 'b')],\n           )"", "">>> mi2 = mi[2:].remove_unused_levels()\n>>> mi2.levels\nFrozenList([[1], ['a', 'b']])""]"
960,..\pandas\reference\api\pandas.Series.list.flatten.html,pandas.Series.list.flatten,Series.list.flatten()[source]# Flatten list values.,Returns: pandas.SeriesThe data from all lists in the series flattened.,"['>>> import pyarrow as pa\n>>> s = pd.Series(\n...     [\n...         [1, 2, 3],\n...         [3],\n...     ],\n...     dtype=pd.ArrowDtype(pa.list_(\n...         pa.int64()\n...     ))\n... )\n>>> s.list.flatten()\n0    1\n1    2\n2    3\n3    3\ndtype: int64[pyarrow]']"
961,..\pandas\reference\api\pandas.Timestamp.replace.html,pandas.Timestamp.replace,"Timestamp.replace(year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, nanosecond=None, tzinfo=<class 'object'>, fold=None)# Implements datetime.replace, handles nanoseconds.","Parameters: yearint, optional monthint, optional dayint, optional hourint, optional minuteint, optional secondint, optional microsecondint, optional nanosecondint, optional tzinfotz-convertible, optional foldint, optional Returns: Timestamp with fields replaced","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651', tz='UTC')\n>>> ts\nTimestamp('2020-03-14 15:32:52.192548651+0000', tz='UTC')"", "">>> ts.replace(year=1999, hour=10)\nTimestamp('1999-03-14 10:32:52.192548651+0000', tz='UTC')"", "">>> import pytz\n>>> ts.replace(tzinfo=pytz.timezone('US/Pacific'))\nTimestamp('2020-03-14 15:32:52.192548651-0700', tz='US/Pacific')"", "">>> pd.NaT.replace(tzinfo=pytz.timezone('US/Pacific'))\nNaT""]"
962,..\pandas\reference\api\pandas.CategoricalIndex.as_unordered.html,pandas.CategoricalIndex.as_unordered,"CategoricalIndex.as_unordered(*args, **kwargs)[source]# Set the Categorical to be unordered.",Returns: CategoricalUnordered Categorical.,"["">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.ordered\nTrue\n>>> ser = ser.cat.as_unordered()\n>>> ser.cat.ordered\nFalse"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a'], ordered=True)\n>>> ci.ordered\nTrue\n>>> ci = ci.as_unordered()\n>>> ci.ordered\nFalse""]"
963,..\pandas\reference\api\pandas.DataFrame.between_time.html,pandas.DataFrame.between_time,"DataFrame.between_time(start_time, end_time, inclusive='both', axis=None)[source]# Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time, you can get the times that are not between the two times.","Parameters: start_timedatetime.time or strInitial time as a time filter limit. end_timedatetime.time or strEnd time as a time filter limit. inclusive{“both”, “neither”, “left”, “right”}, default “both”Include boundaries; whether to set each bound as closed or open. axis{0 or ‘index’, 1 or ‘columns’}, default 0Determine range time on index or columns value. For Series this parameter is unused and defaults to 0. Returns: Series or DataFrameData from the original object filtered to the specified dates range. Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n                     A\n2018-04-09 00:00:00  1\n2018-04-10 00:20:00  2\n2018-04-11 00:40:00  3\n2018-04-12 01:00:00  4"", "">>> ts.between_time('0:15', '0:45')\n                     A\n2018-04-10 00:20:00  2\n2018-04-11 00:40:00  3"", "">>> ts.between_time('0:45', '0:15')\n                     A\n2018-04-09 00:00:00  1\n2018-04-12 01:00:00  4""]"
964,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_on_offset.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_on_offset,CustomBusinessMonthEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
965,..\pandas\reference\api\pandas.date_range.html,pandas.date_range,"pandas.date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, inclusive='both', *, unit=None, **kwargs)[source]# Return a fixed frequency DatetimeIndex. Returns the range of equally spaced time points (where the difference between any two adjacent points is specified by the given frequency) such that they all satisfy start <[=] x <[=] end, where the first one and the last one are, resp., the first and last time points in that range that fall on the boundary of freq (if given as a frequency string) or that are valid for freq (if given as a pandas.tseries.offsets.DateOffset). (If exactly one of start, end, or freq is not specified, this missing parameter can be computed given periods, the number of timesteps in the range. See the note below.) Notes Of the four parameters start, end, periods, and freq, exactly three must be specified. If freq is omitted, the resulting DatetimeIndex will have periods linearly spaced elements between start and end (closed on both sides). To learn more about the frequency strings, please see this link.","Parameters: startstr or datetime-like, optionalLeft bound for generating dates. endstr or datetime-like, optionalRight bound for generating dates. periodsint, optionalNumber of periods to generate. freqstr, Timedelta, datetime.timedelta, or DateOffset, default ‘D’Frequency strings can have multiples, e.g. ‘5h’. See here for a list of frequency aliases. tzstr or tzinfo, optionalTime zone name for returning localized DatetimeIndex, for example ‘Asia/Hong_Kong’. By default, the resulting DatetimeIndex is timezone-naive unless timezone-aware datetime-likes are passed. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. namestr, default NoneName of the resulting DatetimeIndex. inclusive{“both”, “neither”, “left”, “right”}, default “both”Include boundaries; Whether to set each bound as closed or open. Added in version 1.4.0. unitstr, default NoneSpecify the desired resolution of the result. Added in version 2.0.0. **kwargsFor compatibility. Has no effect on the result. Returns: DatetimeIndex","["">>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')"", '>>> pd.date_range(\n...     start=pd.to_datetime(""1/1/2018"").tz_localize(""Europe/Berlin""),\n...     end=pd.to_datetime(""1/08/2018"").tz_localize(""Europe/Berlin""),\n... )\nDatetimeIndex([\'2018-01-01 00:00:00+01:00\', \'2018-01-02 00:00:00+01:00\',\n               \'2018-01-03 00:00:00+01:00\', \'2018-01-04 00:00:00+01:00\',\n               \'2018-01-05 00:00:00+01:00\', \'2018-01-06 00:00:00+01:00\',\n               \'2018-01-07 00:00:00+01:00\', \'2018-01-08 00:00:00+01:00\'],\n              dtype=\'datetime64[ns, Europe/Berlin]\', freq=\'D\')', "">>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')"", "">>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')"", "">>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)"", "">>> pd.date_range(start='1/1/2018', periods=5, freq='ME')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='ME')"", "">>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')"", "">>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3ME')"", "">>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')"", '>>> pd.date_range(start=\'2017-01-01\', end=\'2017-01-04\', inclusive=""both"")\nDatetimeIndex([\'2017-01-01\', \'2017-01-02\', \'2017-01-03\', \'2017-01-04\'],\n              dtype=\'datetime64[ns]\', freq=\'D\')', "">>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')"", "">>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')"", '>>> pd.date_range(start=""2017-01-01"", periods=10, freq=""100YS"", unit=""s"")\nDatetimeIndex([\'2017-01-01\', \'2117-01-01\', \'2217-01-01\', \'2317-01-01\',\n               \'2417-01-01\', \'2517-01-01\', \'2617-01-01\', \'2717-01-01\',\n               \'2817-01-01\', \'2917-01-01\'],\n              dtype=\'datetime64[s]\', freq=\'100YS-JAN\')']"
966,..\pandas\reference\api\pandas.Series.list.len.html,pandas.Series.list.len,Series.list.len()[source]# Return the length of each list in the Series.,Returns: pandas.SeriesThe length of each list.,"['>>> import pyarrow as pa\n>>> s = pd.Series(\n...     [\n...         [1, 2, 3],\n...         [3],\n...     ],\n...     dtype=pd.ArrowDtype(pa.list_(\n...         pa.int64()\n...     ))\n... )\n>>> s.list.len()\n0    3\n1    1\ndtype: int32[pyarrow]']"
967,..\pandas\reference\api\pandas.Timestamp.resolution.html,pandas.Timestamp.resolution,Timestamp.resolution = Timedelta('0 days 00:00:00.000000001')#,No parameters found,[]
968,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.copy.html,pandas.tseries.offsets.QuarterEnd.copy,QuarterEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
969,..\pandas\reference\api\pandas.MultiIndex.reorder_levels.html,pandas.MultiIndex.reorder_levels,MultiIndex.reorder_levels(order)[source]# Rearrange levels using input order. May not drop or duplicate levels.,Parameters: orderlist of int or list of strList representing new level order. Reference level by number (position) or by key (label). Returns: MultiIndex,"["">>> mi = pd.MultiIndex.from_arrays([[1, 2], [3, 4]], names=['x', 'y'])\n>>> mi\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])"", "">>> mi.reorder_levels(order=[1, 0])\nMultiIndex([(3, 1),\n            (4, 2)],\n           names=['y', 'x'])"", "">>> mi.reorder_levels(order=['y', 'x'])\nMultiIndex([(3, 1),\n            (4, 2)],\n           names=['y', 'x'])""]"
970,..\pandas\reference\api\pandas.CategoricalIndex.categories.html,pandas.CategoricalIndex.categories,property CategoricalIndex.categories[source]# The categories of this categorical. Setting assigns new values to each category (effectively a rename of each individual category). The assigned value has to be a list-like object. All items must be unique and the number of items in the new categories must be the same as the number of items in the old categories.,Raises: ValueErrorIf the new categories do not validate as categories or if the number of new categories is unequal the number of old categories,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.categories\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], categories=['b', 'c', 'd'])\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.categories\nIndex(['b', 'c', 'd'], dtype='object')"", "">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.categories\nIndex(['a', 'b'], dtype='object')"", "">>> ci = pd.CategoricalIndex(['a', 'c', 'b', 'a', 'c', 'b'])\n>>> ci.categories\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> ci = pd.CategoricalIndex(['a', 'c'], categories=['c', 'b', 'a'])\n>>> ci.categories\nIndex(['c', 'b', 'a'], dtype='object')""]"
971,..\pandas\reference\api\pandas.DataFrame.bfill.html,pandas.DataFrame.bfill,"DataFrame.bfill(*, axis=None, inplace=False, limit=None, limit_area=None, downcast=<no_default>)[source]# Fill NA/NaN values by using the next valid observation to fill the gap.","Parameters: axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrameAxis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplacebool, default FalseIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area{None, ‘inside’, ‘outside’}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). Added in version 2.2.0. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.2.0. Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.","['>>> s = pd.Series([1, None, None, 2])\n>>> s.bfill()\n0    1.0\n1    2.0\n2    2.0\n3    2.0\ndtype: float64\n>>> s.bfill(limit=1)\n0    1.0\n1    NaN\n2    2.0\n3    2.0\ndtype: float64', "">>> df = pd.DataFrame({'A': [1, None, None, 4], 'B': [None, 5, None, 7]})\n>>> df\n      A     B\n0   1.0   NaN\n1   NaN   5.0\n2   NaN   NaN\n3   4.0   7.0\n>>> df.bfill()\n      A     B\n0   1.0   5.0\n1   4.0   5.0\n2   4.0   7.0\n3   4.0   7.0\n>>> df.bfill(limit=1)\n      A     B\n0   1.0   5.0\n1   NaN   5.0\n2   4.0   7.0\n3   4.0   7.0""]"
972,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_end.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_end,CustomBusinessMonthEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
973,..\pandas\reference\api\pandas.DataFrame.bool.html,pandas.DataFrame.bool,"DataFrame.bool()[source]# Return the bool of a single element Series or DataFrame. Deprecated since version 2.1.0: bool is deprecated and will be removed in future version of pandas. For Series use pandas.Series.item. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).",Returns: boolThe value in the Series or DataFrame.,"['>>> pd.Series([True]).bool()  \nTrue\n>>> pd.Series([False]).bool()  \nFalse', "">>> pd.DataFrame({'col': [True]}).bool()  \nTrue\n>>> pd.DataFrame({'col': [False]}).bool()  \nFalse"", '>>> pd.Series([True]).item()  \nTrue\n>>> pd.Series([False]).item()  \nFalse']"
974,..\pandas\reference\api\pandas.describe_option.html,pandas.describe_option,"pandas.describe_option(pat, _print_desc=False) = <pandas._config.config.CallableDynamicDoc object># Prints the description for one or more registered options. Call with no arguments to get a listing for all registered options. Available options: compute.[use_bottleneck, use_numba, use_numexpr] display.[chop_threshold, colheader_justify, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format] display.html.[border, table_schema, use_mathjax] display.[large_repr, max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions] display.unicode.[ambiguous_as_wide, east_asian_width] display.[width] future.[infer_string, no_silent_downcasting] io.excel.ods.[reader, writer] io.excel.xls.[reader] io.excel.xlsb.[reader] io.excel.xlsm.[reader, writer] io.excel.xlsx.[reader, writer] io.hdf.[default_format, dropna_table] io.parquet.[engine] io.sql.[engine] mode.[chained_assignment, copy_on_write, data_manager, sim_interactive, string_storage, use_inf_as_na] plotting.[backend] plotting.matplotlib.[register_converters] styler.format.[decimal, escape, formatter, na_rep, precision, thousands] styler.html.[mathjax] styler.latex.[environment, hrules, multicol_align, multirow_align] styler.render.[encoding, max_columns, max_elements, max_rows, repr] styler.sparse.[columns, index] Notes Please reference the User Guide for more information. The available options with its descriptions: compute.use_bottleneckboolUse the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True] compute.use_numbaboolUse the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False] compute.use_numexprboolUse the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True] display.chop_thresholdfloat or Noneif set to a float value, all float values smaller than the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None] display.colheader_justify‘left’/’right’Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right] display.date_dayfirstbooleanWhen True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False] display.date_yearfirstbooleanWhen True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False] display.encodingstr/unicodeDefaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8] display.expand_frame_reprbooleanWhether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple “pages” if its width exceeds display.width. [default: True] [currently: True] display.float_formatcallableThe callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None] display.html.borderintA border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1] display.html.table_schemabooleanWhether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False] display.html.use_mathjaxbooleanWhen True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True] display.large_repr‘truncate’/’info’For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table, or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate] display.max_categoriesintThis sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype “category”. [default: 8] [currently: 8] display.max_columnsintIf max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 or None and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection and defaults to 20. [default: 0] [currently: 0] display.max_colwidthint or NoneThe maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a “…” placeholder is embedded in the output. A ‘None’ value means unlimited. [default: 50] [currently: 50] display.max_dir_itemsintThe number of items that will be added to dir(…). ‘None’ value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added. This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100] display.max_info_columnsintmax_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100] display.max_info_rowsintdf.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785] display.max_rowsintIf max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60] display.max_seq_itemsint or NoneWhen pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of “…” to the resulting string. If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100] display.memory_usagebool, string or NoneThis specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,’deep’ [default: True] [currently: True] display.min_rowsintThe numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10] display.multi_sparseboolean“sparsify” MultiIndex display (don’t display repeated elements in outer levels within groups) [default: True] [currently: True] display.notebook_repr_htmlbooleanWhen True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True] display.pprint_nest_depthintControls the number of nested levels to process when pretty-printing [default: 3] [currently: 3] display.precisionintFloating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6] display.show_dimensionsboolean or ‘truncate’Whether to print out dimensions at the end of DataFrame repr. If ‘truncate’ is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate] display.unicode.ambiguous_as_widebooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.unicode.east_asian_widthbooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.widthintWidth of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80] future.infer_string Whether to infer sequence of str objects as pyarrow string dtype, which will be the default in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] future.no_silent_downcasting Whether to opt-in to the future behavior which will not silently downcast results from Series and DataFrame where, mask, and clip methods. Silent downcasting will be removed in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] io.excel.ods.readerstringThe default Excel reader engine for ‘ods’ files. Available options: auto, odf, calamine. [default: auto] [currently: auto] io.excel.ods.writerstringThe default Excel writer engine for ‘ods’ files. Available options: auto, odf. [default: auto] [currently: auto] io.excel.xls.readerstringThe default Excel reader engine for ‘xls’ files. Available options: auto, xlrd, calamine. [default: auto] [currently: auto] io.excel.xlsb.readerstringThe default Excel reader engine for ‘xlsb’ files. Available options: auto, pyxlsb, calamine. [default: auto] [currently: auto] io.excel.xlsm.readerstringThe default Excel reader engine for ‘xlsm’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsm.writerstringThe default Excel writer engine for ‘xlsm’ files. Available options: auto, openpyxl. [default: auto] [currently: auto] io.excel.xlsx.readerstringThe default Excel reader engine for ‘xlsx’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsx.writerstringThe default Excel writer engine for ‘xlsx’ files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto] io.hdf.default_formatformatdefault format writing format, if None, then put will default to ‘fixed’ and append will default to ‘table’ [default: None] [currently: None] io.hdf.dropna_tablebooleandrop ALL nan rows when appending to a table [default: False] [currently: False] io.parquet.enginestringThe default parquet reader/writer engine. Available options: ‘auto’, ‘pyarrow’, ‘fastparquet’, the default is ‘auto’ [default: auto] [currently: auto] io.sql.enginestringThe default sql reader/writer engine. Available options: ‘auto’, ‘sqlalchemy’, the default is ‘auto’ [default: auto] [currently: auto] mode.chained_assignmentstringRaise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn] mode.copy_on_writeboolUse new copy-view behaviour using Copy-on-Write. Defaults to False, unless overridden by the ‘PANDAS_COPY_ON_WRITE’ environment variable (if set to “1” for True, needs to be set before pandas is imported). [default: False] [currently: False] mode.data_managerstringInternal data manager type; can be “block” or “array”. Defaults to “block”, unless overridden by the ‘PANDAS_DATA_MANAGER’ environment variable (needs to be set before pandas is imported). [default: block] [currently: block] (Deprecated, use `` instead.) mode.sim_interactivebooleanWhether to simulate interactive mode for purposes of testing [default: False] [currently: False] mode.string_storagestringThe default storage for StringDtype. This option is ignored if future.infer_string is set to True. [default: python] [currently: python] mode.use_inf_as_nabooleanTrue means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). This option is deprecated in pandas 2.1.0 and will be removed in 3.0. [default: False] [currently: False] (Deprecated, use `` instead.) plotting.backendstrThe plotting backend to use. The default value is “matplotlib”, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib] plotting.matplotlib.register_convertersbool or ‘auto’.Whether to register converters with matplotlib’s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto] styler.format.decimalstrThe character representation for the decimal separator for floats and complex. [default: .] [currently: .] styler.format.escapestr, optionalWhether to escape certain characters according to the given context; html or latex. [default: None] [currently: None] styler.format.formatterstr, callable, dict, optionalA formatter object to be used as default within Styler.format. [default: None] [currently: None] styler.format.na_repstr, optionalThe string representation for values identified as missing. [default: None] [currently: None] styler.format.precisionintThe precision for floats and complex numbers. [default: 6] [currently: 6] styler.format.thousandsstr, optionalThe character representation for thousands separator for floats, int and complex. [default: None] [currently: None] styler.html.mathjaxboolIf False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True] styler.latex.environmentstrThe environment to replace \begin{table}. If “longtable” is used results in a specific longtable environment format. [default: None] [currently: None] styler.latex.hrulesboolWhether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False] styler.latex.multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. “|r” will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r] styler.latex.multirow_align{“c”, “t”, “b”}The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c] styler.render.encodingstrThe encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8] styler.render.max_columnsint, optionalThe maximum number of columns that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.max_elementsintThe maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144] styler.render.max_rowsint, optionalThe maximum number of rows that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.reprstrDetermine which output to use in Jupyter Notebook in {“html”, “latex”}. [default: html] [currently: html] styler.sparse.columnsboolWhether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True] styler.sparse.indexboolWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]","Parameters: patstrRegexp pattern. All matching keys will have their description displayed. _print_descbool, default TrueIf True (default) the description(s) will be printed to stdout. Otherwise, the description(s) will be returned as a unicode string (for testing). Returns: None by default, the description(s) as a unicode string if _print_desc is False","["">>> pd.describe_option('display.max_columns')  \ndisplay.max_columns : int\n    If max_cols is exceeded, switch to truncate view...""]"
975,..\pandas\reference\api\pandas.MultiIndex.set_codes.html,pandas.MultiIndex.set_codes,"MultiIndex.set_codes(codes, *, level=None, verify_integrity=True)[source]# Set new codes on MultiIndex. Defaults to returning new index.","Parameters: codessequence or list of sequenceNew codes to apply. levelint, level name, or sequence of int/level names (default None)Level(s) to set (None for all levels). verify_integritybool, default TrueIf True, checks that levels and codes are compatible. Returns: new index (of same type and class…etc) or NoneThe same type as the caller or None if inplace=True.","['>>> idx = pd.MultiIndex.from_tuples(\n...     [(1, ""one""), (1, ""two""), (2, ""one""), (2, ""two"")], names=[""foo"", ""bar""]\n... )\n>>> idx\nMultiIndex([(1, \'one\'),\n    (1, \'two\'),\n    (2, \'one\'),\n    (2, \'two\')],\n   names=[\'foo\', \'bar\'])', "">>> idx.set_codes([[1, 0, 1, 0], [0, 0, 1, 1]])\nMultiIndex([(2, 'one'),\n            (1, 'one'),\n            (2, 'two'),\n            (1, 'two')],\n           names=['foo', 'bar'])\n>>> idx.set_codes([1, 0, 1, 0], level=0)\nMultiIndex([(2, 'one'),\n            (1, 'two'),\n            (2, 'one'),\n            (1, 'two')],\n           names=['foo', 'bar'])\n>>> idx.set_codes([0, 0, 1, 1], level='bar')\nMultiIndex([(1, 'one'),\n            (1, 'one'),\n            (2, 'two'),\n            (2, 'two')],\n           names=['foo', 'bar'])\n>>> idx.set_codes([[1, 0, 1, 0], [0, 0, 1, 1]], level=[0, 1])\nMultiIndex([(2, 'one'),\n            (1, 'one'),\n            (2, 'two'),\n            (1, 'two')],\n           names=['foo', 'bar'])""]"
976,..\pandas\reference\api\pandas.Timestamp.round.html,pandas.Timestamp.round,"Timestamp.round(freq, ambiguous='raise', nonexistent='raise')# Round the Timestamp to the specified resolution. Notes If the Timestamp has a timezone, rounding will take place relative to the local (“wall”) time and re-localized to the same timezone. When rounding near daylight savings time, use nonexistent and ambiguous to control the re-localization behavior.","Parameters: freqstrFrequency string indicating the rounding resolution. ambiguousbool or {‘raise’, ‘NaT’}, default ‘raise’The behavior is as follows: bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates). ‘NaT’ will return NaT for an ambiguous time. ‘raise’ will raise an AmbiguousTimeError for an ambiguous time. nonexistent{‘raise’, ‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta}, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. ‘shift_forward’ will shift the nonexistent time forward to the closest existing time. ‘shift_backward’ will shift the nonexistent time backward to the closest existing time. ‘NaT’ will return NaT where there are nonexistent times. timedelta objects will shift nonexistent times by the timedelta. ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: a new Timestamp rounded to the given resolution of freq Raises: ValueError if the freq cannot be converted","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')"", "">>> ts.round(freq='h') # hour\nTimestamp('2020-03-14 16:00:00')"", "">>> ts.round(freq='min') # minute\nTimestamp('2020-03-14 15:33:00')"", "">>> ts.round(freq='s') # seconds\nTimestamp('2020-03-14 15:32:52')"", "">>> ts.round(freq='ms') # milliseconds\nTimestamp('2020-03-14 15:32:52.193000')"", "">>> ts.round(freq='5min')\nTimestamp('2020-03-14 15:35:00')"", "">>> ts.round(freq='1h30min')\nTimestamp('2020-03-14 15:00:00')"", '>>> pd.NaT.round()\nNaT', '>>> ts_tz = pd.Timestamp(""2021-10-31 01:30:00"").tz_localize(""Europe/Amsterdam"")', '>>> ts_tz.round(""h"", ambiguous=False)\nTimestamp(\'2021-10-31 02:00:00+0100\', tz=\'Europe/Amsterdam\')', '>>> ts_tz.round(""h"", ambiguous=True)\nTimestamp(\'2021-10-31 02:00:00+0200\', tz=\'Europe/Amsterdam\')']"
977,..\pandas\reference\api\pandas.Series.list.__getitem__.html,pandas.Series.list.__getitem__,Series.list.__getitem__(key)[source]# Index or slice lists in the Series.,Parameters: keyint | sliceIndex or slice of indices to access from each list. Returns: pandas.SeriesThe list at requested index.,"['>>> import pyarrow as pa\n>>> s = pd.Series(\n...     [\n...         [1, 2, 3],\n...         [3],\n...     ],\n...     dtype=pd.ArrowDtype(pa.list_(\n...         pa.int64()\n...     ))\n... )\n>>> s.list[0]\n0    1\n1    3\ndtype: int64[pyarrow]']"
978,..\pandas\reference\api\pandas.CategoricalIndex.codes.html,pandas.CategoricalIndex.codes,"property CategoricalIndex.codes[source]# The category codes of this categorical index. Codes are an array of integers which are the positions of the actual values in the categories array. There is no setter, use the other categorical methods and the normal item setter to change values in the categorical.",Returns: ndarray[int]A non-writable view of the codes array.,"["">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.codes\narray([0, 1], dtype=int8)"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'])\n>>> ci.codes\narray([0, 1, 2, 0, 1, 2], dtype=int8)"", "">>> ci = pd.CategoricalIndex(['a', 'c'], categories=['c', 'b', 'a'])\n>>> ci.codes\narray([2, 0], dtype=int8)""]"
979,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.freqstr.html,pandas.tseries.offsets.QuarterEnd.freqstr,QuarterEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
980,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_start.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_quarter_start,CustomBusinessMonthEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
981,..\pandas\reference\api\pandas.DataFrame.boxplot.html,pandas.DataFrame.boxplot,"DataFrame.boxplot(column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, backend=None, **kwargs)[source]# Make a box plot from DataFrame columns. Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots. For further details see Wikipedia’s entry for boxplot. Notes The return type depends on the return_type parameter: ‘axes’ : object of class matplotlib.axes.Axes ‘dict’ : dict of matplotlib.lines.Line2D objects ‘both’ : a namedtuple with structure (ax, lines) For data grouped with by, return a Series of the above or a numpy array: Series array (for return_type = None) Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned.","Parameters: columnstr or list of str, optionalColumn name or list of names, or vector. Can be any valid input to pandas.DataFrame.groupby(). bystr or array-like, optionalColumn in the DataFrame to pandas.DataFrame.groupby(). One box-plot will be done per value of columns in by. axobject of class matplotlib.axes.Axes, optionalThe matplotlib axes to be used by boxplot. fontsizefloat or strTick label font size in points or as a string (e.g., large). rotfloat, default 0The rotation angle of labels (in degrees) with respect to the screen coordinate system. gridbool, default TrueSetting this to True will show the grid. figsizeA tuple (width, height) in inchesThe size of the figure to create in matplotlib. layouttuple (rows, columns), optionalFor example, (3, 5) will display the subplots using 3 rows and 5 columns, starting from the top-left. return_type{‘axes’, ‘dict’, ‘both’} or None, default ‘axes’The kind of object to return. The default is axes. ‘axes’ returns the matplotlib axes the boxplot is drawn on. ‘dict’ returns a dictionary whose values are the matplotlib Lines of the boxplot. ‘both’ returns a namedtuple with the axes and dict. when grouping with by, a Series mapping columns to return_type is returned. If return_type is None, a NumPy array of axes with the same shape as layout is returned. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. **kwargsAll other plotting keyword arguments to be passed to matplotlib.pyplot.boxplot(). Returns: resultSee Notes.","["">>> np.random.seed(1234)\n>>> df = pd.DataFrame(np.random.randn(10, 4),\n...                   columns=['Col1', 'Col2', 'Col3', 'Col4'])\n>>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])"", "">>> df = pd.DataFrame(np.random.randn(10, 2),\n...                   columns=['Col1', 'Col2'])\n>>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n...                      'B', 'B', 'B', 'B', 'B'])\n>>> boxplot = df.boxplot(by='X')"", "">>> df = pd.DataFrame(np.random.randn(10, 3),\n...                   columns=['Col1', 'Col2', 'Col3'])\n>>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n...                      'B', 'B', 'B', 'B', 'B'])\n>>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A',\n...                      'B', 'A', 'B', 'A', 'B'])\n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y'])"", "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      layout=(2, 1))"", '>>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15)', "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], return_type='axes')\n>>> type(boxplot)\n<class 'matplotlib.axes._axes.Axes'>"", "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      return_type='axes')\n>>> type(boxplot)\n<class 'pandas.core.series.Series'>"", "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      return_type=None)\n>>> type(boxplot)\n<class 'numpy.ndarray'>""]"
982,..\pandas\reference\api\pandas.errors.AbstractMethodError.html,pandas.errors.AbstractMethodError,"exception pandas.errors.AbstractMethodError(class_instance, methodtype='method')[source]# Raise this error instead of NotImplementedError for abstract methods.",No parameters found,"['>>> class Foo:\n...     @classmethod\n...     def classmethod(cls):\n...         raise pd.errors.AbstractMethodError(cls, methodtype=""classmethod"")\n...     def method(self):\n...         raise pd.errors.AbstractMethodError(self)\n>>> test = Foo.classmethod()\nTraceback (most recent call last):\nAbstractMethodError: This classmethod must be defined in the concrete class Foo', '>>> test2 = Foo().method()\nTraceback (most recent call last):\nAbstractMethodError: This classmethod must be defined in the concrete class Foo']"
983,..\pandas\reference\api\pandas.MultiIndex.set_levels.html,pandas.MultiIndex.set_levels,"MultiIndex.set_levels(levels, *, level=None, verify_integrity=True)[source]# Set new levels on MultiIndex. Defaults to returning new index.","Parameters: levelssequence or list of sequenceNew level(s) to apply. levelint, level name, or sequence of int/level names (default None)Level(s) to set (None for all levels). verify_integritybool, default TrueIf True, checks that levels and codes are compatible. Returns: MultiIndex","['>>> idx = pd.MultiIndex.from_tuples(\n...     [\n...         (1, ""one""),\n...         (1, ""two""),\n...         (2, ""one""),\n...         (2, ""two""),\n...         (3, ""one""),\n...         (3, ""two"")\n...     ],\n...     names=[""foo"", ""bar""]\n... )\n>>> idx\nMultiIndex([(1, \'one\'),\n    (1, \'two\'),\n    (2, \'one\'),\n    (2, \'two\'),\n    (3, \'one\'),\n    (3, \'two\')],\n   names=[\'foo\', \'bar\'])', "">>> idx.set_levels([['a', 'b', 'c'], [1, 2]])\nMultiIndex([('a', 1),\n            ('a', 2),\n            ('b', 1),\n            ('b', 2),\n            ('c', 1),\n            ('c', 2)],\n           names=['foo', 'bar'])\n>>> idx.set_levels(['a', 'b', 'c'], level=0)\nMultiIndex([('a', 'one'),\n            ('a', 'two'),\n            ('b', 'one'),\n            ('b', 'two'),\n            ('c', 'one'),\n            ('c', 'two')],\n           names=['foo', 'bar'])\n>>> idx.set_levels(['a', 'b'], level='bar')\nMultiIndex([(1, 'a'),\n            (1, 'b'),\n            (2, 'a'),\n            (2, 'b'),\n            (3, 'a'),\n            (3, 'b')],\n           names=['foo', 'bar'])"", "">>> idx.set_levels([['a', 'b', 'c'], [1, 2, 3, 4]], level=[0, 1])\nMultiIndex([('a', 1),\n    ('a', 2),\n    ('b', 1),\n    ('b', 2),\n    ('c', 1),\n    ('c', 2)],\n   names=['foo', 'bar'])\n>>> idx.set_levels([['a', 'b', 'c'], [1, 2, 3, 4]], level=[0, 1]).levels\nFrozenList([['a', 'b', 'c'], [1, 2, 3, 4]])""]"
984,..\pandas\reference\api\pandas.Timestamp.second.html,pandas.Timestamp.second,Timestamp.second#,No parameters found,[]
985,..\pandas\reference\api\pandas.CategoricalIndex.equals.html,pandas.CategoricalIndex.equals,CategoricalIndex.equals(other)[source]# Determine if two CategoricalIndex objects contain the same elements.,"Returns: boolTrue if two pandas.CategoricalIndex objects have equal elements, False otherwise.","["">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'])\n>>> ci2 = pd.CategoricalIndex(pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c']))\n>>> ci.equals(ci2)\nTrue"", "">>> ci3 = pd.CategoricalIndex(['c', 'b', 'a', 'a', 'b', 'c'])\n>>> ci.equals(ci3)\nFalse"", '>>> ci4 = ci.as_ordered()\n>>> ci.equals(ci4)\nFalse', "">>> ci5 = ci.set_categories(['a', 'b', 'c', 'd'])\n>>> ci.equals(ci5)\nFalse"", "">>> ci6 = ci.set_categories(['b', 'c', 'a'])\n>>> ci.equals(ci6)\nTrue\n>>> ci_ordered = pd.CategoricalIndex(['a', 'b', 'c', 'a', 'b', 'c'],\n...                                  ordered=True)\n>>> ci2_ordered = ci_ordered.set_categories(['b', 'c', 'a'])\n>>> ci_ordered.equals(ci2_ordered)\nFalse""]"
986,..\pandas\reference\api\pandas.DataFrame.clip.html,pandas.DataFrame.clip,"DataFrame.clip(lower=None, upper=None, *, axis=None, inplace=False, **kwargs)[source]# Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.","Parameters: lowerfloat or array-like, default NoneMinimum threshold value. All values below this threshold will be set to it. A missing threshold (e.g NA) will not clip the value. upperfloat or array-like, default NoneMaximum threshold value. All values above this threshold will be set to it. A missing threshold (e.g NA) will not clip the value. axis{{0 or ‘index’, 1 or ‘columns’, None}}, default NoneAlign object with lower and upper along the given axis. For Series this parameter is unused and defaults to None. inplacebool, default FalseWhether to perform the operation in place on the data. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with numpy. Returns: Series or DataFrame or NoneSame type as calling object with the values outside the clip boundaries replaced or None if inplace=True.","["">>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]}\n>>> df = pd.DataFrame(data)\n>>> df\n   col_0  col_1\n0      9     -2\n1     -3     -7\n2      0      6\n3     -1      8\n4      5     -5"", '>>> df.clip(-4, 6)\n   col_0  col_1\n0      6     -2\n1     -3     -4\n2      0      6\n3     -1      6\n4      5     -4', '>>> df.clip([-2, -1], [4, 5])\n    col_0  col_1\n0      4     -1\n1     -2     -1\n2      0      5\n3     -1      5\n4      4     -1', '>>> t = pd.Series([2, -4, -1, 6, 3])\n>>> t\n0    2\n1   -4\n2   -1\n3    6\n4    3\ndtype: int64', '>>> df.clip(t, t + 4, axis=0)\n   col_0  col_1\n0      6      2\n1     -3     -4\n2      0      3\n3      6      8\n4      5      3', '>>> t = pd.Series([2, -4, np.nan, 6, 3])\n>>> t\n0    2.0\n1   -4.0\n2    NaN\n3    6.0\n4    3.0\ndtype: float64', '>>> df.clip(t, axis=0)\ncol_0  col_1\n0      9      2\n1     -3     -4\n2      0      6\n3      6      8\n4      5      3']"
987,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_end.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_end,CustomBusinessMonthEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
988,..\pandas\reference\api\pandas.MultiIndex.sortlevel.html,pandas.MultiIndex.sortlevel,"MultiIndex.sortlevel(level=0, ascending=True, sort_remaining=True, na_position='first')[source]# Sort MultiIndex at the requested level. The result will respect the original ordering of the associated factor at that level.","Parameters: levellist-like, int or str, default 0If a string is given, must be a name of the level. If list-like must be names or ints of levels. ascendingbool, default TrueFalse to sort in descending order. Can also be a list to specify a directed ordering. sort_remainingsort by the remaining levels after level na_position{‘first’ or ‘last’}, default ‘first’Argument ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end. Added in version 2.1.0. Returns: sorted_indexpd.MultiIndexResulting index. indexernp.ndarray[np.intp]Indices of output values in original index.","['>>> mi = pd.MultiIndex.from_arrays([[0, 0], [2, 1]])\n>>> mi\nMultiIndex([(0, 2),\n            (0, 1)],\n           )', '>>> mi.sortlevel()\n(MultiIndex([(0, 1),\n            (0, 2)],\n           ), array([1, 0]))', '>>> mi.sortlevel(sort_remaining=False)\n(MultiIndex([(0, 2),\n            (0, 1)],\n           ), array([0, 1]))', '>>> mi.sortlevel(1)\n(MultiIndex([(0, 1),\n            (0, 2)],\n           ), array([1, 0]))', '>>> mi.sortlevel(1, ascending=False)\n(MultiIndex([(0, 2),\n            (0, 1)],\n           ), array([0, 1]))']"
989,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.html,pandas.tseries.offsets.QuarterEnd,"class pandas.tseries.offsets.QuarterEnd# DateOffset increments between Quarter end dates. startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, … startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, … startingMonth = 3 corresponds to dates like 3/31/2007, 6/30/2007, … Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code startingMonth","Parameters: nint, default 1The number of quarters represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. startingMonthint, default 3A specific integer for the month of the year from which we start quarters.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.QuarterEnd()\nTimestamp('2022-03-31 00:00:00')""]"
990,..\pandas\reference\api\pandas.errors.AttributeConflictWarning.html,pandas.errors.AttributeConflictWarning,exception pandas.errors.AttributeConflictWarning[source]# Warning raised when index attributes conflict when using HDFStore. Occurs when attempting to append an index with a different name than the existing index on an HDFStore or attempting to append an index with a different frequency than the existing index on an HDFStore.,No parameters found,"["">>> idx1 = pd.Index(['a', 'b'], name='name1')\n>>> df1 = pd.DataFrame([[1, 2], [3, 4]], index=idx1)\n>>> df1.to_hdf('file', 'data', 'w', append=True)  \n>>> idx2 = pd.Index(['c', 'd'], name='name2')\n>>> df2 = pd.DataFrame([[5, 6], [7, 8]], index=idx2)\n>>> df2.to_hdf('file', 'data', 'a', append=True)  \nAttributeConflictWarning: the [index_name] attribute of the existing index is\n[name1] which conflicts with the new [name2]...""]"
991,..\pandas\reference\api\pandas.Series.loc.html,pandas.Series.loc,"property Series.loc[source]# Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are: A single label, e.g. 5 or 'a', (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c']. A slice object with labels, e.g. 'a':'f'. Warning Note that contrary to usual python slices, both the start and the stop are included A boolean array of the same length as the axis being sliced, e.g. [True, False, True]. An alignable boolean Series. The index of the key will be aligned before masking. An alignable Index. The Index of the returned selection will be the input. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above) See more at Selection by Label.",Raises: KeyErrorIf any items are not found. IndexingErrorIf an indexed key is passed and its index is unalignable to the frame index.,"["">>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=['cobra', 'viper', 'sidewinder'],\n...                   columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8"", "">>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64"", "">>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8"", "">>> df.loc['cobra', 'shield']\n2"", "">>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64"", '>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8', "">>> df.loc[pd.Series([False, True, False],\n...                  index=['viper', 'sidewinder', 'cobra'])]\n                     max_speed  shield\nsidewinder          7       8"", '>>> df.loc[pd.Index([""cobra"", ""viper""], name=""foo"")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5', "">>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8"", "">>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7"", "">>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n            max_speed  shield\nviper          4       5"", "">>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n            max_speed  shield\ncobra               1       2\nsidewinder          7       8"", "">>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8"", "">>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50"", "">>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50"", "">>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50"", "">>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0"", '>>> df.loc[""viper"", ""shield""] += 5\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       5\nsidewinder          0       0', '>>> shuffled_df = df.loc[[""viper"", ""cobra"", ""sidewinder""]]\n>>> df.loc[:] += shuffled_df\n>>> df\n            max_speed  shield\ncobra              60      20\nviper               0      10\nsidewinder          0       0', "">>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8"", '>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8', "">>> tuples = [\n...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...     ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...           [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36"", "">>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4"", "">>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64"", "">>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64"", "">>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4"", "">>> df.loc[('cobra', 'mark i'), 'shield']\n2"", "">>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36"", "">>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1""]"
992,..\pandas\reference\api\pandas.Timestamp.strftime.html,pandas.Timestamp.strftime,Timestamp.strftime(format)# Return a formatted string of the Timestamp.,Parameters: formatstrFormat string to convert Timestamp to string. See strftime documentation for more information on the format string: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.,"["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.strftime('%Y-%m-%d %X')\n'2020-03-14 15:32:52'""]"
993,..\pandas\reference\api\pandas.CategoricalIndex.html,pandas.CategoricalIndex,"class pandas.CategoricalIndex(data=None, categories=None, ordered=None, dtype=None, copy=False, name=None)[source]# Index based on an underlying Categorical. CategoricalIndex, like Categorical, can only take on a limited, and usually fixed, number of possible values (categories). Also, like Categorical, it might have an order, but numerical operations (additions, divisions, …) are not possible. Attributes codes The category codes of this categorical index. categories The categories of this categorical. ordered Whether the categories have an ordered relationship. Methods rename_categories(*args, **kwargs) Rename categories. reorder_categories(*args, **kwargs) Reorder categories as specified in new_categories. add_categories(*args, **kwargs) Add new categories. remove_categories(*args, **kwargs) Remove the specified categories. remove_unused_categories(*args, **kwargs) Remove categories which are not used. set_categories(*args, **kwargs) Set the categories to the specified new categories. as_ordered(*args, **kwargs) Set the Categorical to be ordered. as_unordered(*args, **kwargs) Set the Categorical to be unordered. map(mapper[, na_action]) Map values using input an input mapping or function. Raises: ValueErrorIf the categories do not validate. TypeErrorIf an explicit ordered=True is given but no categories and the values are not sortable. Notes See the user guide for more.","Parameters: dataarray-like (1-dimensional)The values of the categorical. If categories are given, values not in categories will be replaced with NaN. categoriesindex-like, optionalThe categories for the categorical. Items need to be unique. If the categories are not given here (and also not in dtype), they will be inferred from the data. orderedbool, optionalWhether or not this categorical is treated as an ordered categorical. If not given here or in dtype, the resulting categorical will be unordered. dtypeCategoricalDtype or “category”, optionalIf CategoricalDtype, cannot be used together with categories or ordered. copybool, default FalseMake a copy of input ndarray. nameobject, optionalName to be stored in the index.","['>>> pd.CategoricalIndex([""a"", ""b"", ""c"", ""a"", ""b"", ""c""])\nCategoricalIndex([\'a\', \'b\', \'c\', \'a\', \'b\', \'c\'],\n                 categories=[\'a\', \'b\', \'c\'], ordered=False, dtype=\'category\')', '>>> c = pd.Categorical([""a"", ""b"", ""c"", ""a"", ""b"", ""c""])\n>>> pd.CategoricalIndex(c)\nCategoricalIndex([\'a\', \'b\', \'c\', \'a\', \'b\', \'c\'],\n                 categories=[\'a\', \'b\', \'c\'], ordered=False, dtype=\'category\')', '>>> ci = pd.CategoricalIndex(\n...     [""a"", ""b"", ""c"", ""a"", ""b"", ""c""], ordered=True, categories=[""c"", ""b"", ""a""]\n... )\n>>> ci\nCategoricalIndex([\'a\', \'b\', \'c\', \'a\', \'b\', \'c\'],\n                 categories=[\'c\', \'b\', \'a\'], ordered=True, dtype=\'category\')\n>>> ci.min()\n\'c\'']"
994,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_start.html,pandas.tseries.offsets.CustomBusinessMonthEnd.is_year_start,CustomBusinessMonthEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
995,..\pandas\reference\api\pandas.DataFrame.columns.html,pandas.DataFrame.columns,DataFrame.columns# The column labels of the DataFrame.,No parameters found,"["">>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n>>> df\n     A  B\n0    1  3\n1    2  4\n>>> df.columns\nIndex(['A', 'B'], dtype='object')""]"
996,..\pandas\reference\api\pandas.errors.CategoricalConversionWarning.html,pandas.errors.CategoricalConversionWarning,exception pandas.errors.CategoricalConversionWarning[source]# Warning is raised when reading a partial labeled Stata file using a iterator.,No parameters found,"["">>> from pandas.io.stata import StataReader\n>>> with StataReader('dta_file', chunksize=2) as reader: \n...   for i, block in enumerate(reader):\n...      print(i, block)\n... # CategoricalConversionWarning: One or more series with value labels...""]"
997,..\pandas\reference\api\pandas.Series.lt.html,pandas.Series.lt,"Series.lt(other, level=None, fill_value=None, axis=0)[source]# Return Less than of series and other, element-wise (binary operator lt). Equivalent to series < other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ne    1.0\ndtype: float64\n>>> b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n>>> b\na    0.0\nb    1.0\nc    2.0\nd    NaN\nf    1.0\ndtype: float64\n>>> a.lt(b, fill_value=0)\na    False\nb    False\nc     True\nd    False\ne    False\nf     True\ndtype: bool""]"
998,..\pandas\reference\api\pandas.MultiIndex.swaplevel.html,pandas.MultiIndex.swaplevel,"MultiIndex.swaplevel(i=-2, j=-1)[source]# Swap level i with level j. Calling this method does not change the ordering of the values.","Parameters: iint, str, default -2First level of index to be swapped. Can pass level name as string. Type of parameters can be mixed. jint, str, default -1Second level of index to be swapped. Can pass level name as string. Type of parameters can be mixed. Returns: MultiIndexA new MultiIndex.","["">>> mi = pd.MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n...                    codes=[[0, 0, 1, 1], [0, 1, 0, 1]])\n>>> mi\nMultiIndex([('a', 'bb'),\n            ('a', 'aa'),\n            ('b', 'bb'),\n            ('b', 'aa')],\n           )\n>>> mi.swaplevel(0, 1)\nMultiIndex([('bb', 'a'),\n            ('aa', 'a'),\n            ('bb', 'b'),\n            ('aa', 'b')],\n           )""]"
999,..\pandas\reference\api\pandas.Timestamp.strptime.html,pandas.Timestamp.strptime,"classmethod Timestamp.strptime(string, format)# Function is not implemented. Use pd.to_datetime().",No parameters found,"['>>> pd.Timestamp.strptime(""2023-01-01"", ""%d/%m/%y"")\nTraceback (most recent call last):\nNotImplementedError']"
1000,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_anchored.html,pandas.tseries.offsets.QuarterEnd.is_anchored,QuarterEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1001,..\pandas\reference\api\pandas.CategoricalIndex.map.html,pandas.CategoricalIndex.map,"CategoricalIndex.map(mapper, na_action=None)[source]# Map values using input an input mapping or function. Maps the values (their categories, not the codes) of the index to new categories. If the mapping correspondence is one-to-one the result is a CategoricalIndex which has the same order property as the original, otherwise an Index is returned. If a dict or Series is used any unmapped category is mapped to NaN. Note that if this happens an Index will be returned.","Parameters: mapperfunction, dict, or SeriesMapping correspondence. Returns: pandas.CategoricalIndex or pandas.IndexMapped index.","["">>> idx = pd.CategoricalIndex(['a', 'b', 'c'])\n>>> idx\nCategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'],\n                  ordered=False, dtype='category')\n>>> idx.map(lambda x: x.upper())\nCategoricalIndex(['A', 'B', 'C'], categories=['A', 'B', 'C'],\n                 ordered=False, dtype='category')\n>>> idx.map({'a': 'first', 'b': 'second', 'c': 'third'})\nCategoricalIndex(['first', 'second', 'third'], categories=['first',\n                 'second', 'third'], ordered=False, dtype='category')"", "">>> idx = pd.CategoricalIndex(['a', 'b', 'c'], ordered=True)\n>>> idx\nCategoricalIndex(['a', 'b', 'c'], categories=['a', 'b', 'c'],\n                 ordered=True, dtype='category')\n>>> idx.map({'a': 3, 'b': 2, 'c': 1})\nCategoricalIndex([3, 2, 1], categories=[3, 2, 1], ordered=True,\n                 dtype='category')"", "">>> idx.map({'a': 'first', 'b': 'second', 'c': 'first'})\nIndex(['first', 'second', 'first'], dtype='object')"", "">>> idx.map({'a': 'first', 'b': 'second'})\nIndex(['first', 'second', nan], dtype='object')""]"
1002,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.kwds.html,pandas.tseries.offsets.CustomBusinessMonthEnd.kwds,CustomBusinessMonthEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1003,..\pandas\reference\api\pandas.errors.ChainedAssignmentError.html,pandas.errors.ChainedAssignmentError,"exception pandas.errors.ChainedAssignmentError[source]# Warning raised when trying to set using chained assignment. When the mode.copy_on_write option is enabled, chained assignment can never work. In such a situation, we are always setting into a temporary object that is the result of an indexing operation (getitem), which under Copy-on-Write always behaves as a copy. Thus, assigning through a chain can never update the original Series or DataFrame. For more information on view vs. copy, see the user guide.",No parameters found,"['>>> pd.options.mode.copy_on_write = True\n>>> df = pd.DataFrame({\'A\': [1, 1, 1, 2, 2]}, columns=[\'A\'])\n>>> df[""A""][0:3] = 10 \n... # ChainedAssignmentError: ...\n>>> pd.options.mode.copy_on_write = False']"
1004,..\pandas\reference\api\pandas.Timestamp.time.html,pandas.Timestamp.time,Timestamp.time()# Return time object with same time but with tzinfo=None.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00')\n>>> ts\nTimestamp('2023-01-01 10:00:00')\n>>> ts.time()\ndatetime.time(10, 0)""]"
1005,..\pandas\reference\api\pandas.CategoricalIndex.ordered.html,pandas.CategoricalIndex.ordered,property CategoricalIndex.ordered[source]# Whether the categories have an ordered relationship.,No parameters found,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.ordered\nFalse"", "">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.ordered\nTrue"", "">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.ordered\nTrue"", "">>> cat = pd.Categorical(['a', 'b'], ordered=False)\n>>> cat.ordered\nFalse"", "">>> ci = pd.CategoricalIndex(['a', 'b'], ordered=True)\n>>> ci.ordered\nTrue"", "">>> ci = pd.CategoricalIndex(['a', 'b'], ordered=False)\n>>> ci.ordered\nFalse""]"
1006,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_month_end.html,pandas.tseries.offsets.QuarterEnd.is_month_end,QuarterEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1007,..\pandas\reference\api\pandas.MultiIndex.to_flat_index.html,pandas.MultiIndex.to_flat_index,MultiIndex.to_flat_index()[source]# Convert a MultiIndex to an Index of Tuples containing the level values. Notes This method will simply return the caller if called by anything other than a MultiIndex.,Returns: pd.IndexIndex with the MultiIndex data represented in Tuples.,"["">>> index = pd.MultiIndex.from_product(\n...     [['foo', 'bar'], ['baz', 'qux']],\n...     names=['a', 'b'])\n>>> index.to_flat_index()\nIndex([('foo', 'baz'), ('foo', 'qux'),\n       ('bar', 'baz'), ('bar', 'qux')],\n      dtype='object')""]"
1008,..\pandas\reference\api\pandas.DataFrame.combine.html,pandas.DataFrame.combine,"DataFrame.combine(other, func, fill_value=None, overwrite=True)[source]# Perform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two.","Parameters: otherDataFrameThe DataFrame to merge column-wise. funcfunctionFunction that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns. fill_valuescalar value, default NoneThe value to fill NaNs with prior to passing any column to the merge func. overwritebool, default TrueIf True, columns in self that do not exist in other will be overwritten with NaNs. Returns: DataFrameCombination of the provided DataFrames.","["">>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\n>>> df1.combine(df2, take_smaller)\n   A  B\n0  0  3\n1  0  3"", "">>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine(df2, np.minimum)\n   A  B\n0  1  2\n1  0  3"", "">>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine(df2, take_smaller, fill_value=-5)\n   A    B\n0  0 -5.0\n1  0  4.0"", "">>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]})\n>>> df1.combine(df2, take_smaller, fill_value=-5)\n    A    B\n0  0 -5.0\n1  0  3.0"", "">>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]})\n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2])\n>>> df1.combine(df2, take_smaller)\n     A    B     C\n0  NaN  NaN   NaN\n1  NaN  3.0 -10.0\n2  NaN  3.0   1.0"", '>>> df1.combine(df2, take_smaller, overwrite=False)\n     A    B     C\n0  0.0  NaN   NaN\n1  0.0  3.0 -10.0\n2  NaN  3.0   1.0', "">>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2])\n>>> df2.combine(df1, take_smaller)\n   A    B   C\n0  0.0  NaN NaN\n1  0.0  3.0 NaN\n2  NaN  3.0 NaN"", '>>> df2.combine(df1, take_smaller, overwrite=False)\n     A    B   C\n0  0.0  NaN NaN\n1  0.0  3.0 1.0\n2  NaN  3.0 1.0']"
1009,..\pandas\reference\api\pandas.Series.map.html,pandas.Series.map,"Series.map(arg, na_action=None)[source]# Map values of Series according to an input mapping or function. Used for substituting each value in a Series with another value, that may be derived from a function, a dict or a Series. Notes When arg is a dictionary, values in Series that are not in the dictionary (as keys) are converted to NaN. However, if the dictionary is a dict subclass that defines __missing__ (i.e. provides a method for default values), then this default is used rather than NaN.","Parameters: argfunction, collections.abc.Mapping subclass or SeriesMapping correspondence. na_action{None, ‘ignore’}, default NoneIf ‘ignore’, propagate NaN values, without passing them to the mapping correspondence. Returns: SeriesSame index as caller.","["">>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n>>> s\n0      cat\n1      dog\n2      NaN\n3   rabbit\ndtype: object"", "">>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n0   kitten\n1    puppy\n2      NaN\n3      NaN\ndtype: object"", "">>> s.map('I am a {}'.format)\n0       I am a cat\n1       I am a dog\n2       I am a nan\n3    I am a rabbit\ndtype: object"", "">>> s.map('I am a {}'.format, na_action='ignore')\n0     I am a cat\n1     I am a dog\n2            NaN\n3  I am a rabbit\ndtype: object""]"
1010,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.m_offset.html,pandas.tseries.offsets.CustomBusinessMonthEnd.m_offset,CustomBusinessMonthEnd.m_offset#,No parameters found,[]
1011,..\pandas\reference\api\pandas.errors.ClosedFileError.html,pandas.errors.ClosedFileError,exception pandas.errors.ClosedFileError[source]# Exception is raised when trying to perform an operation on a closed HDFStore file.,No parameters found,"["">>> store = pd.HDFStore('my-store', 'a') \n>>> store.close() \n>>> store.keys() \n... # ClosedFileError: my-store file is not open!""]"
1012,..\pandas\reference\api\pandas.Timestamp.timestamp.html,pandas.Timestamp.timestamp,Timestamp.timestamp()# Return POSIX timestamp as float.,No parameters found,"["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548')\n>>> ts.timestamp()\n1584199972.192548""]"
1013,..\pandas\reference\api\pandas.errors.CSSWarning.html,pandas.errors.CSSWarning,exception pandas.errors.CSSWarning[source]# Warning is raised when converting css styling fails. This can be due to the styling not having an equivalent value or because the styling isn’t properly formatted.,No parameters found,"["">>> df = pd.DataFrame({'A': [1, 1, 1]})\n>>> df.style.applymap(\n...     lambda x: 'background-color: blueGreenRed;'\n... ).to_excel('styled.xlsx')  \nCSSWarning: Unhandled color format: 'blueGreenRed'\n>>> df.style.applymap(\n...     lambda x: 'border: 1px solid red red;'\n... ).to_excel('styled.xlsx')  \nCSSWarning: Unhandled color format: 'blueGreenRed'""]"
1014,..\pandas\reference\api\pandas.DataFrame.combine_first.html,pandas.DataFrame.combine_first,"DataFrame.combine_first(other)[source]# Update null elements with value in the same location in other. Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two. The resulting dataframe contains the ‘first’ dataframe values and overrides the second one values where both first.loc[index, col] and second.loc[index, col] are not missing values, upon calling first.combine_first(second).",Parameters: otherDataFrameProvided DataFrame to use to fill null values. Returns: DataFrameThe result of combining the provided DataFrame with the other object.,"["">>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]})\n>>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]})\n>>> df1.combine_first(df2)\n     A    B\n0  1.0  3.0\n1  0.0  4.0"", "">>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]})\n>>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2])\n>>> df1.combine_first(df2)\n     A    B    C\n0  NaN  4.0  NaN\n1  0.0  3.0  1.0\n2  NaN  3.0  1.0""]"
1015,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_month_start.html,pandas.tseries.offsets.QuarterEnd.is_month_start,QuarterEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1016,..\pandas\reference\api\pandas.CategoricalIndex.remove_categories.html,pandas.CategoricalIndex.remove_categories,"CategoricalIndex.remove_categories(*args, **kwargs)[source]# Remove the specified categories. removals must be included in the old categories. Values which were in the removed categories will be set to NaN",Parameters: removalscategory or list of categoriesThe categories which should be removed. Returns: CategoricalCategorical with removed categories. Raises: ValueErrorIf the removals are not contained in the categories,"["">>> c = pd.Categorical(['a', 'c', 'b', 'c', 'd'])\n>>> c\n['a', 'c', 'b', 'c', 'd']\nCategories (4, object): ['a', 'b', 'c', 'd']"", "">>> c.remove_categories(['d', 'a'])\n[NaN, 'c', 'b', 'c', NaN]\nCategories (2, object): ['b', 'c']""]"
1017,..\pandas\reference\api\pandas.Series.mask.html,pandas.Series.mask,"Series.mask(cond, other=<no_default>, *, inplace=False, axis=None, level=None)[source]# Replace values where the condition is True. Notes The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with True. The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2). For further details and examples see the mask documentation in indexing. The dtype of the object takes precedence. The fill value is casted to the object’s dtype, if this can be done losslessly.","Parameters: condbool Series/DataFrame, array-like, or callableWhere cond is False, keep the original value. Where True, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn’t check it). otherscalar, Series/DataFrame, or callableEntries where cond is True are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn’t check it). If not specified, entries will be filled with the corresponding NULL value (np.nan for numpy dtypes, pd.NA for extension dtypes). inplacebool, default FalseWhether to perform the operation in place on the data. axisint, default NoneAlignment axis if needed. For Series this parameter is unused and defaults to 0. levelint, default NoneAlignment level if needed. Returns: Same type as caller or None if inplace=True.","['>>> s = pd.Series(range(5))\n>>> s.where(s > 0)\n0    NaN\n1    1.0\n2    2.0\n3    3.0\n4    4.0\ndtype: float64\n>>> s.mask(s > 0)\n0    0.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', '>>> s = pd.Series(range(5))\n>>> t = pd.Series([True, False])\n>>> s.where(t, 99)\n0     0\n1    99\n2    99\n3    99\n4    99\ndtype: int64\n>>> s.mask(t, 99)\n0    99\n1     1\n2    99\n3    99\n4    99\ndtype: int64', '>>> s.where(s > 1, 10)\n0    10\n1    10\n2    2\n3    3\n4    4\ndtype: int64\n>>> s.mask(s > 1, 10)\n0     0\n1     1\n2    10\n3    10\n4    10\ndtype: int64', "">>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n>>> df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n>>> m = df % 3 == 0\n>>> df.where(m, -df)\n   A  B\n0  0 -1\n1 -2  3\n2 -4 -5\n3  6 -7\n4 -8  9\n>>> df.where(m, -df) == np.where(m, df, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n>>> df.where(m, -df) == df.mask(~m, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True""]"
1018,..\pandas\reference\api\pandas.MultiIndex.to_frame.html,pandas.MultiIndex.to_frame,"MultiIndex.to_frame(index=True, name=<no_default>, allow_duplicates=False)[source]# Create a DataFrame with the levels of the MultiIndex as columns. Column ordering is determined by the DataFrame constructor with data as a dict.","Parameters: indexbool, default TrueSet the index of the returned DataFrame as the original MultiIndex. namelist / sequence of str, optionalThe passed names should substitute index level names. allow_duplicatesbool, optional default FalseAllow duplicate column labels to be created. Added in version 1.5.0. Returns: DataFrame","["">>> mi = pd.MultiIndex.from_arrays([['a', 'b'], ['c', 'd']])\n>>> mi\nMultiIndex([('a', 'c'),\n            ('b', 'd')],\n           )"", '>>> df = mi.to_frame()\n>>> df\n     0  1\na c  a  c\nb d  b  d', '>>> df = mi.to_frame(index=False)\n>>> df\n   0  1\n0  a  c\n1  b  d', "">>> df = mi.to_frame(name=['x', 'y'])\n>>> df\n     x  y\na c  a  c\nb d  b  d""]"
1019,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.n.html,pandas.tseries.offsets.CustomBusinessMonthEnd.n,CustomBusinessMonthEnd.n#,No parameters found,[]
1020,..\pandas\reference\api\pandas.Timestamp.timetuple.html,pandas.Timestamp.timetuple,"Timestamp.timetuple()# Return time tuple, compatible with time.localtime().",No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00')\n>>> ts\nTimestamp('2023-01-01 10:00:00')\n>>> ts.timetuple()\ntime.struct_time(tm_year=2023, tm_mon=1, tm_mday=1,\ntm_hour=10, tm_min=0, tm_sec=0, tm_wday=6, tm_yday=1, tm_isdst=-1)""]"
1021,..\pandas\reference\api\pandas.errors.DatabaseError.html,pandas.errors.DatabaseError,exception pandas.errors.DatabaseError[source]# Error is raised when executing sql with bad syntax or sql that throws an error.,No parameters found,"['>>> from sqlite3 import connect\n>>> conn = connect(\':memory:\')\n>>> pd.read_sql(\'select * test\', conn) \n... # DatabaseError: Execution failed on sql \'test\': near ""test"": syntax error']"
1022,..\pandas\reference\api\pandas.Timestamp.timetz.html,pandas.Timestamp.timetz,Timestamp.timetz()# Return time object with same time and tzinfo.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00', tz='Europe/Brussels')\n>>> ts\nTimestamp('2023-01-01 10:00:00+0100', tz='Europe/Brussels')\n>>> ts.timetz()\ndatetime.time(10, 0, tzinfo=<DstTzInfo 'Europe/Brussels' CET+1:00:00 STD>)""]"
1023,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.name.html,pandas.tseries.offsets.CustomBusinessMonthEnd.name,CustomBusinessMonthEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1024,..\pandas\reference\api\pandas.CategoricalIndex.remove_unused_categories.html,pandas.CategoricalIndex.remove_unused_categories,"CategoricalIndex.remove_unused_categories(*args, **kwargs)[source]# Remove categories which are not used.",Returns: CategoricalCategorical with unused categories dropped.,"["">>> c = pd.Categorical(['a', 'c', 'b', 'c', 'd'])\n>>> c\n['a', 'c', 'b', 'c', 'd']\nCategories (4, object): ['a', 'b', 'c', 'd']"", "">>> c[2] = 'a'\n>>> c[4] = 'c'\n>>> c\n['a', 'c', 'a', 'c', 'c']\nCategories (4, object): ['a', 'b', 'c', 'd']"", "">>> c.remove_unused_categories()\n['a', 'c', 'a', 'c', 'c']\nCategories (2, object): ['a', 'c']""]"
1025,..\pandas\reference\api\pandas.DataFrame.compare.html,pandas.DataFrame.compare,"DataFrame.compare(other, align_axis=1, keep_shape=False, keep_equal=False, result_names=('self', 'other'))[source]# Compare to another DataFrame and show the differences. Notes Matching NaNs will not appear as a difference. Can only compare identically-labeled (i.e. same shape, identical row and column labels) DataFrames","Parameters: otherDataFrameObject to compare with. align_axis{0 or ‘index’, 1 or ‘columns’}, default 1Determine which axis to align the comparison on. 0, or ‘index’Resulting differences are stacked verticallywith rows drawn alternately from self and other. 1, or ‘columns’Resulting differences are aligned horizontallywith columns drawn alternately from self and other. keep_shapebool, default FalseIf true, all rows and columns are kept. Otherwise, only the ones with different values are kept. keep_equalbool, default FalseIf true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs. result_namestuple, default (‘self’, ‘other’)Set the dataframes names in the comparison. Added in version 1.5.0. Returns: DataFrameDataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with ‘self’ and ‘other’ stacked alternately at the inner level. Raises: ValueErrorWhen the two DataFrames don’t have identical labels or shape.","['>>> df = pd.DataFrame(\n...     {\n...         ""col1"": [""a"", ""a"", ""b"", ""b"", ""a""],\n...         ""col2"": [1.0, 2.0, 3.0, np.nan, 5.0],\n...         ""col3"": [1.0, 2.0, 3.0, 4.0, 5.0]\n...     },\n...     columns=[""col1"", ""col2"", ""col3""],\n... )\n>>> df\n  col1  col2  col3\n0    a   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   3.0\n3    b   NaN   4.0\n4    a   5.0   5.0', "">>> df2 = df.copy()\n>>> df2.loc[0, 'col1'] = 'c'\n>>> df2.loc[2, 'col3'] = 4.0\n>>> df2\n  col1  col2  col3\n0    c   1.0   1.0\n1    a   2.0   2.0\n2    b   3.0   4.0\n3    b   NaN   4.0\n4    a   5.0   5.0"", '>>> df.compare(df2)\n  col1       col3\n  self other self other\n0    a     c  NaN   NaN\n2  NaN   NaN  3.0   4.0', '>>> df.compare(df2, result_names=(""left"", ""right""))\n  col1       col3\n  left right left right\n0    a     c  NaN   NaN\n2  NaN   NaN  3.0   4.0', '>>> df.compare(df2, align_axis=0)\n        col1  col3\n0 self     a   NaN\n  other    c   NaN\n2 self   NaN   3.0\n  other  NaN   4.0', '>>> df.compare(df2, keep_equal=True)\n  col1       col3\n  self other self other\n0    a     c  1.0   1.0\n2    b     b  3.0   4.0', '>>> df.compare(df2, keep_shape=True)\n  col1       col2       col3\n  self other self other self other\n0    a     c  NaN   NaN  NaN   NaN\n1  NaN   NaN  NaN   NaN  NaN   NaN\n2  NaN   NaN  NaN   NaN  3.0   4.0\n3  NaN   NaN  NaN   NaN  NaN   NaN\n4  NaN   NaN  NaN   NaN  NaN   NaN', '>>> df.compare(df2, keep_shape=True, keep_equal=True)\n  col1       col2       col3\n  self other self other self other\n0    a     c  1.0   1.0  1.0   1.0\n1    a     a  2.0   2.0  2.0   2.0\n2    b     b  3.0   3.0  3.0   4.0\n3    b     b  NaN   NaN  4.0   4.0\n4    a     a  5.0   5.0  5.0   5.0']"
1026,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_on_offset.html,pandas.tseries.offsets.QuarterEnd.is_on_offset,QuarterEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1027,..\pandas\reference\api\pandas.MultiIndex.truncate.html,pandas.MultiIndex.truncate,"MultiIndex.truncate(before=None, after=None)[source]# Slice index between two labels / tuples, return new MultiIndex.","Parameters: beforelabel or tuple, can be partial. Default NoneNone defaults to start. afterlabel or tuple, can be partial. Default NoneNone defaults to end. Returns: MultiIndexThe truncated MultiIndex.","["">>> mi = pd.MultiIndex.from_arrays([['a', 'b', 'c'], ['x', 'y', 'z']])\n>>> mi\nMultiIndex([('a', 'x'), ('b', 'y'), ('c', 'z')],\n           )\n>>> mi.truncate(before='a', after='b')\nMultiIndex([('a', 'x'), ('b', 'y')],\n           )""]"
1028,..\pandas\reference\api\pandas.errors.DataError.html,pandas.errors.DataError,"exception pandas.errors.DataError[source]# Exceptionn raised when performing an operation on non-numerical data. For example, calling ohlc on a non-numerical column or a function on a rolling window.",No parameters found,"["">>> ser = pd.Series(['a', 'b', 'c'])\n>>> ser.rolling(2).sum()\nTraceback (most recent call last):\nDataError: No numeric types to aggregate""]"
1029,..\pandas\reference\api\pandas.Series.max.html,pandas.Series.max,"Series.max(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the maximum of the values over the requested axis. If you want the index of the maximum, use idxmax. This is the equivalent of the numpy.ndarray method argmax.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","["">>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64"", '>>> s.max()\n8']"
1030,..\pandas\reference\api\pandas.Timestamp.today.html,pandas.Timestamp.today,classmethod Timestamp.today(tz=None)# Return the current time in the local timezone. This differs from datetime.today() in that it can be localized to a passed timezone.,"Parameters: tzstr or timezone object, default NoneTimezone to localize to.","["">>> pd.Timestamp.today()    \nTimestamp('2020-11-16 22:37:39.969883')"", '>>> pd.NaT.today()\nNaT']"
1031,..\pandas\reference\api\pandas.NA.html,pandas.NA,pandas.NA# alias of <NA>,No parameters found,[]
1032,..\pandas\reference\api\pandas.CategoricalIndex.rename_categories.html,pandas.CategoricalIndex.rename_categories,"CategoricalIndex.rename_categories(*args, **kwargs)[source]# Rename categories.","Parameters: new_categorieslist-like, dict-like or callableNew categories which will replace old categories. list-like: all items must be unique and the number of items in the new categories must match the existing number of categories. dict-like: specifies a mapping from old categories to new. Categories not contained in the mapping are passed through and extra categories in the mapping are ignored. callable : a callable that is called on all items in the old categories and whose return values comprise the new categories. Returns: CategoricalCategorical with renamed categories. Raises: ValueErrorIf new categories are list-like and do not have the same number of items than the current categories or do not validate as categories","["">>> c = pd.Categorical(['a', 'a', 'b'])\n>>> c.rename_categories([0, 1])\n[0, 0, 1]\nCategories (2, int64): [0, 1]"", "">>> c.rename_categories({'a': 'A', 'c': 'C'})\n['A', 'A', 'b']\nCategories (2, object): ['A', 'b']"", "">>> c.rename_categories(lambda x: x.upper())\n['A', 'A', 'B']\nCategories (2, object): ['A', 'B']""]"
1033,..\pandas\reference\api\pandas.DataFrame.convert_dtypes.html,pandas.DataFrame.convert_dtypes,"DataFrame.convert_dtypes(infer_objects=True, convert_string=True, convert_integer=True, convert_boolean=True, convert_floating=True, dtype_backend='numpy_nullable')[source]# Convert columns to the best possible dtypes using dtypes supporting pd.NA. Notes By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA. By using the options convert_string, convert_integer, convert_boolean and convert_floating, it is possible to turn off individual conversions to StringDtype, the integer extension types, BooleanDtype or floating extension types, respectively. For object-dtyped columns, if infer_objects is True, use the inference rules as during normal Series/DataFrame construction.  Then, if possible, convert to StringDtype, BooleanDtype or an appropriate integer or floating extension type, otherwise leave as object. If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. Otherwise, convert to an appropriate floating extension type. In the future, as new dtypes are added that support pd.NA, the results of this method will change to support those new dtypes.","Parameters: infer_objectsbool, default TrueWhether object dtypes should be converted to the best possible types. convert_stringbool, default TrueWhether object dtypes should be converted to StringDtype(). convert_integerbool, default TrueWhether, if possible, conversion can be done to integer extension types. convert_booleanbool, defaults TrueWhether object dtypes should be converted to BooleanDtypes(). convert_floatingbool, defaults TrueWhether, if possible, conversion can be done to floating extension types. If convert_integer is also True, preference will be give to integer dtypes if the floats can be faithfully casted to integers. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: Series or DataFrameCopy of input object with new dtype.","['>>> df = pd.DataFrame(\n...     {\n...         ""a"": pd.Series([1, 2, 3], dtype=np.dtype(""int32"")),\n...         ""b"": pd.Series([""x"", ""y"", ""z""], dtype=np.dtype(""O"")),\n...         ""c"": pd.Series([True, False, np.nan], dtype=np.dtype(""O"")),\n...         ""d"": pd.Series([""h"", ""i"", np.nan], dtype=np.dtype(""O"")),\n...         ""e"": pd.Series([10, np.nan, 20], dtype=np.dtype(""float"")),\n...         ""f"": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(""float"")),\n...     }\n... )', '>>> df\n   a  b      c    d     e      f\n0  1  x   True    h  10.0    NaN\n1  2  y  False    i   NaN  100.5\n2  3  z    NaN  NaN  20.0  200.0', '>>> df.dtypes\na      int32\nb     object\nc     object\nd     object\ne    float64\nf    float64\ndtype: object', '>>> dfn = df.convert_dtypes()\n>>> dfn\n   a  b      c     d     e      f\n0  1  x   True     h    10   <NA>\n1  2  y  False     i  <NA>  100.5\n2  3  z   <NA>  <NA>    20  200.0', '>>> dfn.dtypes\na             Int32\nb    string[python]\nc           boolean\nd    string[python]\ne             Int64\nf           Float64\ndtype: object', '>>> s = pd.Series([""a"", ""b"", np.nan])\n>>> s\n0      a\n1      b\n2    NaN\ndtype: object', '>>> s.convert_dtypes()\n0       a\n1       b\n2    <NA>\ndtype: string']"
1034,..\pandas\reference\api\pandas.errors.DtypeWarning.html,pandas.errors.DtypeWarning,"exception pandas.errors.DtypeWarning[source]# Warning raised when reading different dtypes in a column from a file. Raised for a dtype incompatibility. This can happen whenever read_csv or read_table encounter non-uniform dtypes in a column(s) of a given CSV file. Notes This warning is issued when dealing with larger files because the dtype checking happens per chunk read. Despite the warning, the CSV file is read with mixed types in a single column which will be an object type. See the examples below to better understand this issue.",No parameters found,"["">>> df = pd.DataFrame({'a': (['1'] * 100000 + ['X'] * 100000 +\n...                          ['1'] * 100000),\n...                    'b': ['b'] * 300000})  \n>>> df.to_csv('test.csv', index=False)  \n>>> df2 = pd.read_csv('test.csv')  \n... # DtypeWarning: Columns (0) have mixed types"", "">>> df2.iloc[262140, 0]  \n'1'\n>>> type(df2.iloc[262140, 0])  \n<class 'str'>\n>>> df2.iloc[262150, 0]  \n1\n>>> type(df2.iloc[262150, 0])  \n<class 'int'>"", "">>> df2 = pd.read_csv('test.csv', sep=',', dtype={'a': str})""]"
1035,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.nanos.html,pandas.tseries.offsets.CustomBusinessMonthEnd.nanos,CustomBusinessMonthEnd.nanos#,No parameters found,[]
1036,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_quarter_end.html,pandas.tseries.offsets.QuarterEnd.is_quarter_end,QuarterEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1037,..\pandas\reference\api\pandas.Series.mean.html,pandas.Series.mean,"Series.mean(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the mean of the values over the requested axis.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","['>>> s = pd.Series([1, 2, 3])\n>>> s.mean()\n2.0', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n>>> df\n       a   b\ntiger  1   2\nzebra  2   3\n>>> df.mean()\na   1.5\nb   2.5\ndtype: float64"", '>>> df.mean(axis=1)\ntiger   1.5\nzebra   2.5\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n...                   index=['tiger', 'zebra'])\n>>> df.mean(numeric_only=True)\na   1.5\ndtype: float64""]"
1038,..\pandas\reference\api\pandas.Timestamp.toordinal.html,pandas.Timestamp.toordinal,Timestamp.toordinal()# Return proleptic Gregorian ordinal. January 1 of year 1 is day 1.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:50')\n>>> ts\nTimestamp('2023-01-01 10:00:50')\n>>> ts.toordinal()\n738521""]"
1039,..\pandas\reference\api\pandas.NamedAgg.html,pandas.NamedAgg,"class pandas.NamedAgg(column, aggfunc)[source]# Helper for column specific aggregation with control over output column names. Subclass of typing.NamedTuple. Examples Attributes aggfunc Alias for field number 1 column Alias for field number 0","Parameters: columnHashableColumn label in the DataFrame to apply aggfunc. aggfuncfunction or strFunction to apply to the provided column. If string, the name of a built-in pandas function.","['>>> df = pd.DataFrame({""key"": [1, 1, 2], ""a"": [-1, 0, 1], 1: [10, 11, 12]})\n>>> agg_a = pd.NamedAgg(column=""a"", aggfunc=""min"")\n>>> agg_1 = pd.NamedAgg(column=1, aggfunc=lambda x: np.mean(x))\n>>> df.groupby(""key"").agg(result_a=agg_a, result_1=agg_1)\n     result_a  result_1\nkey\n1          -1      10.5\n2           1      12.0']"
1040,..\pandas\reference\api\pandas.Timestamp.to_datetime64.html,pandas.Timestamp.to_datetime64,Timestamp.to_datetime64()# Return a numpy.datetime64 object with same precision.,No parameters found,"["">>> ts = pd.Timestamp(year=2023, month=1, day=1,\n...                   hour=10, second=15)\n>>> ts\nTimestamp('2023-01-01 10:00:15')\n>>> ts.to_datetime64()\nnumpy.datetime64('2023-01-01T10:00:15.000000')""]"
1041,..\pandas\reference\api\pandas.CategoricalIndex.reorder_categories.html,pandas.CategoricalIndex.reorder_categories,"CategoricalIndex.reorder_categories(*args, **kwargs)[source]# Reorder categories as specified in new_categories. new_categories need to include all old categories and no new category items.","Parameters: new_categoriesIndex-likeThe categories in new order. orderedbool, optionalWhether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information. Returns: CategoricalCategorical with reordered categories. Raises: ValueErrorIf the new categories do not contain all old category items or any new ones","["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser = ser.cat.reorder_categories(['c', 'b', 'a'], ordered=True)\n>>> ser\n0   a\n1   b\n2   c\n3   a\ndtype: category\nCategories (3, object): ['c' < 'b' < 'a']"", "">>> ser.sort_values()\n2   c\n1   b\n0   a\n3   a\ndtype: category\nCategories (3, object): ['c' < 'b' < 'a']"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a'])\n>>> ci\nCategoricalIndex(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c'],\n                 ordered=False, dtype='category')\n>>> ci.reorder_categories(['c', 'b', 'a'], ordered=True)\nCategoricalIndex(['a', 'b', 'c', 'a'], categories=['c', 'b', 'a'],\n                 ordered=True, dtype='category')""]"
1042,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_quarter_start.html,pandas.tseries.offsets.QuarterEnd.is_quarter_start,QuarterEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1043,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.normalize.html,pandas.tseries.offsets.CustomBusinessMonthEnd.normalize,CustomBusinessMonthEnd.normalize#,No parameters found,[]
1044,..\pandas\reference\api\pandas.DataFrame.copy.html,pandas.DataFrame.copy,"DataFrame.copy(deep=True)[source]# Make a copy of this object’s indices and data. When deep=True (default), a new object will be created with a copy of the calling object’s data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When deep=False, a new object will be created without copying the calling object’s data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). Note The deep=False behaviour as described above will change in pandas 3.0. Copy-on-Write will be enabled by default, which means that the “shallow” copy is that is returned with deep=False will still avoid making an eager copy, but changes to the data of the original will no longer be reflected in the shallow copy (or vice versa). Instead, it makes use of a lazy (deferred) copy mechanism that will copy the data only when any changes to the original or shallow copy is made. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Notes When deep=True, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below). While Index objects are copied when deep=True, the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed. Since pandas is not thread safe, see the gotchas when copying in a threading environment. When copy_on_write in pandas config is set to True, the copy_on_write config takes effect even when deep=False. This means that any changes to the copied data would make a new copy of the data upon write (and vice versa). Changes made to either the original or copied variable would not be reflected in the counterpart. See Copy_on_Write for more information.","Parameters: deepbool, default TrueMake a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied. Returns: Series or DataFrameObject type matches caller.","['>>> s = pd.Series([1, 2], index=[""a"", ""b""])\n>>> s\na    1\nb    2\ndtype: int64', '>>> s_copy = s.copy()\n>>> s_copy\na    1\nb    2\ndtype: int64', '>>> s = pd.Series([1, 2], index=[""a"", ""b""])\n>>> deep = s.copy()\n>>> shallow = s.copy(deep=False)', '>>> s is shallow\nFalse\n>>> s.values is shallow.values and s.index is shallow.index\nTrue', '>>> s is deep\nFalse\n>>> s.values is deep.values or s.index is deep.index\nFalse', '>>> s.iloc[0] = 3\n>>> shallow.iloc[1] = 4\n>>> s\na    3\nb    4\ndtype: int64\n>>> shallow\na    3\nb    4\ndtype: int64\n>>> deep\na    1\nb    2\ndtype: int64', '>>> s = pd.Series([[1, 2], [3, 4]])\n>>> deep = s.copy()\n>>> s[0][0] = 10\n>>> s\n0    [10, 2]\n1     [3, 4]\ndtype: object\n>>> deep\n0    [10, 2]\n1     [3, 4]\ndtype: object', '>>> with pd.option_context(""mode.copy_on_write"", True):\n...     s = pd.Series([1, 2], index=[""a"", ""b""])\n...     copy = s.copy(deep=False)\n...     s.iloc[0] = 100\n...     s\na    100\nb      2\ndtype: int64\n>>> copy\na    1\nb    2\ndtype: int64']"
1045,..\pandas\reference\api\pandas.Series.median.html,pandas.Series.median,"Series.median(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the median of the values over the requested axis.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","['>>> s = pd.Series([1, 2, 3])\n>>> s.median()\n2.0', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n>>> df\n       a   b\ntiger  1   2\nzebra  2   3\n>>> df.median()\na   1.5\nb   2.5\ndtype: float64"", '>>> df.median(axis=1)\ntiger   1.5\nzebra   2.5\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n...                   index=['tiger', 'zebra'])\n>>> df.median(numeric_only=True)\na   1.5\ndtype: float64""]"
1046,..\pandas\reference\api\pandas.errors.DuplicateLabelError.html,pandas.errors.DuplicateLabelError,exception pandas.errors.DuplicateLabelError[source]# Error raised when an operation would introduce duplicate labels.,No parameters found,"["">>> s = pd.Series([0, 1, 2], index=['a', 'b', 'c']).set_flags(\n...     allows_duplicate_labels=False\n... )\n>>> s.reindex(['a', 'a', 'b'])\nTraceback (most recent call last):\n   ...\nDuplicateLabelError: Index has duplicates.\n      positions\nlabel\na        [0, 1]""]"
1047,..\pandas\reference\api\pandas.NaT.html,pandas.NaT,pandas.NaT# alias of NaT,No parameters found,[]
1048,..\pandas\reference\api\pandas.Timestamp.to_julian_date.html,pandas.Timestamp.to_julian_date,"Timestamp.to_julian_date()# Convert TimeStamp to a Julian Date. 0 Julian date is noon January 1, 4713 BC.",No parameters found,"["">>> ts = pd.Timestamp('2020-03-14T15:32:52')\n>>> ts.to_julian_date()\n2458923.147824074""]"
1049,..\pandas\reference\api\pandas.CategoricalIndex.set_categories.html,pandas.CategoricalIndex.set_categories,"CategoricalIndex.set_categories(*args, **kwargs)[source]# Set the categories to the specified new categories. new_categories can include new categories (which will result in unused categories) or remove old categories (which results in values set to NaN). If rename=True, the categories will simply be renamed (less or more items than in old categories will result in values set to NaN or in unused categories respectively). This method can be used to perform more than one action of adding, removing, and reordering simultaneously and is therefore faster than performing the individual steps via the more specialised methods. On the other hand this methods does not do checks (e.g., whether the old categories are included in the new categories on a reorder), which can result in surprising changes, for example when using special string dtypes, which does not considers a S1 string equal to a single char python string.","Parameters: new_categoriesIndex-likeThe categories in new order. orderedbool, default FalseWhether or not the categorical is treated as a ordered categorical. If not given, do not change the ordered information. renamebool, default FalseWhether or not the new_categories should be considered as a rename of the old categories or as reordered categories. Returns: Categorical with reordered categories. Raises: ValueErrorIf new_categories does not validate as categories","["">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'A'],\n...                           categories=['a', 'b', 'c'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser\n0   a\n1   b\n2   c\n3   NaN\ndtype: category\nCategories (3, object): ['a' < 'b' < 'c']"", "">>> ser.cat.set_categories(['A', 'B', 'C'], rename=True)\n0   A\n1   B\n2   C\n3   NaN\ndtype: category\nCategories (3, object): ['A' < 'B' < 'C']"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'A'],\n...                          categories=['a', 'b', 'c'], ordered=True)\n>>> ci\nCategoricalIndex(['a', 'b', 'c', nan], categories=['a', 'b', 'c'],\n                 ordered=True, dtype='category')"", "">>> ci.set_categories(['A', 'b', 'c'])\nCategoricalIndex([nan, 'b', 'c', nan], categories=['A', 'b', 'c'],\n                 ordered=True, dtype='category')\n>>> ci.set_categories(['A', 'b', 'c'], rename=True)\nCategoricalIndex(['A', 'b', 'c', nan], categories=['A', 'b', 'c'],\n                 ordered=True, dtype='category')""]"
1050,..\pandas\reference\api\pandas.Series.memory_usage.html,pandas.Series.memory_usage,"Series.memory_usage(index=True, deep=False)[source]# Return the memory usage of the Series. The memory usage can optionally include the contribution of the index and of elements of object dtype.","Parameters: indexbool, default TrueSpecifies whether to include the memory usage of the Series index. deepbool, default FalseIf True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned value. Returns: intBytes of memory consumed.","['>>> s = pd.Series(range(3))\n>>> s.memory_usage()\n152', '>>> s.memory_usage(index=False)\n24', '>>> s = pd.Series([""a"", ""b""])\n>>> s.values\narray([\'a\', \'b\'], dtype=object)\n>>> s.memory_usage()\n144\n>>> s.memory_usage(deep=True)\n244']"
1051,..\pandas\reference\api\pandas.notna.html,pandas.notna,"pandas.notna(obj)[source]# Detect non-missing values for an array-like object. This function takes a scalar or array-like object and indicates whether values are valid (not missing, which is NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).","Parameters: objarray-like or object valueObject to check for not null or non-missing values. Returns: bool or array-like of boolFor scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is valid.","["">>> pd.notna('dog')\nTrue"", '>>> pd.notna(pd.NA)\nFalse', '>>> pd.notna(np.nan)\nFalse', '>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n>>> array\narray([[ 1., nan,  3.],\n       [ 4.,  5., nan]])\n>>> pd.notna(array)\narray([[ True, False,  True],\n       [ True,  True, False]])', '>>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,\n...                          ""2017-07-08""])\n>>> index\nDatetimeIndex([\'2017-07-05\', \'2017-07-06\', \'NaT\', \'2017-07-08\'],\n              dtype=\'datetime64[ns]\', freq=None)\n>>> pd.notna(index)\narray([ True,  True, False,  True])', "">>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])\n>>> df\n     0     1    2\n0  ant   bee  cat\n1  dog  None  fly\n>>> pd.notna(df)\n      0      1     2\n0  True   True  True\n1  True  False  True"", '>>> pd.notna(df[1])\n0     True\n1    False\nName: 1, dtype: bool']"
1052,..\pandas\reference\api\pandas.errors.EmptyDataError.html,pandas.errors.EmptyDataError,exception pandas.errors.EmptyDataError[source]# Exception raised in pd.read_csv when empty data or header is encountered.,No parameters found,['>>> from io import StringIO\n>>> empty = StringIO()\n>>> pd.read_csv(empty)\nTraceback (most recent call last):\nEmptyDataError: No columns to parse from file']
1053,..\pandas\reference\api\pandas.DataFrame.corr.html,pandas.DataFrame.corr,"DataFrame.corr(method='pearson', min_periods=1, numeric_only=False)[source]# Compute pairwise correlation of columns, excluding NA/null values. Notes Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations. Pearson correlation coefficient Kendall rank correlation coefficient Spearman’s rank correlation coefficient","Parameters: method{‘pearson’, ‘kendall’, ‘spearman’} or callableMethod of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarraysand returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable’s behavior. min_periodsint, optionalMinimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: The default value of numeric_only is now False. Returns: DataFrameCorrelation matrix.","["">>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(method=histogram_intersection)\n      dogs  cats\ndogs   1.0   0.3\ncats   0.3   1.0"", "">>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(min_periods=3)\n      dogs  cats\ndogs   1.0   NaN\ncats   NaN   1.0""]"
1054,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_year_end.html,pandas.tseries.offsets.QuarterEnd.is_year_end,QuarterEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1055,..\pandas\reference\api\pandas.Timestamp.to_numpy.html,pandas.Timestamp.to_numpy,"Timestamp.to_numpy(dtype=None, copy=False)# Convert the Timestamp to a NumPy datetime64. This is an alias method for Timestamp.to_datetime64(). The dtype and copy parameters are available here only for compatibility. Their values will not affect the return value.",Returns: numpy.datetime64,"["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts.to_numpy()\nnumpy.datetime64('2020-03-14T15:32:52.192548651')"", "">>> pd.NaT.to_numpy()\nnumpy.datetime64('NaT')""]"
1056,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.rule_code.html,pandas.tseries.offsets.CustomBusinessMonthEnd.rule_code,CustomBusinessMonthEnd.rule_code#,No parameters found,[]
1057,..\pandas\reference\api\pandas.concat.html,pandas.concat,"pandas.concat(objs, *, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, sort=False, copy=None)[source]# Concatenate pandas objects along a particular axis. Allows optional set logic along the other axes. Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number. Notes The keys, levels, and names arguments are all optional. A walkthrough of how this method fits in with other tools for combining pandas objects can be found here. It is not recommended to build DataFrames by adding single rows in a for loop. Build a list of rows and make a DataFrame in a single concat.","Parameters: objsa sequence or mapping of Series or DataFrame objectsIf a mapping is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised. axis{0/’index’, 1/’columns’}, default 0The axis to concatenate along. join{‘inner’, ‘outer’}, default ‘outer’How to handle indexes on other axis (or axes). ignore_indexbool, default FalseIf True, do not use the index values along the concatenation axis. The resulting axis will be labeled 0, …, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join. keyssequence, default NoneIf multiple levels passed, should contain tuples. Construct hierarchical index using the passed keys as the outermost level. levelslist of sequences, default NoneSpecific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys. nameslist, default NoneNames for the levels in the resulting hierarchical index. verify_integritybool, default FalseCheck whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation. sortbool, default FalseSort non-concatenation axis if it is not already aligned. One exception to this is when the non-concatentation axis is a DatetimeIndex and join=’outer’ and the axis is not already aligned. In that case, the non-concatenation axis is always sorted lexicographically. copybool, default TrueIf False, do not copy data unnecessarily. Returns: object, type of objsWhen concatenating all Series along the index (axis=0), a Series is returned. When objs contains at least one DataFrame, a DataFrame is returned. When concatenating along the columns (axis=1), a DataFrame is returned.","["">>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object"", '>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object', "">>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object"", "">>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object"", "">>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4"", "">>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog"", '>>> pd.concat([df1, df3], join=""inner"")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4', "">>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george"", "">>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']"", "">>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4""]"
1058,..\pandas\reference\api\pandas.Series.min.html,pandas.Series.min,"Series.min(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the minimum of the values over the requested axis. If you want the index of the minimum, use idxmin. This is the equivalent of the numpy.ndarray method argmin.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","["">>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64"", '>>> s.min()\n0']"
1059,..\pandas\reference\api\pandas.DataFrame.corrwith.html,pandas.DataFrame.corrwith,"DataFrame.corrwith(other, axis=0, drop=False, method='pearson', numeric_only=False)[source]# Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations.","Parameters: otherDataFrame, SeriesObject with which to compute correlations. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to use. 0 or ‘index’ to compute row-wise, 1 or ‘columns’ for column-wise. dropbool, default FalseDrop missing indices from result. method{‘pearson’, ‘kendall’, ‘spearman’} or callableMethod of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarraysand returning a float. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: The default value of numeric_only is now False. Returns: SeriesPairwise correlations.","['>>> index = [""a"", ""b"", ""c"", ""d"", ""e""]\n>>> columns = [""one"", ""two"", ""three"", ""four""]\n>>> df1 = pd.DataFrame(np.arange(20).reshape(5, 4), index=index, columns=columns)\n>>> df2 = pd.DataFrame(np.arange(16).reshape(4, 4), index=index[:4], columns=columns)\n>>> df1.corrwith(df2)\none      1.0\ntwo      1.0\nthree    1.0\nfour     1.0\ndtype: float64', '>>> df2.corrwith(df1, axis=1)\na    1.0\nb    1.0\nc    1.0\nd    1.0\ne    NaN\ndtype: float64']"
1060,..\pandas\reference\api\pandas.Timestamp.to_period.html,pandas.Timestamp.to_period,Timestamp.to_period(freq=None)# Return an period of which this timestamp is an observation.,No parameters found,"["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> # Year end frequency\n>>> ts.to_period(freq='Y')\nPeriod('2020', 'Y-DEC')"", "">>> # Month end frequency\n>>> ts.to_period(freq='M')\nPeriod('2020-03', 'M')"", "">>> # Weekly frequency\n>>> ts.to_period(freq='W')\nPeriod('2020-03-09/2020-03-15', 'W-SUN')"", "">>> # Quarter end frequency\n>>> ts.to_period(freq='Q')\nPeriod('2020Q1', 'Q-DEC')""]"
1061,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.is_year_start.html,pandas.tseries.offsets.QuarterEnd.is_year_start,QuarterEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1062,..\pandas\reference\api\pandas.notnull.html,pandas.notnull,"pandas.notnull(obj)[source]# Detect non-missing values for an array-like object. This function takes a scalar or array-like object and indicates whether values are valid (not missing, which is NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).","Parameters: objarray-like or object valueObject to check for not null or non-missing values. Returns: bool or array-like of boolFor scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is valid.","["">>> pd.notna('dog')\nTrue"", '>>> pd.notna(pd.NA)\nFalse', '>>> pd.notna(np.nan)\nFalse', '>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n>>> array\narray([[ 1., nan,  3.],\n       [ 4.,  5., nan]])\n>>> pd.notna(array)\narray([[ True, False,  True],\n       [ True,  True, False]])', '>>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,\n...                          ""2017-07-08""])\n>>> index\nDatetimeIndex([\'2017-07-05\', \'2017-07-06\', \'NaT\', \'2017-07-08\'],\n              dtype=\'datetime64[ns]\', freq=None)\n>>> pd.notna(index)\narray([ True,  True, False,  True])', "">>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])\n>>> df\n     0     1    2\n0  ant   bee  cat\n1  dog  None  fly\n>>> pd.notna(df)\n      0      1     2\n0  True   True  True\n1  True  False  True"", '>>> pd.notna(df[1])\n0     True\n1    False\nName: 1, dtype: bool']"
1063,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.agg.html,pandas.core.groupby.DataFrameGroupBy.agg,"DataFrameGroupBy.agg(func=None, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.","Parameters: funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. None, in which case **kwargs are used with Named Aggregation. Here the output has one column for each element in **kwargs. The name of the column is keyword, whereas the value determines the aggregation used to compute the values in the column. Can also accept a Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine. If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group’s index will be passed to the user defined function and optionally available for use. *argsPositional arguments to pass to func. enginestr, default None 'cython' : Runs the function through C-extensions from cython. 'numba' : Runs the function through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function **kwargs If func is None, **kwargs are used to define the output names and aggregations via Named Aggregation. See func entry. Otherwise, keyword arguments to be passed into func. Returns: DataFrame","['>>> data = {""A"": [1, 1, 2, 2],\n...         ""B"": [1, 2, 3, 4],\n...         ""C"": [0.362838, 0.227877, 1.267767, -0.562860]}\n>>> df = pd.DataFrame(data)\n>>> df\n   A  B         C\n0  1  1  0.362838\n1  1  2  0.227877\n2  2  3  1.267767\n3  2  4 -0.562860', "">>> df.groupby('A').agg('min')\n   B         C\nA\n1  1  0.227877\n2  3 -0.562860"", "">>> df.groupby('A').agg(['min', 'max'])\n    B             C\n  min max       min       max\nA\n1   1   2  0.227877  0.362838\n2   3   4 -0.562860  1.267767"", "">>> df.groupby('A').B.agg(['min', 'max'])\n   min  max\nA\n1    1    2\n2    3    4"", "">>> df.groupby('A').agg(lambda x: sum(x) + 2)\n    B          C\nA\n1       5       2.590715\n2       9       2.704907"", "">>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n    B             C\n  min max       sum\nA\n1   1   2  0.590715\n2   3   4  0.704907"", '>>> df.groupby(""A"").agg(\n...     b_min=pd.NamedAgg(column=""B"", aggfunc=""min""),\n...     c_sum=pd.NamedAgg(column=""C"", aggfunc=""sum"")\n... )\n   b_min     c_sum\nA\n1      1  0.590715\n2      3  0.704907', '>>> df.groupby(""A"")[[""B""]].agg(lambda x: x.astype(float).min())\n      B\nA\n1   1.0\n2   3.0']"
1064,..\pandas\reference\api\pandas.errors.IncompatibilityWarning.html,pandas.errors.IncompatibilityWarning,exception pandas.errors.IncompatibilityWarning[source]# Warning raised when trying to use where criteria on an incompatible HDF5 file.,No parameters found,[]
1065,..\pandas\reference\api\pandas.tseries.offsets.CustomBusinessMonthEnd.weekmask.html,pandas.tseries.offsets.CustomBusinessMonthEnd.weekmask,CustomBusinessMonthEnd.weekmask#,No parameters found,[]
1066,..\pandas\reference\api\pandas.Series.mod.html,pandas.Series.mod,"Series.mod(other, level=None, fill_value=None, axis=0)[source]# Return Modulo of series and other, element-wise (binary operator mod). Equivalent to series % other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.mod(b, fill_value=0)\na    0.0\nb    NaN\nc    NaN\nd    0.0\ne    NaN\ndtype: float64""]"
1067,..\pandas\reference\api\pandas.DataFrame.count.html,pandas.DataFrame.count,"DataFrame.count(axis=0, numeric_only=False)[source]# Count non-NA cells for each column or row. The values None, NaN, NaT, pandas.NA are considered NA.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0If 0 or ‘index’ counts are generated for each column. If 1 or ‘columns’ counts are generated for each row. numeric_onlybool, default FalseInclude only float, int or boolean data. Returns: SeriesFor each column/row the number of non-NA/null entries.","['>>> df = pd.DataFrame({""Person"":\n...                    [""John"", ""Myla"", ""Lewis"", ""John"", ""Myla""],\n...                    ""Age"": [24., np.nan, 21., 33, 26],\n...                    ""Single"": [False, True, True, True, False]})\n>>> df\n   Person   Age  Single\n0    John  24.0   False\n1    Myla   NaN    True\n2   Lewis  21.0    True\n3    John  33.0    True\n4    Myla  26.0   False', '>>> df.count()\nPerson    5\nAge       4\nSingle    5\ndtype: int64', "">>> df.count(axis='columns')\n0    3\n1    2\n2    3\n3    3\n4    3\ndtype: int64""]"
1068,..\pandas\reference\api\pandas.Timestamp.to_pydatetime.html,pandas.Timestamp.to_pydatetime,"Timestamp.to_pydatetime(warn=True)# Convert a Timestamp object to a native Python datetime object. If warn=True, issue a warning if nanoseconds is nonzero.",No parameters found,"["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548')\n>>> ts.to_pydatetime()\ndatetime.datetime(2020, 3, 14, 15, 32, 52, 192548)"", '>>> pd.NaT.to_pydatetime()\nNaT']"
1069,..\pandas\reference\api\pandas.option_context.html,pandas.option_context,"class pandas.option_context(*args)[source]# Context manager to temporarily set options in the with statement context. You need to invoke as option_context(pat, val, [(pat, val), ...]). Examples",No parameters found,"["">>> from pandas import option_context\n>>> with option_context('display.max_rows', 10, 'display.max_columns', 5):\n...     pass""]"
1070,..\pandas\reference\api\pandas.errors.IndexingError.html,pandas.errors.IndexingError,exception pandas.errors.IndexingError[source]# Exception is raised when trying to index and there is a mismatch in dimensions.,No parameters found,"['>>> df = pd.DataFrame({\'A\': [1, 1, 1]})\n>>> df.loc[..., ..., \'A\'] \n... # IndexingError: indexer may only contain one \'...\' entry\n>>> df = pd.DataFrame({\'A\': [1, 1, 1]})\n>>> df.loc[1, ..., ...] \n... # IndexingError: Too many indexers\n>>> df[pd.Series([True], dtype=bool)] \n... # IndexingError: Unalignable boolean Series provided as indexer...\n>>> s = pd.Series(range(2),\n...               index = pd.MultiIndex.from_product([[""a"", ""b""], [""c""]]))\n>>> s.loc[""a"", ""c"", ""d""] \n... # IndexingError: Too many indexers']"
1071,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.aggregate.html,pandas.core.groupby.DataFrameGroupBy.aggregate,"DataFrameGroupBy.aggregate(func=None, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.","Parameters: funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. None, in which case **kwargs are used with Named Aggregation. Here the output has one column for each element in **kwargs. The name of the column is keyword, whereas the value determines the aggregation used to compute the values in the column. Can also accept a Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine. If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group’s index will be passed to the user defined function and optionally available for use. *argsPositional arguments to pass to func. enginestr, default None 'cython' : Runs the function through C-extensions from cython. 'numba' : Runs the function through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function **kwargs If func is None, **kwargs are used to define the output names and aggregations via Named Aggregation. See func entry. Otherwise, keyword arguments to be passed into func. Returns: DataFrame","['>>> data = {""A"": [1, 1, 2, 2],\n...         ""B"": [1, 2, 3, 4],\n...         ""C"": [0.362838, 0.227877, 1.267767, -0.562860]}\n>>> df = pd.DataFrame(data)\n>>> df\n   A  B         C\n0  1  1  0.362838\n1  1  2  0.227877\n2  2  3  1.267767\n3  2  4 -0.562860', "">>> df.groupby('A').agg('min')\n   B         C\nA\n1  1  0.227877\n2  3 -0.562860"", "">>> df.groupby('A').agg(['min', 'max'])\n    B             C\n  min max       min       max\nA\n1   1   2  0.227877  0.362838\n2   3   4 -0.562860  1.267767"", "">>> df.groupby('A').B.agg(['min', 'max'])\n   min  max\nA\n1    1    2\n2    3    4"", "">>> df.groupby('A').agg(lambda x: sum(x) + 2)\n    B          C\nA\n1       5       2.590715\n2       9       2.704907"", "">>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n    B             C\n  min max       sum\nA\n1   1   2  0.590715\n2   3   4  0.704907"", '>>> df.groupby(""A"").agg(\n...     b_min=pd.NamedAgg(column=""B"", aggfunc=""min""),\n...     c_sum=pd.NamedAgg(column=""C"", aggfunc=""sum"")\n... )\n   b_min     c_sum\nA\n1      1  0.590715\n2      3  0.704907', '>>> df.groupby(""A"")[[""B""]].agg(lambda x: x.astype(float).min())\n      B\nA\n1   1.0\n2   3.0']"
1072,..\pandas\reference\api\pandas.DataFrame.cov.html,pandas.DataFrame.cov,"DataFrame.cov(min_periods=None, ddof=1, numeric_only=False)[source]# Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN. This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Notes Returns the covariance matrix of the DataFrame’s time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is missing at random) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details.","Parameters: min_periodsint, optionalMinimum number of observations required per pair of columns to have a valid result. ddofint, default 1Delta degrees of freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. This argument is applicable only when no nan is in the dataframe. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: The default value of numeric_only is now False. Returns: DataFrameThe covariance matrix of the series of the DataFrame.","["">>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n...                   columns=['dogs', 'cats'])\n>>> df.cov()\n          dogs      cats\ndogs  0.666667 -1.000000\ncats -1.000000  1.666667"", "">>> np.random.seed(42)\n>>> df = pd.DataFrame(np.random.randn(1000, 5),\n...                   columns=['a', 'b', 'c', 'd', 'e'])\n>>> df.cov()\n          a         b         c         d         e\na  0.998438 -0.020161  0.059277 -0.008943  0.014144\nb -0.020161  1.059352 -0.008543 -0.024738  0.009826\nc  0.059277 -0.008543  1.010670 -0.001486 -0.000271\nd -0.008943 -0.024738 -0.001486  0.921297 -0.013692\ne  0.014144  0.009826 -0.000271 -0.013692  0.977795"", "">>> np.random.seed(42)\n>>> df = pd.DataFrame(np.random.randn(20, 3),\n...                   columns=['a', 'b', 'c'])\n>>> df.loc[df.index[:5], 'a'] = np.nan\n>>> df.loc[df.index[5:10], 'b'] = np.nan\n>>> df.cov(min_periods=12)\n          a         b         c\na  0.316741       NaN -0.150812\nb       NaN  1.248003  0.191417\nc -0.150812  0.191417  0.895202""]"
1073,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.kwds.html,pandas.tseries.offsets.QuarterEnd.kwds,QuarterEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1074,..\pandas\reference\api\pandas.Series.mode.html,pandas.Series.mode,Series.mode(dropna=True)[source]# Return the mode(s) of the Series. The mode is the value that appears most often. There can be multiple modes. Always returns Series even if only one value is returned.,"Parameters: dropnabool, default TrueDon’t consider counts of NaN/NaT. Returns: SeriesModes of the Series in sorted order.","['>>> s = pd.Series([2, 4, 2, 2, 4, None])\n>>> s.mode()\n0    2.0\ndtype: float64', '>>> s = pd.Series([2, 4, 8, 2, 4, None])\n>>> s.mode()\n0    2.0\n1    4.0\ndtype: float64', '>>> s = pd.Series([2, 4, None, None, 4, None])\n>>> s.mode(dropna=False)\n0   NaN\ndtype: float64\n>>> s = pd.Series([2, 4, None, None, 4, None])\n>>> s.mode()\n0    4.0\ndtype: float64']"
1075,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.copy.html,pandas.tseries.offsets.DateOffset.copy,DateOffset.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1076,..\pandas\reference\api\pandas.Timestamp.tz.html,pandas.Timestamp.tz,property Timestamp.tz# Alias for tzinfo.,No parameters found,"["">>> ts = pd.Timestamp(1584226800, unit='s', tz='Europe/Stockholm')\n>>> ts.tz\n<DstTzInfo 'Europe/Stockholm' CET+1:00:00 STD>""]"
1077,..\pandas\reference\api\pandas.Period.asfreq.html,pandas.Period.asfreq,"Period.asfreq(freq, how='E')# Convert Period to desired frequency, at the start or end of the interval.","Parameters: freqstr, BaseOffsetThe desired frequency. If passing a str, it needs to be a valid period alias. how{‘E’, ‘S’, ‘end’, ‘start’}, default ‘end’Start or end of the timespan. Returns: resampledPeriod","["">>> period = pd.Period('2023-1-1', freq='D')\n>>> period.asfreq('h')\nPeriod('2023-01-01 23:00', 'h')""]"
1078,..\pandas\reference\api\pandas.errors.IntCastingNaNError.html,pandas.errors.IntCastingNaNError,exception pandas.errors.IntCastingNaNError[source]# Exception raised when converting (astype) an array with NaN to an integer type.,No parameters found,"['>>> pd.DataFrame(np.array([[1, np.nan], [2, 3]]), dtype=""i8"")\nTraceback (most recent call last):\nIntCastingNaNError: Cannot convert non-finite values (NA or inf) to integer']"
1079,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.all.html,pandas.core.groupby.DataFrameGroupBy.all,"DataFrameGroupBy.all(skipna=True)[source]# Return True if all values in the group are truthful, else False.","Parameters: skipnabool, default TrueFlag to ignore nan values during truth testing. Returns: Series or DataFrameDataFrame or Series of boolean values, where a value is True if all elements are True within its respective group, False otherwise.","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 0], index=lst)\n>>> ser\na    1\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).all()\na     True\nb    False\ndtype: bool"", '>>> data = [[1, 0, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""ostrich"", ""penguin"", ""parrot""])\n>>> df\n         a  b  c\nostrich  1  0  3\npenguin  1  5  6\nparrot   7  8  9\n>>> df.groupby(by=[""a""]).all()\n       b      c\na\n1  False   True\n7   True   True']"
1080,..\pandas\reference\api\pandas.Series.mul.html,pandas.Series.mul,"Series.mul(other, level=None, fill_value=None, axis=0)[source]# Return Multiplication of series and other, element-wise (binary operator mul). Equivalent to series * other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.multiply(b, fill_value=0)\na    1.0\nb    0.0\nc    0.0\nd    0.0\ne    NaN\ndtype: float64""]"
1081,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.n.html,pandas.tseries.offsets.QuarterEnd.n,QuarterEnd.n#,No parameters found,[]
1082,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.freqstr.html,pandas.tseries.offsets.DateOffset.freqstr,DateOffset.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1083,..\pandas\reference\api\pandas.Period.day.html,pandas.Period.day,Period.day# Get day of the month that a Period falls on.,Returns: int,"['>>> p = pd.Period(""2018-03-11"", freq=\'h\')\n>>> p.day\n11']"
1084,..\pandas\reference\api\pandas.errors.InvalidColumnName.html,pandas.errors.InvalidColumnName,"exception pandas.errors.InvalidColumnName[source]# Warning raised by to_stata the column contains a non-valid stata name. Because the column name is an invalid Stata variable, the name needs to be converted.",No parameters found,"['>>> df = pd.DataFrame({""0categories"": pd.Series([2, 2])})\n>>> df.to_stata(\'test\') \n... # InvalidColumnName: Not all pandas column names were valid Stata variable...']"
1085,..\pandas\reference\api\pandas.DataFrame.cummax.html,pandas.DataFrame.cummax,"DataFrame.cummax(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative maximum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative maximum.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: Series or DataFrameReturn cumulative maximum of Series or DataFrame.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cummax()\n0    2.0\n1    NaN\n2    5.0\n3    5.0\n4    5.0\ndtype: float64', '>>> s.cummax(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cummax()\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  3.0  1.0', '>>> df.cummax(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  1.0']"
1086,..\pandas\reference\api\pandas.Timestamp.tzinfo.html,pandas.Timestamp.tzinfo,Timestamp.tzinfo#,No parameters found,[]
1087,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.any.html,pandas.core.groupby.DataFrameGroupBy.any,"DataFrameGroupBy.any(skipna=True)[source]# Return True if any value in the group is truthful, else False.","Parameters: skipnabool, default TrueFlag to ignore nan values during truth testing. Returns: Series or DataFrameDataFrame or Series of boolean values, where a value is True if any element is True within its respective group, False otherwise.","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 0], index=lst)\n>>> ser\na    1\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).any()\na     True\nb    False\ndtype: bool"", '>>> data = [[1, 0, 3], [1, 0, 6], [7, 1, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""ostrich"", ""penguin"", ""parrot""])\n>>> df\n         a  b  c\nostrich  1  0  3\npenguin  1  0  6\nparrot   7  1  9\n>>> df.groupby(by=[""a""]).any()\n       b      c\na\n1  False   True\n7   True   True']"
1088,..\pandas\reference\api\pandas.Timestamp.tzname.html,pandas.Timestamp.tzname,Timestamp.tzname()# Return time zone name.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00', tz='Europe/Brussels')\n>>> ts\nTimestamp('2023-01-01 10:00:00+0100', tz='Europe/Brussels')\n>>> ts.tzname()\n'CET'""]"
1089,..\pandas\reference\api\pandas.Period.dayofweek.html,pandas.Period.dayofweek,"Period.dayofweek# Day of the week the period lies in, with Monday=0 and Sunday=6. If the period frequency is lower than daily (e.g. hourly), and the period spans over multiple days, the day at the start of the period is used. If the frequency is higher than daily (e.g. monthly), the last day of the period is used.",Returns: intDay of the week.,"["">>> per = pd.Period('2017-12-31 22:00', 'h')\n>>> per.day_of_week\n6"", "">>> per = pd.Period('2017-12-31 22:00', '4h')\n>>> per.day_of_week\n6\n>>> per.start_time.day_of_week\n6"", "">>> per = pd.Period('2018-01', 'M')\n>>> per.day_of_week\n2\n>>> per.end_time.day_of_week\n2""]"
1090,..\pandas\reference\api\pandas.DataFrame.cummin.html,pandas.DataFrame.cummin,"DataFrame.cummin(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative minimum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative minimum.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: Series or DataFrameReturn cumulative minimum of Series or DataFrame.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cummin()\n0    2.0\n1    NaN\n2    2.0\n3   -1.0\n4   -1.0\ndtype: float64', '>>> s.cummin(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cummin()\n     A    B\n0  2.0  1.0\n1  2.0  NaN\n2  1.0  0.0', '>>> df.cummin(axis=1)\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0']"
1091,..\pandas\reference\api\pandas.errors.InvalidComparison.html,pandas.errors.InvalidComparison,exception pandas.errors.InvalidComparison[source]# Exception is raised by _validate_comparison_value to indicate an invalid comparison.,No parameters found,[]
1092,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.apply.html,pandas.core.groupby.DataFrameGroupBy.apply,"DataFrameGroupBy.apply(func, *args, include_groups=True, **kwargs)[source]# Apply function func group-wise and combine the results together. The function passed to apply must take a dataframe as its first argument and return a DataFrame, Series or scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method. While apply is a very flexible method, its downside is that using it can be quite a bit slower than using more specific methods like agg or transform. Pandas offers a wide range of method that will be much faster than using apply for their specific purposes, so try to use them before reaching for apply. Notes Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funccallableA callable that takes a dataframe as its first argument, and returns a dataframe, a series or a scalar. In addition the callable may take positional and keyword arguments. include_groupsbool, default TrueWhen True, will attempt to apply func to the groupings in the case that they are columns of the DataFrame. If this raises a TypeError, the result will be computed with the groupings excluded. When False, the groupings will be excluded when applying func. Added in version 2.2.0. Deprecated since version 2.2.0: Setting include_groups to True is deprecated. Only the value False will be allowed in a future version of pandas. args, kwargstuple and dictOptional positional and keyword arguments to pass to func. Returns: Series or DataFrame","["">>> df = pd.DataFrame({'A': 'a a b'.split(),\n...                    'B': [1, 2, 3],\n...                    'C': [4, 6, 5]})\n>>> g1 = df.groupby('A', group_keys=False)\n>>> g2 = df.groupby('A', group_keys=True)"", "">>> g1[['B', 'C']].apply(lambda x: x / x.sum())\n          B    C\n0  0.333333  0.4\n1  0.666667  0.6\n2  1.000000  1.0"", "">>> g2[['B', 'C']].apply(lambda x: x / x.sum())\n            B    C\nA\na 0  0.333333  0.4\n  1  0.666667  0.6\nb 2  1.000000  1.0"", "">>> g1[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n     B    C\nA\na  1.0  2.0\nb  0.0  0.0"", "">>> g2[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n     B    C\nA\na  1.0  2.0\nb  0.0  0.0"", '>>> g1.apply(lambda x: x.C.max() - x.B.min(), include_groups=False)\nA\na    5\nb    2\ndtype: int64']"
1093,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.html,pandas.tseries.offsets.DateOffset,"class pandas.tseries.offsets.DateOffset# Standard kind of date increment used for a date range. Works exactly like the keyword argument form of relativedelta. Note that the positional argument form of relativedelata is not supported. Use of the keyword n is discouraged– you would be better off specifying n in the keywords you use, but regardless it is there for you. n is needed for DateOffset subclasses. DateOffset works as follows.  Each offset specify a set of dates that conform to the DateOffset.  For example, Bday defines this set to be the set of dates that are weekdays (M-F).  To test if a date is in the set of a DateOffset dateOffset we can use the is_on_offset method: dateOffset.is_on_offset(date). If a date is not on a valid date, the rollback and rollforward methods can be used to roll the date to the nearest valid date before/after the date. DateOffsets can be created to move dates forward a given number of valid dates.  For example, Bday(2) can be added to a date to move it two business days forward.  If the date does not start on a valid date, first it is moved to a valid date.  Thus pseudo code is: def __add__(date):   date = rollback(date) # does nothing if date is valid   return date + <n number of periods> When a date offset is created for a negative number of periods, the date is first rolled forward.  The pseudo code is: def __add__(date):   date = rollforward(date) # does nothing if date is valid   return date + <n number of periods> Zero presents a problem.  Should it roll forward or back?  We arbitrarily have it rollforward: date + BDay(0) == BDay.rollforward(date) Since 0 is a bit weird, we suggest avoiding its use. Besides, adding a DateOffsets specified by the singular form of the date component can be used to replace certain component of the timestamp. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code","Parameters: nint, default 1The number of time periods the offset represents. If specified without a temporal pattern, defaults to n days. normalizebool, default FalseWhether to round the result of a DateOffset addition down to the previous midnight. weekdayint {0, 1, …, 6}, default 0A specific integer for the day of the week. 0 is Monday 1 is Tuesday 2 is Wednesday 3 is Thursday 4 is Friday 5 is Saturday 6 is Sunday Instead Weekday type from dateutil.relativedelta can be used. MO is Monday TU is Tuesday WE is Wednesday TH is Thursday FR is Friday SA is Saturday SU is Sunday. **kwdsTemporal parameter that add to or replace the offset value. Parameters that add to the offset (like Timedelta): years months weeks days hours minutes seconds milliseconds microseconds nanoseconds Parameters that replace the offset value: year month day weekday hour minute second microsecond nanosecond.","["">>> from pandas.tseries.offsets import DateOffset\n>>> ts = pd.Timestamp('2017-01-01 09:10:11')\n>>> ts + DateOffset(months=3)\nTimestamp('2017-04-01 09:10:11')"", "">>> ts = pd.Timestamp('2017-01-01 09:10:11')\n>>> ts + DateOffset(months=2)\nTimestamp('2017-03-01 09:10:11')\n>>> ts + DateOffset(day=31)\nTimestamp('2017-01-31 09:10:11')"", "">>> ts + pd.DateOffset(hour=8)\nTimestamp('2017-01-01 08:10:11')""]"
1094,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.name.html,pandas.tseries.offsets.QuarterEnd.name,QuarterEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1095,..\pandas\reference\api\pandas.Series.name.html,pandas.Series.name,property Series.name[source]# Return the name of the Series. The name of a Series becomes its index or column name if it is used to form a DataFrame. It is also used whenever displaying the Series using the interpreter.,"Returns: label (hashable object)The name of the Series, also the column name if part of a DataFrame.","['>>> s = pd.Series([1, 2, 3], dtype=np.int64, name=\'Numbers\')\n>>> s\n0    1\n1    2\n2    3\nName: Numbers, dtype: int64\n>>> s.name = ""Integers""\n>>> s\n0    1\n1    2\n2    3\nName: Integers, dtype: int64', '>>> df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],\n...                   columns=[""Odd Numbers"", ""Even Numbers""])\n>>> df\n   Odd Numbers  Even Numbers\n0            1             2\n1            3             4\n2            5             6\n>>> df[""Even Numbers""].name\n\'Even Numbers\'']"
1096,..\pandas\reference\api\pandas.Timestamp.tz_convert.html,pandas.Timestamp.tz_convert,Timestamp.tz_convert(tz)# Convert timezone-aware Timestamp to another time zone.,"Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile or NoneTime zone for time which Timestamp will be converted to. None will remove timezone holding UTC time. Returns: convertedTimestamp Raises: TypeErrorIf Timestamp is tz-naive.","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651', tz='UTC')\n>>> ts\nTimestamp('2020-03-14 15:32:52.192548651+0000', tz='UTC')"", "">>> ts.tz_convert(tz='Asia/Tokyo')\nTimestamp('2020-03-15 00:32:52.192548651+0900', tz='Asia/Tokyo')"", "">>> ts.astimezone(tz='Asia/Tokyo')\nTimestamp('2020-03-15 00:32:52.192548651+0900', tz='Asia/Tokyo')"", "">>> pd.NaT.tz_convert(tz='Asia/Tokyo')\nNaT""]"
1097,..\pandas\reference\api\pandas.Period.dayofyear.html,pandas.Period.dayofyear,Period.dayofyear# Return the day of the year. This attribute returns the day of the year on which the particular date occurs. The return value ranges between 1 to 365 for regular years and 1 to 366 for leap years.,Returns: intThe day of year.,"['>>> period = pd.Period(""2015-10-23"", freq=\'h\')\n>>> period.day_of_year\n296\n>>> period = pd.Period(""2012-12-31"", freq=\'D\')\n>>> period.day_of_year\n366\n>>> period = pd.Period(""2013-01-01"", freq=\'D\')\n>>> period.day_of_year\n1']"
1098,..\pandas\reference\api\pandas.DataFrame.cumprod.html,pandas.DataFrame.cumprod,"DataFrame.cumprod(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative product over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative product.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: Series or DataFrameReturn cumulative product of Series or DataFrame.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cumprod()\n0     2.0\n1     NaN\n2    10.0\n3   -10.0\n4    -0.0\ndtype: float64', '>>> s.cumprod(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cumprod()\n     A    B\n0  2.0  1.0\n1  6.0  NaN\n2  6.0  0.0', '>>> df.cumprod(axis=1)\n     A    B\n0  2.0  2.0\n1  3.0  NaN\n2  1.0  0.0']"
1099,..\pandas\reference\api\pandas.errors.InvalidIndexError.html,pandas.errors.InvalidIndexError,exception pandas.errors.InvalidIndexError[source]# Exception raised when attempting to use an invalid index key.,No parameters found,"['>>> idx = pd.MultiIndex.from_product([[""x"", ""y""], [0, 1]])\n>>> df = pd.DataFrame([[1, 1, 2, 2],\n...                   [3, 3, 4, 4]], columns=idx)\n>>> df\n    x       y\n    0   1   0   1\n0   1   1   2   2\n1   3   3   4   4\n>>> df[:, 0]\nTraceback (most recent call last):\nInvalidIndexError: (slice(None, None, None), 0)']"
1100,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_anchored.html,pandas.tseries.offsets.DateOffset.is_anchored,DateOffset.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1101,..\pandas\reference\api\pandas.Timestamp.tz_localize.html,pandas.Timestamp.tz_localize,"Timestamp.tz_localize(tz, ambiguous='raise', nonexistent='raise')# Localize the Timestamp to a timezone. Convert naive Timestamp to local time zone or remove timezone from timezone-aware Timestamp.","Parameters: tzstr, pytz.timezone, dateutil.tz.tzfile or NoneTime zone for time which Timestamp will be converted to. None will remove timezone holding local time. ambiguousbool, ‘NaT’, default ‘raise’When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. The behavior is as follows: bool contains flags to determine if time is dst or not (note that this flag is only applicable for ambiguous fall dst dates). ‘NaT’ will return NaT for an ambiguous time. ‘raise’ will raise an AmbiguousTimeError for an ambiguous time. nonexistent‘shift_forward’, ‘shift_backward, ‘NaT’, timedelta, default ‘raise’A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. The behavior is as follows: ‘shift_forward’ will shift the nonexistent time forward to the closest existing time. ‘shift_backward’ will shift the nonexistent time backward to the closest existing time. ‘NaT’ will return NaT where there are nonexistent times. timedelta objects will shift nonexistent times by the timedelta. ‘raise’ will raise an NonExistentTimeError if there are nonexistent times. Returns: localizedTimestamp Raises: TypeErrorIf the Timestamp is tz-aware and tz is not None.","["">>> ts = pd.Timestamp('2020-03-14T15:32:52.192548651')\n>>> ts\nTimestamp('2020-03-14 15:32:52.192548651')"", "">>> ts.tz_localize(tz='Europe/Stockholm')\nTimestamp('2020-03-14 15:32:52.192548651+0100', tz='Europe/Stockholm')"", '>>> pd.NaT.tz_localize()\nNaT']"
1102,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.nanos.html,pandas.tseries.offsets.QuarterEnd.nanos,QuarterEnd.nanos#,No parameters found,[]
1103,..\pandas\reference\api\pandas.errors.InvalidVersion.html,pandas.errors.InvalidVersion,"exception pandas.errors.InvalidVersion[source]# An invalid version was found, users should refer to PEP 440.",No parameters found,"["">>> pd.util.version.Version('1.')\nTraceback (most recent call last):\nInvalidVersion: Invalid version: '1.'""]"
1104,..\pandas\reference\api\pandas.DataFrame.cumsum.html,pandas.DataFrame.cumsum,"DataFrame.cumsum(axis=None, skipna=True, *args, **kwargs)[source]# Return cumulative sum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative sum.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The index or the name of the axis. 0 is equivalent to None or ‘index’. For Series this parameter is unused and defaults to 0. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. *args, **kwargsAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: Series or DataFrameReturn cumulative sum of Series or DataFrame.","['>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64', '>>> s.cumsum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64', '>>> s.cumsum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', "">>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                   columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0"", '>>> df.cumsum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0', '>>> df.cumsum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0']"
1105,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.bfill.html,pandas.core.groupby.DataFrameGroupBy.bfill,DataFrameGroupBy.bfill(limit=None)[source]# Backward fill the values.,"Parameters: limitint, optionalLimit of how many values to fill. Returns: Series or DataFrameObject with missing values filled.","["">>> index = ['Falcon', 'Falcon', 'Parrot', 'Parrot', 'Parrot']\n>>> s = pd.Series([None, 1, None, None, 3], index=index)\n>>> s\nFalcon    NaN\nFalcon    1.0\nParrot    NaN\nParrot    NaN\nParrot    3.0\ndtype: float64\n>>> s.groupby(level=0).bfill()\nFalcon    1.0\nFalcon    1.0\nParrot    3.0\nParrot    3.0\nParrot    3.0\ndtype: float64\n>>> s.groupby(level=0).bfill(limit=1)\nFalcon    1.0\nFalcon    1.0\nParrot    NaN\nParrot    3.0\nParrot    3.0\ndtype: float64"", "">>> df = pd.DataFrame({'A': [1, None, None, None, 4],\n...                    'B': [None, None, 5, None, 7]}, index=index)\n>>> df\n          A         B\nFalcon  1.0       NaN\nFalcon  NaN       NaN\nParrot  NaN       5.0\nParrot  NaN       NaN\nParrot  4.0       7.0\n>>> df.groupby(level=0).bfill()\n          A         B\nFalcon  1.0       NaN\nFalcon  NaN       NaN\nParrot  4.0       5.0\nParrot  4.0       7.0\nParrot  4.0       7.0\n>>> df.groupby(level=0).bfill(limit=1)\n          A         B\nFalcon  1.0       NaN\nFalcon  NaN       NaN\nParrot  NaN       5.0\nParrot  4.0       7.0\nParrot  4.0       7.0""]"
1106,..\pandas\reference\api\pandas.Series.nbytes.html,pandas.Series.nbytes,property Series.nbytes[source]# Return the number of bytes in the underlying data.,No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.nbytes\n24"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.nbytes\n24""]"
1107,..\pandas\reference\api\pandas.Period.daysinmonth.html,pandas.Period.daysinmonth,Period.daysinmonth# Get the total number of days of the month that this period falls on.,Returns: int,"['>>> p = pd.Period(""2018-03-11"", freq=\'h\')\n>>> p.daysinmonth\n31']"
1108,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_month_end.html,pandas.tseries.offsets.DateOffset.is_month_end,DateOffset.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1109,..\pandas\reference\api\pandas.Timestamp.unit.html,pandas.Timestamp.unit,Timestamp.unit# The abbreviation associated with self._creso.,No parameters found,"['>>> pd.Timestamp(""2020-01-01 12:34:56"").unit\n\'s\'', '>>> pd.Timestamp(""2020-01-01 12:34:56.123"").unit\n\'ms\'', '>>> pd.Timestamp(""2020-01-01 12:34:56.123456"").unit\n\'us\'', '>>> pd.Timestamp(""2020-01-01 12:34:56.123456789"").unit\n\'ns\'']"
1110,..\pandas\reference\api\pandas.errors.LossySetitemError.html,pandas.errors.LossySetitemError,exception pandas.errors.LossySetitemError[source]# Raised when trying to do a __setitem__ on an np.ndarray that is not lossless.,No parameters found,[]
1111,..\pandas\reference\api\pandas.DataFrame.describe.html,pandas.DataFrame.describe,"DataFrame.describe(percentiles=None, include=None, exclude=None)[source]# Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Notes For numeric data, the result’s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value’s frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.","Parameters: percentileslist-like of numbers, optionalThe percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles. include‘all’, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored for Series. Here are the options: ‘all’ : All columns of the input will be included in the output. A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category' None (default) : The result will include all numeric columns. excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored for Series. Here are the options: A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category' None (default) : The result will exclude nothing. Returns: Series or DataFrameSummary statistics of the Series or Dataframe provided.","['>>> s = pd.Series([1, 2, 3])\n>>> s.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\ndtype: float64', "">>> s = pd.Series(['a', 'a', 'b', 'c'])\n>>> s.describe()\ncount     4\nunique    3\ntop       a\nfreq      2\ndtype: object"", '>>> s = pd.Series([\n...     np.datetime64(""2000-01-01""),\n...     np.datetime64(""2010-01-01""),\n...     np.datetime64(""2010-01-01"")\n... ])\n>>> s.describe()\ncount                      3\nmean     2006-09-01 08:00:00\nmin      2000-01-01 00:00:00\n25%      2004-12-31 12:00:00\n50%      2010-01-01 00:00:00\n75%      2010-01-01 00:00:00\nmax      2010-01-01 00:00:00\ndtype: object', "">>> df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n...                    'numeric': [1, 2, 3],\n...                    'object': ['a', 'b', 'c']\n...                    })\n>>> df.describe()\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0"", "">>> df.describe(include='all')  \n       categorical  numeric object\ncount            3      3.0      3\nunique           3      NaN      3\ntop              f      NaN      a\nfreq             1      NaN      1\nmean           NaN      2.0    NaN\nstd            NaN      1.0    NaN\nmin            NaN      1.0    NaN\n25%            NaN      1.5    NaN\n50%            NaN      2.0    NaN\n75%            NaN      2.5    NaN\nmax            NaN      3.0    NaN"", '>>> df.numeric.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\nName: numeric, dtype: float64', '>>> df.describe(include=[np.number])\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0', '>>> df.describe(include=[object])  \n       object\ncount       3\nunique      3\ntop         a\nfreq        1', "">>> df.describe(include=['category'])\n       categorical\ncount            3\nunique           3\ntop              d\nfreq             1"", '>>> df.describe(exclude=[np.number])  \n       categorical object\ncount            3      3\nunique           3      3\ntop              f      a\nfreq             1      1', '>>> df.describe(exclude=[object])  \n       categorical  numeric\ncount            3      3.0\nunique           3      NaN\ntop              f      NaN\nfreq             1      NaN\nmean           NaN      2.0\nstd            NaN      1.0\nmin            NaN      1.0\n25%            NaN      1.5\n50%            NaN      2.0\n75%            NaN      2.5\nmax            NaN      3.0']"
1112,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.normalize.html,pandas.tseries.offsets.QuarterEnd.normalize,QuarterEnd.normalize#,No parameters found,[]
1113,..\pandas\reference\api\pandas.Period.days_in_month.html,pandas.Period.days_in_month,Period.days_in_month# Get the total number of days in the month that this period falls on.,Returns: int,"["">>> p = pd.Period('2018-2-17')\n>>> p.days_in_month\n28"", "">>> pd.Period('2018-03-01').days_in_month\n31"", "">>> p = pd.Period('2016-2-17')\n>>> p.days_in_month\n29""]"
1114,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.boxplot.html,pandas.core.groupby.DataFrameGroupBy.boxplot,"DataFrameGroupBy.boxplot(subplots=True, column=None, fontsize=None, rot=0, grid=True, ax=None, figsize=None, layout=None, sharex=False, sharey=True, backend=None, **kwargs)[source]# Make box plots from DataFrameGroupBy data.","Parameters: groupedGrouped DataFrame subplotsbool False - no subplots will be used True - create a subplot for each group. columncolumn name or list of names, or vectorCan be any valid input to groupby. fontsizefloat or str rotlabel rotation angle gridSetting this to True will show the grid axMatplotlib axis object, default None figsizeA tuple (width, height) in inches layouttuple (optional)The layout of the plot: (rows, columns). sharexbool, default FalseWhether x-axes will be shared among subplots. shareybool, default TrueWhether y-axes will be shared among subplots. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. **kwargsAll other plotting keyword arguments to be passed to matplotlib’s boxplot function. Returns: dict of key/value = group key/DataFrame.boxplot return value or DataFrame.boxplot return value in case subplots=figures=False","["">>> import itertools\n>>> tuples = [t for t in itertools.product(range(1000), range(4))]\n>>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])\n>>> data = np.random.randn(len(index), 4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)\n>>> grouped = df.groupby(level='lvl1')\n>>> grouped.boxplot(rot=45, fontsize=12, figsize=(8, 10))"", '>>> grouped.boxplot(subplots=False, rot=45, fontsize=12)']"
1115,..\pandas\reference\api\pandas.Series.ndim.html,pandas.Series.ndim,"property Series.ndim[source]# Number of dimensions of the underlying data, by definition 1.",No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.ndim\n1"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.ndim\n1""]"
1116,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_month_start.html,pandas.tseries.offsets.DateOffset.is_month_start,DateOffset.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1117,..\pandas\reference\api\pandas.errors.MergeError.html,pandas.errors.MergeError,exception pandas.errors.MergeError[source]# Exception raised when merging data. Subclass of ValueError.,No parameters found,"['>>> left = pd.DataFrame({""a"": [""a"", ""b"", ""b"", ""d""],\n...                     ""b"": [""cat"", ""dog"", ""weasel"", ""horse""]},\n...                     index=range(4))\n>>> right = pd.DataFrame({""a"": [""a"", ""b"", ""c"", ""d""],\n...                      ""c"": [""meow"", ""bark"", ""chirp"", ""nay""]},\n...                      index=range(4)).set_index(""a"")\n>>> left.join(right, on=""a"", validate=""one_to_one"",)\nTraceback (most recent call last):\nMergeError: Merge keys are not unique in left dataset; not a one-to-one merge']"
1118,..\pandas\reference\api\pandas.Timestamp.utcfromtimestamp.html,pandas.Timestamp.utcfromtimestamp,classmethod Timestamp.utcfromtimestamp(ts)# Construct a timezone-aware UTC datetime from a POSIX timestamp. Notes Timestamp.utcfromtimestamp behavior differs from datetime.utcfromtimestamp in returning a timezone-aware object.,No parameters found,"["">>> pd.Timestamp.utcfromtimestamp(1584199972)\nTimestamp('2020-03-14 15:32:52+0000', tz='UTC')""]"
1119,..\pandas\reference\api\pandas.DataFrame.diff.html,pandas.DataFrame.diff,"DataFrame.diff(periods=1, axis=0)[source]# First discrete difference of element. Calculates the difference of a DataFrame element compared with another element in the DataFrame (default is element in previous row). Notes For boolean dtypes, this uses operator.xor() rather than operator.sub(). The result is calculated according to current dtype in DataFrame, however dtype of the result is always float64.","Parameters: periodsint, default 1Periods to shift for calculating difference, accepts negative values. axis{0 or ‘index’, 1 or ‘columns’}, default 0Take difference over rows (0) or columns (1). Returns: DataFrameFirst differences of the Series.","["">>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n...                    'b': [1, 1, 2, 3, 5, 8],\n...                    'c': [1, 4, 9, 16, 25, 36]})\n>>> df\n   a  b   c\n0  1  1   1\n1  2  1   4\n2  3  2   9\n3  4  3  16\n4  5  5  25\n5  6  8  36"", '>>> df.diff()\n     a    b     c\n0  NaN  NaN   NaN\n1  1.0  0.0   3.0\n2  1.0  1.0   5.0\n3  1.0  1.0   7.0\n4  1.0  2.0   9.0\n5  1.0  3.0  11.0', '>>> df.diff(axis=1)\n    a  b   c\n0 NaN  0   0\n1 NaN -1   3\n2 NaN -1   7\n3 NaN -1  13\n4 NaN  0  20\n5 NaN  2  28', '>>> df.diff(periods=3)\n     a    b     c\n0  NaN  NaN   NaN\n1  NaN  NaN   NaN\n2  NaN  NaN   NaN\n3  3.0  2.0  15.0\n4  3.0  4.0  21.0\n5  3.0  6.0  27.0', '>>> df.diff(periods=-1)\n     a    b     c\n0 -1.0  0.0  -3.0\n1 -1.0 -1.0  -5.0\n2 -1.0 -1.0  -7.0\n3 -1.0 -2.0  -9.0\n4 -1.0 -3.0 -11.0\n5  NaN  NaN   NaN', "">>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\n>>> df.diff()\n       a\n0    NaN\n1  255.0""]"
1120,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.rule_code.html,pandas.tseries.offsets.QuarterEnd.rule_code,QuarterEnd.rule_code#,No parameters found,[]
1121,..\pandas\reference\api\pandas.Period.day_of_week.html,pandas.Period.day_of_week,"Period.day_of_week# Day of the week the period lies in, with Monday=0 and Sunday=6. If the period frequency is lower than daily (e.g. hourly), and the period spans over multiple days, the day at the start of the period is used. If the frequency is higher than daily (e.g. monthly), the last day of the period is used.",Returns: intDay of the week.,"["">>> per = pd.Period('2017-12-31 22:00', 'h')\n>>> per.day_of_week\n6"", "">>> per = pd.Period('2017-12-31 22:00', '4h')\n>>> per.day_of_week\n6\n>>> per.start_time.day_of_week\n6"", "">>> per = pd.Period('2018-01', 'M')\n>>> per.day_of_week\n2\n>>> per.end_time.day_of_week\n2""]"
1122,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.corr.html,pandas.core.groupby.DataFrameGroupBy.corr,"DataFrameGroupBy.corr(method='pearson', min_periods=1, numeric_only=False)[source]# Compute pairwise correlation of columns, excluding NA/null values. Notes Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations. Pearson correlation coefficient Kendall rank correlation coefficient Spearman’s rank correlation coefficient","Parameters: method{‘pearson’, ‘kendall’, ‘spearman’} or callableMethod of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarraysand returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable’s behavior. min_periodsint, optionalMinimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: The default value of numeric_only is now False. Returns: DataFrameCorrelation matrix.","["">>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(method=histogram_intersection)\n      dogs  cats\ndogs   1.0   0.3\ncats   0.3   1.0"", "">>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(min_periods=3)\n      dogs  cats\ndogs   1.0   NaN\ncats   NaN   1.0""]"
1123,..\pandas\reference\api\pandas.Series.ne.html,pandas.Series.ne,"Series.ne(other, level=None, fill_value=None, axis=0)[source]# Return Not equal to of series and other, element-wise (binary operator ne). Equivalent to series != other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.ne(b, fill_value=0)\na    False\nb     True\nc     True\nd     True\ne     True\ndtype: bool""]"
1124,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_on_offset.html,pandas.tseries.offsets.DateOffset.is_on_offset,DateOffset.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1125,..\pandas\reference\api\pandas.tseries.offsets.QuarterEnd.startingMonth.html,pandas.tseries.offsets.QuarterEnd.startingMonth,QuarterEnd.startingMonth#,No parameters found,[]
1126,..\pandas\reference\api\pandas.errors.NoBufferPresent.html,pandas.errors.NoBufferPresent,exception pandas.errors.NoBufferPresent[source]# Exception is raised in _get_data_buffer to signal that there is no requested buffer.,No parameters found,[]
1127,..\pandas\reference\api\pandas.DataFrame.div.html,pandas.DataFrame.div,"DataFrame.div(other, axis='columns', level=None, fill_value=None)[source]# Get Floating division of dataframe and other, element-wise (binary operator truediv). Equivalent to dataframe / other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1128,..\pandas\reference\api\pandas.Period.day_of_year.html,pandas.Period.day_of_year,Period.day_of_year# Return the day of the year. This attribute returns the day of the year on which the particular date occurs. The return value ranges between 1 to 365 for regular years and 1 to 366 for leap years.,Returns: intThe day of year.,"['>>> period = pd.Period(""2015-10-23"", freq=\'h\')\n>>> period.day_of_year\n296\n>>> period = pd.Period(""2012-12-31"", freq=\'D\')\n>>> period.day_of_year\n366\n>>> period = pd.Period(""2013-01-01"", freq=\'D\')\n>>> period.day_of_year\n1']"
1129,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.corrwith.html,pandas.core.groupby.DataFrameGroupBy.corrwith,"DataFrameGroupBy.corrwith(other, axis=<no_default>, drop=False, method='pearson', numeric_only=False)[source]# Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations.","Parameters: otherDataFrame, SeriesObject with which to compute correlations. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to use. 0 or ‘index’ to compute row-wise, 1 or ‘columns’ for column-wise. dropbool, default FalseDrop missing indices from result. method{‘pearson’, ‘kendall’, ‘spearman’} or callableMethod of correlation: pearson : standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: callable with input two 1d ndarraysand returning a float. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: The default value of numeric_only is now False. Returns: SeriesPairwise correlations.","['>>> index = [""a"", ""b"", ""c"", ""d"", ""e""]\n>>> columns = [""one"", ""two"", ""three"", ""four""]\n>>> df1 = pd.DataFrame(np.arange(20).reshape(5, 4), index=index, columns=columns)\n>>> df2 = pd.DataFrame(np.arange(16).reshape(4, 4), index=index[:4], columns=columns)\n>>> df1.corrwith(df2)\none      1.0\ntwo      1.0\nthree    1.0\nfour     1.0\ndtype: float64', '>>> df2.corrwith(df1, axis=1)\na    1.0\nb    1.0\nc    1.0\nd    1.0\ne    NaN\ndtype: float64']"
1130,..\pandas\reference\api\pandas.Timestamp.utcnow.html,pandas.Timestamp.utcnow,classmethod Timestamp.utcnow()# Return a new Timestamp representing UTC day and time.,No parameters found,"["">>> pd.Timestamp.utcnow()   \nTimestamp('2020-11-16 22:50:18.092888+0000', tz='UTC')""]"
1131,..\pandas\reference\api\pandas.Series.nlargest.html,pandas.Series.nlargest,"Series.nlargest(n=5, keep='first')[source]# Return the largest n elements. Notes Faster than .sort_values(ascending=False).head(n) for small n relative to the size of the Series object.","Parameters: nint, default 5Return this many descending sorted values. keep{‘first’, ‘last’, ‘all’}, default ‘first’When there are duplicate values that cannot all fit in a Series of n elements: first : return the first n occurrences in order of appearance. last : return the last n occurrences in reverse order of appearance. all : keep all occurrences. This can result in a Series of size larger than n. Returns: SeriesThe n largest values in the Series, sorted in decreasing order.","['>>> countries_population = {""Italy"": 59000000, ""France"": 65000000,\n...                         ""Malta"": 434000, ""Maldives"": 434000,\n...                         ""Brunei"": 434000, ""Iceland"": 337000,\n...                         ""Nauru"": 11300, ""Tuvalu"": 11300,\n...                         ""Anguilla"": 11300, ""Montserrat"": 5200}\n>>> s = pd.Series(countries_population)\n>>> s\nItaly       59000000\nFrance      65000000\nMalta         434000\nMaldives      434000\nBrunei        434000\nIceland       337000\nNauru          11300\nTuvalu         11300\nAnguilla       11300\nMontserrat      5200\ndtype: int64', '>>> s.nlargest()\nFrance      65000000\nItaly       59000000\nMalta         434000\nMaldives      434000\nBrunei        434000\ndtype: int64', '>>> s.nlargest(3)\nFrance    65000000\nItaly     59000000\nMalta       434000\ndtype: int64', "">>> s.nlargest(3, keep='last')\nFrance      65000000\nItaly       59000000\nBrunei        434000\ndtype: int64"", "">>> s.nlargest(3, keep='all')\nFrance      65000000\nItaly       59000000\nMalta         434000\nMaldives      434000\nBrunei        434000\ndtype: int64""]"
1132,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_quarter_end.html,pandas.tseries.offsets.DateOffset.is_quarter_end,DateOffset.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1133,..\pandas\reference\api\pandas.tseries.offsets.Second.copy.html,pandas.tseries.offsets.Second.copy,Second.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1134,..\pandas\reference\api\pandas.errors.NullFrequencyError.html,pandas.errors.NullFrequencyError,"exception pandas.errors.NullFrequencyError[source]# Exception raised when a freq cannot be null. Particularly DatetimeIndex.shift, TimedeltaIndex.shift, PeriodIndex.shift.",No parameters found,"['>>> df = pd.DatetimeIndex([""2011-01-01 10:00"", ""2011-01-01""], freq=None)\n>>> df.shift(2)\nTraceback (most recent call last):\nNullFrequencyError: Cannot shift with no freq']"
1135,..\pandas\reference\api\pandas.Period.end_time.html,pandas.Period.end_time,Period.end_time# Get the Timestamp for the end of the period.,Returns: Timestamp,"["">>> pd.Period('2020-01', 'D').end_time\nTimestamp('2020-01-01 23:59:59.999999999')"", "">>> period_index = pd.period_range('2020-1-1 00:00', '2020-3-1 00:00', freq='M')\n>>> s = pd.Series(period_index)\n>>> s\n0   2020-01\n1   2020-02\n2   2020-03\ndtype: period[M]\n>>> s.dt.end_time\n0   2020-01-31 23:59:59.999999999\n1   2020-02-29 23:59:59.999999999\n2   2020-03-31 23:59:59.999999999\ndtype: datetime64[ns]"", '>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.end_time\nDatetimeIndex([\'2023-01-31 23:59:59.999999999\',\n               \'2023-02-28 23:59:59.999999999\',\n               \'2023-03-31 23:59:59.999999999\'],\n               dtype=\'datetime64[ns]\', freq=None)']"
1136,..\pandas\reference\api\pandas.errors.NumbaUtilError.html,pandas.errors.NumbaUtilError,exception pandas.errors.NumbaUtilError[source]# Error raised for unsupported Numba engine routines.,No parameters found,"['>>> df = pd.DataFrame({""key"": [""a"", ""a"", ""b"", ""b""], ""data"": [1, 2, 3, 4]},\n...                   columns=[""key"", ""data""])\n>>> def incorrect_function(x):\n...     return sum(x) * 2.7\n>>> df.groupby(""key"").agg(incorrect_function, engine=""numba"")\nTraceback (most recent call last):\nNumbaUtilError: The first 2 arguments to incorrect_function\nmust be [\'values\', \'index\']']"
1137,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_quarter_start.html,pandas.tseries.offsets.DateOffset.is_quarter_start,DateOffset.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1138,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.count.html,pandas.core.groupby.DataFrameGroupBy.count,"DataFrameGroupBy.count()[source]# Compute count of group, excluding missing values.",Returns: Series or DataFrameCount of values within each group.,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, np.nan], index=lst)\n>>> ser\na    1.0\na    2.0\nb    NaN\ndtype: float64\n>>> ser.groupby(level=0).count()\na    2\nb    0\ndtype: int64"", '>>> data = [[1, np.nan, 3], [1, np.nan, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a         b     c\ncow     1       NaN     3\nhorse   1       NaN     6\nbull    7       8.0     9\n>>> df.groupby(""a"").count()\n    b   c\na\n1   0   2\n7   1   1', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').count()\n2023-01-01    2\n2023-02-01    2\nFreq: MS, dtype: int64""]"
1139,..\pandas\reference\api\pandas.DataFrame.dot.html,pandas.DataFrame.dot,"DataFrame.dot(other)[source]# Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using self @ other. Notes The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here.","Parameters: otherSeries, DataFrame or array-likeThe other object to compute the matrix product with. Returns: Series or DataFrameIf other is a Series, return the matrix product between self and other as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array.","['>>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\n>>> s = pd.Series([1, 1, 2, 1])\n>>> df.dot(s)\n0    -4\n1     5\ndtype: int64', '>>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\n>>> df.dot(other)\n    0   1\n0   1   4\n1   2   2', '>>> df @ other\n    0   1\n0   1   4\n1   2   2', '>>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\n>>> df.dot(arr)\n    0   1\n0   1   4\n1   2   2', '>>> s2 = s.reindex([1, 0, 2, 3])\n>>> df.dot(s2)\n0    -4\n1     5\ndtype: int64']"
1140,..\pandas\reference\api\pandas.tseries.offsets.Second.delta.html,pandas.tseries.offsets.Second.delta,Second.delta#,No parameters found,[]
1141,..\pandas\reference\api\pandas.Series.notna.html,pandas.Series.notna,"Series.notna()[source]# Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.",Returns: SeriesMask of bool values for each element in Series that indicates whether an element is not an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.notna()\n     age   born  name    toy\n0   True  False  True  False\n1   True   True  True   True\n2  False   True  True   True', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.notna()\n0     True\n1     True\n2    False\ndtype: bool']"
1142,..\pandas\reference\api\pandas.Timestamp.utcoffset.html,pandas.Timestamp.utcoffset,Timestamp.utcoffset()# Return utc offset.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00', tz='Europe/Brussels')\n>>> ts\nTimestamp('2023-01-01 10:00:00+0100', tz='Europe/Brussels')\n>>> ts.utcoffset()\ndatetime.timedelta(seconds=3600)""]"
1143,..\pandas\reference\api\pandas.Period.freq.html,pandas.Period.freq,Period.freq#,No parameters found,[]
1144,..\pandas\reference\api\pandas.errors.NumExprClobberingError.html,pandas.errors.NumExprClobberingError,exception pandas.errors.NumExprClobberingError[source]# Exception raised when trying to use a built-in numexpr name as a variable name. eval or query will throw the error if the engine is set to ‘numexpr’. ‘numexpr’ is the default engine value for these methods if the numexpr package is installed.,No parameters found,"['>>> df = pd.DataFrame({\'abs\': [1, 1, 1]})\n>>> df.query(""abs > 2"") \n... # NumExprClobberingError: Variables in expression ""(abs) > (2)"" overlap...\n>>> sin, a = 1, 2\n>>> pd.eval(""sin + a"", engine=\'numexpr\') \n... # NumExprClobberingError: Variables in expression ""(sin) + (a)"" overlap...']"
1145,..\pandas\reference\api\pandas.DataFrame.drop.html,pandas.DataFrame.drop,"DataFrame.drop(labels=None, *, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')[source]# Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. See the user guide for more information about the now unused levels.","Parameters: labelssingle label or list-likeIndex or column labels to drop. A tuple will be used as a single label and not treated as a list-like. axis{0 or ‘index’, 1 or ‘columns’}, default 0Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’). indexsingle label or list-likeAlternative to specifying axis (labels, axis=0 is equivalent to index=labels). columnssingle label or list-likeAlternative to specifying axis (labels, axis=1 is equivalent to columns=labels). levelint or level name, optionalFor MultiIndex, level from which the labels will be removed. inplacebool, default FalseIf False, return a copy. Otherwise, do operation in place and return None. errors{‘ignore’, ‘raise’}, default ‘raise’If ‘ignore’, suppress error and only existing labels are dropped. Returns: DataFrame or NoneReturns DataFrame or None DataFrame with the specified index or column labels removed or None if inplace=True. Raises: KeyErrorIf any of the labels is not found in the selected axis.","["">>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n...                   columns=['A', 'B', 'C', 'D'])\n>>> df\n   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11"", "">>> df.drop(['B', 'C'], axis=1)\n   A   D\n0  0   3\n1  4   7\n2  8  11"", "">>> df.drop(columns=['B', 'C'])\n   A   D\n0  0   3\n1  4   7\n2  8  11"", '>>> df.drop([0, 1])\n   A  B   C   D\n2  8  9  10  11', "">>> midx = pd.MultiIndex(levels=[['llama', 'cow', 'falcon'],\n...                              ['speed', 'weight', 'length']],\n...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n...                         [250, 150], [1.5, 0.8], [320, 250],\n...                         [1, 0.8], [0.3, 0.2]])\n>>> df\n                big     small\nllama   speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8\n        length  0.3     0.2"", "">>> df.drop(index=('falcon', 'weight'))\n                big     small\nllama   speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        length  0.3     0.2"", "">>> df.drop(index='cow', columns='small')\n                big\nllama   speed   45.0\n        weight  200.0\n        length  1.5\nfalcon  speed   320.0\n        weight  1.0\n        length  0.3"", "">>> df.drop(index='length', level=1)\n                big     small\nllama   speed   45.0    30.0\n        weight  200.0   100.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8""]"
1146,..\pandas\reference\api\pandas.Period.freqstr.html,pandas.Period.freqstr,Period.freqstr# Return a string representation of the frequency.,No parameters found,"["">>> pd.Period('2020-01', 'D').freqstr\n'D'""]"
1147,..\pandas\reference\api\pandas.errors.OptionError.html,pandas.errors.OptionError,exception pandas.errors.OptionError[source]# Exception raised for pandas.options. Backwards compatible with KeyError checks.,No parameters found,['>>> pd.options.context\nTraceback (most recent call last):\nOptionError: No such option']
1148,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.cov.html,pandas.core.groupby.DataFrameGroupBy.cov,"DataFrameGroupBy.cov(min_periods=None, ddof=1, numeric_only=False)[source]# Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN. This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Notes Returns the covariance matrix of the DataFrame’s time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is missing at random) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices for more details.","Parameters: min_periodsint, optionalMinimum number of observations required per pair of columns to have a valid result. ddofint, default 1Delta degrees of freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. This argument is applicable only when no nan is in the dataframe. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: The default value of numeric_only is now False. Returns: DataFrameThe covariance matrix of the series of the DataFrame.","["">>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\n...                   columns=['dogs', 'cats'])\n>>> df.cov()\n          dogs      cats\ndogs  0.666667 -1.000000\ncats -1.000000  1.666667"", "">>> np.random.seed(42)\n>>> df = pd.DataFrame(np.random.randn(1000, 5),\n...                   columns=['a', 'b', 'c', 'd', 'e'])\n>>> df.cov()\n          a         b         c         d         e\na  0.998438 -0.020161  0.059277 -0.008943  0.014144\nb -0.020161  1.059352 -0.008543 -0.024738  0.009826\nc  0.059277 -0.008543  1.010670 -0.001486 -0.000271\nd -0.008943 -0.024738 -0.001486  0.921297 -0.013692\ne  0.014144  0.009826 -0.000271 -0.013692  0.977795"", "">>> np.random.seed(42)\n>>> df = pd.DataFrame(np.random.randn(20, 3),\n...                   columns=['a', 'b', 'c'])\n>>> df.loc[df.index[:5], 'a'] = np.nan\n>>> df.loc[df.index[5:10], 'b'] = np.nan\n>>> df.cov(min_periods=12)\n          a         b         c\na  0.316741       NaN -0.150812\nb       NaN  1.248003  0.191417\nc -0.150812  0.191417  0.895202""]"
1149,..\pandas\reference\api\pandas.tseries.offsets.Second.freqstr.html,pandas.tseries.offsets.Second.freqstr,Second.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1150,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_year_end.html,pandas.tseries.offsets.DateOffset.is_year_end,DateOffset.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1151,..\pandas\reference\api\pandas.Timestamp.utctimetuple.html,pandas.Timestamp.utctimetuple,"Timestamp.utctimetuple()# Return UTC time tuple, compatible with time.localtime().",No parameters found,"["">>> ts = pd.Timestamp('2023-01-01 10:00:00', tz='Europe/Brussels')\n>>> ts\nTimestamp('2023-01-01 10:00:00+0100', tz='Europe/Brussels')\n>>> ts.utctimetuple()\ntime.struct_time(tm_year=2023, tm_mon=1, tm_mday=1, tm_hour=9,\ntm_min=0, tm_sec=0, tm_wday=6, tm_yday=1, tm_isdst=0)""]"
1152,..\pandas\reference\api\pandas.Series.notnull.html,pandas.Series.notnull,"Series.notnull()[source]# Series.notnull is an alias for Series.notna. Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.",Returns: SeriesMask of bool values for each element in Series that indicates whether an element is not an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.notna()\n     age   born  name    toy\n0   True  False  True  False\n1   True   True  True   True\n2  False   True  True   True', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.notna()\n0     True\n1     True\n2    False\ndtype: bool']"
1153,..\pandas\reference\api\pandas.DataFrame.droplevel.html,pandas.DataFrame.droplevel,"DataFrame.droplevel(level, axis=0)[source]# Return Series/DataFrame with requested index / column level(s) removed.","Parameters: levelint, str, or list-likeIf a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels. axis{0 or ‘index’, 1 or ‘columns’}, default 0Axis along which the level(s) is removed: 0 or ‘index’: remove level(s) in column. 1 or ‘columns’: remove level(s) in row. For Series this parameter is unused and defaults to 0. Returns: Series/DataFrameSeries/DataFrame with requested index / column level(s) removed.","["">>> df = pd.DataFrame([\n...     [1, 2, 3, 4],\n...     [5, 6, 7, 8],\n...     [9, 10, 11, 12]\n... ]).set_index([0, 1]).rename_axis(['a', 'b'])"", "">>> df.columns = pd.MultiIndex.from_tuples([\n...     ('c', 'e'), ('d', 'f')\n... ], names=['level_1', 'level_2'])"", '>>> df\nlevel_1   c   d\nlevel_2   e   f\na b\n1 2      3   4\n5 6      7   8\n9 10    11  12', "">>> df.droplevel('a')\nlevel_1   c   d\nlevel_2   e   f\nb\n2        3   4\n6        7   8\n10      11  12"", "">>> df.droplevel('level_2', axis=1)\nlevel_1   c   d\na b\n1 2      3   4\n5 6      7   8\n9 10    11  12""]"
1154,..\pandas\reference\api\pandas.Period.hour.html,pandas.Period.hour,Period.hour# Get the hour of the day component of the Period.,"Returns: intThe hour as an integer, between 0 and 23.","['>>> p = pd.Period(""2018-03-11 13:03:12.050000"")\n>>> p.hour\n13', '>>> p = pd.Period(""2018-03-11"", freq=""M"")\n>>> p.hour\n0']"
1155,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.cumcount.html,pandas.core.groupby.DataFrameGroupBy.cumcount,"DataFrameGroupBy.cumcount(ascending=True)[source]# Number each item in each group from 0 to the length of that group - 1. Essentially this is equivalent to self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))","Parameters: ascendingbool, default TrueIf False, number in reverse, from length of group - 1 to 0. Returns: SeriesSequence number of each element within each group.","["">>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n...                   columns=['A'])\n>>> df\n   A\n0  a\n1  a\n2  a\n3  b\n4  b\n5  a\n>>> df.groupby('A').cumcount()\n0    0\n1    1\n2    2\n3    0\n4    1\n5    3\ndtype: int64\n>>> df.groupby('A').cumcount(ascending=False)\n0    3\n1    2\n2    1\n3    1\n4    0\n5    0\ndtype: int64""]"
1156,..\pandas\reference\api\pandas.errors.OutOfBoundsDatetime.html,pandas.errors.OutOfBoundsDatetime,exception pandas.errors.OutOfBoundsDatetime# Raised when the datetime is outside the range that can be represented.,No parameters found,"['>>> pd.to_datetime(""08335394550"")\nTraceback (most recent call last):\nOutOfBoundsDatetime: Parsing ""08335394550"" to datetime overflows,\nat position 0']"
1157,..\pandas\reference\api\pandas.tseries.offsets.Second.html,pandas.tseries.offsets.Second,class pandas.tseries.offsets.Second# Offset n seconds. Examples You can use the parameter n to represent a shift of n seconds. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of seconds represented.","["">>> from pandas.tseries.offsets import Second\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Second(n=10)\nTimestamp('2022-12-09 15:00:10')\n>>> ts - Second(n=10)\nTimestamp('2022-12-09 14:59:50')"", "">>> ts + Second(n=-10)\nTimestamp('2022-12-09 14:59:50')""]"
1158,..\pandas\reference\api\pandas.DataFrame.dropna.html,pandas.DataFrame.dropna,"DataFrame.dropna(*, axis=0, how=<no_default>, thresh=<no_default>, subset=None, inplace=False, ignore_index=False)[source]# Remove missing values. See the User Guide for more on which values are considered missing, and how to work with missing data.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0Determine if rows or columns which contain missing values are removed. 0, or ‘index’ : Drop rows which contain missing values. 1, or ‘columns’ : Drop columns which contain missing value. Only a single axis is allowed. how{‘any’, ‘all’}, default ‘any’Determine if row or column is removed from DataFrame, when we have at least one NA or all NA. ‘any’ : If any NA values are present, drop that row or column. ‘all’ : If all values are NA, drop that row or column. threshint, optionalRequire that many non-NA values. Cannot be combined with how. subsetcolumn label or sequence of labels, optionalLabels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include. inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. Added in version 2.0.0. Returns: DataFrame or NoneDataFrame with NA entries dropped from it or None if inplace=True.","['>>> df = pd.DataFrame({""name"": [\'Alfred\', \'Batman\', \'Catwoman\'],\n...                    ""toy"": [np.nan, \'Batmobile\', \'Bullwhip\'],\n...                    ""born"": [pd.NaT, pd.Timestamp(""1940-04-25""),\n...                             pd.NaT]})\n>>> df\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT', '>>> df.dropna()\n     name        toy       born\n1  Batman  Batmobile 1940-04-25', "">>> df.dropna(axis='columns')\n       name\n0    Alfred\n1    Batman\n2  Catwoman"", "">>> df.dropna(how='all')\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT"", '>>> df.dropna(thresh=2)\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT', "">>> df.dropna(subset=['name', 'toy'])\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT""]"
1159,..\pandas\reference\api\pandas.Timestamp.value.html,pandas.Timestamp.value,Timestamp.value#,No parameters found,[]
1160,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.is_year_start.html,pandas.tseries.offsets.DateOffset.is_year_start,DateOffset.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1161,..\pandas\reference\api\pandas.Period.html,pandas.Period,"class pandas.Period(value=None, freq=None, ordinal=None, year=None, month=None, quarter=None, day=None, hour=None, minute=None, second=None)# Represents a period of time. Examples Attributes day Get day of the month that a Period falls on. day_of_week Day of the week the period lies in, with Monday=0 and Sunday=6. day_of_year Return the day of the year. dayofweek Day of the week the period lies in, with Monday=0 and Sunday=6. dayofyear Return the day of the year. days_in_month Get the total number of days in the month that this period falls on. daysinmonth Get the total number of days of the month that this period falls on. end_time Get the Timestamp for the end of the period. freq freqstr Return a string representation of the frequency. hour Get the hour of the day component of the Period. is_leap_year Return True if the period's year is in a leap year. minute Get minute of the hour component of the Period. month Return the month this Period falls on. ordinal quarter Return the quarter this Period falls on. qyear Fiscal year the Period lies in according to its starting-quarter. second Get the second component of the Period. start_time Get the Timestamp for the start of the period. week Get the week of the year on the given Period. weekday Day of the week the period lies in, with Monday=0 and Sunday=6. weekofyear Get the week of the year on the given Period. year Return the year this Period falls on.","Parameters: valuePeriod, str, datetime, date or pandas.Timestamp, default NoneThe time period represented (e.g., ‘4Q2005’). This represents neither the start or the end of the period, but rather the entire period itself. freqstr, default NoneOne of pandas period strings or corresponding objects. Accepted strings are listed in the period alias section in the user docs. If value is datetime, freq is required. ordinalint, default NoneThe period offset from the proleptic Gregorian epoch. yearint, default NoneYear value of the period. monthint, default 1Month value of the period. quarterint, default NoneQuarter value of the period. dayint, default 1Day value of the period. hourint, default 0Hour value of the period. minuteint, default 0Minute value of the period. secondint, default 0Second value of the period.","["">>> period = pd.Period('2012-1-1', freq='D')\n>>> period\nPeriod('2012-01-01', 'D')""]"
1162,..\pandas\reference\api\pandas.Series.nsmallest.html,pandas.Series.nsmallest,"Series.nsmallest(n=5, keep='first')[source]# Return the smallest n elements. Notes Faster than .sort_values().head(n) for small n relative to the size of the Series object.","Parameters: nint, default 5Return this many ascending sorted values. keep{‘first’, ‘last’, ‘all’}, default ‘first’When there are duplicate values that cannot all fit in a Series of n elements: first : return the first n occurrences in order of appearance. last : return the last n occurrences in reverse order of appearance. all : keep all occurrences. This can result in a Series of size larger than n. Returns: SeriesThe n smallest values in the Series, sorted in increasing order.","['>>> countries_population = {""Italy"": 59000000, ""France"": 65000000,\n...                         ""Brunei"": 434000, ""Malta"": 434000,\n...                         ""Maldives"": 434000, ""Iceland"": 337000,\n...                         ""Nauru"": 11300, ""Tuvalu"": 11300,\n...                         ""Anguilla"": 11300, ""Montserrat"": 5200}\n>>> s = pd.Series(countries_population)\n>>> s\nItaly       59000000\nFrance      65000000\nBrunei        434000\nMalta         434000\nMaldives      434000\nIceland       337000\nNauru          11300\nTuvalu         11300\nAnguilla       11300\nMontserrat      5200\ndtype: int64', '>>> s.nsmallest()\nMontserrat    5200\nNauru        11300\nTuvalu       11300\nAnguilla     11300\nIceland     337000\ndtype: int64', '>>> s.nsmallest(3)\nMontserrat   5200\nNauru       11300\nTuvalu      11300\ndtype: int64', "">>> s.nsmallest(3, keep='last')\nMontserrat   5200\nAnguilla    11300\nTuvalu      11300\ndtype: int64"", "">>> s.nsmallest(3, keep='all')\nMontserrat   5200\nNauru       11300\nTuvalu      11300\nAnguilla    11300\ndtype: int64""]"
1163,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.cummax.html,pandas.core.groupby.DataFrameGroupBy.cummax,"DataFrameGroupBy.cummax(axis=<no_default>, numeric_only=False, **kwargs)[source]# Cumulative max for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 6, 2, 3, 1, 4], index=lst)\n>>> ser\na    1\na    6\na    2\nb    3\nb    1\nb    4\ndtype: int64\n>>> ser.groupby(level=0).cummax()\na    1\na    6\na    6\nb    3\nb    3\nb    4\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 1, 0], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a   b   c\ncow     1   8   2\nhorse   1   1   0\nbull    2   6   9\n>>> df.groupby(""a"").groups\n{1: [\'cow\', \'horse\'], 2: [\'bull\']}\n>>> df.groupby(""a"").cummax()\n        b   c\ncow     8   2\nhorse   8   2\nbull    6   9']"
1164,..\pandas\reference\api\pandas.errors.OutOfBoundsTimedelta.html,pandas.errors.OutOfBoundsTimedelta,exception pandas.errors.OutOfBoundsTimedelta# Raised when encountering a timedelta value that cannot be represented. Representation should be within a timedelta64[ns].,No parameters found,"['>>> pd.date_range(start=""1/1/1700"", freq=""B"", periods=100000)\nTraceback (most recent call last):\nOutOfBoundsTimedelta: Cannot cast 139999 days 00:00:00\nto unit=\'ns\' without overflow.']"
1165,..\pandas\reference\api\pandas.Period.is_leap_year.html,pandas.Period.is_leap_year,Period.is_leap_year# Return True if the period’s year is in a leap year.,No parameters found,"["">>> period = pd.Period('2022-01', 'M')\n>>> period.is_leap_year\nFalse"", "">>> period = pd.Period('2020-01', 'M')\n>>> period.is_leap_year\nTrue""]"
1166,..\pandas\reference\api\pandas.DataFrame.drop_duplicates.html,pandas.DataFrame.drop_duplicates,"DataFrame.drop_duplicates(subset=None, *, keep='first', inplace=False, ignore_index=False)[source]# Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.","Parameters: subsetcolumn label or sequence of labels, optionalOnly consider certain columns for identifying duplicates, by default use all of the columns. keep{‘first’, ‘last’, False}, default ‘first’Determines which duplicates (if any) to keep. ‘first’ : Drop duplicates except for the first occurrence. ‘last’ : Drop duplicates except for the last occurrence. False : Drop all duplicates. inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. Returns: DataFrame or NoneDataFrame with duplicates removed or None if inplace=True.","["">>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0"", '>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0', "">>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5"", "">>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0""]"
1167,..\pandas\reference\api\pandas.Timestamp.week.html,pandas.Timestamp.week,Timestamp.week# Return the week number of the year.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.week\n11']"
1168,..\pandas\reference\api\pandas.Series.nunique.html,pandas.Series.nunique,Series.nunique(dropna=True)[source]# Return number of unique elements in the object. Excludes NA values by default.,"Parameters: dropnabool, default TrueDon’t include NaN in the count. Returns: int","['>>> s = pd.Series([1, 3, 5, 7, 7])\n>>> s\n0    1\n1    3\n2    5\n3    7\n4    7\ndtype: int64', '>>> s.nunique()\n4']"
1169,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.cummin.html,pandas.core.groupby.DataFrameGroupBy.cummin,"DataFrameGroupBy.cummin(axis=<no_default>, numeric_only=False, **kwargs)[source]# Cumulative min for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 6, 2, 3, 0, 4], index=lst)\n>>> ser\na    1\na    6\na    2\nb    3\nb    0\nb    4\ndtype: int64\n>>> ser.groupby(level=0).cummin()\na    1\na    1\na    1\nb    3\nb    0\nb    0\ndtype: int64"", '>>> data = [[1, 0, 2], [1, 1, 5], [6, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""snake"", ""rabbit"", ""turtle""])\n>>> df\n        a   b   c\nsnake   1   0   2\nrabbit  1   1   5\nturtle  6   6   9\n>>> df.groupby(""a"").groups\n{1: [\'snake\', \'rabbit\'], 6: [\'turtle\']}\n>>> df.groupby(""a"").cummin()\n        b   c\nsnake   0   2\nrabbit  0   2\nturtle  6   9']"
1170,..\pandas\reference\api\pandas.errors.ParserError.html,pandas.errors.ParserError,exception pandas.errors.ParserError[source]# Exception that is raised by an error encountered in parsing file contents. This is a generic error raised for errors encountered when functions like read_csv or read_html are parsing contents of a file.,No parameters found,"['>>> data = \'\'\'a,b,c\n... cat,foo,bar\n... dog,foo,""baz\'\'\'\n>>> from io import StringIO\n>>> pd.read_csv(StringIO(data), skipfooter=1, engine=\'python\')\nTraceback (most recent call last):\nParserError: \',\' expected after \'""\'. Error could possibly be due\nto parsing errors in the skipped footer rows']"
1171,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.kwds.html,pandas.tseries.offsets.DateOffset.kwds,DateOffset.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1172,..\pandas\reference\api\pandas.tseries.offsets.Second.is_anchored.html,pandas.tseries.offsets.Second.is_anchored,Second.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
1173,..\pandas\reference\api\pandas.Period.minute.html,pandas.Period.minute,Period.minute# Get minute of the hour component of the Period.,"Returns: intThe minute as an integer, between 0 and 59.","['>>> p = pd.Period(""2018-03-11 13:03:12.050000"")\n>>> p.minute\n3']"
1174,..\pandas\reference\api\pandas.errors.ParserWarning.html,pandas.errors.ParserWarning,"exception pandas.errors.ParserWarning[source]# Warning raised when reading a file that doesn’t use the default ‘c’ parser. Raised by pd.read_csv and pd.read_table when it is necessary to change parsers, generally from the default ‘c’ parser to ‘python’. It happens due to a lack of support or functionality for parsing a particular attribute of a CSV file with the requested engine. Currently, ‘c’ unsupported options include the following parameters: sep other than a single character (e.g. regex separators) skipfooter higher than 0 sep=None with delim_whitespace=False The warning can be avoided by adding engine=’python’ as a parameter in pd.read_csv and pd.read_table methods.",No parameters found,"["">>> import io\n>>> csv = '''a;b;c\n...           1;1,8\n...           1;2,1'''\n>>> df = pd.read_csv(io.StringIO(csv), sep='[;,]')  \n... # ParserWarning: Falling back to the 'python' engine..."", "">>> df = pd.read_csv(io.StringIO(csv), sep='[;,]', engine='python')""]"
1175,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.cumprod.html,pandas.core.groupby.DataFrameGroupBy.cumprod,"DataFrameGroupBy.cumprod(axis=<no_default>, *args, **kwargs)[source]# Cumulative product for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([6, 2, 0], index=lst)\n>>> ser\na    6\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).cumprod()\na    6\na   12\nb    0\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a   b   c\ncow     1   8   2\nhorse   1   2   5\nbull    2   6   9\n>>> df.groupby(""a"").groups\n{1: [\'cow\', \'horse\'], 2: [\'bull\']}\n>>> df.groupby(""a"").cumprod()\n        b   c\ncow     8   2\nhorse  16  10\nbull    6   9']"
1176,..\pandas\reference\api\pandas.Timestamp.weekday.html,pandas.Timestamp.weekday,Timestamp.weekday()# Return the day of the week represented by the date. Monday == 0 … Sunday == 6.,No parameters found,"["">>> ts = pd.Timestamp('2023-01-01')\n>>> ts\nTimestamp('2023-01-01  00:00:00')\n>>> ts.weekday()\n6""]"
1177,..\pandas\reference\api\pandas.tseries.offsets.Second.is_month_end.html,pandas.tseries.offsets.Second.is_month_end,Second.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1178,..\pandas\reference\api\pandas.Series.pad.html,pandas.Series.pad,"Series.pad(*, axis=None, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values by propagating the last valid observation to next valid. Deprecated since version 2.0: Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead.",Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.,[]
1179,..\pandas\reference\api\pandas.Period.month.html,pandas.Period.month,Period.month# Return the month this Period falls on.,No parameters found,"["">>> period = pd.Period('2022-01', 'M')\n>>> period.month\n1""]"
1180,..\pandas\reference\api\pandas.DataFrame.dtypes.html,pandas.DataFrame.dtypes,property DataFrame.dtypes[source]# Return the dtypes in the DataFrame. This returns a Series with the data type of each column. The result’s index is the original DataFrame’s columns. Columns with mixed types are stored with the object dtype. See the User Guide for more.,Returns: pandas.SeriesThe data type of each column.,"["">>> df = pd.DataFrame({'float': [1.0],\n...                    'int': [1],\n...                    'datetime': [pd.Timestamp('20180310')],\n...                    'string': ['foo']})\n>>> df.dtypes\nfloat              float64\nint                  int64\ndatetime    datetime64[ns]\nstring              object\ndtype: object""]"
1181,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.n.html,pandas.tseries.offsets.DateOffset.n,DateOffset.n#,No parameters found,[]
1182,..\pandas\reference\api\pandas.errors.PerformanceWarning.html,pandas.errors.PerformanceWarning,exception pandas.errors.PerformanceWarning[source]# Warning raised when there is a possible performance impact.,No parameters found,"['>>> df = pd.DataFrame({""jim"": [0, 0, 1, 1],\n...                    ""joe"": [""x"", ""x"", ""z"", ""y""],\n...                    ""jolie"": [1, 2, 3, 4]})\n>>> df = df.set_index([""jim"", ""joe""])\n>>> df\n          jolie\njim  joe\n0    x    1\n     x    2\n1    z    3\n     y    4\n>>> df.loc[(1, \'z\')]  \n# PerformanceWarning: indexing past lexsort depth may impact performance.\ndf.loc[(1, \'z\')]\n          jolie\njim  joe\n1    z        3']"
1183,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.cumsum.html,pandas.core.groupby.DataFrameGroupBy.cumsum,"DataFrameGroupBy.cumsum(axis=<no_default>, *args, **kwargs)[source]# Cumulative sum for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([6, 2, 0], index=lst)\n>>> ser\na    6\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).cumsum()\na    6\na    8\nb    0\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""fox"", ""gorilla"", ""lion""])\n>>> df\n          a   b   c\nfox       1   8   2\ngorilla   1   2   5\nlion      2   6   9\n>>> df.groupby(""a"").groups\n{1: [\'fox\', \'gorilla\'], 2: [\'lion\']}\n>>> df.groupby(""a"").cumsum()\n          b   c\nfox       8   2\ngorilla  10   7\nlion      6   9']"
1184,..\pandas\reference\api\pandas.Period.now.html,pandas.Period.now,classmethod Period.now(freq)# Return the period of now’s date.,"Parameters: freqstr, BaseOffsetFrequency to use for the returned period.","["">>> pd.Period.now('h')  \nPeriod('2023-06-12 11:00', 'h')""]"
1185,..\pandas\reference\api\pandas.DataFrame.duplicated.html,pandas.DataFrame.duplicated,"DataFrame.duplicated(subset=None, keep='first')[source]# Return boolean Series denoting duplicate rows. Considering certain columns is optional.","Parameters: subsetcolumn label or sequence of labels, optionalOnly consider certain columns for identifying duplicates, by default use all of the columns. keep{‘first’, ‘last’, False}, default ‘first’Determines which duplicates (if any) to mark. first : Mark duplicates as True except for the first occurrence. last : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True. Returns: SeriesBoolean series for each duplicated rows.","["">>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0"", '>>> df.duplicated()\n0    False\n1     True\n2    False\n3    False\n4    False\ndtype: bool', "">>> df.duplicated(keep='last')\n0     True\n1    False\n2    False\n3    False\n4    False\ndtype: bool"", '>>> df.duplicated(keep=False)\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool', "">>> df.duplicated(subset=['brand'])\n0    False\n1     True\n2    False\n3     True\n4     True\ndtype: bool""]"
1186,..\pandas\reference\api\pandas.Timestamp.weekofyear.html,pandas.Timestamp.weekofyear,Timestamp.weekofyear# Return the week number of the year.,Returns: int,"['>>> ts = pd.Timestamp(2020, 3, 14)\n>>> ts.week\n11']"
1187,..\pandas\reference\api\pandas.errors.PossibleDataLossError.html,pandas.errors.PossibleDataLossError,exception pandas.errors.PossibleDataLossError[source]# Exception raised when trying to open a HDFStore file when already opened.,No parameters found,"['>>> store = pd.HDFStore(\'my-store\', \'a\') \n>>> store.open(""w"") \n... # PossibleDataLossError: Re-opening the file [my-store] with mode [a]...']"
1188,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.name.html,pandas.tseries.offsets.DateOffset.name,DateOffset.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1189,..\pandas\reference\api\pandas.tseries.offsets.Second.is_month_start.html,pandas.tseries.offsets.Second.is_month_start,Second.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1190,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.describe.html,pandas.core.groupby.DataFrameGroupBy.describe,"DataFrameGroupBy.describe(percentiles=None, include=None, exclude=None)[source]# Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Notes For numeric data, the result’s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value’s frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.","Parameters: percentileslist-like of numbers, optionalThe percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles. include‘all’, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored for Series. Here are the options: ‘all’ : All columns of the input will be included in the output. A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category' None (default) : The result will include all numeric columns. excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored for Series. Here are the options: A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category' None (default) : The result will exclude nothing. Returns: Series or DataFrameSummary statistics of the Series or Dataframe provided.","['>>> s = pd.Series([1, 2, 3])\n>>> s.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\ndtype: float64', "">>> s = pd.Series(['a', 'a', 'b', 'c'])\n>>> s.describe()\ncount     4\nunique    3\ntop       a\nfreq      2\ndtype: object"", '>>> s = pd.Series([\n...     np.datetime64(""2000-01-01""),\n...     np.datetime64(""2010-01-01""),\n...     np.datetime64(""2010-01-01"")\n... ])\n>>> s.describe()\ncount                      3\nmean     2006-09-01 08:00:00\nmin      2000-01-01 00:00:00\n25%      2004-12-31 12:00:00\n50%      2010-01-01 00:00:00\n75%      2010-01-01 00:00:00\nmax      2010-01-01 00:00:00\ndtype: object', "">>> df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n...                    'numeric': [1, 2, 3],\n...                    'object': ['a', 'b', 'c']\n...                    })\n>>> df.describe()\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0"", "">>> df.describe(include='all')  \n       categorical  numeric object\ncount            3      3.0      3\nunique           3      NaN      3\ntop              f      NaN      a\nfreq             1      NaN      1\nmean           NaN      2.0    NaN\nstd            NaN      1.0    NaN\nmin            NaN      1.0    NaN\n25%            NaN      1.5    NaN\n50%            NaN      2.0    NaN\n75%            NaN      2.5    NaN\nmax            NaN      3.0    NaN"", '>>> df.numeric.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\nName: numeric, dtype: float64', '>>> df.describe(include=[np.number])\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0', '>>> df.describe(include=[object])  \n       object\ncount       3\nunique      3\ntop         a\nfreq        1', "">>> df.describe(include=['category'])\n       categorical\ncount            3\nunique           3\ntop              d\nfreq             1"", '>>> df.describe(exclude=[np.number])  \n       categorical object\ncount            3      3\nunique           3      3\ntop              f      a\nfreq             1      1', '>>> df.describe(exclude=[object])  \n       categorical  numeric\ncount            3      3.0\nunique           3      NaN\ntop              f      NaN\nfreq             1      NaN\nmean           NaN      2.0\nstd            NaN      1.0\nmin            NaN      1.0\n25%            NaN      1.5\n50%            NaN      2.0\n75%            NaN      2.5\nmax            NaN      3.0']"
1191,..\pandas\reference\api\pandas.Series.pct_change.html,pandas.Series.pct_change,"Series.pct_change(periods=1, fill_method=<no_default>, limit=<no_default>, freq=None, **kwargs)[source]# Fractional change between the current and a prior element. Computes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements. Note Despite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100.","Parameters: periodsint, default 1Periods to shift for forming percent change. fill_method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default ‘pad’How to handle NAs before computing percent changes. Deprecated since version 2.1: All options of fill_method are deprecated except fill_method=None. limitint, default NoneThe number of consecutive NAs to fill before stopping. Deprecated since version 2.1. freqDateOffset, timedelta, or str, optionalIncrement to use from time series API (e.g. ‘ME’ or BDay()). **kwargsAdditional keyword arguments are passed into DataFrame.shift or Series.shift. Returns: Series or DataFrameThe same type as the calling object.","['>>> s = pd.Series([90, 91, 85])\n>>> s\n0    90\n1    91\n2    85\ndtype: int64', '>>> s.pct_change()\n0         NaN\n1    0.011111\n2   -0.065934\ndtype: float64', '>>> s.pct_change(periods=2)\n0         NaN\n1         NaN\n2   -0.055556\ndtype: float64', '>>> s = pd.Series([90, 91, None, 85])\n>>> s\n0    90.0\n1    91.0\n2     NaN\n3    85.0\ndtype: float64', '>>> s.ffill().pct_change()\n0         NaN\n1    0.011111\n2    0.000000\n3   -0.065934\ndtype: float64', "">>> df = pd.DataFrame({\n...     'FR': [4.0405, 4.0963, 4.3149],\n...     'GR': [1.7246, 1.7482, 1.8519],\n...     'IT': [804.74, 810.01, 860.13]},\n...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n>>> df\n                FR      GR      IT\n1980-01-01  4.0405  1.7246  804.74\n1980-02-01  4.0963  1.7482  810.01\n1980-03-01  4.3149  1.8519  860.13"", '>>> df.pct_change()\n                  FR        GR        IT\n1980-01-01       NaN       NaN       NaN\n1980-02-01  0.013810  0.013684  0.006549\n1980-03-01  0.053365  0.059318  0.061876', "">>> df = pd.DataFrame({\n...     '2016': [1769950, 30586265],\n...     '2015': [1500923, 40912316],\n...     '2014': [1371819, 41403351]},\n...     index=['GOOG', 'APPL'])\n>>> df\n          2016      2015      2014\nGOOG   1769950   1500923   1371819\nAPPL  30586265  40912316  41403351"", "">>> df.pct_change(axis='columns', periods=-1)\n          2016      2015  2014\nGOOG  0.179241  0.094112   NaN\nAPPL -0.252395 -0.011860   NaN""]"
1192,..\pandas\reference\api\pandas.Period.ordinal.html,pandas.Period.ordinal,Period.ordinal#,No parameters found,[]
1193,..\pandas\reference\api\pandas.DataFrame.empty.html,pandas.DataFrame.empty,"property DataFrame.empty[source]# Indicator whether Series/DataFrame is empty. True if Series/DataFrame is entirely empty (no items), meaning any of the axes are of length 0. Notes If Series/DataFrame contains only NaNs, it is still not considered empty. See the example below.","Returns: boolIf Series/DataFrame is empty, return True, if not return False.","["">>> df_empty = pd.DataFrame({'A' : []})\n>>> df_empty\nEmpty DataFrame\nColumns: [A]\nIndex: []\n>>> df_empty.empty\nTrue"", "">>> df = pd.DataFrame({'A' : [np.nan]})\n>>> df\n    A\n0 NaN\n>>> df.empty\nFalse\n>>> df.dropna().empty\nTrue"", "">>> ser_empty = pd.Series({'A' : []})\n>>> ser_empty\nA    []\ndtype: object\n>>> ser_empty.empty\nFalse\n>>> ser_empty = pd.Series()\n>>> ser_empty.empty\nTrue""]"
1194,..\pandas\reference\api\pandas.errors.PossiblePrecisionLoss.html,pandas.errors.PossiblePrecisionLoss,exception pandas.errors.PossiblePrecisionLoss[source]# Warning raised by to_stata on a column with a value outside or equal to int64. When the column value is outside or equal to the int64 value the column is converted to a float64 dtype.,No parameters found,"['>>> df = pd.DataFrame({""s"": pd.Series([1, 2**53], dtype=np.int64)})\n>>> df.to_stata(\'test\') \n... # PossiblePrecisionLoss: Column converted from int64 to float64...']"
1195,..\pandas\reference\api\pandas.Timestamp.year.html,pandas.Timestamp.year,Timestamp.year#,No parameters found,[]
1196,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.diff.html,pandas.core.groupby.DataFrameGroupBy.diff,"DataFrameGroupBy.diff(periods=1, axis=<no_default>)[source]# First discrete difference of element. Calculates the difference of each element compared with another element in the group (default is element in previous row).","Parameters: periodsint, default 1Periods to shift for calculating difference, accepts negative values. axisaxis to shift, default 0Take difference over rows (0) or columns (1). Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. Returns: Series or DataFrameFirst differences.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).diff()\na    NaN\na   -5.0\na    6.0\nb    NaN\nb   -1.0\nb    0.0\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).diff()\n         a    b\n  dog  NaN  NaN\n  dog  2.0  3.0\n  dog  2.0  4.0\nmouse  NaN  NaN\nmouse  0.0  0.0\nmouse  1.0 -2.0\nmouse -5.0 -1.0""]"
1197,..\pandas\reference\api\pandas.Series.pipe.html,pandas.Series.pipe,"Series.pipe(func, *args, **kwargs)[source]# Apply chainable functions that expect Series or DataFrames. Notes Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects.","Parameters: funcfunctionFunction to apply to the Series/DataFrame. args, and kwargs are passed into func. Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame. *argsiterable, optionalPositional arguments passed into func. **kwargsmapping, optionalA dictionary of keyword arguments passed into func. Returns: the return type of func.","["">>> data = [[8000, 1000], [9500, np.nan], [5000, 2000]]\n>>> df = pd.DataFrame(data, columns=['Salary', 'Others'])\n>>> df\n   Salary  Others\n0    8000  1000.0\n1    9500     NaN\n2    5000  2000.0"", '>>> def subtract_federal_tax(df):\n...     return df * 0.9\n>>> def subtract_state_tax(df, rate):\n...     return df * (1 - rate)\n>>> def subtract_national_insurance(df, rate, rate_increase):\n...     new_rate = rate + rate_increase\n...     return df * (1 - new_rate)', '>>> subtract_national_insurance(\n...     subtract_state_tax(subtract_federal_tax(df), rate=0.12),\n...     rate=0.05,\n...     rate_increase=0.02)', '>>> (\n...     df.pipe(subtract_federal_tax)\n...     .pipe(subtract_state_tax, rate=0.12)\n...     .pipe(subtract_national_insurance, rate=0.05, rate_increase=0.02)\n... )\n    Salary   Others\n0  5892.48   736.56\n1  6997.32      NaN\n2  3682.80  1473.12', "">>> def subtract_national_insurance(rate, df, rate_increase):\n...     new_rate = rate + rate_increase\n...     return df * (1 - new_rate)\n>>> (\n...     df.pipe(subtract_federal_tax)\n...     .pipe(subtract_state_tax, rate=0.12)\n...     .pipe(\n...         (subtract_national_insurance, 'df'),\n...         rate=0.05,\n...         rate_increase=0.02\n...     )\n... )\n    Salary   Others\n0  5892.48   736.56\n1  6997.32      NaN\n2  3682.80  1473.12""]"
1198,..\pandas\reference\api\pandas.DataFrame.eq.html,pandas.DataFrame.eq,"DataFrame.eq(other, axis='columns', level=None)[source]# Get Equal to of dataframe and other, element-wise (binary operator eq). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).","Parameters: otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}, default ‘columns’Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. Returns: DataFrame of boolResult of the comparison.","["">>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300"", '>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df != pd.Series([100, 250], index=[""cost"", ""revenue""])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True', '>>> df.ne(pd.Series([100, 300], index=[""A"", ""D""]), axis=\'index\')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True', '>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False', "">>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False"", "">>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150"", '>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False', "">>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225"", '>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False']"
1199,..\pandas\reference\api\pandas.Period.quarter.html,pandas.Period.quarter,Period.quarter# Return the quarter this Period falls on.,No parameters found,"["">>> period = pd.Period('2022-04', 'M')\n>>> period.quarter\n2""]"
1200,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.nanos.html,pandas.tseries.offsets.DateOffset.nanos,DateOffset.nanos#,No parameters found,[]
1201,..\pandas\reference\api\pandas.tseries.offsets.Second.is_on_offset.html,pandas.tseries.offsets.Second.is_on_offset,Second.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1202,..\pandas\reference\api\pandas.errors.PyperclipException.html,pandas.errors.PyperclipException,exception pandas.errors.PyperclipException[source]# Exception raised when clipboard functionality is unsupported. Raised by to_clipboard() and read_clipboard().,No parameters found,[]
1203,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.ffill.html,pandas.core.groupby.DataFrameGroupBy.ffill,DataFrameGroupBy.ffill(limit=None)[source]# Forward fill the values.,"Parameters: limitint, optionalLimit of how many values to fill. Returns: Series or DataFrameObject with missing values filled.","['>>> key = [0, 0, 1, 1]\n>>> ser = pd.Series([np.nan, 2, 3, np.nan], index=key)\n>>> ser\n0    NaN\n0    2.0\n1    3.0\n1    NaN\ndtype: float64\n>>> ser.groupby(level=0).ffill()\n0    NaN\n0    2.0\n1    3.0\n1    3.0\ndtype: float64', '>>> df = pd.DataFrame(\n...     {\n...         ""key"": [0, 0, 1, 1, 1],\n...         ""A"": [np.nan, 2, np.nan, 3, np.nan],\n...         ""B"": [2, 3, np.nan, np.nan, np.nan],\n...         ""C"": [np.nan, np.nan, 2, np.nan, np.nan],\n...     }\n... )\n>>> df\n   key    A    B   C\n0    0  NaN  2.0 NaN\n1    0  2.0  3.0 NaN\n2    1  NaN  NaN 2.0\n3    1  3.0  NaN NaN\n4    1  NaN  NaN NaN', '>>> df.groupby(""key"").ffill()\n     A    B   C\n0  NaN  2.0 NaN\n1  2.0  3.0 NaN\n2  NaN  NaN 2.0\n3  3.0  NaN 2.0\n4  3.0  NaN 2.0', '>>> df.T.groupby(np.array([0, 0, 1, 1])).ffill().T\n   key    A    B    C\n0  0.0  0.0  2.0  2.0\n1  0.0  2.0  3.0  3.0\n2  1.0  1.0  NaN  2.0\n3  1.0  3.0  NaN  NaN\n4  1.0  1.0  NaN  NaN', '>>> df.groupby(""key"").ffill(limit=1)\n     A    B    C\n0  NaN  2.0  NaN\n1  2.0  3.0  NaN\n2  NaN  NaN  2.0\n3  3.0  NaN  2.0\n4  3.0  NaN  NaN']"
1204,..\pandas\reference\api\pandas.Period.qyear.html,pandas.Period.qyear,"Period.qyear# Fiscal year the Period lies in according to its starting-quarter. The year and the qyear of the period will be the same if the fiscal and calendar years are the same. When they are not, the fiscal year can be different from the calendar year of the period.",Returns: intThe fiscal year of the period.,"["">>> per = pd.Period('2018Q1', freq='Q')\n>>> per.qyear\n2018\n>>> per.year\n2018"", "">>> per = pd.Period('2018Q1', freq='Q-MAR')\n>>> per.start_time\nTimestamp('2017-04-01 00:00:00')\n>>> per.qyear\n2018\n>>> per.year\n2017""]"
1205,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.normalize.html,pandas.tseries.offsets.DateOffset.normalize,DateOffset.normalize#,No parameters found,[]
1206,..\pandas\reference\api\pandas.errors.PyperclipWindowsException.html,pandas.errors.PyperclipWindowsException,exception pandas.errors.PyperclipWindowsException(message)[source]# Exception raised when clipboard functionality is unsupported by Windows. Access to the clipboard handle would be denied due to some other window process is accessing it.,No parameters found,[]
1207,..\pandas\reference\api\pandas.Series.plot.area.html,pandas.Series.plot.area,"Series.plot.area(x=None, y=None, stacked=True, **kwargs)[source]# Draw a stacked area plot. An area plot displays quantitative data visually. This function wraps the matplotlib area function.","Parameters: xlabel or position, optionalCoordinates for the X axis. By default uses the index. ylabel or position, optionalColumn to plot. By default uses all columns. stackedbool, default TrueArea plots are stacked by default. Set to False to create a unstacked plot. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarrayArea plot, or array of area plots if subplots is True.","["">>> df = pd.DataFrame({\n...     'sales': [3, 2, 3, 9, 10, 6],\n...     'signups': [5, 5, 6, 12, 14, 13],\n...     'visits': [20, 42, 28, 62, 81, 50],\n... }, index=pd.date_range(start='2018/01/01', end='2018/07/01',\n...                        freq='ME'))\n>>> ax = df.plot.area()"", '>>> ax = df.plot.area(stacked=False)', "">>> ax = df.plot.area(y='sales')"", "">>> df = pd.DataFrame({\n...     'sales': [3, 2, 3],\n...     'visits': [20, 42, 28],\n...     'day': [1, 2, 3],\n... })\n>>> ax = df.plot.area(x='day')""]"
1208,..\pandas\reference\api\pandas.to_datetime.html,pandas.to_datetime,"pandas.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=False, format=None, exact=<no_default>, unit=None, infer_datetime_format=<no_default>, origin='unix', cache=True)[source]# Convert argument to datetime. This function converts a scalar, array-like, Series or DataFrame/dict-like to a pandas datetime object. Notes Many input types are supported, and lead to different output types: scalars can be int, float, str, datetime object (from stdlib datetime module or numpy). They are converted to Timestamp when possible, otherwise they are converted to datetime.datetime. None/NaN/null scalars are converted to NaT. array-like can contain int, float, str, datetime objects. They are converted to DatetimeIndex when possible, otherwise they are converted to Index with object dtype, containing datetime.datetime. None/NaN/null entries are converted to NaT in both cases. Series are converted to Series with datetime64 dtype when possible, otherwise they are converted to Series with object dtype, containing datetime.datetime. None/NaN/null entries are converted to NaT in both cases. DataFrame/dict-like are converted to Series with datetime64 dtype. For each row a datetime is created from assembling the various dataframe columns. Column keys can be common abbreviations like [‘year’, ‘month’, ‘day’, ‘minute’, ‘second’, ‘ms’, ‘us’, ‘ns’]) or plurals of the same. The following causes are responsible for datetime.datetime objects being returned (possibly inside an Index or a Series with object dtype) instead of a proper pandas designated type (Timestamp, DatetimeIndex or Series with datetime64 dtype): when any input element is before Timestamp.min or after Timestamp.max, see timestamp limitations. when utc=False (default) and the input is an array-like or Series containing mixed naive/aware datetime, or aware with mixed time offsets. Note that this happens in the (quite frequent) situation when the timezone has a daylight savings policy. In that case you may wish to use utc=True.","Parameters: argint, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-likeThe object to convert to a datetime. If a DataFrame is provided, the method expects minimally the following columns: ""year"", ""month"", ""day"". The column “year” must be specified in 4-digit format. errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’ If 'raise', then invalid parsing will raise an exception. If 'coerce', then invalid parsing will be set as NaT. If 'ignore', then invalid parsing will return the input. dayfirstbool, default FalseSpecify a date parse order if arg is str or is list-like. If True, parses dates with the day first, e.g. ""10/11/12"" is parsed as 2012-11-10. Warning dayfirst=True is not strict, but will prefer to parse with day first. yearfirstbool, default FalseSpecify a date parse order if arg is str or is list-like. If True parses dates with the year first, e.g. ""10/11/12"" is parsed as 2010-11-12. If both dayfirst and yearfirst are True, yearfirst is preceded (same as dateutil). Warning yearfirst=True is not strict, but will prefer to parse with year first. utcbool, default FalseControl timezone-related parsing, localization and conversion. If True, the function always returns a timezone-aware UTC-localized Timestamp, Series or DatetimeIndex. To do this, timezone-naive inputs are localized as UTC, while timezone-aware inputs are converted to UTC. If False (default), inputs will not be coerced to UTC. Timezone-naive inputs will remain naive, while timezone-aware ones will keep their time offsets. Limitations exist for mixed offsets (typically, daylight savings), see Examples section for details. Warning In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless utc=True. Please specify utc=True to opt in to the new behaviour and silence this warning. To create a Series with mixed offsets and object dtype, please use apply and datetime.datetime.strptime. See also: pandas general documentation about timezone conversion and localization. formatstr, default NoneThe strftime to parse time, e.g. ""%d/%m/%Y"". See strftime documentation for more information on choices, though note that ""%f"" will parse all the way up to nanoseconds. You can also pass: “ISO8601”, to parse any ISO8601 time string (not necessarily in exactly the same format); “mixed”, to infer the format for each element individually. This is risky, and you should probably use it along with dayfirst. Note If a DataFrame is passed, then format has no effect. exactbool, default TrueControl how format is used: If True, require an exact format match. If False, allow the format to match anywhere in the target string. Cannot be used alongside format='ISO8601' or format='mixed'. unitstr, default ‘ns’The unit of the arg (D,s,ms,us,ns) denote the unit, which is an integer or float number. This will be based off the origin. Example, with unit='ms' and origin='unix', this would calculate the number of milliseconds to the unix epoch start. infer_datetime_formatbool, default FalseIf True and no format is given, attempt to infer the format of the datetime strings based on the first non-NaN element, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by ~5-10x. Deprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect. originscalar, default ‘unix’Define the reference date. The numeric values would be parsed as number of units (defined by unit) since this reference date. If 'unix' (or POSIX) time; origin is set to 1970-01-01. If 'julian', unit must be 'D', and origin is set to beginning of Julian Calendar. Julian day number 0 is assigned to the day starting at noon on January 1, 4713 BC. If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date string), origin is set to Timestamp identified by origin. If a float or integer, origin is the difference (in units determined by the unit argument) relative to 1970-01-01. cachebool, default TrueIf True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets. The cache is only used when there are at least 50 values. The presence of out-of-bounds values will render the cache unusable and may slow down parsing. Returns: datetimeIf parsing succeeded. Return type depends on input (types in parenthesis correspond to fallback in case of unsuccessful timezone or out-of-range timestamp parsing): scalar: Timestamp (or datetime.datetime) array-like: DatetimeIndex (or Series with object dtype containing datetime.datetime) Series: Series of datetime64 dtype (or Series of object dtype containing datetime.datetime) DataFrame: Series of datetime64 dtype (or Series of object dtype containing datetime.datetime) Raises: ParserErrorWhen parsing a date from string fails. ValueErrorWhen another datetime conversion error happens. For example when one of ‘year’, ‘month’, day’ columns is missing in a DataFrame, or when a Timezone-aware datetime.datetime is found in an array-like of mixed time offsets, and utc=False.","["">>> df = pd.DataFrame({'year': [2015, 2016],\n...                    'month': [2, 3],\n...                    'day': [4, 5]})\n>>> pd.to_datetime(df)\n0   2015-02-04\n1   2016-03-05\ndtype: datetime64[ns]"", "">>> pd.to_datetime(1490195805, unit='s')\nTimestamp('2017-03-22 15:16:45')\n>>> pd.to_datetime(1490195805433502912, unit='ns')\nTimestamp('2017-03-22 15:16:45.433502912')"", "">>> pd.to_datetime([1, 2, 3], unit='D',\n...                origin=pd.Timestamp('1960-01-01'))\nDatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n              dtype='datetime64[ns]', freq=None)"", "">>> pd.to_datetime('2018-10-26 12:00:00.0000000011',\n...                format='%Y-%m-%d %H:%M:%S.%f')\nTimestamp('2018-10-26 12:00:00.000000001')"", "">>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\nNaT"", "">>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])\nDatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n              dtype='datetime64[ns]', freq=None)"", "">>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\nDatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n              dtype='datetime64[ns, UTC-05:00]', freq=None)"", "">>> pd.to_datetime(['2020-10-25 02:00 +0200',\n...                 '2020-10-25 04:00 +0100'])  \nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n      dtype='object')"", '>>> from datetime import datetime\n>>> pd.to_datetime([""2020-01-01 01:00:00-01:00"",\n...                 datetime(2020, 1, 1, 3, 0)])  \nFutureWarning: In a future version of pandas, parsing datetimes with mixed\ntime zones will raise an error unless `utc=True`. Please specify `utc=True`\nto opt in to the new behaviour and silence this warning. To create a `Series`\nwith mixed offsets and `object` dtype, please use `apply` and\n`datetime.datetime.strptime`.\nIndex([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype=\'object\')', "">>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)"", "">>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n...                utc=True)\nDatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)"", "">>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)""]"
1209,..\pandas\reference\api\pandas.DataFrame.equals.html,pandas.DataFrame.equals,"DataFrame.equals(other)[source]# Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The row/column index do not need to have the same type, as long as the values are considered equal. Corresponding columns and index must be of the same dtype.","Parameters: otherSeries or DataFrameThe other Series or DataFrame to be compared with the first. Returns: boolTrue if all elements are the same in both objects, False otherwise.","['>>> df = pd.DataFrame({1: [10], 2: [20]})\n>>> df\n    1   2\n0  10  20', '>>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n>>> exactly_equal\n    1   2\n0  10  20\n>>> df.equals(exactly_equal)\nTrue', '>>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n>>> different_column_type\n   1.0  2.0\n0   10   20\n>>> df.equals(different_column_type)\nTrue', '>>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n>>> different_data_type\n      1     2\n0  10.0  20.0\n>>> df.equals(different_data_type)\nFalse']"
1210,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.fillna.html,pandas.core.groupby.DataFrameGroupBy.fillna,"DataFrameGroupBy.fillna(value=None, method=None, axis=<no_default>, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values using the specified method within groups. Deprecated since version 2.2.0: This method is deprecated and will be removed in a future version. Use the DataFrameGroupBy.ffill() or DataFrameGroupBy.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna() instead.","Parameters: valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame).  Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. Users wanting to use the value argument and not method should prefer DataFrame.fillna() as this will produce the same result and be more performant. method{{‘bfill’, ‘ffill’, None}}, default NoneMethod to use for filling holes. 'ffill' will propagate the last valid observation forward within a group. 'bfill' will use next valid observation to fill the gap. axis{0 or ‘index’, 1 or ‘columns’}Axis along which to fill missing values. When the DataFrameGroupBy axis argument is 0, using axis=1 here will produce the same results as DataFrame.fillna(). When the DataFrameGroupBy axis argument is 1, using axis=0 or axis=1 here will produce the same results. inplacebool, default FalseBroken. Do not set to True. limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill within a group. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Returns: DataFrameObject with missing values filled.","['>>> df = pd.DataFrame(\n...     {\n...         ""key"": [0, 0, 1, 1, 1],\n...         ""A"": [np.nan, 2, np.nan, 3, np.nan],\n...         ""B"": [2, 3, np.nan, np.nan, np.nan],\n...         ""C"": [np.nan, np.nan, 2, np.nan, np.nan],\n...     }\n... )\n>>> df\n   key    A    B   C\n0    0  NaN  2.0 NaN\n1    0  2.0  3.0 NaN\n2    1  NaN  NaN 2.0\n3    1  3.0  NaN NaN\n4    1  NaN  NaN NaN', '>>> df.groupby(""key"").fillna(method=""ffill"")\n     A    B   C\n0  NaN  2.0 NaN\n1  2.0  3.0 NaN\n2  NaN  NaN 2.0\n3  3.0  NaN 2.0\n4  3.0  NaN 2.0', '>>> df.groupby(""key"").fillna(method=""bfill"")\n     A    B   C\n0  2.0  2.0 NaN\n1  2.0  3.0 NaN\n2  3.0  NaN 2.0\n3  3.0  NaN NaN\n4  NaN  NaN NaN', '>>> df.T.groupby(np.array([0, 0, 1, 1])).fillna(method=""ffill"").T\n   key    A    B    C\n0  0.0  0.0  2.0  2.0\n1  0.0  2.0  3.0  3.0\n2  1.0  1.0  NaN  2.0\n3  1.0  3.0  NaN  NaN\n4  1.0  1.0  NaN  NaN', '>>> df.T.groupby(np.array([0, 0, 1, 1])).fillna(method=""bfill"").T\n   key    A    B    C\n0  0.0  NaN  2.0  NaN\n1  0.0  2.0  3.0  NaN\n2  1.0  NaN  2.0  2.0\n3  1.0  3.0  NaN  NaN\n4  1.0  NaN  NaN  NaN', '>>> df.groupby(""key"").fillna(method=""ffill"", limit=1)\n     A    B    C\n0  NaN  2.0  NaN\n1  2.0  3.0  NaN\n2  NaN  NaN  2.0\n3  3.0  NaN  2.0\n4  3.0  NaN  NaN']"
1211,..\pandas\reference\api\pandas.tseries.offsets.Second.is_quarter_end.html,pandas.tseries.offsets.Second.is_quarter_end,Second.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1212,..\pandas\reference\api\pandas.Period.second.html,pandas.Period.second,Period.second# Get the second component of the Period.,Returns: intThe second of the Period (ranges from 0 to 59).,"['>>> p = pd.Period(""2018-03-11 13:03:12.050000"")\n>>> p.second\n12']"
1213,..\pandas\reference\api\pandas.DataFrame.eval.html,pandas.DataFrame.eval,"DataFrame.eval(expr, *, inplace=False, **kwargs)[source]# Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements.  This allows eval to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function. Notes For more details see the API documentation for eval(). For detailed examples see enhancing performance with eval.","Parameters: exprstrThe expression string to evaluate. inplacebool, default FalseIf the expression contains an assignment, whether to perform the operation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargsSee the documentation for eval() for complete details on the keyword arguments accepted by query(). Returns: ndarray, scalar, pandas object, or NoneThe result of the evaluation or None if inplace=True.","["">>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)})\n>>> df\n   A   B\n0  1  10\n1  2   8\n2  3   6\n3  4   4\n4  5   2\n>>> df.eval('A + B')\n0    11\n1    10\n2     9\n3     8\n4     7\ndtype: int64"", "">>> df.eval('C = A + B')\n   A   B   C\n0  1  10  11\n1  2   8  10\n2  3   6   9\n3  4   4   8\n4  5   2   7\n>>> df\n   A   B\n0  1  10\n1  2   8\n2  3   6\n3  4   4\n4  5   2"", "">>> df.eval(\n...     '''\n... C = A + B\n... D = A - B\n... '''\n... )\n   A   B   C  D\n0  1  10  11 -9\n1  2   8  10 -6\n2  3   6   9 -3\n3  4   4   8  0\n4  5   2   7  3""]"
1214,..\pandas\reference\api\pandas.to_numeric.html,pandas.to_numeric,"pandas.to_numeric(arg, errors='raise', downcast=None, dtype_backend=<no_default>)[source]# Convert argument to a numeric type. The default return dtype is float64 or int64 depending on the data supplied. Use the downcast parameter to obtain other dtypes. Please note that precision loss may occur if really large numbers are passed in. Due to the internal limitations of ndarray, if numbers smaller than -9223372036854775808 (np.iinfo(np.int64).min) or larger than 18446744073709551615 (np.iinfo(np.uint64).max) are passed in, it is very likely they will be converted to float so that they can be stored in an ndarray. These warnings apply similarly to Series since it internally leverages ndarray.","Parameters: argscalar, list, tuple, 1-d array, or SeriesArgument to be converted. errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception. If ‘coerce’, then invalid parsing will be set as NaN. If ‘ignore’, then invalid parsing will return the input. Changed in version 2.2. “ignore” is deprecated. Catch exceptions explicitly instead. downcaststr, default NoneCan be ‘integer’, ‘signed’, ‘unsigned’, or ‘float’. If not None, and if the data has been successfully cast to a numerical dtype (or if the data was numeric to begin with), downcast that resulting data to the smallest numerical dtype possible according to the following rules: ‘integer’ or ‘signed’: smallest signed int dtype (min.: np.int8) ‘unsigned’: smallest unsigned int dtype (min.: np.uint8) ‘float’: smallest float dtype (min.: np.float32) As this behaviour is separate from the core conversion to numeric values, any errors raised during the downcasting will be surfaced regardless of the value of the ‘errors’ input. In addition, downcasting will only occur if the size of the resulting data’s dtype is strictly larger than the dtype it is to be cast to, so if none of the dtypes checked satisfy that specification, no downcasting will be performed on the data. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: retNumeric if parsing succeeded. Return type depends on input.  Series if Series, otherwise ndarray.","["">>> s = pd.Series(['1.0', '2', -3])\n>>> pd.to_numeric(s)\n0    1.0\n1    2.0\n2   -3.0\ndtype: float64\n>>> pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -3.0\ndtype: float32\n>>> pd.to_numeric(s, downcast='signed')\n0    1\n1    2\n2   -3\ndtype: int8\n>>> s = pd.Series(['apple', '1.0', '2', -3])\n>>> pd.to_numeric(s, errors='coerce')\n0    NaN\n1    1.0\n2    2.0\n3   -3.0\ndtype: float64"", '>>> s = pd.Series([1, 2, 3], dtype=""Int64"")\n>>> pd.to_numeric(s, downcast=""integer"")\n0    1\n1    2\n2    3\ndtype: Int8\n>>> s = pd.Series([1.0, 2.1, 3.0], dtype=""Float64"")\n>>> pd.to_numeric(s, downcast=""float"")\n0    1.0\n1    2.1\n2    3.0\ndtype: Float32']"
1215,..\pandas\reference\api\pandas.tseries.offsets.Second.is_quarter_start.html,pandas.tseries.offsets.Second.is_quarter_start,Second.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1216,..\pandas\reference\api\pandas.errors.SettingWithCopyError.html,pandas.errors.SettingWithCopyError,"exception pandas.errors.SettingWithCopyError[source]# Exception raised when trying to set on a copied slice from a DataFrame. The mode.chained_assignment needs to be set to set to ‘raise.’ This can happen unintentionally when chained indexing. For more information on evaluation order, see the user guide. For more information on view vs. copy, see the user guide.",No parameters found,"["">>> pd.options.mode.chained_assignment = 'raise'\n>>> df = pd.DataFrame({'A': [1, 1, 1, 2, 2]}, columns=['A'])\n>>> df.loc[0:3]['A'] = 'a' \n... # SettingWithCopyError: A value is trying to be set on a copy of a...""]"
1217,..\pandas\reference\api\pandas.tseries.offsets.DateOffset.rule_code.html,pandas.tseries.offsets.DateOffset.rule_code,DateOffset.rule_code#,No parameters found,[]
1218,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.filter.html,pandas.core.groupby.DataFrameGroupBy.filter,"DataFrameGroupBy.filter(func, dropna=True, *args, **kwargs)[source]# Filter elements from groups that don’t satisfy a criterion. Elements from groups are filtered if they do not satisfy the boolean criterion specified by func. Notes Each subframe is endowed the attribute ‘name’ in case you need to know which group you are working on. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funcfunctionCriterion to apply to each group. Should return True or False. dropnaboolDrop groups that do not pass the filter. True by default; if False, groups that evaluate False are filled with NaNs. Returns: DataFrame","["">>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : [1, 2, 3, 4, 5, 6],\n...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> grouped.filter(lambda x: x['B'].mean() > 3.)\n     A  B    C\n1  bar  2  5.0\n3  bar  4  1.0\n5  bar  6  9.0""]"
1219,..\pandas\reference\api\pandas.Series.plot.bar.html,pandas.Series.plot.bar,"Series.plot.bar(x=None, y=None, **kwargs)[source]# Vertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.","Parameters: xlabel or position, optionalAllows plotting of one column versus another. If not specified, the index of the DataFrame is used. ylabel or position, optionalAllows plotting of one column versus another. If not specified, all numerical columns are used. colorstr, array-like, or dict, optionalThe color for each of the DataFrame’s columns. Possible values are: A single color string referred to by name, RGB or RGBA code,for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used. A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.","["">>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)"", "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)"", '>>> ax = df.plot.bar(stacked=True)', '>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)', '>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={""speed"": ""red"", ""lifespan"": ""green""}\n... )\n>>> axes[1].legend(loc=2)', "">>> ax = df.plot.bar(y='speed', rot=0)"", "">>> ax = df.plot.bar(x='lifespan', rot=0)""]"
1220,..\pandas\reference\api\pandas.errors.SettingWithCopyWarning.html,pandas.errors.SettingWithCopyWarning,"exception pandas.errors.SettingWithCopyWarning[source]# Warning raised when trying to set on a copied slice from a DataFrame. The mode.chained_assignment needs to be set to set to ‘warn.’ ‘Warn’ is the default option. This can happen unintentionally when chained indexing. For more information on evaluation order, see the user guide. For more information on view vs. copy, see the user guide.",No parameters found,"["">>> df = pd.DataFrame({'A': [1, 1, 1, 2, 2]}, columns=['A'])\n>>> df.loc[0:3]['A'] = 'a' \n... # SettingWithCopyWarning: A value is trying to be set on a copy of a...""]"
1221,..\pandas\reference\api\pandas.to_timedelta.html,pandas.to_timedelta,"pandas.to_timedelta(arg, unit=None, errors='raise')[source]# Convert argument to timedelta. Timedeltas are absolute differences in times, expressed in difference units (e.g. days, hours, minutes, seconds). This method converts an argument from a recognized timedelta format / value into a Timedelta type. Notes If the precision is higher than nanoseconds, the precision of the duration is truncated to nanoseconds for string inputs.","Parameters: argstr, timedelta, list-like or SeriesThe data to be converted to timedelta. Changed in version 2.0: Strings with units ‘M’, ‘Y’ and ‘y’ do not represent unambiguous timedelta values and will raise an exception. unitstr, optionalDenotes the unit of the arg for numeric arg. Defaults to ""ns"". Possible values: ‘W’ ‘D’ / ‘days’ / ‘day’ ‘hours’ / ‘hour’ / ‘hr’ / ‘h’ / ‘H’ ‘m’ / ‘minute’ / ‘min’ / ‘minutes’ / ‘T’ ‘s’ / ‘seconds’ / ‘sec’ / ‘second’ / ‘S’ ‘ms’ / ‘milliseconds’ / ‘millisecond’ / ‘milli’ / ‘millis’ / ‘L’ ‘us’ / ‘microseconds’ / ‘microsecond’ / ‘micro’ / ‘micros’ / ‘U’ ‘ns’ / ‘nanoseconds’ / ‘nano’ / ‘nanos’ / ‘nanosecond’ / ‘N’ Must not be specified when arg contains strings and errors=""raise"". Deprecated since version 2.2.0: Units ‘H’, ‘T’, ‘S’, ‘L’, ‘U’ and ‘N’ are deprecated and will be removed in a future version. Please use ‘h’, ‘min’, ‘s’, ‘ms’, ‘us’, and ‘ns’ instead of ‘H’, ‘T’, ‘S’, ‘L’, ‘U’ and ‘N’. errors{‘ignore’, ‘raise’, ‘coerce’}, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception. If ‘coerce’, then invalid parsing will be set as NaT. If ‘ignore’, then invalid parsing will return the input. Returns: timedeltaIf parsing succeeded. Return type depends on input: list-like: TimedeltaIndex of timedelta64 dtype Series: Series of timedelta64 dtype scalar: Timedelta","["">>> pd.to_timedelta('1 days 06:05:01.00003')\nTimedelta('1 days 06:05:01.000030')\n>>> pd.to_timedelta('15.5us')\nTimedelta('0 days 00:00:00.000015500')"", "">>> pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])\nTimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT],\n               dtype='timedelta64[ns]', freq=None)"", "">>> pd.to_timedelta(np.arange(5), unit='s')\nTimedeltaIndex(['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02',\n                '0 days 00:00:03', '0 days 00:00:04'],\n               dtype='timedelta64[ns]', freq=None)\n>>> pd.to_timedelta(np.arange(5), unit='d')\nTimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],\n               dtype='timedelta64[ns]', freq=None)""]"
1222,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.first.html,pandas.core.groupby.DataFrameGroupBy.first,"DataFrameGroupBy.first(numeric_only=False, min_count=-1, skipna=True)[source]# Compute the first entry of each column within each group. Defaults to skipping NA elements.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count valid values are present the result will be NA. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Added in version 2.2.1. Returns: Series or DataFrameFirst values within each group.","['>>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[None, 5, 6], C=[1, 2, 3],\n...                        D=[\'3/11/2000\', \'3/12/2000\', \'3/13/2000\']))\n>>> df[\'D\'] = pd.to_datetime(df[\'D\'])\n>>> df.groupby(""A"").first()\n     B  C          D\nA\n1  5.0  1 2000-03-11\n3  6.0  3 2000-03-13\n>>> df.groupby(""A"").first(min_count=2)\n    B    C          D\nA\n1 NaN  1.0 2000-03-11\n3 NaN  NaN        NaT\n>>> df.groupby(""A"").first(numeric_only=True)\n     B  C\nA\n1  5.0  1\n3  6.0  3']"
1223,..\pandas\reference\api\pandas.Period.start_time.html,pandas.Period.start_time,Period.start_time# Get the Timestamp for the start of the period.,Returns: Timestamp,"["">>> period = pd.Period('2012-1-1', freq='D')\n>>> period\nPeriod('2012-01-01', 'D')"", "">>> period.start_time\nTimestamp('2012-01-01 00:00:00')"", "">>> period.end_time\nTimestamp('2012-01-01 23:59:59.999999999')""]"
1224,..\pandas\reference\api\pandas.tseries.offsets.Day.copy.html,pandas.tseries.offsets.Day.copy,Day.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1225,..\pandas\reference\api\pandas.DataFrame.ewm.html,pandas.DataFrame.ewm,"DataFrame.ewm(com=None, span=None, halflife=None, alpha=None, min_periods=0, adjust=True, ignore_na=False, axis=<no_default>, times=None, method='single')[source]# Provide exponentially weighted (EW) calculations. Exactly one of com, span, halflife, or alpha must be provided if times is not provided. If times is provided, halflife and one of com, span or alpha may be provided. Notes See Windowing Operations for further usage details and examples.","Parameters: comfloat, optionalSpecify decay in terms of center of mass \(\alpha = 1 / (1 + com)\), for \(com \geq 0\). spanfloat, optionalSpecify decay in terms of span \(\alpha = 2 / (span + 1)\), for \(span \geq 1\). halflifefloat, str, timedelta, optionalSpecify decay in terms of half-life \(\alpha = 1 - \exp\left(-\ln(2) / halflife\right)\), for \(halflife > 0\). If times is specified, a timedelta convertible unit over which an observation decays to half its value. Only applicable to mean(), and halflife value will not apply to the other functions. alphafloat, optionalSpecify smoothing factor \(\alpha\) directly \(0 < \alpha \leq 1\). min_periodsint, default 0Minimum number of observations in window required to have a value; otherwise, result is np.nan. adjustbool, default TrueDivide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average). When adjust=True (default), the EW function is calculated using weights \(w_i = (1 - \alpha)^i\). For example, the EW moving average of the series [\(x_0, x_1, ..., x_t\)] would be: \[y_t = \frac{x_t + (1 - \alpha)x_{t-1} + (1 - \alpha)^2 x_{t-2} + ... + (1 - \alpha)^t x_0}{1 + (1 - \alpha) + (1 - \alpha)^2 + ... + (1 - \alpha)^t}\] When adjust=False, the exponentially weighted function is calculated recursively: \[\begin{split}\begin{split}     y_0 &= x_0\\     y_t &= (1 - \alpha) y_{t-1} + \alpha x_t, \end{split}\end{split}\] ignore_nabool, default FalseIgnore missing values when calculating weights. When ignore_na=False (default), weights are based on absolute positions. For example, the weights of \(x_0\) and \(x_2\) used in calculating the final weighted average of [\(x_0\), None, \(x_2\)] are \((1-\alpha)^2\) and \(1\) if adjust=True, and \((1-\alpha)^2\) and \(\alpha\) if adjust=False. When ignore_na=True, weights are based on relative positions. For example, the weights of \(x_0\) and \(x_2\) used in calculating the final weighted average of [\(x_0\), None, \(x_2\)] are \(1-\alpha\) and \(1\) if adjust=True, and \(1-\alpha\) and \(\alpha\) if adjust=False. axis{0, 1}, default 0If 0 or 'index', calculate across the rows. If 1 or 'columns', calculate across the columns. For Series this parameter is unused and defaults to 0. timesnp.ndarray, Series, default NoneOnly applicable to mean(). Times corresponding to the observations. Must be monotonically increasing and datetime64[ns] dtype. If 1-D array like, a sequence with the same shape as the observations. methodstr {‘single’, ‘table’}, default ‘single’ Added in version 1.4.0. Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Only applicable to mean() Returns: pandas.api.typing.ExponentialMovingWindow","["">>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0"", '>>> df.ewm(com=0.5).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213\n>>> df.ewm(alpha=2 / 3).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213', '>>> df.ewm(com=0.5, adjust=True).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213\n>>> df.ewm(com=0.5, adjust=False).mean()\n          B\n0  0.000000\n1  0.666667\n2  1.555556\n3  1.555556\n4  3.650794', '>>> df.ewm(com=0.5, ignore_na=True).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.225000\n>>> df.ewm(com=0.5, ignore_na=False).mean()\n          B\n0  0.000000\n1  0.750000\n2  1.615385\n3  1.615385\n4  3.670213', "">>> times = ['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15', '2020-01-17']\n>>> df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean()\n          B\n0  0.000000\n1  0.585786\n2  1.523889\n3  1.523889\n4  3.233686""]"
1226,..\pandas\reference\api\pandas.tseries.offsets.Second.is_year_end.html,pandas.tseries.offsets.Second.is_year_end,Second.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1227,..\pandas\reference\api\pandas.Series.plot.barh.html,pandas.Series.plot.barh,"Series.plot.barh(x=None, y=None, **kwargs)[source]# Make a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.","Parameters: xlabel or position, optionalAllows plotting of one column versus another. If not specified, the index of the DataFrame is used. ylabel or position, optionalAllows plotting of one column versus another. If not specified, all numerical columns are used. colorstr, array-like, or dict, optionalThe color for each of the DataFrame’s columns. Possible values are: A single color string referred to by name, RGB or RGBA code,for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used. A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.","["">>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')"", "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()"", '>>> ax = df.plot.barh(stacked=True)', '>>> ax = df.plot.barh(color={""speed"": ""red"", ""lifespan"": ""green""})', "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')"", "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')""]"
1228,..\pandas\reference\api\pandas.errors.SpecificationError.html,pandas.errors.SpecificationError,exception pandas.errors.SpecificationError[source]# Exception raised by agg when the functions are ill-specified. The exception raised in two scenarios. The first way is calling agg on a Dataframe or Series using a nested renamer (dict-of-dict). The second way is calling agg on a Dataframe with duplicated functions names without assigning column name.,No parameters found,"["">>> df = pd.DataFrame({'A': [1, 1, 1, 2, 2],\n...                    'B': range(5),\n...                    'C': range(5)})\n>>> df.groupby('A').B.agg({'foo': 'count'}) \n... # SpecificationError: nested renamer is not supported"", "">>> df.groupby('A').agg({'B': {'foo': ['sum', 'max']}}) \n... # SpecificationError: nested renamer is not supported"", "">>> df.groupby('A').agg(['min', 'min']) \n... # SpecificationError: nested renamer is not supported""]"
1229,..\pandas\reference\api\pandas.tseries.api.guess_datetime_format.html,pandas.tseries.api.guess_datetime_format,"pandas.tseries.api.guess_datetime_format(dt_str, dayfirst=False)# Guess the datetime format of a given datetime string.","Parameters: dt_strstrDatetime string to guess the format of. dayfirstbool, default FalseIf True parses dates with the day first, eg 20/01/2005 Warning dayfirst=True is not strict, but will prefer to parse with day first (this is a known bug). Returns: str or Noneretdatetime format string (for strftime or strptime), or None if it can’t be guessed.","["">>> from pandas.tseries.api import guess_datetime_format\n>>> guess_datetime_format('09/13/2023')\n'%m/%d/%Y'"", "">>> guess_datetime_format('2023|September|13')""]"
1230,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.get_group.html,pandas.core.groupby.DataFrameGroupBy.get_group,"DataFrameGroupBy.get_group(name, obj=None)[source]# Construct DataFrame from group with provided name.","Parameters: nameobjectThe name of the group to get as a DataFrame. objDataFrame, default NoneThe DataFrame to take the DataFrame out of.  If it is None, the object groupby was called on will be used. Deprecated since version 2.1.0: The obj is deprecated and will be removed in a future version. Do df.iloc[gb.indices.get(name)] instead of gb.get_group(name, obj=df). Returns: same type as obj","['>>> lst = [\'a\', \'a\', \'b\']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).get_group(""a"")\na    1\na    2\ndtype: int64', '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(by=[""a""]).get_group((1,))\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').get_group('2023-01-01')\n2023-01-01    1\n2023-01-15    2\ndtype: int64""]"
1231,..\pandas\reference\api\pandas.errors.UndefinedVariableError.html,pandas.errors.UndefinedVariableError,"exception pandas.errors.UndefinedVariableError(name, is_local=None)[source]# Exception raised by query or eval when using an undefined variable name. It will also specify whether the undefined variable is local or not.",No parameters found,"['>>> df = pd.DataFrame({\'A\': [1, 1, 1]})\n>>> df.query(""A > x"") \n... # UndefinedVariableError: name \'x\' is not defined\n>>> df.query(""A > @y"") \n... # UndefinedVariableError: local variable \'y\' is not defined\n>>> pd.eval(\'x + 1\') \n... # UndefinedVariableError: name \'x\' is not defined']"
1232,..\pandas\reference\api\pandas.tseries.offsets.Day.delta.html,pandas.tseries.offsets.Day.delta,Day.delta#,No parameters found,[]
1233,..\pandas\reference\api\pandas.DataFrame.expanding.html,pandas.DataFrame.expanding,"DataFrame.expanding(min_periods=1, axis=<no_default>, method='single')[source]# Provide expanding window calculations. Notes See Windowing Operations for further usage details and examples.","Parameters: min_periodsint, default 1Minimum number of observations in window required to have a value; otherwise, result is np.nan. axisint or str, default 0If 0 or 'index', roll across the rows. If 1 or 'columns', roll across the columns. For Series this parameter is unused and defaults to 0. methodstr {‘single’, ‘table’}, default ‘single’Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Added in version 1.3.0. Returns: pandas.api.typing.Expanding","['>>> df = pd.DataFrame({""B"": [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0', '>>> df.expanding(1).sum()\n     B\n0  0.0\n1  1.0\n2  3.0\n3  3.0\n4  7.0\n>>> df.expanding(3).sum()\n     B\n0  NaN\n1  NaN\n2  3.0\n3  3.0\n4  7.0']"
1234,..\pandas\reference\api\pandas.Series.plot.box.html,pandas.Series.plot.box,"Series.plot.box(by=None, **kwargs)[source]# Make a box plot of the DataFrame columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers. For further details see Wikipedia’s entry for boxplot. A consideration when using this chart is that the box and the whiskers can overlap, which is very common when plotting small sets of data.","Parameters: bystr or sequenceColumn in the DataFrame to group by. Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings **kwargsAdditional keywords are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","["">>> data = np.random.randn(25, 4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'))\n>>> ax = df.plot.box()"", '>>> age_list = [8, 10, 12, 14, 72, 74, 76, 78, 20, 25, 30, 35, 60, 85]\n>>> df = pd.DataFrame({""gender"": list(""MMMMMMMMFFFFFF""), ""age"": age_list})\n>>> ax = df.plot.box(column=""age"", by=""gender"", figsize=(10, 8))']"
1235,..\pandas\reference\api\pandas.Period.strftime.html,pandas.Period.strftime,"Period.strftime(fmt)# Returns a formatted string representation of the Period. fmt must be None or a string containing one or several directives. When None, the format will be determined from the frequency of the Period. The method recognizes the same directives as the time.strftime() function of the standard Python distribution, as well as the specific additional directives %f, %F, %q, %l, %u, %n. (formatting & docs originally from scikits.timeries). Directive Meaning Notes %a Locale’s abbreviated weekday name. %A Locale’s full weekday name. %b Locale’s abbreviated month name. %B Locale’s full month name. %c Locale’s appropriate date and time representation. %d Day of the month as a decimal number [01,31]. %f ‘Fiscal’ year without a century  as a decimal number [00,99] (1) %F ‘Fiscal’ year with a century as a decimal number (2) %H Hour (24-hour clock) as a decimal number [00,23]. %I Hour (12-hour clock) as a decimal number [01,12]. %j Day of the year as a decimal number [001,366]. %m Month as a decimal number [01,12]. %M Minute as a decimal number [00,59]. %p Locale’s equivalent of either AM or PM. (3) %q Quarter as a decimal number [1,4] %S Second as a decimal number [00,61]. (4) %l Millisecond as a decimal number [000,999]. %u Microsecond as a decimal number [000000,999999]. %n Nanosecond as a decimal number [000000000,999999999]. %U Week number of the year (Sunday as the first day of the week) as a decimal number [00,53].  All days in a new year preceding the first Sunday are considered to be in week 0. (5) %w Weekday as a decimal number [0(Sunday),6]. %W Week number of the year (Monday as the first day of the week) as a decimal number [00,53].  All days in a new year preceding the first Monday are considered to be in week 0. (5) %x Locale’s appropriate date representation. %X Locale’s appropriate time representation. %y Year without century as a decimal number [00,99]. %Y Year with century as a decimal number. %Z Time zone name (no characters if no time zone exists). %% A literal '%' character. Notes The %f directive is the same as %y if the frequency is not quarterly. Otherwise, it corresponds to the ‘fiscal’ year, as defined by the qyear attribute. The %F directive is the same as %Y if the frequency is not quarterly. Otherwise, it corresponds to the ‘fiscal’ year, as defined by the qyear attribute. The %p directive only affects the output hour field if the %I directive is used to parse the hour. The range really is 0 to 61; this accounts for leap seconds and the (very rare) double leap seconds. The %U and %W directives are only used in calculations when the day of the week and the year are specified.",No parameters found,"["">>> from pandas import Period\n>>> a = Period(freq='Q-JUL', year=2006, quarter=1)\n>>> a.strftime('%F-Q%q')\n'2006-Q1'\n>>> # Output the last month in the quarter of this date\n>>> a.strftime('%b-%Y')\n'Oct-2005'\n>>>\n>>> a = Period(freq='D', year=2001, month=1, day=1)\n>>> a.strftime('%d-%b-%Y')\n'01-Jan-2001'\n>>> a.strftime('%b. %d, %Y was a %A')\n'Jan. 01, 2001 was a Monday'""]"
1236,..\pandas\reference\api\pandas.tseries.frequencies.to_offset.html,pandas.tseries.frequencies.to_offset,"pandas.tseries.frequencies.to_offset(freq, is_period=False)# Return DateOffset object from string or datetime.timedelta object.","Parameters: freqstr, datetime.timedelta, BaseOffset or None Returns: BaseOffset subclass or None Raises: ValueErrorIf freq is an invalid frequency","['>>> from pandas.tseries.frequencies import to_offset\n>>> to_offset(""5min"")\n<5 * Minutes>', '>>> to_offset(""1D1h"")\n<25 * Hours>', '>>> to_offset(""2W"")\n<2 * Weeks: weekday=6>', '>>> to_offset(""2B"")\n<2 * BusinessDays>', '>>> to_offset(pd.Timedelta(days=1))\n<Day>', '>>> to_offset(pd.offsets.Hour())\n<Hour>']"
1237,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.groups.html,pandas.core.groupby.DataFrameGroupBy.groups,property DataFrameGroupBy.groups[source]# Dict {group name -> group labels}.,No parameters found,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).groups\n{'a': ['a', 'a'], 'b': ['b']}"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""])\n>>> df\n   a  b  c\n0  1  2  3\n1  1  5  6\n2  7  8  9\n>>> df.groupby(by=[""a""]).groups\n{1: [0, 1], 7: [2]}', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').groups\n{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}""]"
1238,..\pandas\reference\api\pandas.tseries.offsets.Second.is_year_start.html,pandas.tseries.offsets.Second.is_year_start,Second.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1239,..\pandas\reference\api\pandas.errors.UnsortedIndexError.html,pandas.errors.UnsortedIndexError,exception pandas.errors.UnsortedIndexError[source]# Error raised when slicing a MultiIndex which has not been lexsorted. Subclass of KeyError.,No parameters found,"['>>> df = pd.DataFrame({""cat"": [0, 0, 1, 1],\n...                    ""color"": [""white"", ""white"", ""brown"", ""black""],\n...                    ""lives"": [4, 4, 3, 7]},\n...                   )\n>>> df = df.set_index([""cat"", ""color""])\n>>> df\n            lives\ncat  color\n0    white    4\n     white    4\n1    brown    3\n     black    7\n>>> df.loc[(0, ""black""):(1, ""white"")]\nTraceback (most recent call last):\nUnsortedIndexError: \'Key length (2) was greater\nthan MultiIndex lexsort depth (1)\'']"
1240,..\pandas\reference\api\pandas.tseries.offsets.Day.freqstr.html,pandas.tseries.offsets.Day.freqstr,Day.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1241,..\pandas\reference\api\pandas.Period.to_timestamp.html,pandas.Period.to_timestamp,"Period.to_timestamp(freq=None, how='start')# Return the Timestamp representation of the Period. Uses the target frequency specified at the part of the period specified by how, which is either Start or Finish.","Parameters: freqstr or DateOffsetTarget frequency. Default is ‘D’ if self.freq is week or longer and ‘S’ otherwise. howstr, default ‘S’ (start)One of ‘S’, ‘E’. Can be aliased as case insensitive ‘Start’, ‘Finish’, ‘Begin’, ‘End’. Returns: Timestamp","["">>> period = pd.Period('2023-1-1', freq='D')\n>>> timestamp = period.to_timestamp()\n>>> timestamp\nTimestamp('2023-01-01 00:00:00')""]"
1242,..\pandas\reference\api\pandas.DataFrame.explode.html,pandas.DataFrame.explode,"DataFrame.explode(column, ignore_index=False)[source]# Transform each element of a list-like to a row, replicating index values. Notes This routine will explode list-likes including lists, tuples, sets, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged, and empty list-likes will result in a np.nan for that row. In addition, the ordering of rows in the output will be non-deterministic when exploding sets. Reference the user guide for more examples.","Parameters: columnIndexLabelColumn(s) to explode. For multiple columns, specify a non-empty list with each element be str or tuple, and all specified columns their list-like data on same row of the frame must have matching length. Added in version 1.3.0: Multi-column explode ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, …, n - 1. Returns: DataFrameExploded lists to rows of the subset columns; index will be duplicated for these rows. Raises: ValueError If columns of the frame are not unique. If specified columns to explode is empty list. If specified columns to explode have not matching count of elements rowwise in the frame.","["">>> df = pd.DataFrame({'A': [[0, 1, 2], 'foo', [], [3, 4]],\n...                    'B': 1,\n...                    'C': [['a', 'b', 'c'], np.nan, [], ['d', 'e']]})\n>>> df\n           A  B          C\n0  [0, 1, 2]  1  [a, b, c]\n1        foo  1        NaN\n2         []  1         []\n3     [3, 4]  1     [d, e]"", "">>> df.explode('A')\n     A  B          C\n0    0  1  [a, b, c]\n0    1  1  [a, b, c]\n0    2  1  [a, b, c]\n1  foo  1        NaN\n2  NaN  1         []\n3    3  1     [d, e]\n3    4  1     [d, e]"", "">>> df.explode(list('AC'))\n     A  B    C\n0    0  1    a\n0    1  1    b\n0    2  1    c\n1  foo  1  NaN\n2  NaN  1  NaN\n3    3  1    d\n3    4  1    e""]"
1243,..\pandas\reference\api\pandas.errors.UnsupportedFunctionCall.html,pandas.errors.UnsupportedFunctionCall,"exception pandas.errors.UnsupportedFunctionCall[source]# Exception raised when attempting to call a unsupported numpy function. For example, np.cumsum(groupby_object).",No parameters found,"['>>> df = pd.DataFrame({""A"": [0, 0, 1, 1],\n...                    ""B"": [""x"", ""x"", ""z"", ""y""],\n...                    ""C"": [1, 2, 3, 4]}\n...                   )\n>>> np.cumsum(df.groupby([""A""]))\nTraceback (most recent call last):\nUnsupportedFunctionCall: numpy operations are not valid with groupby.\nUse .groupby(...).cumsum() instead']"
1244,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.head.html,pandas.core.groupby.DataFrameGroupBy.head,"DataFrameGroupBy.head(n=5)[source]# Return first n rows of each group. Similar to .apply(lambda x: x.head(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).",Parameters: nintIf positive: number of entries to include from start of each group. If negative: number of entries to exclude from end of each group. Returns: Series or DataFrameSubset of original Series or DataFrame as determined by n.,"["">>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').head(1)\n   A  B\n0  1  2\n2  5  6\n>>> df.groupby('A').head(-1)\n   A  B\n0  1  2""]"
1245,..\pandas\reference\api\pandas.tseries.offsets.Second.kwds.html,pandas.tseries.offsets.Second.kwds,Second.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1246,..\pandas\reference\api\pandas.Series.plot.density.html,pandas.Series.plot.density,"Series.plot.density(bw_method=None, ind=None, **kwargs)[source]# Generate Kernel Density Estimate plot using Gaussian kernels. In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.","Parameters: bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be ‘scott’, ‘silverman’, a scalar constant or a callable. If None (default), ‘scott’ is used. See scipy.stats.gaussian_kde for more information. indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","['>>> s = pd.Series([1, 2, 2.5, 3, 3.5, 4, 5])\n>>> ax = s.plot.kde()', '>>> ax = s.plot.kde(bw_method=0.3)', '>>> ax = s.plot.kde(bw_method=3)', '>>> ax = s.plot.kde(ind=[1, 2, 3, 4, 5])', "">>> df = pd.DataFrame({\n...     'x': [1, 2, 2.5, 3, 3.5, 4, 5],\n...     'y': [4, 4, 4.5, 5, 5.5, 6, 6],\n... })\n>>> ax = df.plot.kde()"", '>>> ax = df.plot.kde(bw_method=0.3)', '>>> ax = df.plot.kde(bw_method=3)', '>>> ax = df.plot.kde(ind=[1, 2, 3, 4, 5, 6])']"
1247,..\pandas\reference\api\pandas.tseries.offsets.BDay.html,pandas.tseries.offsets.BDay,pandas.tseries.offsets.BDay# alias of BusinessDay,No parameters found,[]
1248,..\pandas\reference\api\pandas.tseries.offsets.Day.html,pandas.tseries.offsets.Day,class pandas.tseries.offsets.Day# Offset n days. Examples You can use the parameter n to represent a shift of n days. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of days represented.","["">>> from pandas.tseries.offsets import Day\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Day()\nTimestamp('2022-12-10 15:00:00')\n>>> ts - Day(4)\nTimestamp('2022-12-05 15:00:00')"", "">>> ts + Day(-4)\nTimestamp('2022-12-05 15:00:00')""]"
1249,..\pandas\reference\api\pandas.Period.week.html,pandas.Period.week,Period.week# Get the week of the year on the given Period.,Returns: int,"['>>> p = pd.Period(""2018-03-11"", ""h"")\n>>> p.week\n10', '>>> p = pd.Period(""2018-02-01"", ""D"")\n>>> p.week\n5', '>>> p = pd.Period(""2018-01-06"", ""D"")\n>>> p.week\n1']"
1250,..\pandas\reference\api\pandas.DataFrame.ffill.html,pandas.DataFrame.ffill,"DataFrame.ffill(*, axis=None, inplace=False, limit=None, limit_area=None, downcast=<no_default>)[source]# Fill NA/NaN values by propagating the last valid observation to next valid.","Parameters: axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrameAxis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplacebool, default FalseIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area{None, ‘inside’, ‘outside’}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). Added in version 2.2.0. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.2.0. Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.","['>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n...                    [3, 4, np.nan, 1],\n...                    [np.nan, np.nan, np.nan, np.nan],\n...                    [np.nan, 3, np.nan, 4]],\n...                   columns=list(""ABCD""))\n>>> df\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  NaN  NaN NaN  NaN\n3  NaN  3.0 NaN  4.0', '>>> df.ffill()\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  3.0  4.0 NaN  1.0\n3  3.0  3.0 NaN  4.0', '>>> ser = pd.Series([1, np.nan, 2, 3])\n>>> ser.ffill()\n0   1.0\n1   1.0\n2   2.0\n3   3.0\ndtype: float64']"
1251,..\pandas\reference\api\pandas.errors.ValueLabelTypeMismatch.html,pandas.errors.ValueLabelTypeMismatch,exception pandas.errors.ValueLabelTypeMismatch[source]# Warning raised by to_stata on a category column that contains non-string values.,No parameters found,"['>>> df = pd.DataFrame({""categories"": pd.Series([""a"", 2], dtype=""category"")})\n>>> df.to_stata(\'test\') \n... # ValueLabelTypeMismatch: Stata value labels (pandas categories) must be str...']"
1252,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.hist.html,pandas.core.groupby.DataFrameGroupBy.hist,"DataFrameGroupBy.hist(column=None, by=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False, sharey=False, figsize=None, layout=None, bins=10, backend=None, legend=False, **kwargs)[source]# Make a histogram of the DataFrame’s columns. A histogram is a representation of the distribution of data. This function calls matplotlib.pyplot.hist(), on each series in the DataFrame, resulting in one histogram per column.","Parameters: dataDataFrameThe pandas object holding the data. columnstr or sequence, optionalIf passed, will be used to limit data to a subset of columns. byobject, optionalIf passed, then used to form histograms for separate groups. gridbool, default TrueWhether to show axis grid lines. xlabelsizeint, default NoneIf specified changes the x-axis label size. xrotfloat, default NoneRotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise. ylabelsizeint, default NoneIf specified changes the y-axis label size. yrotfloat, default NoneRotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise. axMatplotlib axes object, default NoneThe axes to plot the histogram on. sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure. shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to invisible. figsizetuple, optionalThe size in inches of the figure to create. Uses the value in matplotlib.rcParams by default. layouttuple, optionalTuple of (rows, columns) for the layout of the histograms. binsint or sequence, default 10Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. legendbool, default FalseWhether to show the legend. **kwargsAll other plotting keyword arguments to be passed to matplotlib.pyplot.hist(). Returns: matplotlib.AxesSubplot or numpy.ndarray of them","["">>> data = {'length': [1.5, 0.5, 1.2, 0.9, 3],\n...         'width': [0.7, 0.2, 0.15, 0.2, 1.1]}\n>>> index = ['pig', 'rabbit', 'duck', 'chicken', 'horse']\n>>> df = pd.DataFrame(data, index=index)\n>>> hist = df.hist(bins=3)""]"
1253,..\pandas\reference\api\pandas.tseries.offsets.Second.n.html,pandas.tseries.offsets.Second.n,Second.n#,No parameters found,[]
1254,..\pandas\reference\api\pandas.tseries.offsets.BMonthBegin.html,pandas.tseries.offsets.BMonthBegin,pandas.tseries.offsets.BMonthBegin# alias of BusinessMonthBegin,No parameters found,[]
1255,..\pandas\reference\api\pandas.Period.weekday.html,pandas.Period.weekday,"Period.weekday# Day of the week the period lies in, with Monday=0 and Sunday=6. If the period frequency is lower than daily (e.g. hourly), and the period spans over multiple days, the day at the start of the period is used. If the frequency is higher than daily (e.g. monthly), the last day of the period is used.",Returns: intDay of the week.,"["">>> per = pd.Period('2017-12-31 22:00', 'h')\n>>> per.dayofweek\n6"", "">>> per = pd.Period('2017-12-31 22:00', '4h')\n>>> per.dayofweek\n6\n>>> per.start_time.dayofweek\n6"", "">>> per = pd.Period('2018-01', 'M')\n>>> per.dayofweek\n2\n>>> per.end_time.dayofweek\n2""]"
1256,..\pandas\reference\api\pandas.Series.plot.hist.html,pandas.Series.plot.hist,"Series.plot.hist(by=None, bins=10, **kwargs)[source]# Draw one histogram of the DataFrame’s columns. A histogram is a representation of the distribution of data. This function groups the values of all given Series in the DataFrame into bins and draws all bins in one matplotlib.axes.Axes. This is useful when the DataFrame’s Series are in a similar scale.","Parameters: bystr or sequence, optionalColumn in the DataFrame to group by. Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings binsint, default 10Number of histogram bins to be used. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: class:matplotlib.AxesSubplotReturn a histogram plot.","["">>> df = pd.DataFrame(np.random.randint(1, 7, 6000), columns=['one'])\n>>> df['two'] = df['one'] + np.random.randint(1, 7, 6000)\n>>> ax = df.plot.hist(bins=12, alpha=0.5)"", '>>> age_list = [8, 10, 12, 14, 72, 74, 76, 78, 20, 25, 30, 35, 60, 85]\n>>> df = pd.DataFrame({""gender"": list(""MMMMMMMMFFFFFF""), ""age"": age_list})\n>>> ax = df.plot.hist(column=[""age""], by=""gender"", figsize=(10, 8))']"
1257,..\pandas\reference\api\pandas.eval.html,pandas.eval,"pandas.eval(expr, parser='pandas', engine=None, local_dict=None, global_dict=None, resolvers=(), level=0, target=None, inplace=False)[source]# Evaluate a Python expression as a string using various backends. The following arithmetic operations are supported: +, -, *, /, **, %, // (python engine only) along with the following boolean operations: | (or), & (and), and ~ (not). Additionally, the 'pandas' parser allows the use of and, or, and not with the same semantics as the corresponding bitwise operators.  Series and DataFrame objects are supported and behave as they would with plain ol’ Python evaluation. Notes The dtype of any objects involved in an arithmetic % operation are recursively cast to float64. See the enhancing performance documentation for more details.","Parameters: exprstrThe expression to evaluate. This string cannot contain any Python statements, only Python expressions. parser{‘pandas’, ‘python’}, default ‘pandas’The parser to use to construct the syntax tree from the expression. The default of 'pandas' parses code slightly different than standard Python. Alternatively, you can parse an expression using the 'python' parser to retain strict Python semantics.  See the enhancing performance documentation for more details. engine{‘python’, ‘numexpr’}, default ‘numexpr’The engine used to evaluate the expression. Supported engines are None : tries to use numexpr, falls back to python 'numexpr' : This default engine evaluates pandas objects using numexpr for large speed ups in complex expressions with large frames. 'python' : Performs operations as if you had eval’d in top level python. This engine is generally not that useful. More backends may be available in the future. local_dictdict or None, optionalA dictionary of local variables, taken from locals() by default. global_dictdict or None, optionalA dictionary of global variables, taken from globals() by default. resolverslist of dict-like or None, optionalA list of objects implementing the __getitem__ special method that you can use to inject an additional collection of namespaces to use for variable lookup. For example, this is used in the query() method to inject the DataFrame.index and DataFrame.columns variables that refer to their respective DataFrame instance attributes. levelint, optionalThe number of prior stack frames to traverse and add to the current scope. Most users will not need to change this parameter. targetobject, optional, default NoneThis is the target object for assignment. It is used when there is variable assignment in the expression. If so, then target must support item assignment with string keys, and if a copy is being returned, it must also support .copy(). inplacebool, default FalseIf target is provided, and the expression mutates target, whether to modify target inplace. Otherwise, return a copy of target with the mutation. Returns: ndarray, numeric scalar, DataFrame, Series, or NoneThe completion value of evaluating the given code or None if inplace=True. Raises: ValueErrorThere are many instances where such an error can be raised: target=None, but the expression is multiline. The expression is multiline, but not all them have item assignment. An example of such an arrangement is this: a = b + 1 a + 2 Here, there are expressions on different lines, making it multiline, but the last line has no variable assigned to the output of a + 2. inplace=True, but the expression is missing item assignment. Item assignment is provided, but the target does not support string item assignment. Item assignment is provided and inplace=False, but the target does not support the .copy() method","['>>> df = pd.DataFrame({""animal"": [""dog"", ""pig""], ""age"": [10, 20]})\n>>> df\n  animal  age\n0    dog   10\n1    pig   20', '>>> pd.eval(""double_age = df.age * 2"", target=df)\n  animal  age  double_age\n0    dog   10          20\n1    pig   20          40']"
1258,..\pandas\reference\api\pandas.DataFrame.fillna.html,pandas.DataFrame.fillna,"DataFrame.fillna(value=None, *, method=None, axis=None, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values using the specified method.","Parameters: valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame).  Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. method{‘backfill’, ‘bfill’, ‘ffill’, None}, default NoneMethod to use for filling holes in reindexed Series: ffill: propagate last valid observation forward to next valid. backfill / bfill: use next valid observation to fill gap. Deprecated since version 2.1.0: Use ffill or bfill instead. axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrameAxis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplacebool, default FalseIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.2.0. Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.","['>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n...                    [3, 4, np.nan, 1],\n...                    [np.nan, np.nan, np.nan, np.nan],\n...                    [np.nan, 3, np.nan, 4]],\n...                   columns=list(""ABCD""))\n>>> df\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  NaN  NaN NaN  NaN\n3  NaN  3.0 NaN  4.0', '>>> df.fillna(0)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  0.0\n3  0.0  3.0  0.0  4.0', '>>> values = {""A"": 0, ""B"": 1, ""C"": 2, ""D"": 3}\n>>> df.fillna(value=values)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  2.0  1.0\n2  0.0  1.0  2.0  3.0\n3  0.0  3.0  2.0  4.0', '>>> df.fillna(value=values, limit=1)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  NaN  1.0\n2  NaN  1.0  NaN  3.0\n3  NaN  3.0  NaN  4.0', '>>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(""ABCE""))\n>>> df.fillna(df2)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  NaN\n3  0.0  3.0  0.0  4.0']"
1259,..\pandas\reference\api\pandas.tseries.offsets.Day.is_anchored.html,pandas.tseries.offsets.Day.is_anchored,Day.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
1260,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.idxmax.html,pandas.core.groupby.DataFrameGroupBy.idxmax,"DataFrameGroupBy.idxmax(axis=<no_default>, skipna=True, numeric_only=False)[source]# Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Notes This method is the DataFrame version of ndarray.argmax.","Parameters: axis{{0 or ‘index’, 1 or ‘columns’}}, default NoneThe axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. If axis is not provided, grouper’s axis is used. Changed in version 2.0.0. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Returns: SeriesIndexes of maxima along the specified axis. Raises: ValueError If the row/column is empty","["">>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n...                    'co2_emissions': [37.2, 19.66, 1712]},\n...                   index=['Pork', 'Wheat Products', 'Beef'])"", '>>> df\n                consumption  co2_emissions\nPork                  10.51         37.20\nWheat Products       103.11         19.66\nBeef                  55.48       1712.00', '>>> df.idxmax()\nconsumption     Wheat Products\nco2_emissions             Beef\ndtype: object', '>>> df.idxmax(axis=""columns"")\nPork              co2_emissions\nWheat Products     consumption\nBeef              co2_emissions\ndtype: object']"
1261,..\pandas\reference\api\pandas.tseries.offsets.BMonthEnd.html,pandas.tseries.offsets.BMonthEnd,pandas.tseries.offsets.BMonthEnd# alias of BusinessMonthEnd,No parameters found,[]
1262,..\pandas\reference\api\pandas.tseries.offsets.Second.name.html,pandas.tseries.offsets.Second.name,Second.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1263,..\pandas\reference\api\pandas.Period.weekofyear.html,pandas.Period.weekofyear,Period.weekofyear# Get the week of the year on the given Period.,Returns: int,"['>>> p = pd.Period(""2018-03-11"", ""h"")\n>>> p.weekofyear\n10', '>>> p = pd.Period(""2018-02-01"", ""D"")\n>>> p.weekofyear\n5', '>>> p = pd.Period(""2018-01-06"", ""D"")\n>>> p.weekofyear\n1']"
1264,..\pandas\reference\api\pandas.Series.plot.html,pandas.Series.plot,"Series.plot(*args, **kwargs)[source]# Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used. Notes See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)","Parameters: dataSeries or DataFrameThe object for which the method is called. xlabel or position, default NoneOnly used if data is a DataFrame. ylabel, position or list of label, positions, default NoneAllows plotting of one column versus another. Only used if data is a DataFrame. kindstrThe kind of plot to produce: ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only) axmatplotlib axes object, default NoneAn axes of the current figure. subplotsbool or sequence of iterables, default FalseWhether to group columns into subplots: False : No subplots will be used True : Make separate subplots for each column. sequence of iterables of column labels: Create a subplot for each group of columns. For example [(‘a’, ‘c’), (‘b’, ‘d’)] will create 2 subplots: one with columns ‘a’ and ‘c’, and one with columns ‘b’ and ‘d’. Remaining columns that aren’t specified will be plotted in additional subplots (one per column). Added in version 1.5.0. sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure. shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to invisible. layouttuple, optional(rows, columns) for the layout of subplots. figsizea tuple (width, height) in inchesSize of a figure object. use_indexbool, default TrueUse index as ticks for x axis. titlestr or listTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot. gridbool, default None (matlab style default)Axis grid lines. legendbool or {‘reverse’}Place legend on axis subplots. stylelist or dictThe matplotlib line style per column. logxbool or ‘sym’, default FalseUse log scaling or symlog scaling on x axis. logybool or ‘sym’ default FalseUse log scaling or symlog scaling on y axis. loglogbool or ‘sym’, default FalseUse log scaling or symlog scaling on both x and y axes. xtickssequenceValues to use for the xticks. ytickssequenceValues to use for the yticks. xlim2-tuple/listSet the x limits of the current axes. ylim2-tuple/listSet the y limits of the current axes. xlabellabel, optionalName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. ylabellabel, optionalName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. rotfloat, default NoneRotation for ticks (xticks for vertical, yticks for horizontal plots). fontsizefloat, default NoneFont size for xticks and yticks. colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If string, load colormap with that name from matplotlib. colorbarbool, optionalIf True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots). positionfloatSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center). tablebool, Series or DataFrame, default FalseIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table. yerrDataFrame, Series, array-like, dict and strSee Plotting with Error Bars for detail. xerrDataFrame, Series, array-like, dict and strEquivalent to yerr. stackedbool, default False in line and bar plots, and True in area plotIf True, create stacked plot. secondary_ybool or sequence, default FalseWhether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis. mark_rightbool, default TrueWhen using a secondary_y axis, automatically mark the column labels with “(right)” in the legend. include_boolbool, default is FalseIf True, boolean values can be plotted. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes or numpy.ndarray of themIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.","['>>> ser = pd.Series([1, 2, 3, 3])\n>>> plot = ser.plot(kind=\'hist\', title=""My plot"")', '>>> df = pd.DataFrame({\'length\': [1.5, 0.5, 1.2, 0.9, 3],\n...                   \'width\': [0.7, 0.2, 0.15, 0.2, 1.1]},\n...                   index=[\'pig\', \'rabbit\', \'duck\', \'chicken\', \'horse\'])\n>>> plot = df.plot(title=""DataFrame Plot"")', '>>> lst = [-1, -2, -3, 1, 2, 3]\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> plot = ser.groupby(lambda x: x > 0).plot(title=""SeriesGroupBy Plot"")', '>>> df = pd.DataFrame({""col1"" : [1, 2, 3, 4],\n...                   ""col2"" : [""A"", ""B"", ""A"", ""B""]})\n>>> plot = df.groupby(""col2"").plot(kind=""bar"", title=""DataFrameGroupBy Plot"")']"
1265,..\pandas\reference\api\pandas.ExcelFile.book.html,pandas.ExcelFile.book,property ExcelFile.book[source]#,No parameters found,[]
1266,..\pandas\reference\api\pandas.DataFrame.filter.html,pandas.DataFrame.filter,"DataFrame.filter(items=None, like=None, regex=None, axis=None)[source]# Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Notes The items, like, and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with [].","Parameters: itemslist-likeKeep labels from axis which are in items. likestrKeep labels from axis for which “like in label == True”. regexstr (regular expression)Keep labels from axis for which re.search(regex, label) == True. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneThe axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, ‘columns’ for DataFrame. For Series this parameter is unused and defaults to None. Returns: same type as input object","["">>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6"", "">>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6"", "">>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6"", "">>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6""]"
1267,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.idxmin.html,pandas.core.groupby.DataFrameGroupBy.idxmin,"DataFrameGroupBy.idxmin(axis=<no_default>, skipna=True, numeric_only=False)[source]# Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Notes This method is the DataFrame version of ndarray.argmin.","Parameters: axis{{0 or ‘index’, 1 or ‘columns’}}, default NoneThe axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. If axis is not provided, grouper’s axis is used. Changed in version 2.0.0. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Returns: SeriesIndexes of minima along the specified axis. Raises: ValueError If the row/column is empty","["">>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n...                    'co2_emissions': [37.2, 19.66, 1712]},\n...                   index=['Pork', 'Wheat Products', 'Beef'])"", '>>> df\n                consumption  co2_emissions\nPork                  10.51         37.20\nWheat Products       103.11         19.66\nBeef                  55.48       1712.00', '>>> df.idxmin()\nconsumption                Pork\nco2_emissions    Wheat Products\ndtype: object', '>>> df.idxmin(axis=""columns"")\nPork                consumption\nWheat Products    co2_emissions\nBeef                consumption\ndtype: object']"
1268,..\pandas\reference\api\pandas.Series.plot.kde.html,pandas.Series.plot.kde,"Series.plot.kde(bw_method=None, ind=None, **kwargs)[source]# Generate Kernel Density Estimate plot using Gaussian kernels. In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.","Parameters: bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be ‘scott’, ‘silverman’, a scalar constant or a callable. If None (default), ‘scott’ is used. See scipy.stats.gaussian_kde for more information. indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","['>>> s = pd.Series([1, 2, 2.5, 3, 3.5, 4, 5])\n>>> ax = s.plot.kde()', '>>> ax = s.plot.kde(bw_method=0.3)', '>>> ax = s.plot.kde(bw_method=3)', '>>> ax = s.plot.kde(ind=[1, 2, 3, 4, 5])', "">>> df = pd.DataFrame({\n...     'x': [1, 2, 2.5, 3, 3.5, 4, 5],\n...     'y': [4, 4, 4.5, 5, 5.5, 6, 6],\n... })\n>>> ax = df.plot.kde()"", '>>> ax = df.plot.kde(bw_method=0.3)', '>>> ax = df.plot.kde(bw_method=3)', '>>> ax = df.plot.kde(ind=[1, 2, 3, 4, 5, 6])']"
1269,..\pandas\reference\api\pandas.Period.year.html,pandas.Period.year,Period.year# Return the year this Period falls on.,No parameters found,"["">>> period = pd.Period('2022-01', 'M')\n>>> period.year\n2022""]"
1270,..\pandas\reference\api\pandas.tseries.offsets.Day.is_month_end.html,pandas.tseries.offsets.Day.is_month_end,Day.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1271,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.copy.html,pandas.tseries.offsets.BQuarterBegin.copy,BQuarterBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1272,..\pandas\reference\api\pandas.ExcelFile.html,pandas.ExcelFile,"class pandas.ExcelFile(path_or_buffer, engine=None, storage_options=None, engine_kwargs=None)[source]# Class for parsing tabular Excel sheets into DataFrame objects. See read_excel for more documentation. Examples Attributes book sheet_names","Parameters: path_or_bufferstr, bytes, path object (pathlib.Path or py._path.local.LocalPath),A file-like object, xlrd workbook or openpyxl workbook. If a string or path object, expected to be a path to a .xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file. enginestr, default NoneIf io is not a buffer or path, this must be set to identify io. Supported engines: xlrd, openpyxl, odf, pyxlsb, calamine Engine compatibility : xlrd supports old-style Excel files (.xls). openpyxl supports newer Excel file formats. odf supports OpenDocument file formats (.odf, .ods, .odt). pyxlsb supports Binary Excel files. calamine supports Excel (.xls, .xlsx, .xlsm, .xlsb) and OpenDocument (.ods) file formats. Changed in version 1.2.0: The engine xlrd now only supports old-style .xls files. When engine=None, the following logic will be used to determine the engine: If path_or_buffer is an OpenDocument format (.odf, .ods, .odt), then odf will be used. Otherwise if path_or_buffer is an xls format, xlrd will be used. Otherwise if path_or_buffer is in xlsb format, pyxlsb will be used. Added in version 1.3.0. Otherwise if openpyxl is installed, then openpyxl will be used. Otherwise if xlrd >= 2.0 is installed, a ValueError will be raised. Warning Please do not report issues when using xlrd to read .xlsx files. This is not supported, switch to using openpyxl instead. engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine.","['>>> file = pd.ExcelFile(\'myfile.xlsx\')  \n>>> with pd.ExcelFile(""myfile.xls"") as xls:  \n...     df1 = pd.read_excel(xls, ""Sheet1"")']"
1273,..\pandas\reference\api\pandas.tseries.offsets.Second.nanos.html,pandas.tseries.offsets.Second.nanos,Second.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
1274,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.indices.html,pandas.core.groupby.DataFrameGroupBy.indices,property DataFrameGroupBy.indices[source]# Dict {group name -> group indices}.,No parameters found,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).indices\n{'a': array([0, 1]), 'b': array([2])}"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(by=[""a""]).indices\n{1: array([0, 1]), 7: array([2])}', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').indices\ndefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],\nTimestamp('2023-02-01 00:00:00'): [2, 3]})""]"
1275,..\pandas\reference\api\pandas.PeriodDtype.freq.html,pandas.PeriodDtype.freq,property PeriodDtype.freq[source]# The frequency object of this PeriodDtype.,No parameters found,"["">>> dtype = pd.PeriodDtype(freq='D')\n>>> dtype.freq\n<Day>""]"
1276,..\pandas\reference\api\pandas.DataFrame.first.html,pandas.DataFrame.first,"DataFrame.first(offset)[source]# Select initial periods of time series data based on a date offset. Deprecated since version 2.1: first() is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function can select the first few rows based on a date offset.","Parameters: offsetstr, DateOffset or dateutil.relativedeltaThe offset length of the data that will be selected. For instance, ‘1ME’ will display all the rows having their index within the first month. Returns: Series or DataFrameA subset of the caller. Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n            A\n2018-04-09  1\n2018-04-11  2\n2018-04-13  3\n2018-04-15  4"", "">>> ts.first('3D')\n            A\n2018-04-09  1\n2018-04-11  2""]"
1277,..\pandas\reference\api\pandas.Series.plot.line.html,pandas.Series.plot.line,"Series.plot.line(x=None, y=None, **kwargs)[source]# Plot Series or DataFrame as lines. This function is useful to plot lines using DataFrame’s values as coordinates.","Parameters: xlabel or position, optionalAllows plotting of one column versus another. If not specified, the index of the DataFrame is used. ylabel or position, optionalAllows plotting of one column versus another. If not specified, all numerical columns are used. colorstr, array-like, or dict, optionalThe color for each of the DataFrame’s columns. Possible values are: A single color string referred to by name, RGB or RGBA code,for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used. A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color lines for column a in green and lines for column b in red. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.","['>>> s = pd.Series([1, 3, 2])\n>>> s.plot.line()', "">>> df = pd.DataFrame({\n...    'pig': [20, 18, 489, 675, 1776],\n...    'horse': [4, 25, 281, 600, 1900]\n...    }, index=[1990, 1997, 2003, 2009, 2014])\n>>> lines = df.plot.line()"", "">>> axes = df.plot.line(subplots=True)\n>>> type(axes)\n<class 'numpy.ndarray'>"", '>>> axes = df.plot.line(\n...     subplots=True, color={""pig"": ""pink"", ""horse"": ""#742802""}\n... )', "">>> lines = df.plot.line(x='pig', y='horse')""]"
1278,..\pandas\reference\api\pandas.tseries.offsets.Day.is_month_start.html,pandas.tseries.offsets.Day.is_month_start,Day.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1279,..\pandas\reference\api\pandas.PeriodDtype.html,pandas.PeriodDtype,"class pandas.PeriodDtype(freq)[source]# An ExtensionDtype for Period data. This is not an actual numpy dtype, but a duck type. Attributes freq The frequency object of this PeriodDtype. Methods None",Parameters: freqstr or DateOffsetThe frequency of this PeriodDtype.,"["">>> pd.PeriodDtype(freq='D')\nperiod[D]"", '>>> pd.PeriodDtype(freq=pd.offsets.MonthEnd())\nperiod[M]']"
1280,..\pandas\reference\api\pandas.ExcelFile.parse.html,pandas.ExcelFile.parse,"ExcelFile.parse(sheet_name=0, header=0, names=None, index_col=None, usecols=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, parse_dates=False, date_parser=<no_default>, date_format=None, thousands=None, comment=None, skipfooter=0, dtype_backend=<no_default>, **kwds)[source]# Parse specified sheet(s) into a DataFrame. Equivalent to read_excel(ExcelFile, …)  See the read_excel docstring for more info on accepted parameters.",Returns: DataFrame or dict of DataFramesDataFrame from the passed in Excel file.,"["">>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n>>> df.to_excel('myfile.xlsx')  \n>>> file = pd.ExcelFile('myfile.xlsx')  \n>>> file.parse()""]"
1281,..\pandas\reference\api\pandas.Series.plot.pie.html,pandas.Series.plot.pie,Series.plot.pie(**kwargs)[source]# Generate a pie plot. A pie plot is a proportional representation of the numerical data in a column. This function wraps matplotlib.pyplot.pie() for the specified column. If no column reference is passed and subplots=True a pie plot is drawn for each numerical column independently.,"Parameters: yint or label, optionalLabel or position of the column to plot. If not provided, subplots=True argument must be passed. **kwargsKeyword arguments to pass on to DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themA NumPy array is returned when subplots is True.","["">>> df = pd.DataFrame({'mass': [0.330, 4.87 , 5.97],\n...                    'radius': [2439.7, 6051.8, 6378.1]},\n...                   index=['Mercury', 'Venus', 'Earth'])\n>>> plot = df.plot.pie(y='mass', figsize=(5, 5))"", '>>> plot = df.plot.pie(subplots=True, figsize=(11, 6))']"
1282,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.freqstr.html,pandas.tseries.offsets.BQuarterBegin.freqstr,BQuarterBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1283,..\pandas\reference\api\pandas.DataFrame.first_valid_index.html,pandas.DataFrame.first_valid_index,"DataFrame.first_valid_index()[source]# Return index for first non-NA value or None, if no non-NA value is found.",Returns: type of index,"['>>> s = pd.Series([None, 3, 4])\n>>> s.first_valid_index()\n1\n>>> s.last_valid_index()\n2', '>>> s = pd.Series([None, None])\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', '>>> s = pd.Series()\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', "">>> df = pd.DataFrame({'A': [None, None, 2], 'B': [None, 3, 4]})\n>>> df\n     A      B\n0  NaN    NaN\n1  NaN    3.0\n2  2.0    4.0\n>>> df.first_valid_index()\n1\n>>> df.last_valid_index()\n2"", "">>> df = pd.DataFrame({'A': [None, None, None], 'B': [None, None, None]})\n>>> df\n     A      B\n0  None   None\n1  None   None\n2  None   None\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone"", '>>> df = pd.DataFrame()\n>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone']"
1284,..\pandas\reference\api\pandas.tseries.offsets.Second.normalize.html,pandas.tseries.offsets.Second.normalize,Second.normalize#,No parameters found,[]
1285,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.last.html,pandas.core.groupby.DataFrameGroupBy.last,"DataFrameGroupBy.last(numeric_only=False, min_count=-1, skipna=True)[source]# Compute the last entry of each column within each group. Defaults to skipping NA elements.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count valid values are present the result will be NA. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Added in version 2.2.1. Returns: Series or DataFrameLast of values within each group.","['>>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))\n>>> df.groupby(""A"").last()\n     B  C\nA\n1  5.0  2\n3  6.0  3']"
1286,..\pandas\reference\api\pandas.tseries.offsets.Day.is_on_offset.html,pandas.tseries.offsets.Day.is_on_offset,Day.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1287,..\pandas\reference\api\pandas.PeriodIndex.asfreq.html,pandas.PeriodIndex.asfreq,"PeriodIndex.asfreq(freq=None, how='E')[source]# Convert the PeriodArray to the specified frequency freq. Equivalent to applying pandas.Period.asfreq() with the given arguments to each Period in this PeriodArray.","Parameters: freqstrA frequency. howstr {‘E’, ‘S’}, default ‘E’Whether the elements should be aligned to the end or start within pa period. ‘E’, ‘END’, or ‘FINISH’ for end, ‘S’, ‘START’, or ‘BEGIN’ for start. January 31st (‘END’) vs. January 1st (‘START’) for example. Returns: PeriodArrayThe transformed PeriodArray with the new frequency.","["">>> pidx = pd.period_range('2010-01-01', '2015-01-01', freq='Y')\n>>> pidx\nPeriodIndex(['2010', '2011', '2012', '2013', '2014', '2015'],\ndtype='period[Y-DEC]')"", "">>> pidx.asfreq('M')\nPeriodIndex(['2010-12', '2011-12', '2012-12', '2013-12', '2014-12',\n'2015-12'], dtype='period[M]')"", "">>> pidx.asfreq('M', how='S')\nPeriodIndex(['2010-01', '2011-01', '2012-01', '2013-01', '2014-01',\n'2015-01'], dtype='period[M]')""]"
1288,..\pandas\reference\api\pandas.tseries.offsets.Second.rule_code.html,pandas.tseries.offsets.Second.rule_code,Second.rule_code#,No parameters found,[]
1289,..\pandas\reference\api\pandas.DataFrame.floordiv.html,pandas.DataFrame.floordiv,"DataFrame.floordiv(other, axis='columns', level=None, fill_value=None)[source]# Get Integer division of dataframe and other, element-wise (binary operator floordiv). Equivalent to dataframe // other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rfloordiv. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1290,..\pandas\reference\api\pandas.ExcelFile.sheet_names.html,pandas.ExcelFile.sheet_names,property ExcelFile.sheet_names[source]#,No parameters found,[]
1291,..\pandas\reference\api\pandas.Series.pop.html,pandas.Series.pop,Series.pop(item)[source]# Return item and drops from series. Raise KeyError if not found.,Parameters: itemlabelIndex of the element that needs to be removed. Returns: Value that is popped from series.,"['>>> ser = pd.Series([1, 2, 3])', '>>> ser.pop(0)\n1', '>>> ser\n1    2\n2    3\ndtype: int64']"
1292,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.html,pandas.tseries.offsets.BQuarterBegin,"class pandas.tseries.offsets.BQuarterBegin# DateOffset increments between the first business day of each Quarter. startingMonth = 1 corresponds to dates like 1/01/2007, 4/01/2007, … startingMonth = 2 corresponds to dates like 2/01/2007, 5/01/2007, … startingMonth = 3 corresponds to dates like 3/01/2007, 6/01/2007, … Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code startingMonth","Parameters: nint, default 1The number of quarters represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. startingMonthint, default 3A specific integer for the month of the year from which we start quarters.","["">>> from pandas.tseries.offsets import BQuarterBegin\n>>> ts = pd.Timestamp('2020-05-24 05:01:15')\n>>> ts + BQuarterBegin()\nTimestamp('2020-06-01 05:01:15')\n>>> ts + BQuarterBegin(2)\nTimestamp('2020-09-01 05:01:15')\n>>> ts + BQuarterBegin(startingMonth=2)\nTimestamp('2020-08-03 05:01:15')\n>>> ts + BQuarterBegin(-1)\nTimestamp('2020-03-02 05:01:15')""]"
1293,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.max.html,pandas.core.groupby.DataFrameGroupBy.max,"DataFrameGroupBy.max(numeric_only=False, min_count=-1, engine=None, engine_kwargs=None)[source]# Compute max of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. enginestr, default None None 'cython' : Runs rolling apply through C-extensions from cython. 'numba'Runs rolling apply through JIT compiled code from numba.Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogiland parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply groupby aggregation. Returns: Series or DataFrameComputed max of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).max()\na    2\nb    4\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").max()\n    b  c\na\n1   8  5\n2   6  9']"
1294,..\pandas\reference\api\pandas.tseries.offsets.Day.is_quarter_end.html,pandas.tseries.offsets.Day.is_quarter_end,Day.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1295,..\pandas\reference\api\pandas.DataFrame.from_dict.html,pandas.DataFrame.from_dict,"classmethod DataFrame.from_dict(data, orient='columns', dtype=None, columns=None)[source]# Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification.","Parameters: datadictOf the form {field : array-like} or {field : dict}. orient{‘columns’, ‘index’, ‘tight’}, default ‘columns’The “orientation” of the data. If the keys of the passed dict should be the columns of the resulting DataFrame, pass ‘columns’ (default). Otherwise if the keys should be rows, pass ‘index’. If ‘tight’, assume a dict with keys [‘index’, ‘columns’, ‘data’, ‘index_names’, ‘column_names’]. Added in version 1.4.0: ‘tight’ as an allowed value for the orient argument dtypedtype, default NoneData type to force after DataFrame construction, otherwise infer. columnslist, default NoneColumn labels to use when orient='index'. Raises a ValueError if used with orient='columns' or orient='tight'. Returns: DataFrame","["">>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n>>> pd.DataFrame.from_dict(data)\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d"", "">>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n>>> pd.DataFrame.from_dict(data, orient='index')\n       0  1  2  3\nrow_1  3  2  1  0\nrow_2  a  b  c  d"", "">>> pd.DataFrame.from_dict(data, orient='index',\n...                        columns=['A', 'B', 'C', 'D'])\n       A  B  C  D\nrow_1  3  2  1  0\nrow_2  a  b  c  d"", "">>> data = {'index': [('a', 'b'), ('a', 'c')],\n...         'columns': [('x', 1), ('y', 2)],\n...         'data': [[1, 3], [2, 4]],\n...         'index_names': ['n1', 'n2'],\n...         'column_names': ['z1', 'z2']}\n>>> pd.DataFrame.from_dict(data, orient='tight')\nz1     x  y\nz2     1  2\nn1 n2\na  b   1  3\n   c   2  4""]"
1296,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_anchored.html,pandas.tseries.offsets.BQuarterBegin.is_anchored,BQuarterBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1297,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.copy.html,pandas.tseries.offsets.SemiMonthBegin.copy,SemiMonthBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1298,..\pandas\reference\api\pandas.PeriodIndex.day.html,pandas.PeriodIndex.day,property PeriodIndex.day[source]# The days of the period.,No parameters found,"["">>> idx = pd.PeriodIndex(['2020-01-31', '2020-02-28'], freq='D')\n>>> idx.day\nIndex([31, 28], dtype='int64')""]"
1299,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.mean.html,pandas.core.groupby.DataFrameGroupBy.mean,"DataFrameGroupBy.mean(numeric_only=False, engine=None, engine_kwargs=None)[source]# Compute mean of groups, excluding missing values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None and defaults to False. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}} Added in version 1.4.0. Returns: pandas.Series or pandas.DataFrame","["">>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n...                    'B': [np.nan, 2, 3, 4, 5],\n...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])"", "">>> df.groupby('A').mean()\n     B         C\nA\n1  3.0  1.333333\n2  4.0  1.500000"", "">>> df.groupby(['A', 'B']).mean()\n         C\nA B\n1 2.0  2.0\n  4.0  1.0\n2 3.0  1.0\n  5.0  2.0"", "">>> df.groupby('A')['B'].mean()\nA\n1    3.0\n2    4.0\nName: B, dtype: float64""]"
1300,..\pandas\reference\api\pandas.Series.pow.html,pandas.Series.pow,"Series.pow(other, level=None, fill_value=None, axis=0)[source]# Return Exponential power of series and other, element-wise (binary operator pow). Equivalent to series ** other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.pow(b, fill_value=0)\na    1.0\nb    1.0\nc    1.0\nd    0.0\ne    NaN\ndtype: float64""]"
1301,..\pandas\reference\api\pandas.tseries.offsets.Day.is_quarter_start.html,pandas.tseries.offsets.Day.is_quarter_start,Day.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1302,..\pandas\reference\api\pandas.ExcelWriter.html,pandas.ExcelWriter,"class pandas.ExcelWriter(path, engine=None, date_format=None, datetime_format=None, mode='w', storage_options=None, if_sheet_exists=None, engine_kwargs=None)[source]# Class for writing DataFrame objects into excel sheets. Default is to use: xlsxwriter for xlsx files if xlsxwriter is installed otherwise openpyxl odswriter for ods files See DataFrame.to_excel for typical usage. The writer should be used as a context manager. Otherwise, call close() to save and close any opened file handles. Notes For compatibility with CSV writers, ExcelWriter serializes lists and dicts to strings before writing. Examples Default usage: To write to separate sheets in a single file: You can set the date format or datetime format: You can also append to an existing Excel file: Here, the if_sheet_exists parameter can be set to replace a sheet if it already exists: You can also write multiple DataFrames to a single sheet. Note that the if_sheet_exists parameter needs to be set to overlay: You can store Excel file in RAM: You can pack Excel file into zip archive: You can specify additional arguments to the underlying engine: In append mode, engine_kwargs are passed through to openpyxl’s load_workbook: Attributes book Book instance. date_format Format string for dates written into Excel files (e.g. 'YYYY-MM-DD'). datetime_format Format string for dates written into Excel files (e.g. 'YYYY-MM-DD'). engine Name of engine. if_sheet_exists How to behave when writing to a sheet that already exists in append mode. sheets Mapping of sheet names to sheet objects. supported_extensions Extensions that writer engine supports.","Parameters: pathstr or typing.BinaryIOPath to xls or xlsx or ods file. enginestr (optional)Engine to use for writing. If None, defaults to io.excel.<extension>.writer.  NOTE: can only be passed as a keyword argument. date_formatstr, default NoneFormat string for dates written into Excel files (e.g. ‘YYYY-MM-DD’). datetime_formatstr, default NoneFormat string for datetime objects written into Excel files. (e.g. ‘YYYY-MM-DD HH:MM:SS’). mode{‘w’, ‘a’}, default ‘w’File mode to use (write or append). Append does not work with fsspec URLs. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. if_sheet_exists{‘error’, ‘new’, ‘replace’, ‘overlay’}, default ‘error’How to behave when trying to write to a sheet that already exists (append mode only). error: raise a ValueError. new: Create a new sheet, with a name determined by the engine. replace: Delete the contents of the sheet before writing to it. overlay: Write contents to the existing sheet without first removing, but possibly over top of, the existing contents. Added in version 1.3.0. Changed in version 1.4.0: Added overlay option engine_kwargsdict, optionalKeyword arguments to be passed into the engine. These will be passed to the following functions of the respective engines: xlsxwriter: xlsxwriter.Workbook(file, **engine_kwargs) openpyxl (write mode): openpyxl.Workbook(**engine_kwargs) openpyxl (append mode): openpyxl.load_workbook(file, **engine_kwargs) odswriter: odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs) Added in version 1.3.0.","['>>> df = pd.DataFrame([[""ABC"", ""XYZ""]], columns=[""Foo"", ""Bar""])  \n>>> with pd.ExcelWriter(""path_to_file.xlsx"") as writer:\n...     df.to_excel(writer)', '>>> df1 = pd.DataFrame([[""AAA"", ""BBB""]], columns=[""Spam"", ""Egg""])  \n>>> df2 = pd.DataFrame([[""ABC"", ""XYZ""]], columns=[""Foo"", ""Bar""])  \n>>> with pd.ExcelWriter(""path_to_file.xlsx"") as writer:\n...     df1.to_excel(writer, sheet_name=""Sheet1"")  \n...     df2.to_excel(writer, sheet_name=""Sheet2"")', '>>> from datetime import date, datetime  \n>>> df = pd.DataFrame(\n...     [\n...         [date(2014, 1, 31), date(1999, 9, 24)],\n...         [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],\n...     ],\n...     index=[""Date"", ""Datetime""],\n...     columns=[""X"", ""Y""],\n... )  \n>>> with pd.ExcelWriter(\n...     ""path_to_file.xlsx"",\n...     date_format=""YYYY-MM-DD"",\n...     datetime_format=""YYYY-MM-DD HH:MM:SS""\n... ) as writer:\n...     df.to_excel(writer)', '>>> with pd.ExcelWriter(""path_to_file.xlsx"", mode=""a"", engine=""openpyxl"") as writer:\n...     df.to_excel(writer, sheet_name=""Sheet3"")', '>>> with ExcelWriter(\n...     ""path_to_file.xlsx"",\n...     mode=""a"",\n...     engine=""openpyxl"",\n...     if_sheet_exists=""replace"",\n... ) as writer:\n...     df.to_excel(writer, sheet_name=""Sheet1"")', '>>> with ExcelWriter(""path_to_file.xlsx"",\n...     mode=""a"",\n...     engine=""openpyxl"",\n...     if_sheet_exists=""overlay"",\n... ) as writer:\n...     df1.to_excel(writer, sheet_name=""Sheet1"")\n...     df2.to_excel(writer, sheet_name=""Sheet1"", startcol=3)', '>>> import io\n>>> df = pd.DataFrame([[""ABC"", ""XYZ""]], columns=[""Foo"", ""Bar""])\n>>> buffer = io.BytesIO()\n>>> with pd.ExcelWriter(buffer) as writer:\n...     df.to_excel(writer)', '>>> import zipfile  \n>>> df = pd.DataFrame([[""ABC"", ""XYZ""]], columns=[""Foo"", ""Bar""])  \n>>> with zipfile.ZipFile(""path_to_file.zip"", ""w"") as zf:\n...     with zf.open(""filename.xlsx"", ""w"") as buffer:\n...         with pd.ExcelWriter(buffer) as writer:\n...             df.to_excel(writer)', '>>> with pd.ExcelWriter(\n...     ""path_to_file.xlsx"",\n...     engine=""xlsxwriter"",\n...     engine_kwargs={""options"": {""nan_inf_to_errors"": True}}\n... ) as writer:\n...     df.to_excel(writer)', '>>> with pd.ExcelWriter(\n...     ""path_to_file.xlsx"",\n...     engine=""openpyxl"",\n...     mode=""a"",\n...     engine_kwargs={""keep_vba"": True}\n... ) as writer:\n...     df.to_excel(writer, sheet_name=""Sheet2"")']"
1303,..\pandas\reference\api\pandas.DataFrame.from_records.html,pandas.DataFrame.from_records,"classmethod DataFrame.from_records(data, index=None, exclude=None, columns=None, coerce_float=False, nrows=None)[source]# Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame.","Parameters: datastructured ndarray, sequence of tuples or dicts, or DataFrameStructured input data. Deprecated since version 2.1.0: Passing a DataFrame is deprecated. indexstr, list of fields, array-likeField of array to use as the index, alternately a specific set of input labels to use. excludesequence, default NoneColumns or fields to exclude. columnssequence, default NoneColumn names to use. If the passed data do not have names associated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_floatbool, default FalseAttempt to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. nrowsint, default NoneNumber of rows to read if data is an iterator. Returns: DataFrame","["">>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n>>> pd.DataFrame.from_records(data)\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d"", "">>> data = [{'col_1': 3, 'col_2': 'a'},\n...         {'col_1': 2, 'col_2': 'b'},\n...         {'col_1': 1, 'col_2': 'c'},\n...         {'col_1': 0, 'col_2': 'd'}]\n>>> pd.DataFrame.from_records(data)\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d"", "">>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n>>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d""]"
1304,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.median.html,pandas.core.groupby.DataFrameGroupBy.median,"DataFrameGroupBy.median(numeric_only=False)[source]# Compute median of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None and defaults to False. Returns: Series or DataFrameMedian of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).median()\na    7.0\nb    3.0\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).median()\n         a    b\ndog    3.0  4.0\nmouse  7.0  3.0"", "">>> ser = pd.Series([1, 2, 3, 3, 4, 5],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').median()\n2023-01-01    2.0\n2023-02-01    4.0\nFreq: MS, dtype: float64""]"
1305,..\pandas\reference\api\pandas.Series.prod.html,pandas.Series.prod,"Series.prod(axis=None, skipna=True, numeric_only=False, min_count=0, **kwargs)[source]# Return the product of the values over the requested axis.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","['>>> pd.Series([], dtype=""float64"").prod()\n1.0', '>>> pd.Series([], dtype=""float64"").prod(min_count=1)\nnan', '>>> pd.Series([np.nan]).prod()\n1.0', '>>> pd.Series([np.nan]).prod(min_count=1)\nnan']"
1306,..\pandas\reference\api\pandas.factorize.html,pandas.factorize,"pandas.factorize(values, sort=False, use_na_sentinel=True, size_hint=None)[source]# Encode the object as an enumerated type or categorical variable. This method is useful for obtaining a numeric representation of an array when all that matters is identifying distinct values. factorize is available as both a top-level function pandas.factorize(), and as a method Series.factorize() and Index.factorize(). Notes Reference the user guide for more examples.","Parameters: valuessequenceA 1-D sequence. Sequences that aren’t pandas objects are coerced to ndarrays before factorization. sortbool, default FalseSort uniques and shuffle codes to maintain the relationship. use_na_sentinelbool, default TrueIf True, the sentinel -1 will be used for NaN values. If False, NaN values will be encoded as non-negative integers and will not drop the NaN from the uniques of the values. Added in version 1.5.0. size_hintint, optionalHint to the hashtable sizer. Returns: codesndarrayAn integer ndarray that’s an indexer into uniques. uniques.take(codes) will have the same values as values. uniquesndarray, Index, or CategoricalThe unique valid values. When values is Categorical, uniques is a Categorical. When values is some other pandas object, an Index is returned. Otherwise, a 1-D ndarray is returned. Note Even if there’s a missing value in values, uniques will not contain an entry for it.","['>>> codes, uniques = pd.factorize(np.array([\'b\', \'b\', \'a\', \'c\', \'b\'], dtype=""O""))\n>>> codes\narray([0, 0, 1, 2, 0])\n>>> uniques\narray([\'b\', \'a\', \'c\'], dtype=object)', '>>> codes, uniques = pd.factorize(np.array([\'b\', \'b\', \'a\', \'c\', \'b\'], dtype=""O""),\n...                               sort=True)\n>>> codes\narray([1, 1, 0, 2, 1])\n>>> uniques\narray([\'a\', \'b\', \'c\'], dtype=object)', '>>> codes, uniques = pd.factorize(np.array([\'b\', None, \'a\', \'c\', \'b\'], dtype=""O""))\n>>> codes\narray([ 0, -1,  1,  2,  0])\n>>> uniques\narray([\'b\', \'a\', \'c\'], dtype=object)', "">>> cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])\n>>> codes, uniques = pd.factorize(cat)\n>>> codes\narray([0, 0, 1])\n>>> uniques\n['a', 'c']\nCategories (3, object): ['a', 'b', 'c']"", "">>> cat = pd.Series(['a', 'a', 'c'])\n>>> codes, uniques = pd.factorize(cat)\n>>> codes\narray([0, 0, 1])\n>>> uniques\nIndex(['a', 'c'], dtype='object')"", '>>> values = np.array([1, 2, 1, np.nan])\n>>> codes, uniques = pd.factorize(values)  # default: use_na_sentinel=True\n>>> codes\narray([ 0,  1,  0, -1])\n>>> uniques\narray([1., 2.])', '>>> codes, uniques = pd.factorize(values, use_na_sentinel=False)\n>>> codes\narray([0, 1, 0, 2])\n>>> uniques\narray([ 1.,  2., nan])']"
1307,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_month_end.html,pandas.tseries.offsets.BQuarterBegin.is_month_end,BQuarterBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1308,..\pandas\reference\api\pandas.PeriodIndex.dayofweek.html,pandas.PeriodIndex.dayofweek,"property PeriodIndex.dayofweek[source]# The day of the week with Monday=0, Sunday=6.",No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-01"", ""2023-01-02"", ""2023-01-03""], freq=""D"")\n>>> idx.weekday\nIndex([6, 0, 1], dtype=\'int64\')']"
1309,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.day_of_month.html,pandas.tseries.offsets.SemiMonthBegin.day_of_month,SemiMonthBegin.day_of_month#,No parameters found,[]
1310,..\pandas\reference\api\pandas.tseries.offsets.Day.is_year_end.html,pandas.tseries.offsets.Day.is_year_end,Day.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1311,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.min.html,pandas.core.groupby.DataFrameGroupBy.min,"DataFrameGroupBy.min(numeric_only=False, min_count=-1, engine=None, engine_kwargs=None)[source]# Compute min of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. enginestr, default None None 'cython' : Runs rolling apply through C-extensions from cython. 'numba'Runs rolling apply through JIT compiled code from numba.Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogiland parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply groupby aggregation. Returns: Series or DataFrameComputed min of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).min()\na    1\nb    3\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").min()\n    b  c\na\n1   2  2\n2   5  8']"
1312,..\pandas\reference\api\pandas.DataFrame.ge.html,pandas.DataFrame.ge,"DataFrame.ge(other, axis='columns', level=None)[source]# Get Greater than or equal to of dataframe and other, element-wise (binary operator ge). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).","Parameters: otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}, default ‘columns’Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. Returns: DataFrame of boolResult of the comparison.","["">>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300"", '>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df != pd.Series([100, 250], index=[""cost"", ""revenue""])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True', '>>> df.ne(pd.Series([100, 300], index=[""A"", ""D""]), axis=\'index\')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True', '>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False', "">>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False"", "">>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150"", '>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False', "">>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225"", '>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False']"
1313,..\pandas\reference\api\pandas.Series.product.html,pandas.Series.product,"Series.product(axis=None, skipna=True, numeric_only=False, min_count=0, **kwargs)[source]# Return the product of the values over the requested axis.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","['>>> pd.Series([], dtype=""float64"").prod()\n1.0', '>>> pd.Series([], dtype=""float64"").prod(min_count=1)\nnan', '>>> pd.Series([np.nan]).prod()\n1.0', '>>> pd.Series([np.nan]).prod(min_count=1)\nnan']"
1314,..\pandas\reference\api\pandas.Flags.html,pandas.Flags,"class pandas.Flags(obj, *, allows_duplicate_labels)[source]# Flags that apply to pandas objects. Examples Attributes can be set in two ways: Attributes allows_duplicate_labels Whether this object allows duplicate labels.","Parameters: objSeries or DataFrameThe object these flags are associated with. allows_duplicate_labelsbool, default TrueWhether to allow duplicate labels in this object. By default, duplicate labels are permitted. Setting this to False will cause an errors.DuplicateLabelError to be raised when index (or columns for DataFrame) is not unique, or any subsequent operation on introduces duplicates. See Disallowing Duplicate Labels for more. Warning This is an experimental feature. Currently, many methods fail to propagate the allows_duplicate_labels value. In future versions it is expected that every method taking or returning one or more DataFrame or Series objects will propagate allows_duplicate_labels.","['>>> df = pd.DataFrame()\n>>> df.flags\n<Flags(allows_duplicate_labels=True)>\n>>> df.flags.allows_duplicate_labels = False\n>>> df.flags\n<Flags(allows_duplicate_labels=False)>', "">>> df.flags['allows_duplicate_labels'] = True\n>>> df.flags\n<Flags(allows_duplicate_labels=True)>""]"
1315,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.ngroup.html,pandas.core.groupby.DataFrameGroupBy.ngroup,"DataFrameGroupBy.ngroup(ascending=True)[source]# Number each group from 0 to the number of groups - 1. This is the enumerative complement of cumcount.  Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed. Groups with missing keys (where pd.isna() is True) will be labeled with NaN and will be skipped from the count.","Parameters: ascendingbool, default TrueIf False, number in reverse, from number of group - 1 to 0. Returns: SeriesUnique numbers for each group.","['>>> df = pd.DataFrame({""color"": [""red"", None, ""red"", ""blue"", ""blue"", ""red""]})\n>>> df\n   color\n0    red\n1   None\n2    red\n3   blue\n4   blue\n5    red\n>>> df.groupby(""color"").ngroup()\n0    1.0\n1    NaN\n2    1.0\n3    0.0\n4    0.0\n5    1.0\ndtype: float64\n>>> df.groupby(""color"", dropna=False).ngroup()\n0    1\n1    2\n2    1\n3    0\n4    0\n5    1\ndtype: int64\n>>> df.groupby(""color"", dropna=False).ngroup(ascending=False)\n0    1\n1    0\n2    1\n3    2\n4    2\n5    1\ndtype: int64']"
1316,..\pandas\reference\api\pandas.DataFrame.get.html,pandas.DataFrame.get,"DataFrame.get(key, default=None)[source]# Get item from object for given key (ex: DataFrame column). Returns default value if not found.",Parameters: keyobject Returns: same type as items contained in object,"['>>> df = pd.DataFrame(\n...     [\n...         [24.3, 75.7, ""high""],\n...         [31, 87.8, ""high""],\n...         [22, 71.6, ""medium""],\n...         [35, 95, ""medium""],\n...     ],\n...     columns=[""temp_celsius"", ""temp_fahrenheit"", ""windspeed""],\n...     index=pd.date_range(start=""2014-02-12"", end=""2014-02-15"", freq=""D""),\n... )', '>>> df\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium', '>>> df.get([""temp_celsius"", ""windspeed""])\n            temp_celsius windspeed\n2014-02-12          24.3      high\n2014-02-13          31.0      high\n2014-02-14          22.0    medium\n2014-02-15          35.0    medium', "">>> ser = df['windspeed']\n>>> ser.get('2014-02-13')\n'high'"", '>>> df.get([""temp_celsius"", ""temp_kelvin""], default=""default_value"")\n\'default_value\'', "">>> ser.get('2014-02-10', '[unknown]')\n'[unknown]'""]"
1317,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_month_start.html,pandas.tseries.offsets.BQuarterBegin.is_month_start,BQuarterBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1318,..\pandas\reference\api\pandas.PeriodIndex.dayofyear.html,pandas.PeriodIndex.dayofyear,property PeriodIndex.dayofyear[source]# The ordinal day of the year.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-10"", ""2023-02-01"", ""2023-03-01""], freq=""D"")\n>>> idx.dayofyear\nIndex([10, 32, 60], dtype=\'int64\')', '>>> idx = pd.PeriodIndex([""2023"", ""2024"", ""2025""], freq=""Y"")\n>>> idx\nPeriodIndex([\'2023\', \'2024\', \'2025\'], dtype=\'period[Y-DEC]\')\n>>> idx.dayofyear\nIndex([365, 366, 365], dtype=\'int64\')']"
1319,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.freqstr.html,pandas.tseries.offsets.SemiMonthBegin.freqstr,SemiMonthBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1320,..\pandas\reference\api\pandas.Series.quantile.html,pandas.Series.quantile,"Series.quantile(q=0.5, interpolation='linear')[source]# Return value at the given quantile.","Parameters: qfloat or array-like, default 0.5 (50% quantile)The quantile(s) to compute, which can lie in range: 0 <= q <= 1. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j: linear: i + (j - i) * (x-i)/(j-i), where (x-i)/(j-i) is the fractional part of the index surrounded by i > j. lower: i. higher: j. nearest: i or j whichever is nearest. midpoint: (i + j) / 2. Returns: float or SeriesIf q is an array, a Series will be returned where the index is q and the values are the quantiles, otherwise a float will be returned.","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s.quantile(.5)\n2.5\n>>> s.quantile([.25, .5, .75])\n0.25    1.75\n0.50    2.50\n0.75    3.25\ndtype: float64']"
1321,..\pandas\reference\api\pandas.Float32Dtype.html,pandas.Float32Dtype,class pandas.Float32Dtype[source]# An ExtensionDtype for float32 data. This dtype uses pd.NA as missing value indicator. Attributes None Methods None,No parameters found,"['>>> ser = pd.Series([2.25, pd.NA], dtype=pd.Float32Dtype())\n>>> ser.dtype\nFloat32Dtype()', '>>> ser = pd.Series([2.25, pd.NA], dtype=pd.Float64Dtype())\n>>> ser.dtype\nFloat64Dtype()']"
1322,..\pandas\reference\api\pandas.tseries.offsets.Day.is_year_start.html,pandas.tseries.offsets.Day.is_year_start,Day.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1323,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.nth.html,pandas.core.groupby.DataFrameGroupBy.nth,"property DataFrameGroupBy.nth[source]# Take the nth row from each group if n is an int, otherwise a subset of rows. Can be either a call or an index. dropna is not available with index notation. Index notation accepts a comma separated list of integers and slices. If dropna, will take the nth non-null row, dropna is either ‘all’ or ‘any’; this is equivalent to calling dropna(how=dropna) before the groupby.","Parameters: nint, slice or list of ints and slicesA single nth value for the row or a list of nth values or slices. Changed in version 1.4.0: Added slice and lists containing slices. Added index notation. dropna{‘any’, ‘all’, None}, default NoneApply the specified dropna operation before counting which row is the nth row. Only supported if n is an int. Returns: Series or DataFrameN-th value within each group.","["">>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n...                    'B': [np.nan, 2, 3, 4, 5]}, columns=['A', 'B'])\n>>> g = df.groupby('A')\n>>> g.nth(0)\n   A   B\n0  1 NaN\n2  2 3.0\n>>> g.nth(1)\n   A   B\n1  1 2.0\n4  2 5.0\n>>> g.nth(-1)\n   A   B\n3  1 4.0\n4  2 5.0\n>>> g.nth([0, 1])\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0\n4  2 5.0\n>>> g.nth(slice(None, -1))\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0"", '>>> g.nth[0, 1]\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0\n4  2 5.0\n>>> g.nth[:-1]\n   A   B\n0  1 NaN\n1  1 2.0\n2  2 3.0', "">>> g.nth(0, dropna='any')\n   A   B\n1  1 2.0\n2  2 3.0"", "">>> g.nth(3, dropna='any')\nEmpty DataFrame\nColumns: [A, B]\nIndex: []""]"
1324,..\pandas\reference\api\pandas.DataFrame.groupby.html,pandas.DataFrame.groupby,"DataFrame.groupby(by=None, axis=<no_default>, level=None, as_index=True, sort=True, group_keys=True, observed=<no_default>, dropna=True)[source]# Group DataFrame using a mapper or by a Series of columns. A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. Notes See the user guide for more detailed usage and examples, including splitting an object into groups, iterating through groups, selecting a group, aggregation, and more.","Parameters: bymapping, function, label, pd.Grouper or list of suchUsed to determine the groups for the groupby. If by is a function, it’s called on each value of the object’s index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series’ values are first aligned; see .align() method). If a list or ndarray of length equal to the selected axis is passed (see the groupby user guide), the values are used as-is to determine the groups. A label or list of labels may be passed to group by the columns in self. Notice that a tuple is interpreted as a (single) key. axis{0 or ‘index’, 1 or ‘columns’}, default 0Split along rows (0) or columns (1). For Series this parameter is unused and defaults to 0. Deprecated since version 2.1.0: Will be removed and behave like axis=0 in a future version. For axis=1, do frame.T.groupby(...) instead. levelint, level name, or sequence of such, default NoneIf the axis is a MultiIndex (hierarchical), group by a particular level or levels. Do not specify both by and level. as_indexbool, default TrueReturn object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively “SQL-style” grouped output. This argument has no effect on filtrations (see the filtrations in the user guide), such as head(), tail(), nth() and in transformations (see the transformations in the user guide). sortbool, default TrueSort group keys. Get better performance by turning this off. Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group. If False, the groups will appear in the same order as they did in the original DataFrame. This argument has no effect on filtrations (see the filtrations in the user guide), such as head(), tail(), nth() and in transformations (see the transformations in the user guide). Changed in version 2.0.0: Specifying sort=False with an ordered categorical grouper will no longer sort the values. group_keysbool, default TrueWhen calling apply and the by argument produces a like-indexed (i.e. a transform) result, add group keys to index to identify pieces. By default group keys are not included when the result’s index (and column) labels match the inputs, and are included otherwise. Changed in version 1.5.0: Warns that group_keys will no longer be ignored when the result from apply is a like-indexed Series or DataFrame. Specify group_keys explicitly to include the group keys or not. Changed in version 2.0.0: group_keys now defaults to True. observedbool, default FalseThis only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. Deprecated since version 2.1.0: The default value will change to True in a future version of pandas. dropnabool, default TrueIf True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups. Returns: pandas.api.typing.DataFrameGroupByReturns a groupby object that contains information about the groups.","["">>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> df\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> df.groupby(['Animal']).mean()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0"", '>>> arrays = [[\'Falcon\', \'Falcon\', \'Parrot\', \'Parrot\'],\n...           [\'Captive\', \'Wild\', \'Captive\', \'Wild\']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=(\'Animal\', \'Type\'))\n>>> df = pd.DataFrame({\'Max Speed\': [390., 350., 30., 20.]},\n...                   index=index)\n>>> df\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> df.groupby(level=0).mean()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> df.groupby(level=""Type"").mean()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0', '>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> df = pd.DataFrame(l, columns=[""a"", ""b"", ""c""])', '>>> df.groupby(by=[""b""]).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5', '>>> df.groupby(by=[""b""], dropna=False).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4', '>>> l = [[""a"", 12, 12], [None, 12.3, 33.], [""b"", 12.3, 123], [""a"", 1, 1]]\n>>> df = pd.DataFrame(l, columns=[""a"", ""b"", ""c""])', '>>> df.groupby(by=""a"").sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0', '>>> df.groupby(by=""a"", dropna=False).sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0', '>>> df = pd.DataFrame({\'Animal\': [\'Falcon\', \'Falcon\',\n...                               \'Parrot\', \'Parrot\'],\n...                    \'Max Speed\': [380., 370., 24., 26.]})\n>>> df.groupby(""Animal"", group_keys=True)[[\'Max Speed\']].apply(lambda x: x)\n          Max Speed\nAnimal\nFalcon 0      380.0\n       1      370.0\nParrot 2       24.0\n       3       26.0', '>>> df.groupby(""Animal"", group_keys=False)[[\'Max Speed\']].apply(lambda x: x)\n   Max Speed\n0      380.0\n1      370.0\n2       24.0\n3       26.0']"
1325,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_on_offset.html,pandas.tseries.offsets.BQuarterBegin.is_on_offset,BQuarterBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1326,..\pandas\reference\api\pandas.PeriodIndex.daysinmonth.html,pandas.PeriodIndex.daysinmonth,property PeriodIndex.daysinmonth[source]# The number of days in the month.,No parameters found,"["">>> period = pd.period_range('2020-1-1 00:00', '2020-3-1 00:00', freq='M')\n>>> s = pd.Series(period)\n>>> s\n0   2020-01\n1   2020-02\n2   2020-03\ndtype: period[M]\n>>> s.dt.days_in_month\n0    31\n1    29\n2    31\ndtype: int64"", '>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.days_in_month   # It can be also entered as `daysinmonth`\nIndex([31, 28, 31], dtype=\'int64\')']"
1327,..\pandas\reference\api\pandas.tseries.offsets.Day.kwds.html,pandas.tseries.offsets.Day.kwds,Day.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1328,..\pandas\reference\api\pandas.Series.radd.html,pandas.Series.radd,"Series.radd(other, level=None, fill_value=None, axis=0)[source]# Return Addition of series and other, element-wise (binary operator radd). Equivalent to other + series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.add(b, fill_value=0)\na    2.0\nb    1.0\nc    1.0\nd    1.0\ne    NaN\ndtype: float64""]"
1329,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.nunique.html,pandas.core.groupby.DataFrameGroupBy.nunique,DataFrameGroupBy.nunique(dropna=True)[source]# Return DataFrame with counts of unique elements in each position.,"Parameters: dropnabool, default TrueDon’t include NaN in the counts. Returns: nunique: DataFrame","["">>> df = pd.DataFrame({'id': ['spam', 'egg', 'egg', 'spam',\n...                           'ham', 'ham'],\n...                    'value1': [1, 5, 5, 2, 5, 5],\n...                    'value2': list('abbaxy')})\n>>> df\n     id  value1 value2\n0  spam       1      a\n1   egg       5      b\n2   egg       5      b\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y"", "">>> df.groupby('id').nunique()\n      value1  value2\nid\negg        1       1\nham        1       2\nspam       2       1"", "">>> df.groupby('id').filter(lambda g: (g.nunique() > 1).any())\n     id  value1 value2\n0  spam       1      a\n3  spam       2      a\n4   ham       5      x\n5   ham       5      y""]"
1330,..\pandas\reference\api\pandas.Float64Dtype.html,pandas.Float64Dtype,class pandas.Float64Dtype[source]# An ExtensionDtype for float64 data. This dtype uses pd.NA as missing value indicator. Attributes None Methods None,No parameters found,"['>>> ser = pd.Series([2.25, pd.NA], dtype=pd.Float32Dtype())\n>>> ser.dtype\nFloat32Dtype()', '>>> ser = pd.Series([2.25, pd.NA], dtype=pd.Float64Dtype())\n>>> ser.dtype\nFloat64Dtype()']"
1331,..\pandas\reference\api\pandas.DataFrame.gt.html,pandas.DataFrame.gt,"DataFrame.gt(other, axis='columns', level=None)[source]# Get Greater than of dataframe and other, element-wise (binary operator gt). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).","Parameters: otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}, default ‘columns’Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. Returns: DataFrame of boolResult of the comparison.","["">>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300"", '>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df != pd.Series([100, 250], index=[""cost"", ""revenue""])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True', '>>> df.ne(pd.Series([100, 300], index=[""A"", ""D""]), axis=\'index\')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True', '>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False', "">>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False"", "">>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150"", '>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False', "">>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225"", '>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False']"
1332,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.html,pandas.tseries.offsets.SemiMonthBegin,class pandas.tseries.offsets.SemiMonthBegin# Two DateOffset’s per month repeating on the first day of the month & day_of_month. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. day_of_month freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. day_of_monthint, {1, 3,…,27}, default 15A specific integer for the day of the month.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.SemiMonthBegin()\nTimestamp('2022-01-15 00:00:00')""]"
1333,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_quarter_end.html,pandas.tseries.offsets.BQuarterBegin.is_quarter_end,BQuarterBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1334,..\pandas\reference\api\pandas.PeriodIndex.days_in_month.html,pandas.PeriodIndex.days_in_month,property PeriodIndex.days_in_month[source]# The number of days in the month.,No parameters found,"["">>> period = pd.period_range('2020-1-1 00:00', '2020-3-1 00:00', freq='M')\n>>> s = pd.Series(period)\n>>> s\n0   2020-01\n1   2020-02\n2   2020-03\ndtype: period[M]\n>>> s.dt.days_in_month\n0    31\n1    29\n2    31\ndtype: int64"", '>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.days_in_month   # It can be also entered as `daysinmonth`\nIndex([31, 28, 31], dtype=\'int64\')']"
1335,..\pandas\reference\api\pandas.Series.rank.html,pandas.Series.rank,"Series.rank(axis=0, method='average', numeric_only=False, na_option='keep', ascending=True, pct=False)[source]# Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0Index to direct ranking. For Series this parameter is unused and defaults to 0. method{‘average’, ‘min’, ‘max’, ‘first’, ‘dense’}, default ‘average’How to rank the group of records that have the same value (i.e. ties): average: average rank of the group min: lowest rank in the group max: highest rank in the group first: ranks assigned in order they appear in the array dense: like ‘min’, but rank always increases by 1 between groups. numeric_onlybool, default FalseFor DataFrame objects, rank only numeric columns if set to True. Changed in version 2.0.0: The default value of numeric_only is now False. na_option{‘keep’, ‘top’, ‘bottom’}, default ‘keep’How to rank NaN values: keep: assign NaN rank to NaN values top: assign lowest rank to NaN values bottom: assign highest rank to NaN values ascendingbool, default TrueWhether or not the elements should be ranked in ascending order. pctbool, default FalseWhether or not to display the returned rankings in percentile form. Returns: same type as callerReturn a Series or DataFrame with data ranks as values.","["">>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n...                                    'spider', 'snake'],\n...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n>>> df\n    Animal  Number_legs\n0      cat          4.0\n1  penguin          2.0\n2      dog          4.0\n3   spider          8.0\n4    snake          NaN"", '>>> s = pd.Series(range(5), index=list(""abcde""))\n>>> s[""d""] = s[""b""]\n>>> s.rank()\na    1.0\nb    2.5\nc    4.0\nd    2.5\ne    5.0\ndtype: float64', "">>> df['default_rank'] = df['Number_legs'].rank()\n>>> df['max_rank'] = df['Number_legs'].rank(method='max')\n>>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n>>> df['pct_rank'] = df['Number_legs'].rank(pct=True)\n>>> df\n    Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n0      cat          4.0           2.5       3.0        2.5     0.625\n1  penguin          2.0           1.0       1.0        1.0     0.250\n2      dog          4.0           2.5       3.0        2.5     0.625\n3   spider          8.0           4.0       4.0        4.0     1.000\n4    snake          NaN           NaN       NaN        5.0       NaN""]"
1336,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_quarter_start.html,pandas.tseries.offsets.BQuarterBegin.is_quarter_start,BQuarterBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1337,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.ohlc.html,pandas.core.groupby.DataFrameGroupBy.ohlc,"DataFrameGroupBy.ohlc()[source]# Compute open, high, low and close values of a group, excluding missing values. For multiple groupings, the result index will be a MultiIndex","Returns: DataFrameOpen, high, low and close values within each group.","["">>> lst = ['SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC', 'SPX', 'CAC',]\n>>> ser = pd.Series([3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 0.1, 0.5], index=lst)\n>>> ser\nSPX     3.4\nCAC     9.0\nSPX     7.2\nCAC     5.2\nSPX     8.8\nCAC     9.4\nSPX     0.1\nCAC     0.5\ndtype: float64\n>>> ser.groupby(level=0).ohlc()\n     open  high  low  close\nCAC   9.0   9.4  0.5    0.5\nSPX   3.4   8.8  0.1    0.1"", "">>> data = {2022: [1.2, 2.3, 8.9, 4.5, 4.4, 3, 2 , 1],\n...         2023: [3.4, 9.0, 7.2, 5.2, 8.8, 9.4, 8.2, 1.0]}\n>>> df = pd.DataFrame(data, index=['SPX', 'CAC', 'SPX', 'CAC',\n...                   'SPX', 'CAC', 'SPX', 'CAC'])\n>>> df\n     2022  2023\nSPX   1.2   3.4\nCAC   2.3   9.0\nSPX   8.9   7.2\nCAC   4.5   5.2\nSPX   4.4   8.8\nCAC   3.0   9.4\nSPX   2.0   8.2\nCAC   1.0   1.0\n>>> df.groupby(level=0).ohlc()\n    2022                 2023\n    open high  low close open high  low close\nCAC  2.3  4.5  1.0   1.0  9.0  9.4  1.0   1.0\nSPX  1.2  8.9  1.2   2.0  3.4  8.8  3.4   8.2"", "">>> ser = pd.Series([1, 3, 2, 4, 3, 5],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').ohlc()\n            open  high  low  close\n2023-01-01     1     3    1      2\n2023-02-01     4     5    3      5""]"
1338,..\pandas\reference\api\pandas.from_dummies.html,pandas.from_dummies,"pandas.from_dummies(data, sep=None, default_category=None)[source]# Create a categorical DataFrame from a DataFrame of dummy variables. Inverts the operation performed by get_dummies(). Added in version 1.5.0. Notes The columns of the passed dummy data should only include 1’s and 0’s, or boolean values.","Parameters: dataDataFrameData which contains dummy-coded variables in form of integer columns of 1’s and 0’s. sepstr, default NoneSeparator used in the column names of the dummy categories they are character indicating the separation of the categorical names from the prefixes. For example, if your column names are ‘prefix_A’ and ‘prefix_B’, you can strip the underscore by specifying sep=’_’. default_categoryNone, Hashable or dict of Hashables, default NoneThe default category is the implied category when a value has none of the listed categories specified with a one, i.e. if all dummies in a row are zero. Can be a single value for all variables or a dict directly mapping the default categories to a prefix of a variable. Returns: DataFrameCategorical data decoded from the dummy input-data. Raises: ValueError When the input DataFrame data contains NA values. When the input DataFrame data contains column names with separators that do not match the separator specified with sep. When a dict passed to default_category does not include an implied category for each prefix. When a value in data has more than one category assigned to it. When default_category=None and a value in data has no category assigned to it. TypeError When the input data is not of type DataFrame. When the input DataFrame data contains non-dummy data. When the passed sep is of a wrong data type. When the passed default_category is of a wrong data type.","['>>> df = pd.DataFrame({""a"": [1, 0, 0, 1], ""b"": [0, 1, 0, 0],\n...                    ""c"": [0, 0, 1, 0]})', '>>> df\n   a  b  c\n0  1  0  0\n1  0  1  0\n2  0  0  1\n3  1  0  0', '>>> pd.from_dummies(df)\n0     a\n1     b\n2     c\n3     a', '>>> df = pd.DataFrame({""col1_a"": [1, 0, 1], ""col1_b"": [0, 1, 0],\n...                    ""col2_a"": [0, 1, 0], ""col2_b"": [1, 0, 0],\n...                    ""col2_c"": [0, 0, 1]})', '>>> df\n      col1_a  col1_b  col2_a  col2_b  col2_c\n0       1       0       0       1       0\n1       0       1       1       0       0\n2       1       0       0       0       1', '>>> pd.from_dummies(df, sep=""_"")\n    col1    col2\n0    a       b\n1    b       a\n2    a       c', '>>> df = pd.DataFrame({""col1_a"": [1, 0, 0], ""col1_b"": [0, 1, 0],\n...                    ""col2_a"": [0, 1, 0], ""col2_b"": [1, 0, 0],\n...                    ""col2_c"": [0, 0, 0]})', '>>> df\n      col1_a  col1_b  col2_a  col2_b  col2_c\n0       1       0       0       1       0\n1       0       1       1       0       0\n2       0       0       0       0       0', '>>> pd.from_dummies(df, sep=""_"", default_category={""col1"": ""d"", ""col2"": ""e""})\n    col1    col2\n0    a       b\n1    b       a\n2    d       e']"
1339,..\pandas\reference\api\pandas.DataFrame.head.html,pandas.DataFrame.head,"DataFrame.head(n=5)[source]# Return the first n rows. This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of n, this function returns all rows except the last |n| rows, equivalent to df[:n]. If n is larger than the number of rows, this function returns all rows.","Parameters: nint, default 5Number of rows to select. Returns: same type as callerThe first n rows of the caller object.","["">>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n>>> df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra"", '>>> df.head()\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey', '>>> df.head(3)\n      animal\n0  alligator\n1        bee\n2     falcon', '>>> df.head(-3)\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot']"
1340,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_anchored.html,pandas.tseries.offsets.SemiMonthBegin.is_anchored,SemiMonthBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1341,..\pandas\reference\api\pandas.tseries.offsets.Day.n.html,pandas.tseries.offsets.Day.n,Day.n#,No parameters found,[]
1342,..\pandas\reference\api\pandas.PeriodIndex.day_of_week.html,pandas.PeriodIndex.day_of_week,"property PeriodIndex.day_of_week[source]# The day of the week with Monday=0, Sunday=6.",No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-01"", ""2023-01-02"", ""2023-01-03""], freq=""D"")\n>>> idx.weekday\nIndex([6, 0, 1], dtype=\'int64\')']"
1343,..\pandas\reference\api\pandas.Series.ravel.html,pandas.Series.ravel,"Series.ravel(order='C')[source]# Return the flattened underlying data as an ndarray or ExtensionArray. Deprecated since version 2.2.0: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use to_numpy() for conversion to a numpy array instead.",Returns: numpy.ndarray or ExtensionArrayFlattened data of the Series.,"['>>> s = pd.Series([1, 2, 3])\n>>> s.ravel()  \narray([1, 2, 3])']"
1344,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.pct_change.html,pandas.core.groupby.DataFrameGroupBy.pct_change,"DataFrameGroupBy.pct_change(periods=1, fill_method=<no_default>, limit=<no_default>, freq=None, axis=<no_default>)[source]# Calculate pct_change of each value to previous entry in group.",Returns: Series or DataFramePercentage changes within each group.,"["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).pct_change()\na         NaN\na    1.000000\nb         NaN\nb    0.333333\ndtype: float64"", '>>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a  b  c\n    tuna   1  2  3\n  salmon   1  5  6\n catfish   2  5  8\ngoldfish   2  6  9\n>>> df.groupby(""a"").pct_change()\n            b  c\n    tuna    NaN    NaN\n  salmon    1.5  1.000\n catfish    NaN    NaN\ngoldfish    0.2  0.125']"
1345,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_year_end.html,pandas.tseries.offsets.BQuarterBegin.is_year_end,BQuarterBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1346,..\pandas\reference\api\pandas.get_dummies.html,pandas.get_dummies,"pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)[source]# Convert categorical variable into dummy/indicator variables. Each variable is converted in as many 0/1 variables as there are different values. Columns in the output are each named after a value; if the input is a DataFrame, the name of the original variable is prepended to the value. Notes Reference the user guide for more examples.","Parameters: dataarray-like, Series, or DataFrameData of which to get dummy indicators. prefixstr, list of str, or dict of str, default NoneString to append DataFrame column names. Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternatively, prefix can be a dictionary mapping column names to prefixes. prefix_sepstr, default ‘_’If appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix. dummy_nabool, default FalseAdd a column to indicate NaNs, if False NaNs are ignored. columnslist-like, default NoneColumn names in the DataFrame to be encoded. If columns is None then all the columns with object, string, or category dtype will be converted. sparsebool, default FalseWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False). drop_firstbool, default FalseWhether to get k-1 dummies out of k categorical levels by removing the first level. dtypedtype, default boolData type for new columns. Only a single dtype is allowed. Returns: DataFrameDummy-coded data. If data contains other columns than the dummy-coded one(s), these will be prepended, unaltered, to the result.","["">>> s = pd.Series(list('abca'))"", '>>> pd.get_dummies(s)\n       a      b      c\n0   True  False  False\n1  False   True  False\n2  False  False   True\n3   True  False  False', "">>> s1 = ['a', 'b', np.nan]"", '>>> pd.get_dummies(s1)\n       a      b\n0   True  False\n1  False   True\n2  False  False', '>>> pd.get_dummies(s1, dummy_na=True)\n       a      b    NaN\n0   True  False  False\n1  False   True  False\n2  False  False   True', "">>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n...                    'C': [1, 2, 3]})"", "">>> pd.get_dummies(df, prefix=['col1', 'col2'])\n   C  col1_a  col1_b  col2_a  col2_b  col2_c\n0  1    True   False   False    True   False\n1  2   False    True    True   False   False\n2  3    True   False   False   False    True"", "">>> pd.get_dummies(pd.Series(list('abcaa')))\n       a      b      c\n0   True  False  False\n1  False   True  False\n2  False  False   True\n3   True  False  False\n4   True  False  False"", "">>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)\n       b      c\n0  False  False\n1   True  False\n2  False   True\n3  False  False\n4  False  False"", "">>> pd.get_dummies(pd.Series(list('abc')), dtype=float)\n     a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0""]"
1347,..\pandas\reference\api\pandas.DataFrame.hist.html,pandas.DataFrame.hist,"DataFrame.hist(column=None, by=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False, sharey=False, figsize=None, layout=None, bins=10, backend=None, legend=False, **kwargs)[source]# Make a histogram of the DataFrame’s columns. A histogram is a representation of the distribution of data. This function calls matplotlib.pyplot.hist(), on each series in the DataFrame, resulting in one histogram per column.","Parameters: dataDataFrameThe pandas object holding the data. columnstr or sequence, optionalIf passed, will be used to limit data to a subset of columns. byobject, optionalIf passed, then used to form histograms for separate groups. gridbool, default TrueWhether to show axis grid lines. xlabelsizeint, default NoneIf specified changes the x-axis label size. xrotfloat, default NoneRotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise. ylabelsizeint, default NoneIf specified changes the y-axis label size. yrotfloat, default NoneRotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise. axMatplotlib axes object, default NoneThe axes to plot the histogram on. sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure. shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to invisible. figsizetuple, optionalThe size in inches of the figure to create. Uses the value in matplotlib.rcParams by default. layouttuple, optionalTuple of (rows, columns) for the layout of the histograms. binsint or sequence, default 10Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. legendbool, default FalseWhether to show the legend. **kwargsAll other plotting keyword arguments to be passed to matplotlib.pyplot.hist(). Returns: matplotlib.AxesSubplot or numpy.ndarray of them","["">>> data = {'length': [1.5, 0.5, 1.2, 0.9, 3],\n...         'width': [0.7, 0.2, 0.15, 0.2, 1.1]}\n>>> index = ['pig', 'rabbit', 'duck', 'chicken', 'horse']\n>>> df = pd.DataFrame(data, index=index)\n>>> hist = df.hist(bins=3)""]"
1348,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_month_end.html,pandas.tseries.offsets.SemiMonthBegin.is_month_end,SemiMonthBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1349,..\pandas\reference\api\pandas.get_option.html,pandas.get_option,"pandas.get_option(pat) = <pandas._config.config.CallableDynamicDoc object># Retrieves the value of the specified option. Available options: compute.[use_bottleneck, use_numba, use_numexpr] display.[chop_threshold, colheader_justify, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format] display.html.[border, table_schema, use_mathjax] display.[large_repr, max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions] display.unicode.[ambiguous_as_wide, east_asian_width] display.[width] future.[infer_string, no_silent_downcasting] io.excel.ods.[reader, writer] io.excel.xls.[reader] io.excel.xlsb.[reader] io.excel.xlsm.[reader, writer] io.excel.xlsx.[reader, writer] io.hdf.[default_format, dropna_table] io.parquet.[engine] io.sql.[engine] mode.[chained_assignment, copy_on_write, data_manager, sim_interactive, string_storage, use_inf_as_na] plotting.[backend] plotting.matplotlib.[register_converters] styler.format.[decimal, escape, formatter, na_rep, precision, thousands] styler.html.[mathjax] styler.latex.[environment, hrules, multicol_align, multirow_align] styler.render.[encoding, max_columns, max_elements, max_rows, repr] styler.sparse.[columns, index] Notes Please reference the User Guide for more information. The available options with its descriptions: compute.use_bottleneckboolUse the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True] compute.use_numbaboolUse the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False] compute.use_numexprboolUse the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True] display.chop_thresholdfloat or Noneif set to a float value, all float values smaller than the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None] display.colheader_justify‘left’/’right’Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right] display.date_dayfirstbooleanWhen True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False] display.date_yearfirstbooleanWhen True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False] display.encodingstr/unicodeDefaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8] display.expand_frame_reprbooleanWhether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple “pages” if its width exceeds display.width. [default: True] [currently: True] display.float_formatcallableThe callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None] display.html.borderintA border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1] display.html.table_schemabooleanWhether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False] display.html.use_mathjaxbooleanWhen True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True] display.large_repr‘truncate’/’info’For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table, or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate] display.max_categoriesintThis sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype “category”. [default: 8] [currently: 8] display.max_columnsintIf max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 or None and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection and defaults to 20. [default: 0] [currently: 0] display.max_colwidthint or NoneThe maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a “…” placeholder is embedded in the output. A ‘None’ value means unlimited. [default: 50] [currently: 50] display.max_dir_itemsintThe number of items that will be added to dir(…). ‘None’ value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added. This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100] display.max_info_columnsintmax_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100] display.max_info_rowsintdf.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785] display.max_rowsintIf max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60] display.max_seq_itemsint or NoneWhen pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of “…” to the resulting string. If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100] display.memory_usagebool, string or NoneThis specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,’deep’ [default: True] [currently: True] display.min_rowsintThe numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10] display.multi_sparseboolean“sparsify” MultiIndex display (don’t display repeated elements in outer levels within groups) [default: True] [currently: True] display.notebook_repr_htmlbooleanWhen True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True] display.pprint_nest_depthintControls the number of nested levels to process when pretty-printing [default: 3] [currently: 3] display.precisionintFloating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6] display.show_dimensionsboolean or ‘truncate’Whether to print out dimensions at the end of DataFrame repr. If ‘truncate’ is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate] display.unicode.ambiguous_as_widebooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.unicode.east_asian_widthbooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.widthintWidth of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80] future.infer_string Whether to infer sequence of str objects as pyarrow string dtype, which will be the default in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] future.no_silent_downcasting Whether to opt-in to the future behavior which will not silently downcast results from Series and DataFrame where, mask, and clip methods. Silent downcasting will be removed in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] io.excel.ods.readerstringThe default Excel reader engine for ‘ods’ files. Available options: auto, odf, calamine. [default: auto] [currently: auto] io.excel.ods.writerstringThe default Excel writer engine for ‘ods’ files. Available options: auto, odf. [default: auto] [currently: auto] io.excel.xls.readerstringThe default Excel reader engine for ‘xls’ files. Available options: auto, xlrd, calamine. [default: auto] [currently: auto] io.excel.xlsb.readerstringThe default Excel reader engine for ‘xlsb’ files. Available options: auto, pyxlsb, calamine. [default: auto] [currently: auto] io.excel.xlsm.readerstringThe default Excel reader engine for ‘xlsm’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsm.writerstringThe default Excel writer engine for ‘xlsm’ files. Available options: auto, openpyxl. [default: auto] [currently: auto] io.excel.xlsx.readerstringThe default Excel reader engine for ‘xlsx’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsx.writerstringThe default Excel writer engine for ‘xlsx’ files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto] io.hdf.default_formatformatdefault format writing format, if None, then put will default to ‘fixed’ and append will default to ‘table’ [default: None] [currently: None] io.hdf.dropna_tablebooleandrop ALL nan rows when appending to a table [default: False] [currently: False] io.parquet.enginestringThe default parquet reader/writer engine. Available options: ‘auto’, ‘pyarrow’, ‘fastparquet’, the default is ‘auto’ [default: auto] [currently: auto] io.sql.enginestringThe default sql reader/writer engine. Available options: ‘auto’, ‘sqlalchemy’, the default is ‘auto’ [default: auto] [currently: auto] mode.chained_assignmentstringRaise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn] mode.copy_on_writeboolUse new copy-view behaviour using Copy-on-Write. Defaults to False, unless overridden by the ‘PANDAS_COPY_ON_WRITE’ environment variable (if set to “1” for True, needs to be set before pandas is imported). [default: False] [currently: False] mode.data_managerstringInternal data manager type; can be “block” or “array”. Defaults to “block”, unless overridden by the ‘PANDAS_DATA_MANAGER’ environment variable (needs to be set before pandas is imported). [default: block] [currently: block] (Deprecated, use `` instead.) mode.sim_interactivebooleanWhether to simulate interactive mode for purposes of testing [default: False] [currently: False] mode.string_storagestringThe default storage for StringDtype. This option is ignored if future.infer_string is set to True. [default: python] [currently: python] mode.use_inf_as_nabooleanTrue means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). This option is deprecated in pandas 2.1.0 and will be removed in 3.0. [default: False] [currently: False] (Deprecated, use `` instead.) plotting.backendstrThe plotting backend to use. The default value is “matplotlib”, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib] plotting.matplotlib.register_convertersbool or ‘auto’.Whether to register converters with matplotlib’s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto] styler.format.decimalstrThe character representation for the decimal separator for floats and complex. [default: .] [currently: .] styler.format.escapestr, optionalWhether to escape certain characters according to the given context; html or latex. [default: None] [currently: None] styler.format.formatterstr, callable, dict, optionalA formatter object to be used as default within Styler.format. [default: None] [currently: None] styler.format.na_repstr, optionalThe string representation for values identified as missing. [default: None] [currently: None] styler.format.precisionintThe precision for floats and complex numbers. [default: 6] [currently: 6] styler.format.thousandsstr, optionalThe character representation for thousands separator for floats, int and complex. [default: None] [currently: None] styler.html.mathjaxboolIf False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True] styler.latex.environmentstrThe environment to replace \begin{table}. If “longtable” is used results in a specific longtable environment format. [default: None] [currently: None] styler.latex.hrulesboolWhether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False] styler.latex.multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. “|r” will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r] styler.latex.multirow_align{“c”, “t”, “b”}The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c] styler.render.encodingstrThe encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8] styler.render.max_columnsint, optionalThe maximum number of columns that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.max_elementsintThe maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144] styler.render.max_rowsint, optionalThe maximum number of rows that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.reprstrDetermine which output to use in Jupyter Notebook in {“html”, “latex”}. [default: html] [currently: html] styler.sparse.columnsboolWhether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True] styler.sparse.indexboolWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]","Parameters: patstrRegexp which should match a single option. Note: partial matches are supported for convenience, but unless you use the full option name (e.g. x.y.z.option_name), your code may break in future versions if new options with similar names are introduced. Returns: resultthe value of the option Raises: OptionErrorif no such option exists","["">>> pd.get_option('display.max_columns')  \n4""]"
1350,..\pandas\reference\api\pandas.PeriodIndex.day_of_year.html,pandas.PeriodIndex.day_of_year,property PeriodIndex.day_of_year[source]# The ordinal day of the year.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-10"", ""2023-02-01"", ""2023-03-01""], freq=""D"")\n>>> idx.dayofyear\nIndex([10, 32, 60], dtype=\'int64\')', '>>> idx = pd.PeriodIndex([""2023"", ""2024"", ""2025""], freq=""Y"")\n>>> idx\nPeriodIndex([\'2023\', \'2024\', \'2025\'], dtype=\'period[Y-DEC]\')\n>>> idx.dayofyear\nIndex([365, 366, 365], dtype=\'int64\')']"
1351,..\pandas\reference\api\pandas.Series.rdiv.html,pandas.Series.rdiv,"Series.rdiv(other, level=None, fill_value=None, axis=0)[source]# Return Floating division of series and other, element-wise (binary operator rtruediv). Equivalent to other / series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.divide(b, fill_value=0)\na    1.0\nb    inf\nc    inf\nd    0.0\ne    NaN\ndtype: float64""]"
1352,..\pandas\reference\api\pandas.tseries.offsets.Day.name.html,pandas.tseries.offsets.Day.name,Day.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1353,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.pipe.html,pandas.core.groupby.DataFrameGroupBy.pipe,"DataFrameGroupBy.pipe(func, *args, **kwargs)[source]# Apply a func with arguments to this GroupBy object and return its result. Use .pipe when you want to improve readability by chaining together functions that expect Series, DataFrames, GroupBy or Resampler objects. Instead of writing You can write which is much more readable. Notes See more here","Parameters: funccallable or tuple of (callable, str)Function to apply to this GroupBy object or, alternatively, a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the GroupBy object. argsiterable, optionalPositional arguments passed into func. kwargsdict, optionalA dictionary of keyword arguments passed into func. Returns: the return type of func.","['>>> h = lambda x, arg2, arg3: x + 1 - arg2 * arg3\n>>> g = lambda x, arg1: x * 5 / arg1\n>>> f = lambda x: x ** 4\n>>> df = pd.DataFrame([[""a"", 4], [""b"", 5]], columns=[""group"", ""value""])\n>>> h(g(f(df.groupby(\'group\')), arg1=1), arg2=2, arg3=3)', "">>> (df.groupby('group')\n...    .pipe(f)\n...    .pipe(g, arg1=1)\n...    .pipe(h, arg2=2, arg3=3))"", "">>> df = pd.DataFrame({'A': 'a b a b'.split(), 'B': [1, 2, 3, 4]})\n>>> df\n   A  B\n0  a  1\n1  b  2\n2  a  3\n3  b  4"", "">>> df.groupby('A').pipe(lambda x: x.max() - x.min())\n   B\nA\na  2\nb  2""]"
1354,..\pandas\reference\api\pandas.DataFrame.html,pandas.DataFrame,"class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)[source]# Two-dimensional, size-mutable, potentially heterogeneous tabular data. Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure. Notes Please reference the User Guide for more information. Examples Constructing DataFrame from a dictionary. Notice that the inferred dtype is int64. To enforce a single dtype: Constructing DataFrame from a dictionary including Series: Constructing DataFrame from numpy ndarray: Constructing DataFrame from a numpy ndarray that has labeled columns: Constructing DataFrame from dataclass: Constructing DataFrame from Series/DataFrame: Attributes T The transpose of the DataFrame. at Access a single value for a row/column label pair. attrs Dictionary of global attributes of this dataset. axes Return a list representing the axes of the DataFrame. columns The column labels of the DataFrame. dtypes Return the dtypes in the DataFrame. empty Indicator whether Series/DataFrame is empty. flags Get the properties associated with this pandas object. iat Access a single value for a row/column pair by integer position. iloc (DEPRECATED) Purely integer-location based indexing for selection by position. index The index (row labels) of the DataFrame. loc Access a group of rows and columns by label(s) or a boolean array. ndim Return an int representing the number of axes / array dimensions. shape Return a tuple representing the dimensionality of the DataFrame. size Return an int representing the number of elements in this object. style Returns a Styler object. values Return a Numpy representation of the DataFrame.","Parameters: datandarray (structured or homogeneous), Iterable, dict, or DataFrameDict can contain Series, arrays, constants, dataclass or list-like objects. If data is a dict, column order follows insertion-order. If a dict contains Series which have an index defined, it is aligned by its index. This alignment also occurs if data is a Series or a DataFrame itself. Alignment is done on Series/DataFrame inputs. If data is a list of dicts, column order follows insertion-order. indexIndex or array-likeIndex to use for resulting frame. Will default to RangeIndex if no indexing information part of input data and no index provided. columnsIndex or array-likeColumn labels to use for resulting frame when data does not have them, defaulting to RangeIndex(0, 1, 2, …, n). If data contains column labels, will perform column selection instead. dtypedtype, default NoneData type to force. Only a single dtype is allowed. If None, infer. copybool or None, default NoneCopy data from inputs. For dict data, the default of None behaves like copy=True.  For DataFrame or 2d ndarray input, the default of None behaves like copy=False. If data is a dict containing one or more Series (possibly of different dtypes), copy=False will ensure that these inputs are not copied. Changed in version 1.3.0.","["">>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4"", '>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object', '>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object', "">>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0"", "">>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9"", '>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(""a"", ""i4""), (""b"", ""i4""), (""c"", ""i4"")])\n>>> df3 = pd.DataFrame(data, columns=[\'c\', \'a\'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7', '>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(""Point"", [(""x"", int), (""y"", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3', '>>> ser = pd.Series([1, 2, 3], index=[""a"", ""b"", ""c""])\n>>> df = pd.DataFrame(data=ser, index=[""a"", ""c""])\n>>> df\n   0\na  1\nc  3', '>>> df1 = pd.DataFrame([1, 2, 3], index=[""a"", ""b"", ""c""], columns=[""x""])\n>>> df2 = pd.DataFrame(data=df1, index=[""a"", ""c""])\n>>> df2\n   x\na  1\nc  3']"
1355,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.is_year_start.html,pandas.tseries.offsets.BQuarterBegin.is_year_start,BQuarterBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1356,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_month_start.html,pandas.tseries.offsets.SemiMonthBegin.is_month_start,SemiMonthBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1357,..\pandas\reference\api\pandas.Grouper.html,pandas.Grouper,"class pandas.Grouper(*args, **kwargs)[source]# A Grouper allows the user to specify a groupby instruction for an object. This specification will select a column via the key parameter, or if the level and/or axis parameters are given, a level of the index of the target object. If axis and/or level are passed as keywords to both Grouper and groupby, the values passed to Grouper take precedence.","Parameters: keystr, defaults to NoneGroupby key, which selects the grouping column of the target. levelname/number, defaults to NoneThe level for the target index. freqstr / frequency object, defaults to NoneThis will groupby the specified frequency if the target selection (via key or level) is a datetime-like object. For full specification of available frequencies, please see here. axisstr, int, defaults to 0Number/name of the axis. sortbool, default to FalseWhether to sort the resulting labels. closed{‘left’ or ‘right’}Closed end of interval. Only when freq parameter is passed. label{‘left’ or ‘right’}Interval boundary to use for labeling. Only when freq parameter is passed. convention{‘start’, ‘end’, ‘e’, ‘s’}If grouper is PeriodIndex and freq parameter is passed. originTimestamp or str, default ‘start_day’The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If string, must be one of the following: ‘epoch’: origin is 1970-01-01 ‘start’: origin is the first value of the timeseries ‘start_day’: origin is the first day at midnight of the timeseries ‘end’: origin is the last value of the timeseries ‘end_day’: origin is the ceiling midnight of the last day Added in version 1.3.0. offsetTimedelta or str, default is NoneAn offset timedelta added to the origin. dropnabool, default TrueIf True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups. Returns: Grouper or pandas.api.typing.TimeGrouperA TimeGrouper is returned if freq is not None. Otherwise, a Grouper is returned.","['>>> df = pd.DataFrame(\n...     {\n...         ""Animal"": [""Falcon"", ""Parrot"", ""Falcon"", ""Falcon"", ""Parrot""],\n...         ""Speed"": [100, 5, 200, 300, 15],\n...     }\n... )\n>>> df\n   Animal  Speed\n0  Falcon    100\n1  Parrot      5\n2  Falcon    200\n3  Falcon    300\n4  Parrot     15\n>>> df.groupby(pd.Grouper(key=""Animal"")).mean()\n        Speed\nAnimal\nFalcon  200.0\nParrot   10.0', '>>> df = pd.DataFrame(\n...    {\n...        ""Publish date"": [\n...             pd.Timestamp(""2000-01-02""),\n...             pd.Timestamp(""2000-01-02""),\n...             pd.Timestamp(""2000-01-09""),\n...             pd.Timestamp(""2000-01-16"")\n...         ],\n...         ""ID"": [0, 1, 2, 3],\n...         ""Price"": [10, 20, 30, 40]\n...     }\n... )\n>>> df\n  Publish date  ID  Price\n0   2000-01-02   0     10\n1   2000-01-02   1     20\n2   2000-01-09   2     30\n3   2000-01-16   3     40\n>>> df.groupby(pd.Grouper(key=""Publish date"", freq=""1W"")).mean()\n               ID  Price\nPublish date\n2000-01-02    0.5   15.0\n2000-01-09    2.0   30.0\n2000-01-16    3.0   40.0', "">>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n>>> rng = pd.date_range(start, end, freq='7min')\n>>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n>>> ts\n2000-10-01 23:30:00     0\n2000-10-01 23:37:00     3\n2000-10-01 23:44:00     6\n2000-10-01 23:51:00     9\n2000-10-01 23:58:00    12\n2000-10-02 00:05:00    15\n2000-10-02 00:12:00    18\n2000-10-02 00:19:00    21\n2000-10-02 00:26:00    24\nFreq: 7min, dtype: int64"", "">>> ts.groupby(pd.Grouper(freq='17min')).sum()\n2000-10-01 23:14:00     0\n2000-10-01 23:31:00     9\n2000-10-01 23:48:00    21\n2000-10-02 00:05:00    54\n2000-10-02 00:22:00    24\nFreq: 17min, dtype: int64"", "">>> ts.groupby(pd.Grouper(freq='17min', origin='epoch')).sum()\n2000-10-01 23:18:00     0\n2000-10-01 23:35:00    18\n2000-10-01 23:52:00    27\n2000-10-02 00:09:00    39\n2000-10-02 00:26:00    24\nFreq: 17min, dtype: int64"", "">>> ts.groupby(pd.Grouper(freq='17min', origin='2000-01-01')).sum()\n2000-10-01 23:24:00     3\n2000-10-01 23:41:00    15\n2000-10-01 23:58:00    45\n2000-10-02 00:15:00    45\nFreq: 17min, dtype: int64"", "">>> ts.groupby(pd.Grouper(freq='17min', origin='start')).sum()\n2000-10-01 23:30:00     9\n2000-10-01 23:47:00    21\n2000-10-02 00:04:00    54\n2000-10-02 00:21:00    24\nFreq: 17min, dtype: int64"", "">>> ts.groupby(pd.Grouper(freq='17min', offset='23h30min')).sum()\n2000-10-01 23:30:00     9\n2000-10-01 23:47:00    21\n2000-10-02 00:04:00    54\n2000-10-02 00:21:00    24\nFreq: 17min, dtype: int64"", "">>> ts.groupby(pd.Grouper(freq='17min', offset='2min')).sum()\n2000-10-01 23:16:00     0\n2000-10-01 23:33:00     9\n2000-10-01 23:50:00    36\n2000-10-02 00:07:00    39\n2000-10-02 00:24:00    24\nFreq: 17min, dtype: int64""]"
1358,..\pandas\reference\api\pandas.PeriodIndex.end_time.html,pandas.PeriodIndex.end_time,property PeriodIndex.end_time[source]# Get the Timestamp for the end of the period.,Returns: Timestamp,"["">>> pd.Period('2020-01', 'D').end_time\nTimestamp('2020-01-01 23:59:59.999999999')"", "">>> period_index = pd.period_range('2020-1-1 00:00', '2020-3-1 00:00', freq='M')\n>>> s = pd.Series(period_index)\n>>> s\n0   2020-01\n1   2020-02\n2   2020-03\ndtype: period[M]\n>>> s.dt.end_time\n0   2020-01-31 23:59:59.999999999\n1   2020-02-29 23:59:59.999999999\n2   2020-03-31 23:59:59.999999999\ndtype: datetime64[ns]"", '>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.end_time\nDatetimeIndex([\'2023-01-31 23:59:59.999999999\',\n               \'2023-02-28 23:59:59.999999999\',\n               \'2023-03-31 23:59:59.999999999\'],\n               dtype=\'datetime64[ns]\', freq=None)']"
1359,..\pandas\reference\api\pandas.Series.reindex.html,pandas.Series.reindex,"Series.reindex(index=None, *, axis=None, method=None, copy=None, level=None, fill_value=None, limit=None, tolerance=None)[source]# Conform Series to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.","Parameters: indexarray-like, optionalNew labels for the index. Preferably an Index object to avoid duplicating data. axisint or str, optionalUnused. method{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. None (default): don’t fill gaps pad / ffill: Propagate last valid observation forward to next valid. backfill / bfill: Use next valid observation to fill gap. nearest: Use nearest valid observations to fill gap. copybool, default TrueReturn a new object, even if the passed indexes are the same. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any “compatible” value. limitint, default NoneMaximum number of consecutive elements to forward or backward fill. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: Series with changed index.","["">>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n...                   index=index)\n>>> df\n           http_status  response_time\nFirefox            200           0.04\nChrome             200           0.02\nSafari             404           0.07\nIE10               404           0.08\nKonqueror          301           1.00"", "">>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n...              'Chrome']\n>>> df.reindex(new_index)\n               http_status  response_time\nSafari               404.0           0.07\nIceweasel              NaN            NaN\nComodo Dragon          NaN            NaN\nIE10                 404.0           0.08\nChrome               200.0           0.02"", '>>> df.reindex(new_index, fill_value=0)\n               http_status  response_time\nSafari                 404           0.07\nIceweasel                0           0.00\nComodo Dragon            0           0.00\nIE10                   404           0.08\nChrome                 200           0.02', "">>> df.reindex(new_index, fill_value='missing')\n              http_status response_time\nSafari                404          0.07\nIceweasel         missing       missing\nComodo Dragon     missing       missing\nIE10                  404          0.08\nChrome                200          0.02"", "">>> df.reindex(columns=['http_status', 'user_agent'])\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN"", '>>> df.reindex([\'http_status\', \'user_agent\'], axis=""columns"")\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN', '>>> date_index = pd.date_range(\'1/1/2010\', periods=6, freq=\'D\')\n>>> df2 = pd.DataFrame({""prices"": [100, 101, np.nan, 100, 89, 88]},\n...                    index=date_index)\n>>> df2\n            prices\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0', "">>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n>>> df2.reindex(date_index2)\n            prices\n2009-12-29     NaN\n2009-12-30     NaN\n2009-12-31     NaN\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN"", "">>> df2.reindex(date_index2, method='bfill')\n            prices\n2009-12-29   100.0\n2009-12-30   100.0\n2009-12-31   100.0\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN""]"
1360,..\pandas\reference\api\pandas.HDFStore.append.html,pandas.HDFStore.append,"HDFStore.append(key, value, format=None, axes=None, index=True, append=True, complib=None, complevel=None, columns=None, min_itemsize=None, nan_rep=None, chunksize=None, expectedrows=None, dropna=None, data_columns=None, encoding=None, errors='strict')[source]# Append to Table in file. Node must already exist and be Table format. Notes Does not check if data being appended overlaps with existing data in the table, so be careful","Parameters: keystr value{Series, DataFrame} format‘table’ is the defaultFormat to use when storing object in HDFStore.  Value can be one of: 'table'Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. indexbool, default TrueWrite DataFrame index as a column. appendbool, default TrueAppend the input data to the existing. data_columnslist of columns, or True, default NoneList of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See here. min_itemsizedict of columns that specify minimum str sizes nan_repstr to use as str nan representation chunksizesize to chunk the writing expectedrowsexpected TOTAL row size of this table encodingdefault None, provide an encoding for str dropnabool, default False, optionalDo not write an ALL nan row to the store settable by the option ‘io.hdf.dropna_table’.","['>>> df1 = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df1, format=\'table\')  \n>>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=[\'A\', \'B\'])\n>>> store.append(\'data\', df2)  \n>>> store.close()  \n   A  B\n0  1  2\n1  3  4\n0  5  6\n1  7  8']"
1361,..\pandas\reference\api\pandas.DataFrame.iat.html,pandas.DataFrame.iat,"property DataFrame.iat[source]# Access a single value for a row/column pair by integer position. Similar to iloc, in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.",Raises: IndexErrorWhen integer position is out of bounds.,"["">>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n0   0   2   3\n1   0   4   1\n2  10  20  30"", '>>> df.iat[1, 2]\n1', '>>> df.iat[1, 2] = 10\n>>> df.iat[1, 2]\n10', '>>> df.loc[0].iat[1]\n2']"
1362,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.kwds.html,pandas.tseries.offsets.BQuarterBegin.kwds,BQuarterBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1363,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.plot.html,pandas.core.groupby.DataFrameGroupBy.plot,"property DataFrameGroupBy.plot[source]# Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used. Notes See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)","Parameters: dataSeries or DataFrameThe object for which the method is called. xlabel or position, default NoneOnly used if data is a DataFrame. ylabel, position or list of label, positions, default NoneAllows plotting of one column versus another. Only used if data is a DataFrame. kindstrThe kind of plot to produce: ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only) axmatplotlib axes object, default NoneAn axes of the current figure. subplotsbool or sequence of iterables, default FalseWhether to group columns into subplots: False : No subplots will be used True : Make separate subplots for each column. sequence of iterables of column labels: Create a subplot for each group of columns. For example [(‘a’, ‘c’), (‘b’, ‘d’)] will create 2 subplots: one with columns ‘a’ and ‘c’, and one with columns ‘b’ and ‘d’. Remaining columns that aren’t specified will be plotted in additional subplots (one per column). Added in version 1.5.0. sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure. shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to invisible. layouttuple, optional(rows, columns) for the layout of subplots. figsizea tuple (width, height) in inchesSize of a figure object. use_indexbool, default TrueUse index as ticks for x axis. titlestr or listTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot. gridbool, default None (matlab style default)Axis grid lines. legendbool or {‘reverse’}Place legend on axis subplots. stylelist or dictThe matplotlib line style per column. logxbool or ‘sym’, default FalseUse log scaling or symlog scaling on x axis. logybool or ‘sym’ default FalseUse log scaling or symlog scaling on y axis. loglogbool or ‘sym’, default FalseUse log scaling or symlog scaling on both x and y axes. xtickssequenceValues to use for the xticks. ytickssequenceValues to use for the yticks. xlim2-tuple/listSet the x limits of the current axes. ylim2-tuple/listSet the y limits of the current axes. xlabellabel, optionalName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. ylabellabel, optionalName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. rotfloat, default NoneRotation for ticks (xticks for vertical, yticks for horizontal plots). fontsizefloat, default NoneFont size for xticks and yticks. colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If string, load colormap with that name from matplotlib. colorbarbool, optionalIf True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots). positionfloatSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center). tablebool, Series or DataFrame, default FalseIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table. yerrDataFrame, Series, array-like, dict and strSee Plotting with Error Bars for detail. xerrDataFrame, Series, array-like, dict and strEquivalent to yerr. stackedbool, default False in line and bar plots, and True in area plotIf True, create stacked plot. secondary_ybool or sequence, default FalseWhether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis. mark_rightbool, default TrueWhen using a secondary_y axis, automatically mark the column labels with “(right)” in the legend. include_boolbool, default is FalseIf True, boolean values can be plotted. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes or numpy.ndarray of themIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.","['>>> ser = pd.Series([1, 2, 3, 3])\n>>> plot = ser.plot(kind=\'hist\', title=""My plot"")', '>>> df = pd.DataFrame({\'length\': [1.5, 0.5, 1.2, 0.9, 3],\n...                   \'width\': [0.7, 0.2, 0.15, 0.2, 1.1]},\n...                   index=[\'pig\', \'rabbit\', \'duck\', \'chicken\', \'horse\'])\n>>> plot = df.plot(title=""DataFrame Plot"")', '>>> lst = [-1, -2, -3, 1, 2, 3]\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> plot = ser.groupby(lambda x: x > 0).plot(title=""SeriesGroupBy Plot"")', '>>> df = pd.DataFrame({""col1"" : [1, 2, 3, 4],\n...                   ""col2"" : [""A"", ""B"", ""A"", ""B""]})\n>>> plot = df.groupby(""col2"").plot(kind=""bar"", title=""DataFrameGroupBy Plot"")']"
1364,..\pandas\reference\api\pandas.tseries.offsets.Day.nanos.html,pandas.tseries.offsets.Day.nanos,Day.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
1365,..\pandas\reference\api\pandas.PeriodIndex.freq.html,pandas.PeriodIndex.freq,property PeriodIndex.freq[source]#,No parameters found,[]
1366,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_on_offset.html,pandas.tseries.offsets.SemiMonthBegin.is_on_offset,SemiMonthBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1367,..\pandas\reference\api\pandas.Series.reindex_like.html,pandas.Series.reindex_like,"Series.reindex_like(other, method=None, copy=None, limit=None, tolerance=None)[source]# Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Notes Same as calling .reindex(index=other.index, columns=other.columns,...).","Parameters: otherObject of the same data typeIts row and column indices are used to define the new indices of this object. method{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. None (default): don’t fill gaps pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap. copybool, default TrueReturn a new object, even if the passed indexes are the same. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True limitint, default NoneMaximum number of consecutive labels to fill for inexact matches. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: Series or DataFrameSame type as caller, but with changed indices on each axis.","["">>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n...                     [31, 87.8, 'high'],\n...                     [22, 71.6, 'medium'],\n...                     [35, 95, 'medium']],\n...                    columns=['temp_celsius', 'temp_fahrenheit',\n...                             'windspeed'],\n...                    index=pd.date_range(start='2014-02-12',\n...                                        end='2014-02-15', freq='D'))"", '>>> df1\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium', "">>> df2 = pd.DataFrame([[28, 'low'],\n...                     [30, 'low'],\n...                     [35.1, 'medium']],\n...                    columns=['temp_celsius', 'windspeed'],\n...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n...                                            '2014-02-15']))"", '>>> df2\n            temp_celsius windspeed\n2014-02-12          28.0       low\n2014-02-13          30.0       low\n2014-02-15          35.1    medium', '>>> df2.reindex_like(df1)\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          28.0              NaN       low\n2014-02-13          30.0              NaN       low\n2014-02-14           NaN              NaN       NaN\n2014-02-15          35.1              NaN    medium']"
1368,..\pandas\reference\api\pandas.HDFStore.get.html,pandas.HDFStore.get,HDFStore.get(key)[source]# Retrieve pandas object stored in file.,Parameters: keystr Returns: objectSame type as object stored in file.,"['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df)  \n>>> store.get(\'data\')  \n>>> store.close()']"
1369,..\pandas\reference\api\pandas.DataFrame.idxmax.html,pandas.DataFrame.idxmax,"DataFrame.idxmax(axis=0, skipna=True, numeric_only=False)[source]# Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Notes This method is the DataFrame version of ndarray.argmax.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Returns: SeriesIndexes of maxima along the specified axis. Raises: ValueError If the row/column is empty","["">>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n...                     'co2_emissions': [37.2, 19.66, 1712]},\n...                   index=['Pork', 'Wheat Products', 'Beef'])"", '>>> df\n                consumption  co2_emissions\nPork                  10.51         37.20\nWheat Products       103.11         19.66\nBeef                  55.48       1712.00', '>>> df.idxmax()\nconsumption     Wheat Products\nco2_emissions             Beef\ndtype: object', '>>> df.idxmax(axis=""columns"")\nPork              co2_emissions\nWheat Products     consumption\nBeef              co2_emissions\ndtype: object']"
1370,..\pandas\reference\api\pandas.PeriodIndex.freqstr.html,pandas.PeriodIndex.freqstr,"property PeriodIndex.freqstr[source]# Return the frequency object as a string if it’s set, otherwise None.",No parameters found,"['>>> idx = pd.DatetimeIndex([""1/1/2020 10:00:00+00:00""], freq=""D"")\n>>> idx.freqstr\n\'D\'', '>>> idx = pd.DatetimeIndex([""2018-01-01"", ""2018-01-03"", ""2018-01-05""],\n...                        freq=""infer"")\n>>> idx.freqstr\n\'2D\'', '>>> idx = pd.PeriodIndex([""2023-1"", ""2023-2"", ""2023-3""], freq=""M"")\n>>> idx.freqstr\n\'M\'']"
1371,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.prod.html,pandas.core.groupby.DataFrameGroupBy.prod,"DataFrameGroupBy.prod(numeric_only=False, min_count=0)[source]# Compute prod of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. Returns: Series or DataFrameComputed prod of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).prod()\na    2\nb   12\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").prod()\n     b    c\na\n1   16   10\n2   30   72']"
1372,..\pandas\reference\api\pandas.Series.rename.html,pandas.Series.rename,"Series.rename(index=None, *, axis=None, copy=None, inplace=False, level=None, errors='ignore')[source]# Alter Series index labels or name. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don’t throw an error. Alternatively, change Series.name with a scalar value. See the user guide for more.","Parameters: indexscalar, hashable sequence, dict-like or function optionalFunctions or dict-like are transformations to apply to the index. Scalar or hashable sequence-like will alter the Series.name attribute. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. copybool, default TrueAlso copy underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True inplacebool, default FalseWhether to return a new Series. If True the value of copy is ignored. levelint or level name, default NoneIn case of MultiIndex, only rename labels in the specified level. errors{‘ignore’, ‘raise’}, default ‘ignore’If ‘raise’, raise KeyError when a dict-like mapper or index contains labels that are not present in the index being transformed. If ‘ignore’, existing keys will be renamed and extra keys will be ignored. Returns: Series or NoneSeries with index labels or name altered or None if inplace=True.","['>>> s = pd.Series([1, 2, 3])\n>>> s\n0    1\n1    2\n2    3\ndtype: int64\n>>> s.rename(""my_name"")  # scalar, changes Series.name\n0    1\n1    2\n2    3\nName: my_name, dtype: int64\n>>> s.rename(lambda x: x ** 2)  # function, changes labels\n0    1\n1    2\n4    3\ndtype: int64\n>>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n0    1\n3    2\n5    3\ndtype: int64']"
1373,..\pandas\reference\api\pandas.tseries.offsets.Day.normalize.html,pandas.tseries.offsets.Day.normalize,Day.normalize#,No parameters found,[]
1374,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_quarter_end.html,pandas.tseries.offsets.SemiMonthBegin.is_quarter_end,SemiMonthBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1375,..\pandas\reference\api\pandas.HDFStore.groups.html,pandas.HDFStore.groups,HDFStore.groups()[source]# Return a list of all the top-level nodes. Each node returned is not a pandas storage object.,Returns: listList of objects.,"['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df)  \n>>> print(store.groups())  \n>>> store.close()  \n[/data (Group) \'\'\n  children := [\'axis0\' (Array), \'axis1\' (Array), \'block0_values\' (Array),\n  \'block0_items\' (Array)]]']"
1376,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.n.html,pandas.tseries.offsets.BQuarterBegin.n,BQuarterBegin.n#,No parameters found,[]
1377,..\pandas\reference\api\pandas.DataFrame.idxmin.html,pandas.DataFrame.idxmin,"DataFrame.idxmin(axis=0, skipna=True, numeric_only=False)[source]# Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Notes This method is the DataFrame version of ndarray.argmin.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Returns: SeriesIndexes of minima along the specified axis. Raises: ValueError If the row/column is empty","["">>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n...                     'co2_emissions': [37.2, 19.66, 1712]},\n...                   index=['Pork', 'Wheat Products', 'Beef'])"", '>>> df\n                consumption  co2_emissions\nPork                  10.51         37.20\nWheat Products       103.11         19.66\nBeef                  55.48       1712.00', '>>> df.idxmin()\nconsumption                Pork\nco2_emissions    Wheat Products\ndtype: object', '>>> df.idxmin(axis=""columns"")\nPork                consumption\nWheat Products    co2_emissions\nBeef                consumption\ndtype: object']"
1378,..\pandas\reference\api\pandas.PeriodIndex.from_fields.html,pandas.PeriodIndex.from_fields,"classmethod PeriodIndex.from_fields(*, year=None, quarter=None, month=None, day=None, hour=None, minute=None, second=None, freq=None)[source]#",No parameters found,[]
1379,..\pandas\reference\api\pandas.HDFStore.info.html,pandas.HDFStore.info,HDFStore.info()[source]# Print detailed information on the store.,Returns: str,"['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df)  \n>>> print(store.info())  \n>>> store.close()  \n<class \'pandas.io.pytables.HDFStore\'>\nFile path: store.h5\n/data    frame    (shape->[2,2])']"
1380,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.quantile.html,pandas.core.groupby.DataFrameGroupBy.quantile,"DataFrameGroupBy.quantile(q=0.5, interpolation='linear', numeric_only=False)[source]# Return group values at the given quantile, a la numpy.percentile.","Parameters: qfloat or array-like, default 0.5 (50% quantile)Value(s) between 0 and 1 providing the quantile(s) to compute. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}Method to use when the desired quantile falls between two points. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameReturn type determined by caller of GroupBy object.","["">>> df = pd.DataFrame([\n...     ['a', 1], ['a', 2], ['a', 3],\n...     ['b', 1], ['b', 3], ['b', 5]\n... ], columns=['key', 'val'])\n>>> df.groupby('key').quantile()\n    val\nkey\na    2.0\nb    3.0""]"
1381,..\pandas\reference\api\pandas.Series.rename_axis.html,pandas.Series.rename_axis,"Series.rename_axis(mapper=<no_default>, *, index=<no_default>, axis=0, copy=True, inplace=False)[source]# Set the name of the axis for the index or columns. Notes DataFrame.rename_axis supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored. The second calling convention will modify the names of the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels. We highly recommend using keyword arguments to clarify your intent.","Parameters: mapperscalar, list-like, optionalValue to set the axis name attribute. index, columnsscalar, list-like, dict-like or function, optionalA scalar, list-like, dict-like or functions transformations to apply to that axis’ values. Note that the columns parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects. Use either mapper and axis to specify the axis to target with mapper, or index and/or columns. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to rename. For Series this parameter is unused and defaults to 0. copybool, default NoneAlso copy underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True inplacebool, default FalseModifies the object directly, instead of creating a new Series or DataFrame. Returns: Series, DataFrame, or NoneThe same type as the caller or None if inplace=True.","['>>> s = pd.Series([""dog"", ""cat"", ""monkey""])\n>>> s\n0       dog\n1       cat\n2    monkey\ndtype: object\n>>> s.rename_axis(""animal"")\nanimal\n0    dog\n1    cat\n2    monkey\ndtype: object', '>>> df = pd.DataFrame({""num_legs"": [4, 4, 2],\n...                    ""num_arms"": [0, 0, 2]},\n...                   [""dog"", ""cat"", ""monkey""])\n>>> df\n        num_legs  num_arms\ndog            4         0\ncat            4         0\nmonkey         2         2\n>>> df = df.rename_axis(""animal"")\n>>> df\n        num_legs  num_arms\nanimal\ndog            4         0\ncat            4         0\nmonkey         2         2\n>>> df = df.rename_axis(""limbs"", axis=""columns"")\n>>> df\nlimbs   num_legs  num_arms\nanimal\ndog            4         0\ncat            4         0\nmonkey         2         2', "">>> df.index = pd.MultiIndex.from_product([['mammal'],\n...                                        ['dog', 'cat', 'monkey']],\n...                                       names=['type', 'name'])\n>>> df\nlimbs          num_legs  num_arms\ntype   name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2"", "">>> df.rename_axis(index={'type': 'class'})\nlimbs          num_legs  num_arms\nclass  name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2"", '>>> df.rename_axis(columns=str.upper)\nLIMBS          num_legs  num_arms\ntype   name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2']"
1382,..\pandas\reference\api\pandas.tseries.offsets.Day.rule_code.html,pandas.tseries.offsets.Day.rule_code,Day.rule_code#,No parameters found,[]
1383,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.name.html,pandas.tseries.offsets.BQuarterBegin.name,BQuarterBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1384,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_quarter_start.html,pandas.tseries.offsets.SemiMonthBegin.is_quarter_start,SemiMonthBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1385,..\pandas\reference\api\pandas.DataFrame.iloc.html,pandas.DataFrame.iloc,"property DataFrame.iloc[source]# Purely integer-location based indexing for selection by position. Deprecated since version 2.2.0: Returning a tuple from a callable is deprecated. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. Allowed inputs are: An integer, e.g. 5. A list or array of integers, e.g. [4, 3, 0]. A slice object with ints, e.g. 1:7. A boolean array. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don’t have a reference to the calling object, but would like to base your selection on some value. A tuple of row and column indexes. The tuple elements consist of one of the above inputs, e.g. (0, 1). .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics). See more at Selection by Position.",No parameters found,"["">>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000}]\n>>> df = pd.DataFrame(mydict)\n>>> df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000"", "">>> type(df.iloc[0])\n<class 'pandas.core.series.Series'>\n>>> df.iloc[0]\na    1\nb    2\nc    3\nd    4\nName: 0, dtype: int64"", "">>> df.iloc[[0]]\n   a  b  c  d\n0  1  2  3  4\n>>> type(df.iloc[[0]])\n<class 'pandas.core.frame.DataFrame'>"", '>>> df.iloc[[0, 1]]\n     a    b    c    d\n0    1    2    3    4\n1  100  200  300  400', '>>> df.iloc[:3]\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000', '>>> df.iloc[[True, False, True]]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000', '>>> df.iloc[lambda x: x.index % 2 == 0]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000', '>>> df.iloc[0, 1]\n2', '>>> df.iloc[[0, 2], [1, 3]]\n      b     d\n0     2     4\n2  2000  4000', '>>> df.iloc[1:3, 0:3]\n      a     b     c\n1   100   200   300\n2  1000  2000  3000', '>>> df.iloc[:, [True, False, True, False]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000', '>>> df.iloc[:, lambda df: [0, 2]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000']"
1386,..\pandas\reference\api\pandas.HDFStore.keys.html,pandas.HDFStore.keys,HDFStore.keys(include='pandas')[source]# Return a list of keys corresponding to objects stored in HDFStore.,"Parameters: includestr, default ‘pandas’When kind equals ‘pandas’ return pandas objects. When kind equals ‘native’ return native HDF5 Table objects. Returns: listList of ABSOLUTE path-names (e.g. have the leading ‘/’). Raises: raises ValueError if kind has an illegal value","['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df)  \n>>> store.get(\'data\')  \n>>> print(store.keys())  \n[\'/data1\', \'/data2\']\n>>> store.close()']"
1387,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.rank.html,pandas.core.groupby.DataFrameGroupBy.rank,"DataFrameGroupBy.rank(method='average', ascending=True, na_option='keep', pct=False, axis=<no_default>)[source]# Provide the rank of values within each group.","Parameters: method{‘average’, ‘min’, ‘max’, ‘first’, ‘dense’}, default ‘average’ average: average rank of group. min: lowest rank in group. max: highest rank in group. first: ranks assigned in order they appear in the array. dense: like ‘min’, but rank always increases by 1 between groups. ascendingbool, default TrueFalse for ranks by high (1) to low (N). na_option{‘keep’, ‘top’, ‘bottom’}, default ‘keep’ keep: leave NA values where they are. top: smallest rank if ascending. bottom: smallest rank if descending. pctbool, default FalseCompute percentage rank of data within each group. axisint, default 0The axis of the object over which to compute the rank. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. Returns: DataFrame with ranking of values within each group","['>>> df = pd.DataFrame(\n...     {\n...         ""group"": [""a"", ""a"", ""a"", ""a"", ""a"", ""b"", ""b"", ""b"", ""b"", ""b""],\n...         ""value"": [2, 4, 2, 3, 5, 1, 2, 4, 1, 5],\n...     }\n... )\n>>> df\n  group  value\n0     a      2\n1     a      4\n2     a      2\n3     a      3\n4     a      5\n5     b      1\n6     b      2\n7     b      4\n8     b      1\n9     b      5\n>>> for method in [\'average\', \'min\', \'max\', \'dense\', \'first\']:\n...     df[f\'{method}_rank\'] = df.groupby(\'group\')[\'value\'].rank(method)\n>>> df\n  group  value  average_rank  min_rank  max_rank  dense_rank  first_rank\n0     a      2           1.5       1.0       2.0         1.0         1.0\n1     a      4           4.0       4.0       4.0         3.0         4.0\n2     a      2           1.5       1.0       2.0         1.0         2.0\n3     a      3           3.0       3.0       3.0         2.0         3.0\n4     a      5           5.0       5.0       5.0         4.0         5.0\n5     b      1           1.5       1.0       2.0         1.0         1.0\n6     b      2           3.0       3.0       3.0         2.0         3.0\n7     b      4           4.0       4.0       4.0         3.0         4.0\n8     b      1           1.5       1.0       2.0         1.0         2.0\n9     b      5           5.0       5.0       5.0         4.0         5.0']"
1388,..\pandas\reference\api\pandas.Series.reorder_levels.html,pandas.Series.reorder_levels,Series.reorder_levels(order)[source]# Rearrange index levels using input order. May not drop or duplicate levels.,Parameters: orderlist of int representing new level orderReference level by number or key. Returns: type of caller (new object),"['>>> arrays = [np.array([""dog"", ""dog"", ""cat"", ""cat"", ""bird"", ""bird""]),\n...           np.array([""white"", ""black"", ""white"", ""black"", ""white"", ""black""])]\n>>> s = pd.Series([1, 2, 3, 3, 5, 2], index=arrays)\n>>> s\ndog   white    1\n      black    2\ncat   white    3\n      black    3\nbird  white    5\n      black    2\ndtype: int64\n>>> s.reorder_levels([1, 0])\nwhite  dog     1\nblack  dog     2\nwhite  cat     3\nblack  cat     3\nwhite  bird    5\nblack  bird    2\ndtype: int64']"
1389,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.nanos.html,pandas.tseries.offsets.BQuarterBegin.nanos,BQuarterBegin.nanos#,No parameters found,[]
1390,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_year_end.html,pandas.tseries.offsets.SemiMonthBegin.is_year_end,SemiMonthBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1391,..\pandas\reference\api\pandas.tseries.offsets.Easter.copy.html,pandas.tseries.offsets.Easter.copy,Easter.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1392,..\pandas\reference\api\pandas.DataFrame.index.html,pandas.DataFrame.index,"DataFrame.index# The index (row labels) of the DataFrame. The index of a DataFrame is a series of labels that identify each row. The labels can be integers, strings, or any other hashable type. The index is used for label-based access and alignment, and can be accessed or modified using this attribute.",Returns: pandas.IndexThe index labels of the DataFrame.,"["">>> df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Aritra'],\n...                    'Age': [25, 30, 35],\n...                    'Location': ['Seattle', 'New York', 'Kona']},\n...                   index=([10, 20, 30]))\n>>> df.index\nIndex([10, 20, 30], dtype='int64')"", '>>> df.index = [100, 200, 300]\n>>> df\n    Name  Age Location\n100  Alice   25  Seattle\n200    Bob   30 New York\n300  Aritra  35    Kona']"
1393,..\pandas\reference\api\pandas.PeriodIndex.from_ordinals.html,pandas.PeriodIndex.from_ordinals,"classmethod PeriodIndex.from_ordinals(ordinals, *, freq, name=None)[source]#",No parameters found,[]
1394,..\pandas\reference\api\pandas.HDFStore.put.html,pandas.HDFStore.put,"HDFStore.put(key, value, format=None, index=True, append=False, complib=None, complevel=None, min_itemsize=None, nan_rep=None, data_columns=None, encoding=None, errors='strict', track_times=True, dropna=False)[source]# Store object in HDFStore.","Parameters: keystr value{Series, DataFrame} format‘fixed(f)|table(t)’, default is ‘fixed’Format to use when storing object in HDFStore. Value can be one of: 'fixed'Fixed format.  Fast writing/reading. Not-appendable, nor searchable. 'table'Table format.  Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. indexbool, default TrueWrite DataFrame index as a column. appendbool, default FalseThis will force Table format, append the input data to the existing. data_columnslist of columns or True, default NoneList of columns to create as data columns, or True to use all columns. See here. encodingstr, default NoneProvide an encoding for strings. track_timesbool, default TrueParameter is propagated to ‘create_table’ method of ‘PyTables’. If set to False it enables to have the same h5 files (same hashes) independent on creation time. dropnabool, default False, optionalRemove missing values.","['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df)']"
1395,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.resample.html,pandas.core.groupby.DataFrameGroupBy.resample,"DataFrameGroupBy.resample(rule, *args, include_groups=True, **kwargs)[source]# Provide resampling when using a TimeGrouper. Given a grouper, the function resamples it according to a string “string” -> “frequency”. See the frequency aliases documentation for more details.","Parameters: rulestr or DateOffsetThe offset string or object representing target grouper conversion. *argsPossible arguments are how, fill_method, limit, kind and on, and other arguments of TimeGrouper. include_groupsbool, default TrueWhen True, will attempt to include the groupings in the operation in the case that they are columns of the DataFrame. If this raises a TypeError, the result will be computed with the groupings excluded. When False, the groupings will be excluded when applying func. Added in version 2.2.0. Deprecated since version 2.2.0: Setting include_groups to True is deprecated. Only the value False will be allowed in a future version of pandas. **kwargsPossible arguments are how, fill_method, limit, kind and on, and other arguments of TimeGrouper. Returns: pandas.api.typing.DatetimeIndexResamplerGroupby, pandas.api.typing.PeriodIndexResamplerGroupby, or pandas.api.typing.TimedeltaIndexResamplerGroupbyReturn a new groupby object, with type depending on the data being resampled.","["">>> idx = pd.date_range('1/1/2000', periods=4, freq='min')\n>>> df = pd.DataFrame(data=4 * [range(2)],\n...                   index=idx,\n...                   columns=['a', 'b'])\n>>> df.iloc[2, 0] = 5\n>>> df\n                    a  b\n2000-01-01 00:00:00  0  1\n2000-01-01 00:01:00  0  1\n2000-01-01 00:02:00  5  1\n2000-01-01 00:03:00  0  1"", "">>> df.groupby('a').resample('3min', include_groups=False).sum()\n                         b\na\n0   2000-01-01 00:00:00  2\n    2000-01-01 00:03:00  1\n5   2000-01-01 00:00:00  1"", "">>> df.groupby('a').resample('30s', include_groups=False).sum()\n                    b\na\n0   2000-01-01 00:00:00  1\n    2000-01-01 00:00:30  0\n    2000-01-01 00:01:00  1\n    2000-01-01 00:01:30  0\n    2000-01-01 00:02:00  0\n    2000-01-01 00:02:30  0\n    2000-01-01 00:03:00  1\n5   2000-01-01 00:02:00  1"", "">>> df.groupby('a').resample('ME', include_groups=False).sum()\n            b\na\n0   2000-01-31  3\n5   2000-01-31  1"", "">>> (\n...     df.groupby('a')\n...     .resample('3min', closed='right', include_groups=False)\n...     .sum()\n... )\n                         b\na\n0   1999-12-31 23:57:00  1\n    2000-01-01 00:00:00  2\n5   2000-01-01 00:00:00  1"", "">>> (\n...     df.groupby('a')\n...     .resample('3min', closed='right', label='right', include_groups=False)\n...     .sum()\n... )\n                         b\na\n0   2000-01-01 00:00:00  1\n    2000-01-01 00:03:00  2\n5   2000-01-01 00:03:00  1""]"
1396,..\pandas\reference\api\pandas.HDFStore.select.html,pandas.HDFStore.select,"HDFStore.select(key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, auto_close=False)[source]# Retrieve pandas object stored in file, optionally based on where criteria. Warning Pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle when using the “fixed” format. Loading pickled data received from untrusted sources can be unsafe. See: https://docs.python.org/3/library/pickle.html for more.","Parameters: keystrObject being retrieved from file. wherelist or NoneList of Term (or convertible) objects, optional. startint or NoneRow number to start selection. stopint, default NoneRow number to stop selection. columnslist or NoneA list of columns that if not None, will limit the return columns. iteratorbool or FalseReturns an iterator. chunksizeint or NoneNumber or rows to include in iteration, return an iterator. auto_closebool or FalseShould automatically close the store when finished. Returns: objectRetrieved object from file.","['>>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df)  \n>>> store.get(\'data\')  \n>>> print(store.keys())  \n[\'/data1\', \'/data2\']\n>>> store.select(\'/data1\')  \n   A  B\n0  1  2\n1  3  4\n>>> store.select(\'/data1\', where=\'columns == A\')  \n   A\n0  1\n1  3\n>>> store.close()']"
1397,..\pandas\reference\api\pandas.PeriodIndex.hour.html,pandas.PeriodIndex.hour,property PeriodIndex.hour[source]# The hour of the period.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-01 10:00"", ""2023-01-01 11:00""], freq=\'h\')\n>>> idx.hour\nIndex([10, 11], dtype=\'int64\')']"
1398,..\pandas\reference\api\pandas.Series.repeat.html,pandas.Series.repeat,"Series.repeat(repeats, axis=None)[source]# Repeat elements of a Series. Returns a new Series where each element of the current Series is repeated consecutively a given number of times.",Parameters: repeatsint or array of intsThe number of repetitions for each element. This should be a non-negative integer. Repeating 0 times will return an empty Series. axisNoneUnused. Parameter needed for compatibility with DataFrame. Returns: SeriesNewly created Series with repeated elements.,"["">>> s = pd.Series(['a', 'b', 'c'])\n>>> s\n0    a\n1    b\n2    c\ndtype: object\n>>> s.repeat(2)\n0    a\n0    a\n1    b\n1    b\n2    c\n2    c\ndtype: object\n>>> s.repeat([1, 2, 3])\n0    a\n1    b\n1    b\n2    c\n2    c\n2    c\ndtype: object""]"
1399,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.is_year_start.html,pandas.tseries.offsets.SemiMonthBegin.is_year_start,SemiMonthBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1400,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.rolling.html,pandas.core.groupby.DataFrameGroupBy.rolling,"DataFrameGroupBy.rolling(*args, **kwargs)[source]# Return a rolling grouper, providing rolling functionality per group.","Parameters: windowint, timedelta, str, offset, or BaseIndexer subclassSize of the moving window. If an integer, the fixed number of observations used for each window. If a timedelta, str, or offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link. If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods, center, closed and step will be passed to get_window_bounds. min_periodsint, default NoneMinimum number of observations in window required to have a value; otherwise, result is np.nan. For a window that is specified by an offset, min_periods will default to 1. For a window that is specified by an integer, min_periods will default to the size of the window. centerbool, default FalseIf False, set the window labels as the right edge of the window index. If True, set the window labels as the center of the window index. win_typestr, default NoneIf None, all points are evenly weighted. If a string, it must be a valid scipy.signal window function. Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature. onstr, optionalFor a DataFrame, a column label or Index level on which to calculate the rolling window, rather than the DataFrame’s index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axisint or str, default 0If 0 or 'index', roll across the rows. If 1 or 'columns', roll across the columns. For Series this parameter is unused and defaults to 0. closedstr, default NoneIf 'right', the first point in the window is excluded from calculations. If 'left', the last point in the window is excluded from calculations. If 'both', no points in the window are excluded from calculations. If 'neither', the first and last points in the window are excluded from calculations. Default None ('right'). methodstr {‘single’, ‘table’}, default ‘single’Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Returns: pandas.api.typing.RollingGroupbyReturn a new grouper with our rolling appended.","["">>> df = pd.DataFrame({'A': [1, 1, 2, 2],\n...                    'B': [1, 2, 3, 4],\n...                    'C': [0.362, 0.227, 1.267, -0.562]})\n>>> df\n      A  B      C\n0     1  1  0.362\n1     1  2  0.227\n2     2  3  1.267\n3     2  4 -0.562"", "">>> df.groupby('A').rolling(2).sum()\n    B      C\nA\n1 0  NaN    NaN\n  1  3.0  0.589\n2 2  NaN    NaN\n  3  7.0  0.705"", "">>> df.groupby('A').rolling(2, min_periods=1).sum()\n    B      C\nA\n1 0  1.0  0.362\n  1  3.0  0.589\n2 2  3.0  1.267\n  3  7.0  0.705"", "">>> df.groupby('A').rolling(2, on='B').sum()\n    B      C\nA\n1 0  1    NaN\n  1  2  0.589\n2 2  3    NaN\n  3  4  0.705""]"
1401,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.normalize.html,pandas.tseries.offsets.BQuarterBegin.normalize,BQuarterBegin.normalize#,No parameters found,[]
1402,..\pandas\reference\api\pandas.DataFrame.infer_objects.html,pandas.DataFrame.infer_objects,"DataFrame.infer_objects(copy=None)[source]# Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.","Parameters: copybool, default TrueWhether to make a copy for non-object or non-inferable columns or Series. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: same type as input object","['>>> df = pd.DataFrame({""A"": [""a"", 1, 2, 3]})\n>>> df = df.iloc[1:]\n>>> df\n   A\n1  1\n2  2\n3  3', '>>> df.dtypes\nA    object\ndtype: object', '>>> df.infer_objects().dtypes\nA    int64\ndtype: object']"
1403,..\pandas\reference\api\pandas.tseries.offsets.Easter.freqstr.html,pandas.tseries.offsets.Easter.freqstr,Easter.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1404,..\pandas\reference\api\pandas.HDFStore.walk.html,pandas.HDFStore.walk,"HDFStore.walk(where='/')[source]# Walk the pytables group hierarchy for pandas objects. This generator will yield the group path, subgroups and pandas object names for each group. Any non-pandas PyTables objects that are not a group will be ignored. The where group itself is listed first (preorder), then each of its child groups (following an alphanumerical order) is also traversed, following the same procedure.","Parameters: wherestr, default “/”Group where to start walking. Yields: pathstrFull path to a group (without trailing ‘/’). groupslistNames (strings) of the groups contained in path. leaveslistNames (strings) of the pandas objects contained in path.","['>>> df1 = pd.DataFrame([[1, 2], [3, 4]], columns=[\'A\', \'B\'])\n>>> store = pd.HDFStore(""store.h5"", \'w\')  \n>>> store.put(\'data\', df1, format=\'table\')  \n>>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=[\'A\', \'B\'])\n>>> store.append(\'data\', df2)  \n>>> store.close()  \n>>> for group in store.walk():  \n...     print(group)  \n>>> store.close()']"
1405,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.sample.html,pandas.core.groupby.DataFrameGroupBy.sample,"DataFrameGroupBy.sample(n=None, frac=None, replace=False, weights=None, random_state=None)[source]# Return a random sample of items from each group. You can use random_state for reproducibility.","Parameters: nint, optionalNumber of items to return for each group. Cannot be used with frac and must be no larger than the smallest group unless replace is True. Default is one if frac is None. fracfloat, optionalFraction of items to return. Cannot be used with n. replacebool, default FalseAllow or disallow sampling of the same row more than once. weightslist-like, optionalDefault None results in equal probability weighting. If passed a list-like then values must have the same length as the underlying DataFrame or Series object and will be used as sampling probabilities after normalization within each group. Values must be non-negative with at least one positive element within each group. random_stateint, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optionalIf int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given. Changed in version 1.4.0: np.random.Generator objects now accepted Returns: Series or DataFrameA new object of same type as caller containing items randomly sampled within each group from the caller object.","['>>> df = pd.DataFrame(\n...     {""a"": [""red""] * 2 + [""blue""] * 2 + [""black""] * 2, ""b"": range(6)}\n... )\n>>> df\n       a  b\n0    red  0\n1    red  1\n2   blue  2\n3   blue  3\n4  black  4\n5  black  5', '>>> df.groupby(""a"").sample(n=1, random_state=1)\n       a  b\n4  black  4\n2   blue  2\n1    red  1', '>>> df.groupby(""a"")[""b""].sample(frac=0.5, random_state=2)\n5    5\n2    2\n0    0\nName: b, dtype: int64', '>>> df.groupby(""a"").sample(\n...     n=1,\n...     weights=[1, 1, 1, 0, 0, 1],\n...     random_state=1,\n... )\n       a  b\n5  black  5\n2   blue  2\n0    red  0']"
1406,..\pandas\reference\api\pandas.DataFrame.info.html,pandas.DataFrame.info,"DataFrame.info(verbose=None, buf=None, max_cols=None, memory_usage=None, show_counts=None)[source]# Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.","Parameters: verbosebool, optionalWhether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed. bufwritable buffer, defaults to sys.stdoutWhere to send the output. By default, the output is printed to sys.stdout. Pass a writable buffer if you need to further process the output. max_colsint, optionalWhen to switch from the verbose to the truncated output. If the DataFrame has more than max_cols columns, the truncated output is used. By default, the setting in pandas.options.display.max_info_columns is used. memory_usagebool, str, optionalSpecifies whether total memory usage of the DataFrame elements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting. True always show memory usage. False never shows memory usage. A value of ‘deep’ is equivalent to “True with deep introspection”. Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources. See the Frequently Asked Questions for more details. show_countsbool, optionalWhether to show the non-null counts. By default, this is shown only if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns. A value of True always shows the counts, and False never shows the counts. Returns: NoneThis method prints a summary of a DataFrame and returns None.","['>>> int_values = [1, 2, 3, 4, 5]\n>>> text_values = [\'alpha\', \'beta\', \'gamma\', \'delta\', \'epsilon\']\n>>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n>>> df = pd.DataFrame({""int_col"": int_values, ""text_col"": text_values,\n...                   ""float_col"": float_values})\n>>> df\n    int_col text_col  float_col\n0        1    alpha       0.00\n1        2     beta       0.25\n2        3    gamma       0.50\n3        4    delta       0.75\n4        5  epsilon       1.00', "">>> df.info(verbose=True)\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype\n---  ------     --------------  -----\n 0   int_col    5 non-null      int64\n 1   text_col   5 non-null      object\n 2   float_col  5 non-null      float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 248.0+ bytes"", "">>> df.info(verbose=False)\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nColumns: 3 entries, int_col to float_col\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 248.0+ bytes"", '>>> import io\n>>> buffer = io.StringIO()\n>>> df.info(buf=buffer)\n>>> s = buffer.getvalue()\n>>> with open(""df_info.txt"", ""w"",\n...           encoding=""utf-8"") as f:  \n...     f.write(s)\n260', "">>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6)\n>>> df = pd.DataFrame({\n...     'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n...     'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6),\n...     'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6)\n... })\n>>> df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 3 columns):\n #   Column    Non-Null Count    Dtype\n---  ------    --------------    -----\n 0   column_1  1000000 non-null  object\n 1   column_2  1000000 non-null  object\n 2   column_3  1000000 non-null  object\ndtypes: object(3)\nmemory usage: 22.9+ MB"", "">>> df.info(memory_usage='deep')\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 3 columns):\n #   Column    Non-Null Count    Dtype\n---  ------    --------------    -----\n 0   column_1  1000000 non-null  object\n 1   column_2  1000000 non-null  object\n 2   column_3  1000000 non-null  object\ndtypes: object(3)\nmemory usage: 165.9 MB""]"
1407,..\pandas\reference\api\pandas.Index.all.html,pandas.Index.all,"Index.all(*args, **kwargs)[source]# Return whether all elements are Truthy. Notes Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero.",Parameters: *argsRequired for compatibility with numpy. **kwargsRequired for compatibility with numpy. Returns: bool or array-like (if axis is specified)A single element array-like may be converted to bool.,"['>>> pd.Index([1, 2, 3]).all()\nTrue', '>>> pd.Index([0, 1, 2]).all()\nFalse']"
1408,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.rule_code.html,pandas.tseries.offsets.BQuarterBegin.rule_code,BQuarterBegin.rule_code#,No parameters found,[]
1409,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.kwds.html,pandas.tseries.offsets.SemiMonthBegin.kwds,SemiMonthBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1410,..\pandas\reference\api\pandas.Series.replace.html,pandas.Series.replace,"Series.replace(to_replace=None, value=<no_default>, *, inplace=False, limit=None, regex=False, method=<no_default>)[source]# Replace values given in to_replace with value. Values of the Series/DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value. Notes Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.","Parameters: to_replacestr, regex, list, dict, Series, int, float, or NoneHow to find the values that will be replaced. numeric, str or regex: numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value list of str, regex, or numeric: First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn’t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above. dict: Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value ‘a’ with ‘b’ and ‘y’ with ‘z’. To use a dict in this way, the optional value parameter should not be given. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column ‘a’ and the value ‘z’ in column ‘b’ and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column ‘a’ for the value ‘b’ and replace it with NaN. The optional value parameter should not be specified to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions. None: This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series. See the examples section for examples of each of these. valuescalar, dict, list, str, regex, default NoneValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed. inplacebool, default FalseIf True, performs operation inplace and returns None. limitint, default NoneMaximum size gap to forward or backward fill. Deprecated since version 2.1.0. regexbool or same types as to_replace, default FalseWhether to interpret to_replace and/or value as regular expressions. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None. method{‘pad’, ‘ffill’, ‘bfill’}The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None. Deprecated since version 2.1.0. Returns: Series/DataFrameObject after replacement. Raises: AssertionError If regex is not a bool and to_replace is not None. TypeError If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced ValueError If a list or an ndarray is passed to to_replace and value but they are not the same length.","['>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64', "">>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e"", '>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e', '>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e', "">>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64"", '>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e', "">>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e"", "">>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e"", "">>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz"", "">>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz"", "">>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz"", "">>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz"", "">>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz"", "">>> s = pd.Series([10, 'a', 'a', 'b', 'a'])"", "">>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object"", "">>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object"", "">>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object"", "">>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': ['a', 'b', 'c', 'd', 'e'],\n...                    'C': ['f', 'g', 'h', 'i', 'j']})"", "">>> df.replace(to_replace='^[a-g]', value='e', regex=True)\n    A  B  C\n0  0  e  e\n1  1  e  e\n2  2  e  h\n3  3  e  i\n4  4  e  j"", "">>> df.replace(to_replace={'B': '^[a-c]', 'C': '^[h-j]'}, value='e', regex=True)\n    A  B  C\n0  0  e  f\n1  1  e  g\n2  2  e  e\n3  3  d  e\n4  4  e  e""]"
1411,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.sem.html,pandas.core.groupby.DataFrameGroupBy.sem,"DataFrameGroupBy.sem(ddof=1, numeric_only=False)[source]# Compute standard error of the mean of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameStandard error of the mean of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([5, 10, 8, 14], index=lst)\n>>> ser\na     5\na    10\nb     8\nb    14\ndtype: int64\n>>> ser.groupby(level=0).sem()\na    2.5\nb    3.0\ndtype: float64"", '>>> data = [[1, 12, 11], [1, 15, 2], [2, 5, 8], [2, 6, 12]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a   b   c\n    tuna   1  12  11\n  salmon   1  15   2\n catfish   2   5   8\ngoldfish   2   6  12\n>>> df.groupby(""a"").sem()\n      b  c\na\n1    1.5  4.5\n2    0.5  2.0', "">>> ser = pd.Series([1, 3, 2, 4, 3, 8],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').sem()\n2023-01-01    0.577350\n2023-02-01    1.527525\nFreq: MS, dtype: float64""]"
1412,..\pandas\reference\api\pandas.tseries.offsets.Easter.html,pandas.tseries.offsets.Easter,class pandas.tseries.offsets.Easter# DateOffset for the Easter holiday using logic defined in dateutil. Right now uses the revised method which is valid in years 1583-4099. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of years represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.Easter()\nTimestamp('2022-04-17 00:00:00')""]"
1413,..\pandas\reference\api\pandas.PeriodIndex.html,pandas.PeriodIndex,"class pandas.PeriodIndex(data=None, ordinal=None, freq=None, dtype=None, copy=False, name=None, **fields)[source]# Immutable ndarray holding ordinal values indicating regular periods in time. Index keys are boxed to Period objects which carries the metadata (eg, frequency information). Attributes day The days of the period. dayofweek The day of the week with Monday=0, Sunday=6. day_of_week The day of the week with Monday=0, Sunday=6. dayofyear The ordinal day of the year. day_of_year The ordinal day of the year. days_in_month The number of days in the month. daysinmonth The number of days in the month. end_time Get the Timestamp for the end of the period. freqstr Return the frequency object as a string if it's set, otherwise None. hour The hour of the period. is_leap_year Logical indicating if the date belongs to a leap year. minute The minute of the period. month The month as January=1, December=12. quarter The quarter of the date. second The second of the period. start_time Get the Timestamp for the start of the period. week The week ordinal of the year. weekday The day of the week with Monday=0, Sunday=6. weekofyear The week ordinal of the year. year The year of the period. freq qyear Methods asfreq([freq, how]) Convert the PeriodArray to the specified frequency freq. strftime(*args, **kwargs) Convert to Index using specified date_format. to_timestamp([freq, how]) Cast to DatetimeArray/Index. from_fields from_ordinals","Parameters: dataarray-like (1d int np.ndarray or PeriodArray), optionalOptional period-like data to construct index with. copyboolMake a copy of input ndarray. freqstr or period object, optionalOne of pandas period strings or corresponding objects. yearint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. monthint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. quarterint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. dayint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. hourint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. minuteint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. secondint, array, or Series, default None Deprecated since version 2.2.0: Use PeriodIndex.from_fields instead. dtypestr or PeriodDtype, default None","["">>> idx = pd.PeriodIndex.from_fields(year=[2000, 2002], quarter=[1, 3])\n>>> idx\nPeriodIndex(['2000Q1', '2002Q3'], dtype='period[Q-DEC]')""]"
1414,..\pandas\reference\api\pandas.DataFrame.insert.html,pandas.DataFrame.insert,"DataFrame.insert(loc, column, value, allow_duplicates=<no_default>)[source]# Insert column into DataFrame at specified location. Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True.","Parameters: locintInsertion index. Must verify 0 <= loc <= len(columns). columnstr, number, or hashable objectLabel of the inserted column. valueScalar, Series, or array-likeContent of the inserted column. allow_duplicatesbool, optional, default lib.no_defaultAllow duplicate column labels to be created.","['>>> df = pd.DataFrame({\'col1\': [1, 2], \'col2\': [3, 4]})\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n>>> df.insert(1, ""newcol"", [99, 99])\n>>> df\n   col1  newcol  col2\n0     1      99     3\n1     2      99     4\n>>> df.insert(0, ""col1"", [100, 100], allow_duplicates=True)\n>>> df\n   col1  col1  newcol  col2\n0   100     1      99     3\n1   100     2      99     4', '>>> df.insert(0, ""col0"", pd.Series([5, 6], index=[1, 2]))\n>>> df\n   col0  col1  col1  newcol  col2\n0   NaN   100     1      99     3\n1   5.0   100     2      99     4']"
1415,..\pandas\reference\api\pandas.Index.any.html,pandas.Index.any,"Index.any(*args, **kwargs)[source]# Return whether any element is Truthy. Notes Not a Number (NaN), positive infinity and negative infinity evaluate to True because these are not equal to zero.",Parameters: *argsRequired for compatibility with numpy. **kwargsRequired for compatibility with numpy. Returns: bool or array-like (if axis is specified)A single element array-like may be converted to bool.,"['>>> index = pd.Index([0, 1, 2])\n>>> index.any()\nTrue', '>>> index = pd.Index([0, 0, 0])\n>>> index.any()\nFalse']"
1416,..\pandas\reference\api\pandas.PeriodIndex.is_leap_year.html,pandas.PeriodIndex.is_leap_year,property PeriodIndex.is_leap_year[source]# Logical indicating if the date belongs to a leap year.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023"", ""2024"", ""2025""], freq=""Y"")\n>>> idx.is_leap_year\narray([False,  True, False])']"
1417,..\pandas\reference\api\pandas.tseries.offsets.BQuarterBegin.startingMonth.html,pandas.tseries.offsets.BQuarterBegin.startingMonth,BQuarterBegin.startingMonth#,No parameters found,[]
1418,..\pandas\reference\api\pandas.DataFrame.interpolate.html,pandas.DataFrame.interpolate,"DataFrame.interpolate(method='linear', *, axis=0, limit=None, inplace=False, limit_direction=None, limit_area=None, downcast=<no_default>, **kwargs)[source]# Fill NaN values using an interpolation method. Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex. Notes The ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’ and ‘akima’ methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation.","Parameters: methodstr, default ‘linear’Interpolation technique to use. One of: ‘linear’: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. ‘time’: Works on daily and higher resolution data to interpolate given length of interval. ‘index’, ‘values’: use the actual numerical values of the index. ‘pad’: Fill in NaNs using existing values. ‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘barycentric’, ‘polynomial’: Passed to scipy.interpolate.interp1d, whereas ‘spline’ is passed to scipy.interpolate.UnivariateSpline. These methods use the numerical values of the index.  Both ‘polynomial’ and ‘spline’ require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5). Note that, slinear method in Pandas refers to the Scipy first order spline instead of Pandas first order spline. ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’, ‘akima’, ‘cubicspline’: Wrappers around the SciPy interpolation methods of similar names. See Notes. ‘from_derivatives’: Refers to scipy.interpolate.BPoly.from_derivatives. axis{{0 or ‘index’, 1 or ‘columns’, None}}, default NoneAxis to interpolate along. For Series this parameter is unused and defaults to 0. limitint, optionalMaximum number of consecutive NaNs to fill. Must be greater than 0. inplacebool, default FalseUpdate the data in place if possible. limit_direction{{‘forward’, ‘backward’, ‘both’}}, OptionalConsecutive NaNs will be filled in this direction. If limit is specified: If ‘method’ is ‘pad’ or ‘ffill’, ‘limit_direction’ must be ‘forward’. If ‘method’ is ‘backfill’ or ‘bfill’, ‘limit_direction’ must be ‘backwards’. If ‘limit’ is not specified: If ‘method’ is ‘backfill’ or ‘bfill’, the default is ‘backward’ else the default is ‘forward’ raises ValueError if limit_direction is ‘forward’ or ‘both’ andmethod is ‘backfill’ or ‘bfill’. raises ValueError if limit_direction is ‘backward’ or ‘both’ andmethod is ‘pad’ or ‘ffill’. limit_area{{None, ‘inside’, ‘outside’}}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). downcastoptional, ‘infer’ or None, defaults to NoneDowncast dtypes if possible. Deprecated since version 2.1.0. ``**kwargs``optionalKeyword arguments to pass on to the interpolating function. Returns: Series or DataFrame or NoneReturns the same object type as the caller, interpolated at some or all NaN values or None if inplace=True.","['>>> s = pd.Series([0, 1, np.nan, 3])\n>>> s\n0    0.0\n1    1.0\n2    NaN\n3    3.0\ndtype: float64\n>>> s.interpolate()\n0    0.0\n1    1.0\n2    2.0\n3    3.0\ndtype: float64', "">>> s = pd.Series([0, 2, np.nan, 8])\n>>> s.interpolate(method='polynomial', order=2)\n0    0.000000\n1    2.000000\n2    4.666667\n3    8.000000\ndtype: float64"", "">>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0),\n...                    (np.nan, 2.0, np.nan, np.nan),\n...                    (2.0, 3.0, np.nan, 9.0),\n...                    (np.nan, 4.0, -4.0, 16.0)],\n...                   columns=list('abcd'))\n>>> df\n     a    b    c     d\n0  0.0  NaN -1.0   1.0\n1  NaN  2.0  NaN   NaN\n2  2.0  3.0  NaN   9.0\n3  NaN  4.0 -4.0  16.0\n>>> df.interpolate(method='linear', limit_direction='forward', axis=0)\n     a    b    c     d\n0  0.0  NaN -1.0   1.0\n1  1.0  2.0 -2.0   5.0\n2  2.0  3.0 -3.0   9.0\n3  2.0  4.0 -4.0  16.0"", "">>> df['d'].interpolate(method='polynomial', order=2)\n0     1.0\n1     4.0\n2     9.0\n3    16.0\nName: d, dtype: float64""]"
1419,..\pandas\reference\api\pandas.Series.resample.html,pandas.Series.resample,"Series.resample(rule, axis=<no_default>, closed=None, label=None, convention=<no_default>, kind=<no_default>, on=None, level=None, origin='start_day', offset=None, group_keys=False)[source]# Resample time-series data. Convenience method for frequency conversion and resampling of time series. The object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or the caller must pass the label of a datetime-like series/index to the on/level keyword parameter. Notes See the user guide for more. To learn more about the offset strings, please see this link.","Parameters: ruleDateOffset, Timedelta or strThe offset string or object representing target conversion. axis{0 or ‘index’, 1 or ‘columns’}, default 0Which axis to use for up- or down-sampling. For Series this parameter is unused and defaults to 0. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex. Deprecated since version 2.0.0: Use frame.T.resample(…) instead. closed{‘right’, ‘left’}, default NoneWhich side of bin interval is closed. The default is ‘left’ for all frequency offsets except for ‘ME’, ‘YE’, ‘QE’, ‘BME’, ‘BA’, ‘BQE’, and ‘W’ which all have a default of ‘right’. label{‘right’, ‘left’}, default NoneWhich bin edge label to label bucket with. The default is ‘left’ for all frequency offsets except for ‘ME’, ‘YE’, ‘QE’, ‘BME’, ‘BA’, ‘BQE’, and ‘W’ which all have a default of ‘right’. convention{‘start’, ‘end’, ‘s’, ‘e’}, default ‘start’For PeriodIndex only, controls whether to use the start or end of rule. Deprecated since version 2.2.0: Convert PeriodIndex to DatetimeIndex before resampling instead. kind{‘timestamp’, ‘period’}, optional, default NonePass ‘timestamp’ to convert the resulting index to a DateTimeIndex or ‘period’ to convert it to a PeriodIndex. By default the input representation is retained. Deprecated since version 2.2.0: Convert index to desired type explicitly instead. onstr, optionalFor a DataFrame, column to use instead of index for resampling. Column must be datetime-like. levelstr or int, optionalFor a MultiIndex, level (name or number) to use for resampling. level must be datetime-like. originTimestamp or str, default ‘start_day’The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If string, must be one of the following: ‘epoch’: origin is 1970-01-01 ‘start’: origin is the first value of the timeseries ‘start_day’: origin is the first day at midnight of the timeseries ‘end’: origin is the last value of the timeseries ‘end_day’: origin is the ceiling midnight of the last day Added in version 1.3.0. Note Only takes effect for Tick-frequencies (i.e. fixed frequencies like days, hours, and minutes, rather than months or quarters). offsetTimedelta or str, default is NoneAn offset timedelta added to the origin. group_keysbool, default FalseWhether to include the group keys in the result index when using .apply() on the resampled object. Added in version 1.5.0: Not specifying group_keys will retain values-dependent behavior from pandas 1.4 and earlier (see pandas 1.5.0 Release notes for examples). Changed in version 2.0.0: group_keys now defaults to False. Returns: pandas.api.typing.ResamplerResampler object.","["">>> index = pd.date_range('1/1/2000', periods=9, freq='min')\n>>> series = pd.Series(range(9), index=index)\n>>> series\n2000-01-01 00:00:00    0\n2000-01-01 00:01:00    1\n2000-01-01 00:02:00    2\n2000-01-01 00:03:00    3\n2000-01-01 00:04:00    4\n2000-01-01 00:05:00    5\n2000-01-01 00:06:00    6\n2000-01-01 00:07:00    7\n2000-01-01 00:08:00    8\nFreq: min, dtype: int64"", "">>> series.resample('3min').sum()\n2000-01-01 00:00:00     3\n2000-01-01 00:03:00    12\n2000-01-01 00:06:00    21\nFreq: 3min, dtype: int64"", "">>> series.resample('3min', label='right').sum()\n2000-01-01 00:03:00     3\n2000-01-01 00:06:00    12\n2000-01-01 00:09:00    21\nFreq: 3min, dtype: int64"", "">>> series.resample('3min', label='right', closed='right').sum()\n2000-01-01 00:00:00     0\n2000-01-01 00:03:00     6\n2000-01-01 00:06:00    15\n2000-01-01 00:09:00    15\nFreq: 3min, dtype: int64"", "">>> series.resample('30s').asfreq()[0:5]   # Select first 5 rows\n2000-01-01 00:00:00   0.0\n2000-01-01 00:00:30   NaN\n2000-01-01 00:01:00   1.0\n2000-01-01 00:01:30   NaN\n2000-01-01 00:02:00   2.0\nFreq: 30s, dtype: float64"", "">>> series.resample('30s').ffill()[0:5]\n2000-01-01 00:00:00    0\n2000-01-01 00:00:30    0\n2000-01-01 00:01:00    1\n2000-01-01 00:01:30    1\n2000-01-01 00:02:00    2\nFreq: 30s, dtype: int64"", "">>> series.resample('30s').bfill()[0:5]\n2000-01-01 00:00:00    0\n2000-01-01 00:00:30    1\n2000-01-01 00:01:00    1\n2000-01-01 00:01:30    2\n2000-01-01 00:02:00    2\nFreq: 30s, dtype: int64"", "">>> def custom_resampler(arraylike):\n...     return np.sum(arraylike) + 5\n...\n>>> series.resample('3min').apply(custom_resampler)\n2000-01-01 00:00:00     8\n2000-01-01 00:03:00    17\n2000-01-01 00:06:00    26\nFreq: 3min, dtype: int64"", "">>> d = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n...      'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n>>> df = pd.DataFrame(d)\n>>> df['week_starting'] = pd.date_range('01/01/2018',\n...                                     periods=8,\n...                                     freq='W')\n>>> df\n   price  volume week_starting\n0     10      50    2018-01-07\n1     11      60    2018-01-14\n2      9      40    2018-01-21\n3     13     100    2018-01-28\n4     14      50    2018-02-04\n5     18     100    2018-02-11\n6     17      40    2018-02-18\n7     19      50    2018-02-25\n>>> df.resample('ME', on='week_starting').mean()\n               price  volume\nweek_starting\n2018-01-31     10.75    62.5\n2018-02-28     17.00    60.0"", "">>> days = pd.date_range('1/1/2000', periods=4, freq='D')\n>>> d2 = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n...       'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n>>> df2 = pd.DataFrame(\n...     d2,\n...     index=pd.MultiIndex.from_product(\n...         [days, ['morning', 'afternoon']]\n...     )\n... )\n>>> df2\n                      price  volume\n2000-01-01 morning       10      50\n           afternoon     11      60\n2000-01-02 morning        9      40\n           afternoon     13     100\n2000-01-03 morning       14      50\n           afternoon     18     100\n2000-01-04 morning       17      40\n           afternoon     19      50\n>>> df2.resample('D', level=0).sum()\n            price  volume\n2000-01-01     21     110\n2000-01-02     22     140\n2000-01-03     32     150\n2000-01-04     36      90"", "">>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n>>> rng = pd.date_range(start, end, freq='7min')\n>>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n>>> ts\n2000-10-01 23:30:00     0\n2000-10-01 23:37:00     3\n2000-10-01 23:44:00     6\n2000-10-01 23:51:00     9\n2000-10-01 23:58:00    12\n2000-10-02 00:05:00    15\n2000-10-02 00:12:00    18\n2000-10-02 00:19:00    21\n2000-10-02 00:26:00    24\nFreq: 7min, dtype: int64"", "">>> ts.resample('17min').sum()\n2000-10-01 23:14:00     0\n2000-10-01 23:31:00     9\n2000-10-01 23:48:00    21\n2000-10-02 00:05:00    54\n2000-10-02 00:22:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='epoch').sum()\n2000-10-01 23:18:00     0\n2000-10-01 23:35:00    18\n2000-10-01 23:52:00    27\n2000-10-02 00:09:00    39\n2000-10-02 00:26:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='2000-01-01').sum()\n2000-10-01 23:24:00     3\n2000-10-01 23:41:00    15\n2000-10-01 23:58:00    45\n2000-10-02 00:15:00    45\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='start').sum()\n2000-10-01 23:30:00     9\n2000-10-01 23:47:00    21\n2000-10-02 00:04:00    54\n2000-10-02 00:21:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', offset='23h30min').sum()\n2000-10-01 23:30:00     9\n2000-10-01 23:47:00    21\n2000-10-02 00:04:00    54\n2000-10-02 00:21:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='end').sum()\n2000-10-01 23:35:00     0\n2000-10-01 23:52:00    18\n2000-10-02 00:09:00    27\n2000-10-02 00:26:00    63\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='end_day').sum()\n2000-10-01 23:38:00     3\n2000-10-01 23:55:00    15\n2000-10-02 00:12:00    45\n2000-10-02 00:29:00    45\nFreq: 17min, dtype: int64""]"
1420,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_anchored.html,pandas.tseries.offsets.Easter.is_anchored,Easter.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1421,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.shift.html,pandas.core.groupby.DataFrameGroupBy.shift,"DataFrameGroupBy.shift(periods=1, freq=None, axis=<no_default>, fill_value=<no_default>, suffix=None)[source]# Shift each group by periods observations. If freq is passed, the index will be increased using the periods and the freq.","Parameters: periodsint | Sequence[int], default 1Number of periods to shift. If a list of values, shift each group by each period. freqstr, optionalFrequency string. axisaxis to shift, default 0Shift direction. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. fill_valueoptionalThe scalar value to use for newly introduced missing values. Changed in version 2.1.0: Will raise a ValueError if freq is provided too. suffixstr, optionalA string to add to each shifted column if there are multiple periods. Ignored otherwise. Returns: Series or DataFrameObject shifted within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).shift(1)\na    NaN\na    1.0\nb    NaN\nb    3.0\ndtype: float64"", '>>> data = [[1, 2, 3], [1, 5, 6], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tuna"", ""salmon"", ""catfish"", ""goldfish""])\n>>> df\n           a  b  c\n    tuna   1  2  3\n  salmon   1  5  6\n catfish   2  5  8\ngoldfish   2  6  9\n>>> df.groupby(""a"").shift(1)\n              b    c\n    tuna    NaN  NaN\n  salmon    2.0  3.0\n catfish    NaN  NaN\ngoldfish    5.0  8.0']"
1422,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.n.html,pandas.tseries.offsets.SemiMonthBegin.n,SemiMonthBegin.n#,No parameters found,[]
1423,..\pandas\reference\api\pandas.Index.append.html,pandas.Index.append,Index.append(other)[source]# Append a collection of Index options together.,Parameters: otherIndex or list/tuple of indices Returns: Index,"["">>> idx = pd.Index([1, 2, 3])\n>>> idx.append(pd.Index([4]))\nIndex([1, 2, 3, 4], dtype='int64')""]"
1424,..\pandas\reference\api\pandas.PeriodIndex.minute.html,pandas.PeriodIndex.minute,property PeriodIndex.minute[source]# The minute of the period.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-01 10:30:00"",\n...                       ""2023-01-01 11:50:00""], freq=\'min\')\n>>> idx.minute\nIndex([30, 50], dtype=\'int64\')']"
1425,..\pandas\reference\api\pandas.Index.argmax.html,pandas.Index.argmax,"Index.argmax(axis=None, skipna=True, *args, **kwargs)[source]# Return int position of the largest value in the Series. If the maximum is achieved in multiple locations, the first row position is returned.","Parameters: axis{None}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values when showing the result. *args, **kwargsAdditional arguments and keywords for compatibility with NumPy. Returns: intRow position of the maximum value.","["">>> s = pd.Series({'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0})\n>>> s\nCorn Flakes              100.0\nAlmond Delight           110.0\nCinnamon Toast Crunch    120.0\nCocoa Puff               110.0\ndtype: float64"", '>>> s.argmax()\n2\n>>> s.argmin()\n0']"
1426,..\pandas\reference\api\pandas.DataFrame.isin.html,pandas.DataFrame.isin,DataFrame.isin(values)[source]# Whether each element in the DataFrame is contained in values.,"Parameters: valuesiterable, Series, DataFrame or dictThe result will only be true at a location if all the labels match. If values is a Series, that’s the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match. Returns: DataFrameDataFrame of booleans showing whether each element in the DataFrame is contained in values.","["">>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n...                   index=['falcon', 'dog'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0"", '>>> df.isin([0, 2])\n        num_legs  num_wings\nfalcon      True       True\ndog        False       True', '>>> ~df.isin([0, 2])\n        num_legs  num_wings\nfalcon     False      False\ndog         True      False', "">>> df.isin({'num_wings': [0, 3]})\n        num_legs  num_wings\nfalcon     False      False\ndog        False       True"", "">>> other = pd.DataFrame({'num_legs': [8, 3], 'num_wings': [0, 2]},\n...                      index=['spider', 'falcon'])\n>>> df.isin(other)\n        num_legs  num_wings\nfalcon     False       True\ndog        False      False""]"
1427,..\pandas\reference\api\pandas.Series.reset_index.html,pandas.Series.reset_index,"Series.reset_index(level=None, *, drop=False, name=<no_default>, inplace=False, allow_duplicates=False)[source]# Generate a new DataFrame or Series with the index reset. This is useful when the index needs to be treated as a column, or when the index is meaningless and needs to be reset to the default before another operation.","Parameters: levelint, str, tuple, or list, default optionalFor a Series with a MultiIndex, only remove the specified levels from the index. Removes all levels by default. dropbool, default FalseJust reset the index, without inserting it as a column in the new DataFrame. nameobject, optionalThe name to use for the column containing the original Series values. Uses self.name by default. This argument is ignored when drop is True. inplacebool, default FalseModify the Series in place (do not create a new object). allow_duplicatesbool, default FalseAllow duplicate column labels to be created. Added in version 1.5.0. Returns: Series or DataFrame or NoneWhen drop is False (the default), a DataFrame is returned. The newly created columns will come first in the DataFrame, followed by the original Series values. When drop is True, a Series is returned. In either case, if inplace=True, no value is returned.","["">>> s = pd.Series([1, 2, 3, 4], name='foo',\n...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))"", '>>> s.reset_index()\n  idx  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4', "">>> s.reset_index(name='values')\n  idx  values\n0   a       1\n1   b       2\n2   c       3\n3   d       4"", '>>> s.reset_index(drop=True)\n0    1\n1    2\n2    3\n3    4\nName: foo, dtype: int64', "">>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n...           np.array(['one', 'two', 'one', 'two'])]\n>>> s2 = pd.Series(\n...     range(4), name='foo',\n...     index=pd.MultiIndex.from_arrays(arrays,\n...                                     names=['a', 'b']))"", "">>> s2.reset_index(level='a')\n       a  foo\nb\none  bar    0\ntwo  bar    1\none  baz    2\ntwo  baz    3"", '>>> s2.reset_index()\n     a    b  foo\n0  bar  one    0\n1  bar  two    1\n2  baz  one    2\n3  baz  two    3']"
1428,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.copy.html,pandas.tseries.offsets.BQuarterEnd.copy,BQuarterEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1429,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_month_end.html,pandas.tseries.offsets.Easter.is_month_end,Easter.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1430,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.name.html,pandas.tseries.offsets.SemiMonthBegin.name,SemiMonthBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1431,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.size.html,pandas.core.groupby.DataFrameGroupBy.size,DataFrameGroupBy.size()[source]# Compute group sizes.,Returns: DataFrame or SeriesNumber of rows in each group as a Series if as_index is True or a DataFrame if as_index is False.,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na     1\na     2\nb     3\ndtype: int64\n>>> ser.groupby(level=0).size()\na    2\nb    1\ndtype: int64"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(""a"").size()\na\n1    2\n7    1\ndtype: int64', "">>> ser = pd.Series([1, 2, 3], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\ndtype: int64\n>>> ser.resample('MS').size()\n2023-01-01    2\n2023-02-01    1\nFreq: MS, dtype: int64""]"
1432,..\pandas\reference\api\pandas.PeriodIndex.month.html,pandas.PeriodIndex.month,"property PeriodIndex.month[source]# The month as January=1, December=12.",No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.month\nIndex([1, 2, 3], dtype=\'int64\')']"
1433,..\pandas\reference\api\pandas.Index.argmin.html,pandas.Index.argmin,"Index.argmin(axis=None, skipna=True, *args, **kwargs)[source]# Return int position of the smallest value in the Series. If the minimum is achieved in multiple locations, the first row position is returned.","Parameters: axis{None}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values when showing the result. *args, **kwargsAdditional arguments and keywords for compatibility with NumPy. Returns: intRow position of the minimum value.","["">>> s = pd.Series({'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0})\n>>> s\nCorn Flakes              100.0\nAlmond Delight           110.0\nCinnamon Toast Crunch    120.0\nCocoa Puff               110.0\ndtype: float64"", '>>> s.argmax()\n2\n>>> s.argmin()\n0']"
1434,..\pandas\reference\api\pandas.DataFrame.isna.html,pandas.DataFrame.isna,"DataFrame.isna()[source]# Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).",Returns: DataFrameMask of bool values for each element in DataFrame that indicates whether an element is an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool']"
1435,..\pandas\reference\api\pandas.PeriodIndex.quarter.html,pandas.PeriodIndex.quarter,property PeriodIndex.quarter[source]# The quarter of the date.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.quarter\nIndex([1, 1, 1], dtype=\'int64\')']"
1436,..\pandas\reference\api\pandas.Series.rfloordiv.html,pandas.Series.rfloordiv,"Series.rfloordiv(other, level=None, fill_value=None, axis=0)[source]# Return Integer division of series and other, element-wise (binary operator rfloordiv). Equivalent to other // series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.floordiv(b, fill_value=0)\na    1.0\nb    inf\nc    inf\nd    0.0\ne    NaN\ndtype: float64""]"
1437,..\pandas\reference\api\pandas.Index.argsort.html,pandas.Index.argsort,"Index.argsort(*args, **kwargs)[source]# Return the integer indices that would sort the index.",Parameters: *argsPassed to numpy.ndarray.argsort. **kwargsPassed to numpy.ndarray.argsort. Returns: np.ndarray[np.intp]Integer indices that would sort the index if used as an indexer.,"["">>> idx = pd.Index(['b', 'a', 'd', 'c'])\n>>> idx\nIndex(['b', 'a', 'd', 'c'], dtype='object')"", '>>> order = idx.argsort()\n>>> order\narray([1, 0, 3, 2])', "">>> idx[order]\nIndex(['a', 'b', 'c', 'd'], dtype='object')""]"
1438,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.nanos.html,pandas.tseries.offsets.SemiMonthBegin.nanos,SemiMonthBegin.nanos#,No parameters found,[]
1439,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.freqstr.html,pandas.tseries.offsets.BQuarterEnd.freqstr,BQuarterEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1440,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.skew.html,pandas.core.groupby.DataFrameGroupBy.skew,"DataFrameGroupBy.skew(axis=<no_default>, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased skew within groups. Normalized by N-1.","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Axis for the function to be applied on. Specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. **kwargsAdditional keyword arguments to be passed to the function. Returns: DataFrame","['>>> arrays = [[\'falcon\', \'parrot\', \'cockatoo\', \'kiwi\',\n...            \'lion\', \'monkey\', \'rabbit\'],\n...           [\'bird\', \'bird\', \'bird\', \'bird\',\n...            \'mammal\', \'mammal\', \'mammal\']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=(\'name\', \'class\'))\n>>> df = pd.DataFrame({\'max_speed\': [389.0, 24.0, 70.0, np.nan,\n...                                  80.5, 21.5, 15.0]},\n...                   index=index)\n>>> df\n                max_speed\nname     class\nfalcon   bird        389.0\nparrot   bird         24.0\ncockatoo bird         70.0\nkiwi     bird          NaN\nlion     mammal       80.5\nmonkey   mammal       21.5\nrabbit   mammal       15.0\n>>> gb = df.groupby([""class""])\n>>> gb.skew()\n        max_speed\nclass\nbird     1.628296\nmammal   1.669046\n>>> gb.skew(skipna=False)\n        max_speed\nclass\nbird          NaN\nmammal   1.669046']"
1441,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_month_start.html,pandas.tseries.offsets.Easter.is_month_start,Easter.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1442,..\pandas\reference\api\pandas.DataFrame.isnull.html,pandas.DataFrame.isnull,"DataFrame.isnull()[source]# DataFrame.isnull is an alias for DataFrame.isna. Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).",Returns: DataFrameMask of bool values for each element in DataFrame that indicates whether an element is an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool']"
1443,..\pandas\reference\api\pandas.PeriodIndex.qyear.html,pandas.PeriodIndex.qyear,property PeriodIndex.qyear[source]#,No parameters found,[]
1444,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.std.html,pandas.core.groupby.DataFrameGroupBy.std,"DataFrameGroupBy.std(ddof=1, engine=None, engine_kwargs=None, numeric_only=False)[source]# Compute standard deviation of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}} Added in version 1.4.0. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameStandard deviation of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).std()\na    3.21455\nb    0.57735\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).std()\n              a         b\ndog    2.000000  3.511885\nmouse  2.217356  1.500000""]"
1445,..\pandas\reference\api\pandas.Index.asof.html,pandas.Index.asof,"final Index.asof(label)[source]# Return the label from the index, or, if not present, the previous one. Assuming that the index is sorted, return the passed index label if it is in the index, or return the previous index label if the passed one is not in the index.",Parameters: labelobjectThe label up to which the method returns the latest index label. Returns: objectThe passed label if it is in the index. The previous label if the passed label is not in the sorted index or NaN if there is no such label.,"["">>> idx = pd.Index(['2013-12-31', '2014-01-02', '2014-01-03'])\n>>> idx.asof('2014-01-01')\n'2013-12-31'"", "">>> idx.asof('2014-01-02')\n'2014-01-02'"", "">>> idx.asof('1999-01-02')\nnan"", "">>> idx_not_sorted = pd.Index(['2013-12-31', '2015-01-02',\n...                            '2014-01-03'])\n>>> idx_not_sorted.asof('2013-12-31')\nTraceback (most recent call last):\nValueError: index must be monotonic increasing or decreasing""]"
1446,..\pandas\reference\api\pandas.Series.rmod.html,pandas.Series.rmod,"Series.rmod(other, level=None, fill_value=None, axis=0)[source]# Return Modulo of series and other, element-wise (binary operator rmod). Equivalent to other % series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.mod(b, fill_value=0)\na    0.0\nb    NaN\nc    NaN\nd    0.0\ne    NaN\ndtype: float64""]"
1447,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.normalize.html,pandas.tseries.offsets.SemiMonthBegin.normalize,SemiMonthBegin.normalize#,No parameters found,[]
1448,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.html,pandas.tseries.offsets.BQuarterEnd,"class pandas.tseries.offsets.BQuarterEnd# DateOffset increments between the last business day of each Quarter. startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, … startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, … startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, … Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code startingMonth","Parameters: nint, default 1The number of quarters represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. startingMonthint, default 3A specific integer for the month of the year from which we start quarters.","["">>> from pandas.tseries.offsets import BQuarterEnd\n>>> ts = pd.Timestamp('2020-05-24 05:01:15')\n>>> ts + BQuarterEnd()\nTimestamp('2020-06-30 05:01:15')\n>>> ts + BQuarterEnd(2)\nTimestamp('2020-09-30 05:01:15')\n>>> ts + BQuarterEnd(1, startingMonth=2)\nTimestamp('2020-05-29 05:01:15')\n>>> ts + BQuarterEnd(startingMonth=2)\nTimestamp('2020-05-29 05:01:15')""]"
1449,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_on_offset.html,pandas.tseries.offsets.Easter.is_on_offset,Easter.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1450,..\pandas\reference\api\pandas.DataFrame.items.html,pandas.DataFrame.items,"DataFrame.items()[source]# Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.","Yields: labelobjectThe column names for the DataFrame being iterated over. contentSeriesThe column entries belonging to each label, as a Series.","["">>> df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'],\n...                   'population': [1864, 22000, 80000]},\n...                   index=['panda', 'polar', 'koala'])\n>>> df\n        species   population\npanda   bear      1864\npolar   bear      22000\nkoala   marsupial 80000\n>>> for label, content in df.items():\n...     print(f'label: {label}')\n...     print(f'content: {content}', sep='\\n')\n...\nlabel: species\ncontent:\npanda         bear\npolar         bear\nkoala    marsupial\nName: species, dtype: object\nlabel: population\ncontent:\npanda     1864\npolar    22000\nkoala    80000\nName: population, dtype: int64""]"
1451,..\pandas\reference\api\pandas.PeriodIndex.second.html,pandas.PeriodIndex.second,property PeriodIndex.second[source]# The second of the period.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-01 10:00:30"",\n...                       ""2023-01-01 10:00:31""], freq=\'s\')\n>>> idx.second\nIndex([30, 31], dtype=\'int64\')']"
1452,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_quarter_end.html,pandas.tseries.offsets.Easter.is_quarter_end,Easter.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1453,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_anchored.html,pandas.tseries.offsets.BQuarterEnd.is_anchored,BQuarterEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1454,..\pandas\reference\api\pandas.Index.asof_locs.html,pandas.Index.asof_locs,"Index.asof_locs(where, mask)[source]# Return the locations (indices) of labels in the index. As in the pandas.Index.asof(), if the label (a particular entry in where) is not in the index, the latest index label up to the passed label is chosen and its index returned. If all of the labels in the index are later than a label in where, -1 is returned. mask is used to ignore NA values in the index during calculation.",Parameters: whereIndexAn Index consisting of an array of timestamps. masknp.ndarray[bool]Array of booleans denoting where values in the original data are not NA. Returns: np.ndarray[np.intp]An array of locations (indices) of the labels from the index which correspond to the return values of pandas.Index.asof() for every element in where.,"["">>> idx = pd.date_range('2023-06-01', periods=3, freq='D')\n>>> where = pd.DatetimeIndex(['2023-05-30 00:12:00', '2023-06-01 00:00:00',\n...                           '2023-06-02 23:59:59'])\n>>> mask = np.ones(3, dtype=bool)\n>>> idx.asof_locs(where, mask)\narray([-1,  0,  1])"", '>>> mask[1] = False\n>>> idx.asof_locs(where, mask)\narray([-1,  0,  0])']"
1455,..\pandas\reference\api\pandas.Series.rmul.html,pandas.Series.rmul,"Series.rmul(other, level=None, fill_value=None, axis=0)[source]# Return Multiplication of series and other, element-wise (binary operator rmul). Equivalent to other * series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.multiply(b, fill_value=0)\na    1.0\nb    0.0\nc    0.0\nd    0.0\ne    NaN\ndtype: float64""]"
1456,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthBegin.rule_code.html,pandas.tseries.offsets.SemiMonthBegin.rule_code,SemiMonthBegin.rule_code#,No parameters found,[]
1457,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.sum.html,pandas.core.groupby.DataFrameGroupBy.sum,"DataFrameGroupBy.sum(numeric_only=False, min_count=0, engine=None, engine_kwargs=None)[source]# Compute sum of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. enginestr, default None None 'cython' : Runs rolling apply through C-extensions from cython. 'numba'Runs rolling apply through JIT compiled code from numba.Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogiland parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply groupby aggregation. Returns: Series or DataFrameComputed sum of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).sum()\na    3\nb    7\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").sum()\n     b   c\na\n1   10   7\n2   11  17']"
1458,..\pandas\reference\api\pandas.DataFrame.iterrows.html,pandas.DataFrame.iterrows,"DataFrame.iterrows()[source]# Iterate over DataFrame rows as (index, Series) pairs. Notes Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally faster than iterrows. You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.",Yields: indexlabel or tuple of labelThe index of the row. A tuple for a MultiIndex. dataSeriesThe data of the row as a Series.,"["">>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])\n>>> row = next(df.iterrows())[1]\n>>> row\nint      1.0\nfloat    1.5\nName: 0, dtype: float64\n>>> print(row['int'].dtype)\nfloat64\n>>> print(df['int'].dtype)\nint64""]"
1459,..\pandas\reference\api\pandas.PeriodIndex.start_time.html,pandas.PeriodIndex.start_time,property PeriodIndex.start_time[source]# Get the Timestamp for the start of the period.,Returns: Timestamp,"["">>> period = pd.Period('2012-1-1', freq='D')\n>>> period\nPeriod('2012-01-01', 'D')"", "">>> period.start_time\nTimestamp('2012-01-01 00:00:00')"", "">>> period.end_time\nTimestamp('2012-01-01 23:59:59.999999999')""]"
1460,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.tail.html,pandas.core.groupby.DataFrameGroupBy.tail,"DataFrameGroupBy.tail(n=5)[source]# Return last n rows of each group. Similar to .apply(lambda x: x.tail(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).",Parameters: nintIf positive: number of entries to include from end of each group. If negative: number of entries to exclude from start of each group. Returns: Series or DataFrameSubset of original Series or DataFrame as determined by n.,"["">>> df = pd.DataFrame([['a', 1], ['a', 2], ['b', 1], ['b', 2]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').tail(1)\n   A  B\n1  a  2\n3  b  2\n>>> df.groupby('A').tail(-1)\n   A  B\n1  a  2\n3  b  2""]"
1461,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.copy.html,pandas.tseries.offsets.SemiMonthEnd.copy,SemiMonthEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1462,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_month_end.html,pandas.tseries.offsets.BQuarterEnd.is_month_end,BQuarterEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1463,..\pandas\reference\api\pandas.DataFrame.itertuples.html,pandas.DataFrame.itertuples,"DataFrame.itertuples(index=True, name='Pandas')[source]# Iterate over DataFrame rows as namedtuples. Notes The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore.","Parameters: indexbool, default TrueIf True, return the index as the first element of the tuple. namestr or None, default “Pandas”The name of the returned namedtuples or None to return regular tuples. Returns: iteratorAn object to iterate over namedtuples for each row in the DataFrame with the first field possibly being the index and following fields being the column values.","["">>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]},\n...                   index=['dog', 'hawk'])\n>>> df\n      num_legs  num_wings\ndog          4          0\nhawk         2          2\n>>> for row in df.itertuples():\n...     print(row)\n...\nPandas(Index='dog', num_legs=4, num_wings=0)\nPandas(Index='hawk', num_legs=2, num_wings=2)"", '>>> for row in df.itertuples(index=False):\n...     print(row)\n...\nPandas(num_legs=4, num_wings=0)\nPandas(num_legs=2, num_wings=2)', "">>> for row in df.itertuples(name='Animal'):\n...     print(row)\n...\nAnimal(Index='dog', num_legs=4, num_wings=0)\nAnimal(Index='hawk', num_legs=2, num_wings=2)""]"
1464,..\pandas\reference\api\pandas.Index.astype.html,pandas.Index.astype,"Index.astype(dtype, copy=True)[source]# Create an Index with values cast to dtypes. The class of a new Index is determined by dtype. When conversion is impossible, a TypeError exception is raised.","Parameters: dtypenumpy dtype or pandas typeNote that any signed integer dtype is treated as 'int64', and any unsigned integer dtype is treated as 'uint64', regardless of the size. copybool, default TrueBy default, astype always returns a newly allocated object. If copy is set to False and internal requirements on dtype are satisfied, the original data is used to create a new Index or the original Index is returned. Returns: IndexIndex with values cast to specified dtype.","["">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.astype('float')\nIndex([1.0, 2.0, 3.0], dtype='float64')""]"
1465,..\pandas\reference\api\pandas.Series.rolling.html,pandas.Series.rolling,"Series.rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=<no_default>, closed=None, step=None, method='single')[source]# Provide rolling window calculations. Notes See Windowing Operations for further usage details and examples.","Parameters: windowint, timedelta, str, offset, or BaseIndexer subclassSize of the moving window. If an integer, the fixed number of observations used for each window. If a timedelta, str, or offset, the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see this link. If a BaseIndexer subclass, the window boundaries based on the defined get_window_bounds method. Additional rolling keyword arguments, namely min_periods, center, closed and step will be passed to get_window_bounds. min_periodsint, default NoneMinimum number of observations in window required to have a value; otherwise, result is np.nan. For a window that is specified by an offset, min_periods will default to 1. For a window that is specified by an integer, min_periods will default to the size of the window. centerbool, default FalseIf False, set the window labels as the right edge of the window index. If True, set the window labels as the center of the window index. win_typestr, default NoneIf None, all points are evenly weighted. If a string, it must be a valid scipy.signal window function. Certain Scipy window types require additional parameters to be passed in the aggregation function. The additional parameters must match the keywords specified in the Scipy window type method signature. onstr, optionalFor a DataFrame, a column label or Index level on which to calculate the rolling window, rather than the DataFrame’s index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axisint or str, default 0If 0 or 'index', roll across the rows. If 1 or 'columns', roll across the columns. For Series this parameter is unused and defaults to 0. Deprecated since version 2.1.0: The axis keyword is deprecated. For axis=1, transpose the DataFrame first instead. closedstr, default NoneIf 'right', the first point in the window is excluded from calculations. If 'left', the last point in the window is excluded from calculations. If 'both', the no points in the window are excluded from calculations. If 'neither', the first and last points in the window are excluded from calculations. Default None ('right'). stepint, default None Added in version 1.5.0. Evaluate the window at every step result, equivalent to slicing as [::step]. window must be an integer. Using a step argument other than None or 1 will produce a result with a different shape than the input. methodstr {‘single’, ‘table’}, default ‘single’ Added in version 1.3.0. Execute the rolling operation per single column or row ('single') or over the entire object ('table'). This argument is only implemented when specifying engine='numba' in the method call. Returns: pandas.api.typing.Window or pandas.api.typing.RollingAn instance of Window is returned if win_type is passed. Otherwise, an instance of Rolling is returned.","["">>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n>>> df\n     B\n0  0.0\n1  1.0\n2  2.0\n3  NaN\n4  4.0"", '>>> df.rolling(2).sum()\n     B\n0  NaN\n1  1.0\n2  3.0\n3  NaN\n4  NaN', "">>> df_time = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n...                        index=[pd.Timestamp('20130101 09:00:00'),\n...                               pd.Timestamp('20130101 09:00:02'),\n...                               pd.Timestamp('20130101 09:00:03'),\n...                               pd.Timestamp('20130101 09:00:05'),\n...                               pd.Timestamp('20130101 09:00:06')])"", '>>> df_time\n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0', "">>> df_time.rolling('2s').sum()\n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  NaN\n2013-01-01 09:00:06  4.0"", '>>> indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2)\n>>> df.rolling(window=indexer, min_periods=1).sum()\n     B\n0  1.0\n1  3.0\n2  2.0\n3  4.0\n4  4.0', '>>> df.rolling(2, min_periods=1).sum()\n     B\n0  0.0\n1  1.0\n2  3.0\n3  2.0\n4  4.0', '>>> df.rolling(3, min_periods=1, center=True).sum()\n     B\n0  1.0\n1  3.0\n2  3.0\n3  6.0\n4  4.0', '>>> df.rolling(3, min_periods=1, center=False).sum()\n     B\n0  0.0\n1  1.0\n2  3.0\n3  3.0\n4  6.0', '>>> df.rolling(2, min_periods=1, step=2).sum()\n     B\n0  0.0\n2  3.0\n4  4.0', "">>> df.rolling(2, win_type='gaussian').sum(std=3)\n          B\n0       NaN\n1  0.986207\n2  2.958621\n3       NaN\n4       NaN"", "">>> df = pd.DataFrame({\n...     'A': [pd.to_datetime('2020-01-01'),\n...           pd.to_datetime('2020-01-01'),\n...           pd.to_datetime('2020-01-02'),],\n...     'B': [1, 2, 3], },\n...     index=pd.date_range('2020', periods=3))"", '>>> df\n                    A  B\n2020-01-01 2020-01-01  1\n2020-01-02 2020-01-01  2\n2020-01-03 2020-01-02  3', "">>> df.rolling('2D', on='A').sum()\n                    A    B\n2020-01-01 2020-01-01  1.0\n2020-01-02 2020-01-01  3.0\n2020-01-03 2020-01-02  6.0""]"
1466,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_quarter_start.html,pandas.tseries.offsets.Easter.is_quarter_start,Easter.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1467,..\pandas\reference\api\pandas.PeriodIndex.strftime.html,pandas.PeriodIndex.strftime,"PeriodIndex.strftime(*args, **kwargs)[source]# Convert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in python string format doc. Formats supported by the C strftime API but not by the python string format doc (such as “%R”, “%r”) are not officially supported and should be preferably replaced with their supported equivalents (such as “%H:%M”, “%I:%M:%S %p”). Note that PeriodIndex support additional directives, detailed in Period.strftime.",Parameters: date_formatstrDate format string (e.g. “%Y-%m-%d”). Returns: ndarray[object]NumPy ndarray of formatted strings.,"['>>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),\n...                     periods=3, freq=\'s\')\n>>> rng.strftime(\'%B %d, %Y, %r\')\nIndex([\'March 10, 2018, 09:00:00 AM\', \'March 10, 2018, 09:00:01 AM\',\n       \'March 10, 2018, 09:00:02 AM\'],\n      dtype=\'object\')']"
1468,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.take.html,pandas.core.groupby.DataFrameGroupBy.take,"DataFrameGroupBy.take(indices, axis=<no_default>, **kwargs)[source]# Return the elements in the given positional indices in each group. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. If a requested index does not exist for some group, this method will raise. To get similar behavior that ignores indices that don’t exist, see DataFrameGroupBy.nth().","Parameters: indicesarray-likeAn array of ints indicating which positions to take. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns. Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. **kwargsFor compatibility with numpy.take(). Has no effect on the output. Returns: DataFrameAn DataFrame containing the elements taken from each group.","["">>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n...                    ('parrot', 'bird', 24.0),\n...                    ('lion', 'mammal', 80.5),\n...                    ('monkey', 'mammal', np.nan),\n...                    ('rabbit', 'mammal', 15.0)],\n...                   columns=['name', 'class', 'max_speed'],\n...                   index=[4, 3, 2, 1, 0])\n>>> df\n     name   class  max_speed\n4  falcon    bird      389.0\n3  parrot    bird       24.0\n2    lion  mammal       80.5\n1  monkey  mammal        NaN\n0  rabbit  mammal       15.0\n>>> gb = df.groupby([1, 1, 2, 2, 2])"", '>>> gb.take([0, 1])\n       name   class  max_speed\n1 4  falcon    bird      389.0\n  3  parrot    bird       24.0\n2 2    lion  mammal       80.5\n  1  monkey  mammal        NaN', '>>> gb.take([1, 0])\n       name   class  max_speed\n1 3  parrot    bird       24.0\n  4  falcon    bird      389.0\n2 1  monkey  mammal        NaN\n  2    lion  mammal       80.5', '>>> gb.take([-1, -2])\n       name   class  max_speed\n1 3  parrot    bird       24.0\n  4  falcon    bird      389.0\n2 0  rabbit  mammal       15.0\n  1  monkey  mammal        NaN']"
1469,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.day_of_month.html,pandas.tseries.offsets.SemiMonthEnd.day_of_month,SemiMonthEnd.day_of_month#,No parameters found,[]
1470,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_month_start.html,pandas.tseries.offsets.BQuarterEnd.is_month_start,BQuarterEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1471,..\pandas\reference\api\pandas.DataFrame.join.html,pandas.DataFrame.join,"DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False, validate=None)[source]# Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. Notes Parameters on, lsuffix, and rsuffix are not supported when passing a list of DataFrame objects.","Parameters: otherDataFrame, Series, or a list containing any combination of themIndex should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. onstr, list of str, or array-like, optionalColumn or index level name(s) in the caller to join on the index in other, otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘left’How to handle the operation of the two objects. left: use calling frame’s index (or column if on is specified) right: use other’s index. outer: form union of calling frame’s index (or column if on is specified) with other’s index, and sort it lexicographically. inner: form intersection of calling frame’s index (or column if on is specified) with other’s index, preserving the order of the calling’s one. cross: creates the cartesian product from both frames, preserves the order of the left keys. lsuffixstr, default ‘’Suffix to use from left frame’s overlapping columns. rsuffixstr, default ‘’Suffix to use from right frame’s overlapping columns. sortbool, default FalseOrder result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword). validatestr, optionalIf specified, checks if join is of specified type. “one_to_one” or “1:1”: check if join keys are unique in both left and right datasets. “one_to_many” or “1:m”: check if join keys are unique in left dataset. “many_to_one” or “m:1”: check if join keys are unique in right dataset. “many_to_many” or “m:m”: allowed, but does not result in checks. Added in version 1.5.0. Returns: DataFrameA dataframe containing columns from both the caller and other.","["">>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})"", '>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K2  A2\n3  K3  A3\n4  K4  A4\n5  K5  A5', "">>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n...                       'B': ['B0', 'B1', 'B2']})"", '>>> other\n  key   B\n0  K0  B0\n1  K1  B1\n2  K2  B2', "">>> df.join(other, lsuffix='_caller', rsuffix='_other')\n  key_caller   A key_other    B\n0         K0  A0        K0   B0\n1         K1  A1        K1   B1\n2         K2  A2        K2   B2\n3         K3  A3       NaN  NaN\n4         K4  A4       NaN  NaN\n5         K5  A5       NaN  NaN"", "">>> df.set_index('key').join(other.set_index('key'))\n      A    B\nkey\nK0   A0   B0\nK1   A1   B1\nK2   A2   B2\nK3   A3  NaN\nK4   A4  NaN\nK5   A5  NaN"", "">>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K2  A2   B2\n3  K3  A3  NaN\n4  K4  A4  NaN\n5  K5  A5  NaN"", "">>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})"", '>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K1  A2\n3  K3  A3\n4  K0  A4\n5  K1  A5', "">>> df.join(other.set_index('key'), on='key', validate='m:1')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K1  A2   B1\n3  K3  A3  NaN\n4  K0  A4   B0\n5  K1  A5   B1""]"
1472,..\pandas\reference\api\pandas.PeriodIndex.to_timestamp.html,pandas.PeriodIndex.to_timestamp,"PeriodIndex.to_timestamp(freq=None, how='start')[source]# Cast to DatetimeArray/Index.","Parameters: freqstr or DateOffset, optionalTarget frequency. The default is ‘D’ for week or longer, ‘s’ otherwise. how{‘s’, ‘e’, ‘start’, ‘end’}Whether to use the start or end of the time period being converted. Returns: DatetimeArray/Index","['>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.to_timestamp()\nDatetimeIndex([\'2023-01-01\', \'2023-02-01\', \'2023-03-01\'],\ndtype=\'datetime64[ns]\', freq=\'MS\')']"
1473,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_year_end.html,pandas.tseries.offsets.Easter.is_year_end,Easter.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1474,..\pandas\reference\api\pandas.Index.copy.html,pandas.Index.copy,"Index.copy(name=None, deep=False)[source]# Make a copy of this object. Name is set on the new object. Notes In most cases, there should be no functional difference from using deep, but if deep is passed it will attempt to deepcopy.","Parameters: nameLabel, optionalSet name for new object. deepbool, default False Returns: IndexIndex refer to new object which is a copy of this object.","["">>> idx = pd.Index(['a', 'b', 'c'])\n>>> new_idx = idx.copy()\n>>> idx is new_idx\nFalse""]"
1475,..\pandas\reference\api\pandas.Series.round.html,pandas.Series.round,"Series.round(decimals=0, *args, **kwargs)[source]# Round each value in a Series to the given number of decimals.","Parameters: decimalsint, default 0Number of decimal places to round to. If decimals is negative, it specifies the number of positions to the left of the decimal point. *args, **kwargsAdditional arguments and keywords have no effect but might be accepted for compatibility with NumPy. Returns: SeriesRounded values of the Series.","['>>> s = pd.Series([0.1, 1.3, 2.7])\n>>> s.round()\n0    0.0\n1    1.0\n2    3.0\ndtype: float64']"
1476,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.transform.html,pandas.core.groupby.DataFrameGroupBy.transform,"DataFrameGroupBy.transform(func, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Call function producing a same-indexed DataFrame on each group. Returns a DataFrame having the same indexes as the original object filled with the transformed values. Notes Each group is endowed the attribute ‘name’ in case you need to know which group you are working on. The current implementation imposes three requirements on f: f must return a value that either has the same shape as the input subframe or can be broadcast to the shape of the input subframe. For example, if f returns a scalar it will be broadcast to have the same shape as the input subframe. if this is a DataFrame, f must support application column-by-column in the subframe. If f also supports application to the entire subframe, then a fast path is used starting from the second chunk. f must not mutate groups. Mutation is not supported and may produce unexpected results. See Mutating with User Defined Function (UDF) methods for more details. When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below. Changed in version 2.0.0: When using .transform on a grouped DataFrame and the transformation function returns a DataFrame, pandas now aligns the result’s index with the input’s index. You can call .to_numpy() on the result of the transformation function to avoid alignment.","Parameters: ffunction, strFunction to apply to each group. See the Notes section below for requirements. Accepted inputs are: String Python function Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine. If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group’s index will be passed to the user defined function and optionally available for use. If a string is chosen, then it needs to be the name of the groupby method you want to use. *argsPositional arguments to pass to func. enginestr, default None 'cython' : Runs the function through C-extensions from cython. 'numba' : Runs the function through JIT compiled code from numba. None : Defaults to 'cython' or the global setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function **kwargsKeyword arguments to be passed into func. Returns: DataFrame","["">>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : ['one', 'one', 'two', 'three',\n...                           'two', 'two'],\n...                    'C' : [1, 5, 5, 2, 5, 5],\n...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')[['C', 'D']]\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n        C         D\n0 -1.154701 -0.577350\n1  0.577350  0.000000\n2  0.577350  1.154701\n3 -1.154701 -1.000000\n4  0.577350 -0.577350\n5  0.577350  1.000000"", '>>> grouped.transform(lambda x: x.max() - x.min())\n    C    D\n0  4.0  6.0\n1  3.0  8.0\n2  4.0  6.0\n3  3.0  8.0\n4  4.0  6.0\n5  3.0  8.0', '>>> grouped.transform(""mean"")\n    C    D\n0  3.666667  4.0\n1  4.000000  5.0\n2  3.666667  4.0\n3  4.000000  5.0\n4  3.666667  4.0\n5  4.000000  5.0', '>>> grouped.transform(lambda x: x.astype(int).max())\nC  D\n0  5  8\n1  5  9\n2  5  8\n3  5  9\n4  5  8\n5  5  9']"
1477,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.freqstr.html,pandas.tseries.offsets.SemiMonthEnd.freqstr,SemiMonthEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1478,..\pandas\reference\api\pandas.Index.delete.html,pandas.Index.delete,Index.delete(loc)[source]# Make new Index with passed location(-s) deleted.,"Parameters: locint or list of intLocation of item(-s) which will be deleted. Use a list of locations to delete more than one value at the same time. Returns: IndexWill be same type as self, except for RangeIndex.","["">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.delete(1)\nIndex(['a', 'c'], dtype='object')"", "">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.delete([0, 2])\nIndex(['b'], dtype='object')""]"
1479,..\pandas\reference\api\pandas.DataFrame.keys.html,pandas.DataFrame.keys,"DataFrame.keys()[source]# Get the ‘info axis’ (see Indexing for more). This is index for Series, columns for DataFrame.",Returns: IndexInfo axis.,"["">>> d = pd.DataFrame(data={'A': [1, 2, 3], 'B': [0, 4, 8]},\n...                  index=['a', 'b', 'c'])\n>>> d\n   A  B\na  1  0\nb  2  4\nc  3  8\n>>> d.keys()\nIndex(['A', 'B'], dtype='object')""]"
1480,..\pandas\reference\api\pandas.tseries.offsets.Easter.is_year_start.html,pandas.tseries.offsets.Easter.is_year_start,Easter.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1481,..\pandas\reference\api\pandas.PeriodIndex.week.html,pandas.PeriodIndex.week,property PeriodIndex.week[source]# The week ordinal of the year.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.week  # It can be written `weekofyear`\nIndex([5, 9, 13], dtype=\'int64\')']"
1482,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.value_counts.html,pandas.core.groupby.DataFrameGroupBy.value_counts,"DataFrameGroupBy.value_counts(subset=None, normalize=False, sort=True, ascending=False, dropna=True)[source]# Return a Series or DataFrame containing counts of unique rows. Added in version 1.4.0. Notes If the groupby as_index is True then the returned Series will have a MultiIndex with one level per input column. If the groupby as_index is False then the returned DataFrame will have an additional column with the value_counts. The column is labelled ‘count’ or ‘proportion’, depending on the normalize parameter. By default, rows that contain any NA values are omitted from the result. By default, the result will be in descending order so that the first element of each group is the most frequently-occurring row.","Parameters: subsetlist-like, optionalColumns to use when counting unique combinations. normalizebool, default FalseReturn proportions rather than frequencies. sortbool, default TrueSort by frequencies. ascendingbool, default FalseSort in ascending order. dropnabool, default TrueDon’t include counts of rows that contain NA values. Returns: Series or DataFrameSeries if the groupby as_index is True, otherwise DataFrame.","["">>> df = pd.DataFrame({\n...     'gender': ['male', 'male', 'female', 'male', 'female', 'male'],\n...     'education': ['low', 'medium', 'high', 'low', 'high', 'low'],\n...     'country': ['US', 'FR', 'US', 'FR', 'FR', 'FR']\n... })"", '>>> df\n        gender  education   country\n0       male    low         US\n1       male    medium      FR\n2       female  high        US\n3       male    low         FR\n4       female  high        FR\n5       male    low         FR', "">>> df.groupby('gender').value_counts()\ngender  education  country\nfemale  high       FR         1\n                   US         1\nmale    low        FR         2\n                   US         1\n        medium     FR         1\nName: count, dtype: int64"", "">>> df.groupby('gender').value_counts(ascending=True)\ngender  education  country\nfemale  high       FR         1\n                   US         1\nmale    low        US         1\n        medium     FR         1\n        low        FR         2\nName: count, dtype: int64"", "">>> df.groupby('gender').value_counts(normalize=True)\ngender  education  country\nfemale  high       FR         0.50\n                   US         0.50\nmale    low        FR         0.50\n                   US         0.25\n        medium     FR         0.25\nName: proportion, dtype: float64"", "">>> df.groupby('gender', as_index=False).value_counts()\n   gender education country  count\n0  female      high      FR      1\n1  female      high      US      1\n2    male       low      FR      2\n3    male       low      US      1\n4    male    medium      FR      1"", "">>> df.groupby('gender', as_index=False).value_counts(normalize=True)\n   gender education country  proportion\n0  female      high      FR        0.50\n1  female      high      US        0.50\n2    male       low      FR        0.50\n3    male       low      US        0.25\n4    male    medium      FR        0.25""]"
1483,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_on_offset.html,pandas.tseries.offsets.BQuarterEnd.is_on_offset,BQuarterEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1484,..\pandas\reference\api\pandas.Series.rpow.html,pandas.Series.rpow,"Series.rpow(other, level=None, fill_value=None, axis=0)[source]# Return Exponential power of series and other, element-wise (binary operator rpow). Equivalent to other ** series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.pow(b, fill_value=0)\na    1.0\nb    1.0\nc    1.0\nd    0.0\ne    NaN\ndtype: float64""]"
1485,..\pandas\reference\api\pandas.tseries.offsets.Easter.kwds.html,pandas.tseries.offsets.Easter.kwds,Easter.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1486,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.var.html,pandas.core.groupby.DataFrameGroupBy.var,"DataFrameGroupBy.var(ddof=1, engine=None, engine_kwargs=None, numeric_only=False)[source]# Compute variance of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex.","Parameters: ddofint, default 1Degrees of freedom. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}} Added in version 1.4.0. numeric_onlybool, default FalseInclude only float, int or boolean data. Added in version 1.5.0. Changed in version 2.0.0: numeric_only now defaults to False. Returns: Series or DataFrameVariance of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).var()\na    10.333333\nb     0.333333\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).var()\n              a          b\ndog    4.000000  12.333333\nmouse  4.916667   2.250000""]"
1487,..\pandas\reference\api\pandas.DataFrame.kurt.html,pandas.DataFrame.kurt,"DataFrame.kurt(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher’s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","["">>> s = pd.Series([1, 2, 2, 3], index=['cat', 'dog', 'dog', 'mouse'])\n>>> s\ncat    1\ndog    2\ndog    2\nmouse  3\ndtype: int64\n>>> s.kurt()\n1.5"", "">>> df = pd.DataFrame({'a': [1, 2, 2, 3], 'b': [3, 4, 4, 4]},\n...                   index=['cat', 'dog', 'dog', 'mouse'])\n>>> df\n       a   b\n  cat  1   3\n  dog  2   4\n  dog  2   4\nmouse  3   4\n>>> df.kurt()\na   1.5\nb   4.0\ndtype: float64"", '>>> df.kurt(axis=None).round(6)\n-0.988693', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [3, 4], 'd': [1, 2]},\n...                   index=['cat', 'dog'])\n>>> df.kurt(axis=1)\ncat   -6.0\ndog   -6.0\ndtype: float64""]"
1488,..\pandas\reference\api\pandas.PeriodIndex.weekday.html,pandas.PeriodIndex.weekday,"property PeriodIndex.weekday[source]# The day of the week with Monday=0, Sunday=6.",No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01-01"", ""2023-01-02"", ""2023-01-03""], freq=""D"")\n>>> idx.weekday\nIndex([6, 0, 1], dtype=\'int64\')']"
1489,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_quarter_end.html,pandas.tseries.offsets.BQuarterEnd.is_quarter_end,BQuarterEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1490,..\pandas\reference\api\pandas.Series.rsub.html,pandas.Series.rsub,"Series.rsub(other, level=None, fill_value=None, axis=0)[source]# Return Subtraction of series and other, element-wise (binary operator rsub). Equivalent to other - series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.subtract(b, fill_value=0)\na    0.0\nb    1.0\nc    1.0\nd   -1.0\ne    NaN\ndtype: float64""]"
1491,..\pandas\reference\api\pandas.tseries.offsets.Easter.n.html,pandas.tseries.offsets.Easter.n,Easter.n#,No parameters found,[]
1492,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.html,pandas.tseries.offsets.SemiMonthEnd,class pandas.tseries.offsets.SemiMonthEnd# Two DateOffset’s per month repeating on the last day of the month & day_of_month. Examples If you want to get the result for the current month: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. day_of_month freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. day_of_monthint, {1, 3,…,27}, default 15A specific integer for the day of the month.","["">>> ts = pd.Timestamp(2022, 1, 14)\n>>> ts + pd.offsets.SemiMonthEnd()\nTimestamp('2022-01-15 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 15)\n>>> ts + pd.offsets.SemiMonthEnd()\nTimestamp('2022-01-31 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 31)\n>>> ts + pd.offsets.SemiMonthEnd()\nTimestamp('2022-02-15 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 15)\n>>> pd.offsets.SemiMonthEnd().rollforward(ts)\nTimestamp('2022-01-15 00:00:00')""]"
1493,..\pandas\reference\api\pandas.Index.difference.html,pandas.Index.difference,"final Index.difference(other, sort=None)[source]# Return a new Index with elements of index not in other. This is the set difference of two Index objects.","Parameters: otherIndex or array-like sortbool or None, default NoneWhether to sort the resulting index. By default, the values are attempted to be sorted, but any TypeError from incomparable elements is caught by pandas. None : Attempt to sort the result, but catch any TypeErrors from comparing incomparable elements. False : Do not sort the result. True : Sort the result (which may raise TypeError). Returns: Index","["">>> idx1 = pd.Index([2, 1, 3, 4])\n>>> idx2 = pd.Index([3, 4, 5, 6])\n>>> idx1.difference(idx2)\nIndex([1, 2], dtype='int64')\n>>> idx1.difference(idx2, sort=False)\nIndex([2, 1], dtype='int64')""]"
1494,..\pandas\reference\api\pandas.core.groupby.DataFrameGroupBy.__iter__.html,pandas.core.groupby.DataFrameGroupBy.__iter__,DataFrameGroupBy.__iter__()[source]# Groupby iterator.,"Returns: Generator yielding sequence of (name, subsetted object) for each group","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> for x, y in ser.groupby(level=0):\n...     print(f'{x}\\n{y}\\n')\na\na    1\na    2\ndtype: int64\nb\nb    3\ndtype: int64"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""])\n>>> df\n   a  b  c\n0  1  2  3\n1  1  5  6\n2  7  8  9\n>>> for x, y in df.groupby(by=[""a""]):\n...     print(f\'{x}\\n{y}\\n\')\n(1,)\n   a  b  c\n0  1  2  3\n1  1  5  6\n(7,)\n   a  b  c\n2  7  8  9', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> for x, y in ser.resample('MS'):\n...     print(f'{x}\\n{y}\\n')\n2023-01-01 00:00:00\n2023-01-01    1\n2023-01-15    2\ndtype: int64\n2023-02-01 00:00:00\n2023-02-01    3\n2023-02-15    4\ndtype: int64""]"
1495,..\pandas\reference\api\pandas.DataFrame.kurtosis.html,pandas.DataFrame.kurtosis,"DataFrame.kurtosis(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher’s definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","["">>> s = pd.Series([1, 2, 2, 3], index=['cat', 'dog', 'dog', 'mouse'])\n>>> s\ncat    1\ndog    2\ndog    2\nmouse  3\ndtype: int64\n>>> s.kurt()\n1.5"", "">>> df = pd.DataFrame({'a': [1, 2, 2, 3], 'b': [3, 4, 4, 4]},\n...                   index=['cat', 'dog', 'dog', 'mouse'])\n>>> df\n       a   b\n  cat  1   3\n  dog  2   4\n  dog  2   4\nmouse  3   4\n>>> df.kurt()\na   1.5\nb   4.0\ndtype: float64"", '>>> df.kurt(axis=None).round(6)\n-0.988693', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [3, 4], 'd': [1, 2]},\n...                   index=['cat', 'dog'])\n>>> df.kurt(axis=1)\ncat   -6.0\ndog   -6.0\ndtype: float64""]"
1496,..\pandas\reference\api\pandas.Series.rtruediv.html,pandas.Series.rtruediv,"Series.rtruediv(other, level=None, fill_value=None, axis=0)[source]# Return Floating division of series and other, element-wise (binary operator rtruediv). Equivalent to other / series, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.divide(b, fill_value=0)\na    1.0\nb    inf\nc    inf\nd    0.0\ne    NaN\ndtype: float64""]"
1497,..\pandas\reference\api\pandas.Index.drop.html,pandas.Index.drop,"Index.drop(labels, errors='raise')[source]# Make new Index with passed list of labels deleted.","Parameters: labelsarray-like or scalar errors{‘ignore’, ‘raise’}, default ‘raise’If ‘ignore’, suppress error and existing labels are dropped. Returns: IndexWill be same type as self, except for RangeIndex. Raises: KeyErrorIf not all of the labels are found in the selected axis","["">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.drop(['a'])\nIndex(['b', 'c'], dtype='object')""]"
1498,..\pandas\reference\api\pandas.tseries.offsets.Easter.name.html,pandas.tseries.offsets.Easter.name,Easter.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1499,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_anchored.html,pandas.tseries.offsets.SemiMonthEnd.is_anchored,SemiMonthEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1500,..\pandas\reference\api\pandas.PeriodIndex.weekofyear.html,pandas.PeriodIndex.weekofyear,property PeriodIndex.weekofyear[source]# The week ordinal of the year.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023-01"", ""2023-02"", ""2023-03""], freq=""M"")\n>>> idx.week  # It can be written `weekofyear`\nIndex([5, 9, 13], dtype=\'int64\')']"
1501,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_quarter_start.html,pandas.tseries.offsets.BQuarterEnd.is_quarter_start,BQuarterEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1502,..\pandas\reference\api\pandas.DataFrame.last.html,pandas.DataFrame.last,"DataFrame.last(offset)[source]# Select final periods of time series data based on a date offset. Deprecated since version 2.1: last() is deprecated and will be removed in a future version. Please create a mask and filter using .loc instead. For a DataFrame with a sorted DatetimeIndex, this function selects the last few rows based on a date offset. Notes Deprecated since version 2.1.0: Please create a mask and filter using .loc instead","Parameters: offsetstr, DateOffset, dateutil.relativedeltaThe offset length of the data that will be selected. For instance, ‘3D’ will display all the rows having their index within the last 3 days. Returns: Series or DataFrameA subset of the caller. Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='2D')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n            A\n2018-04-09  1\n2018-04-11  2\n2018-04-13  3\n2018-04-15  4"", "">>> ts.last('3D')  \n            A\n2018-04-13  3\n2018-04-15  4""]"
1503,..\pandas\reference\api\pandas.Series.sample.html,pandas.Series.sample,"Series.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)[source]# Return a random sample of items from an axis of object. You can use random_state for reproducibility. Notes If frac > 1, replacement should be set to True.","Parameters: nint, optionalNumber of items from axis to return. Cannot be used with frac. Default = 1 if frac = None. fracfloat, optionalFraction of axis items to return. Cannot be used with n. replacebool, default FalseAllow or disallow sampling of the same row more than once. weightsstr or ndarray-like, optionalDefault ‘None’ results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_stateint, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optionalIf int, array-like, or BitGenerator, seed for random number generator. If np.random.RandomState or np.random.Generator, use as given. Changed in version 1.4.0: np.random.Generator objects now accepted axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneAxis to sample. Accepts axis number or name. Default is stat axis for given data type. For Series this parameter is unused and defaults to None. ignore_indexbool, default FalseIf True, the resulting index will be labeled 0, 1, …, n - 1. Added in version 1.3.0. Returns: Series or DataFrameA new object of same type as caller containing n items randomly sampled from the caller object.","["">>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n...                    'num_wings': [2, 0, 0, 0],\n...                    'num_specimen_seen': [10, 2, 1, 8]},\n...                   index=['falcon', 'dog', 'spider', 'fish'])\n>>> df\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\ndog            4          0                  2\nspider         8          0                  1\nfish           0          0                  8"", "">>> df['num_legs'].sample(n=3, random_state=1)\nfish      0\nspider    8\nfalcon    2\nName: num_legs, dtype: int64"", '>>> df.sample(frac=0.5, replace=True, random_state=1)\n      num_legs  num_wings  num_specimen_seen\ndog          4          0                  2\nfish         0          0                  8', '>>> df.sample(frac=2, replace=True, random_state=1)\n        num_legs  num_wings  num_specimen_seen\ndog            4          0                  2\nfish           0          0                  8\nfalcon         2          2                 10\nfalcon         2          2                 10\nfish           0          0                  8\ndog            4          0                  2\nfish           0          0                  8\ndog            4          0                  2', "">>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\nfish           0          0                  8""]"
1504,..\pandas\reference\api\pandas.Index.droplevel.html,pandas.Index.droplevel,"final Index.droplevel(level=0)[source]# Return index with requested level(s) removed. If resulting index has only 1 level left, the result will be of Index type, not MultiIndex. The original index is not modified inplace.","Parameters: levelint, str, or list-like, default 0If a string is given, must be the name of a level If list-like, elements must be names or indexes of levels. Returns: Index or MultiIndex","["">>> mi = pd.MultiIndex.from_arrays(\n... [[1, 2], [3, 4], [5, 6]], names=['x', 'y', 'z'])\n>>> mi\nMultiIndex([(1, 3, 5),\n            (2, 4, 6)],\n           names=['x', 'y', 'z'])"", "">>> mi.droplevel()\nMultiIndex([(3, 5),\n            (4, 6)],\n           names=['y', 'z'])"", "">>> mi.droplevel(2)\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])"", "">>> mi.droplevel('z')\nMultiIndex([(1, 3),\n            (2, 4)],\n           names=['x', 'y'])"", "">>> mi.droplevel(['x', 'y'])\nIndex([5, 6], dtype='int64', name='z')""]"
1505,..\pandas\reference\api\pandas.tseries.offsets.Easter.nanos.html,pandas.tseries.offsets.Easter.nanos,Easter.nanos#,No parameters found,[]
1506,..\pandas\reference\api\pandas.DataFrame.last_valid_index.html,pandas.DataFrame.last_valid_index,"DataFrame.last_valid_index()[source]# Return index for last non-NA value or None, if no non-NA value is found.",Returns: type of index,"['>>> s = pd.Series([None, 3, 4])\n>>> s.first_valid_index()\n1\n>>> s.last_valid_index()\n2', '>>> s = pd.Series([None, None])\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', '>>> s = pd.Series()\n>>> print(s.first_valid_index())\nNone\n>>> print(s.last_valid_index())\nNone', "">>> df = pd.DataFrame({'A': [None, None, 2], 'B': [None, 3, 4]})\n>>> df\n     A      B\n0  NaN    NaN\n1  NaN    3.0\n2  2.0    4.0\n>>> df.first_valid_index()\n1\n>>> df.last_valid_index()\n2"", "">>> df = pd.DataFrame({'A': [None, None, None], 'B': [None, None, None]})\n>>> df\n     A      B\n0  None   None\n1  None   None\n2  None   None\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone"", '>>> df = pd.DataFrame()\n>>> df\nEmpty DataFrame\nColumns: []\nIndex: []\n>>> print(df.first_valid_index())\nNone\n>>> print(df.last_valid_index())\nNone']"
1507,..\pandas\reference\api\pandas.PeriodIndex.year.html,pandas.PeriodIndex.year,property PeriodIndex.year[source]# The year of the period.,No parameters found,"['>>> idx = pd.PeriodIndex([""2023"", ""2024"", ""2025""], freq=""Y"")\n>>> idx.year\nIndex([2023, 2024, 2025], dtype=\'int64\')']"
1508,..\pandas\reference\api\pandas.Series.searchsorted.html,pandas.Series.searchsorted,"Series.searchsorted(value, side='left', sorter=None)[source]# Find indices where elements should be inserted to maintain order. Find the indices into a sorted Series self such that, if the corresponding elements in value were inserted before the indices, the order of self would be preserved. Note The Series must be monotonically sorted, otherwise wrong locations will likely be returned. Pandas does not check this for you. Notes Binary search is used to find the required insertion points.","Parameters: valuearray-like or scalarValues to insert into self. side{‘left’, ‘right’}, optionalIf ‘left’, the index of the first suitable location found is given. If ‘right’, return the last such index.  If there is no suitable index, return either 0 or N (where N is the length of self). sorter1-D array-like, optionalOptional array of integer indices that sort self into ascending order. They are typically the result of np.argsort. Returns: int or array of intA scalar or array of insertion points with the same shape as value.","['>>> ser = pd.Series([1, 2, 3])\n>>> ser\n0    1\n1    2\n2    3\ndtype: int64', '>>> ser.searchsorted(4)\n3', '>>> ser.searchsorted([0, 4])\narray([0, 3])', "">>> ser.searchsorted([1, 3], side='left')\narray([0, 2])"", "">>> ser.searchsorted([1, 3], side='right')\narray([1, 3])"", "">>> ser = pd.Series(pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000']))\n>>> ser\n0   2000-03-11\n1   2000-03-12\n2   2000-03-13\ndtype: datetime64[ns]"", "">>> ser.searchsorted('3/14/2000')\n3"", "">>> ser = pd.Categorical(\n...     ['apple', 'bread', 'bread', 'cheese', 'milk'], ordered=True\n... )\n>>> ser\n['apple', 'bread', 'bread', 'cheese', 'milk']\nCategories (4, object): ['apple' < 'bread' < 'cheese' < 'milk']"", "">>> ser.searchsorted('bread')\n1"", "">>> ser.searchsorted(['bread'], side='right')\narray([3])"", '>>> ser = pd.Series([2, 1, 3])\n>>> ser\n0    2\n1    1\n2    3\ndtype: int64', '>>> ser.searchsorted(1)  \n0  # wrong result, correct would be 1']"
1509,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_month_end.html,pandas.tseries.offsets.SemiMonthEnd.is_month_end,SemiMonthEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1510,..\pandas\reference\api\pandas.Index.dropna.html,pandas.Index.dropna,Index.dropna(how='any')[source]# Return Index without NA/NaN values.,"Parameters: how{‘any’, ‘all’}, default ‘any’If the Index is a MultiIndex, drop the value when any or all levels are NaN. Returns: Index","["">>> idx = pd.Index([1, np.nan, 3])\n>>> idx.dropna()\nIndex([1.0, 3.0], dtype='float64')""]"
1511,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_year_end.html,pandas.tseries.offsets.BQuarterEnd.is_year_end,BQuarterEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1512,..\pandas\reference\api\pandas.tseries.offsets.Easter.normalize.html,pandas.tseries.offsets.Easter.normalize,Easter.normalize#,No parameters found,[]
1513,..\pandas\reference\api\pandas.DataFrame.le.html,pandas.DataFrame.le,"DataFrame.le(other, axis='columns', level=None)[source]# Get Less than or equal to of dataframe and other, element-wise (binary operator le). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).","Parameters: otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}, default ‘columns’Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. Returns: DataFrame of boolResult of the comparison.","["">>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300"", '>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df != pd.Series([100, 250], index=[""cost"", ""revenue""])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True', '>>> df.ne(pd.Series([100, 300], index=[""A"", ""D""]), axis=\'index\')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True', '>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False', "">>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False"", "">>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150"", '>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False', "">>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225"", '>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False']"
1514,..\pandas\reference\api\pandas.period_range.html,pandas.period_range,"pandas.period_range(start=None, end=None, periods=None, freq=None, name=None)[source]# Return a fixed frequency PeriodIndex. The day (calendar) is the default frequency. Notes Of the three parameters: start, end, and periods, exactly two must be specified. To learn more about the frequency strings, please see this link.","Parameters: startstr, datetime, date, pandas.Timestamp, or period-like, default NoneLeft bound for generating periods. endstr, datetime, date, pandas.Timestamp, or period-like, default NoneRight bound for generating periods. periodsint, default NoneNumber of periods to generate. freqstr or DateOffset, optionalFrequency alias. By default the freq is taken from start or end if those are Period objects. Otherwise, the default is ""D"" for daily frequency. namestr, default NoneName of the resulting PeriodIndex. Returns: PeriodIndex","["">>> pd.period_range(start='2017-01-01', end='2018-01-01', freq='M')\nPeriodIndex(['2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06',\n         '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12',\n         '2018-01'],\n        dtype='period[M]')"", "">>> pd.period_range(start=pd.Period('2017Q1', freq='Q'),\n...                 end=pd.Period('2017Q2', freq='Q'), freq='M')\nPeriodIndex(['2017-03', '2017-04', '2017-05', '2017-06'],\n            dtype='period[M]')""]"
1515,..\pandas\reference\api\pandas.Series.sem.html,pandas.Series.sem,"Series.sem(axis=None, skipna=True, ddof=1, numeric_only=False, **kwargs)[source]# Return unbiased standard error of the mean over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument","Parameters: axis{index (0)}For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.sem with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. Returns: scalar or Series (if level specified)","['>>> s = pd.Series([1, 2, 3])\n>>> s.sem().round(6)\n0.57735', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n>>> df\n       a   b\ntiger  1   2\nzebra  2   3\n>>> df.sem()\na   0.5\nb   0.5\ndtype: float64"", '>>> df.sem(axis=1)\ntiger   0.5\nzebra   0.5\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n...                   index=['tiger', 'zebra'])\n>>> df.sem(numeric_only=True)\na   0.5\ndtype: float64""]"
1516,..\pandas\reference\api\pandas.DataFrame.loc.html,pandas.DataFrame.loc,"property DataFrame.loc[source]# Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are: A single label, e.g. 5 or 'a', (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c']. A slice object with labels, e.g. 'a':'f'. Warning Note that contrary to usual python slices, both the start and the stop are included A boolean array of the same length as the axis being sliced, e.g. [True, False, True]. An alignable boolean Series. The index of the key will be aligned before masking. An alignable Index. The Index of the returned selection will be the input. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above) See more at Selection by Label.",Raises: KeyErrorIf any items are not found. IndexingErrorIf an indexed key is passed and its index is unalignable to the frame index.,"["">>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=['cobra', 'viper', 'sidewinder'],\n...                   columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8"", "">>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64"", "">>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8"", "">>> df.loc['cobra', 'shield']\n2"", "">>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64"", '>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8', "">>> df.loc[pd.Series([False, True, False],\n...                  index=['viper', 'sidewinder', 'cobra'])]\n                     max_speed  shield\nsidewinder          7       8"", '>>> df.loc[pd.Index([""cobra"", ""viper""], name=""foo"")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5', "">>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8"", "">>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7"", "">>> df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]\n            max_speed  shield\nviper          4       5"", "">>> df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]\n            max_speed  shield\ncobra               1       2\nsidewinder          7       8"", "">>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8"", "">>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50"", "">>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50"", "">>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50"", "">>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0"", '>>> df.loc[""viper"", ""shield""] += 5\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       5\nsidewinder          0       0', '>>> shuffled_df = df.loc[[""viper"", ""cobra"", ""sidewinder""]]\n>>> df.loc[:] += shuffled_df\n>>> df\n            max_speed  shield\ncobra              60      20\nviper               0      10\nsidewinder          0       0', "">>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...                   index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8"", '>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8', "">>> tuples = [\n...     ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...     ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...     ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...           [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36"", "">>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4"", "">>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64"", "">>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64"", "">>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4"", "">>> df.loc[('cobra', 'mark i'), 'shield']\n2"", "">>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36"", "">>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1""]"
1517,..\pandas\reference\api\pandas.pivot.html,pandas.pivot,"pandas.pivot(data, *, columns, index=<no_default>, values=<no_default>)[source]# Return reshaped DataFrame organized by given index / column values. Reshape data (produce a “pivot” table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping. Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods. Reference the user guide for more examples.","Parameters: dataDataFrame columnsstr or object or a list of strColumn to use to make new frame’s columns. indexstr or object or a list of str, optionalColumn to use to make new frame’s index. If not given, uses existing index. valuesstr, object or a list of the previous, optionalColumn(s) to use for populating new frame’s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns. Returns: DataFrameReturns reshaped DataFrame. Raises: ValueError:When there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.","["">>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n...                            'two'],\n...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n...                    'baz': [1, 2, 3, 4, 5, 6],\n...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n>>> df\n    foo   bar  baz  zoo\n0   one   A    1    x\n1   one   B    2    y\n2   one   C    3    z\n3   two   A    4    q\n4   two   B    5    w\n5   two   C    6    t"", "">>> df.pivot(index='foo', columns='bar', values='baz')\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6"", "">>> df.pivot(index='foo', columns='bar')['baz']\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6"", "">>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n      baz       zoo\nbar   A  B  C   A  B  C\nfoo\none   1  2  3   x  y  z\ntwo   4  5  6   q  w  t"", '>>> df = pd.DataFrame({\n...        ""lev1"": [1, 1, 1, 2, 2, 2],\n...        ""lev2"": [1, 1, 2, 1, 1, 2],\n...        ""lev3"": [1, 2, 1, 2, 1, 2],\n...        ""lev4"": [1, 2, 3, 4, 5, 6],\n...        ""values"": [0, 1, 2, 3, 4, 5]})\n>>> df\n    lev1 lev2 lev3 lev4 values\n0   1    1    1    1    0\n1   1    1    2    2    1\n2   1    2    1    3    2\n3   2    1    2    4    3\n4   2    1    1    5    4\n5   2    2    2    6    5', '>>> df.pivot(index=""lev1"", columns=[""lev2"", ""lev3""], values=""values"")\nlev2    1         2\nlev3    1    2    1    2\nlev1\n1     0.0  1.0  2.0  NaN\n2     4.0  3.0  NaN  5.0', '>>> df.pivot(index=[""lev1"", ""lev2""], columns=[""lev3""], values=""values"")\n      lev3    1    2\nlev1  lev2\n   1     1  0.0  1.0\n         2  2.0  NaN\n   2     1  4.0  3.0\n         2  NaN  5.0', '>>> df = pd.DataFrame({""foo"": [\'one\', \'one\', \'two\', \'two\'],\n...                    ""bar"": [\'A\', \'A\', \'B\', \'C\'],\n...                    ""baz"": [1, 2, 3, 4]})\n>>> df\n   foo bar  baz\n0  one   A    1\n1  one   A    2\n2  two   B    3\n3  two   C    4', "">>> df.pivot(index='foo', columns='bar', values='baz')\nTraceback (most recent call last):\n   ...\nValueError: Index contains duplicate entries, cannot reshape""]"
1518,..\pandas\reference\api\pandas.Index.drop_duplicates.html,pandas.Index.drop_duplicates,"Index.drop_duplicates(*, keep='first')[source]# Return Index with duplicate values removed.","Parameters: keep{‘first’, ‘last’, False}, default ‘first’ ‘first’ : Drop duplicates except for the first occurrence. ‘last’ : Drop duplicates except for the last occurrence. False : Drop all duplicates. Returns: Index","["">>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'])"", "">>> idx.drop_duplicates(keep='first')\nIndex(['lama', 'cow', 'beetle', 'hippo'], dtype='object')"", "">>> idx.drop_duplicates(keep='last')\nIndex(['cow', 'beetle', 'lama', 'hippo'], dtype='object')"", "">>> idx.drop_duplicates(keep=False)\nIndex(['cow', 'beetle', 'hippo'], dtype='object')""]"
1519,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.is_year_start.html,pandas.tseries.offsets.BQuarterEnd.is_year_start,BQuarterEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1520,..\pandas\reference\api\pandas.tseries.offsets.Easter.rule_code.html,pandas.tseries.offsets.Easter.rule_code,Easter.rule_code#,No parameters found,[]
1521,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_month_start.html,pandas.tseries.offsets.SemiMonthEnd.is_month_start,SemiMonthEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1522,..\pandas\reference\api\pandas.Series.set_axis.html,pandas.Series.set_axis,"Series.set_axis(labels, *, axis=0, copy=None)[source]# Assign desired index to given axis. Indexes for row labels can be changed by assigning a list-like or Index.","Parameters: labelslist-like, IndexThe values for the new index. axis{0 or ‘index’}, default 0The axis to update. The value 0 identifies the rows. For Series this parameter is unused and defaults to 0. copybool, default TrueWhether to make a copy of the underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: SeriesAn object of type Series.","['>>> s = pd.Series([1, 2, 3])\n>>> s\n0    1\n1    2\n2    3\ndtype: int64', "">>> s.set_axis(['a', 'b', 'c'], axis=0)\na    1\nb    2\nc    3\ndtype: int64""]"
1523,..\pandas\reference\api\pandas.pivot_table.html,pandas.pivot_table,"pandas.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=<no_default>, sort=True)[source]# Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame. Notes Reference the user guide for more examples.","Parameters: dataDataFrame valueslist-like or scalar, optionalColumn or columns to aggregate. indexcolumn, Grouper, array, or list of the previousKeys to group by on the pivot table index. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. columnscolumn, Grouper, array, or list of the previousKeys to group by on the pivot table column. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. aggfuncfunction, list of functions, dict, default “mean”If a list of functions is passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves). If a dict is passed, the key is column to aggregate and the value is function or list of functions. If margin=True, aggfunc will be used to calculate the partial aggregates. fill_valuescalar, default NoneValue to replace missing values with (in the resulting pivot table, after aggregation). marginsbool, default FalseIf margins=True, special All columns and rows will be added with partial group aggregates across the categories on the rows and columns. dropnabool, default TrueDo not include columns whose entries are all NaN. If True, rows with a NaN value in any column will be omitted before computing margins. margins_namestr, default ‘All’Name of the row / column that will contain the totals when margins is True. observedbool, default FalseThis only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. Deprecated since version 2.2.0: The default value of False is deprecated and will change to True in a future version of pandas. sortbool, default TrueSpecifies if the result should be sorted. Added in version 1.3.0. Returns: DataFrameAn Excel style pivot table.","['>>> df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",\n...                          ""bar"", ""bar"", ""bar"", ""bar""],\n...                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",\n...                          ""one"", ""one"", ""two"", ""two""],\n...                    ""C"": [""small"", ""large"", ""large"", ""small"",\n...                          ""small"", ""large"", ""small"", ""small"",\n...                          ""large""],\n...                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n...                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n>>> df\n     A    B      C  D  E\n0  foo  one  small  1  2\n1  foo  one  large  2  4\n2  foo  one  large  2  5\n3  foo  two  small  3  5\n4  foo  two  small  3  6\n5  bar  one  large  4  6\n6  bar  one  small  5  8\n7  bar  two  small  6  9\n8  bar  two  large  7  9', '>>> table = pd.pivot_table(df, values=\'D\', index=[\'A\', \'B\'],\n...                        columns=[\'C\'], aggfunc=""sum"")\n>>> table\nC        large  small\nA   B\nbar one    4.0    5.0\n    two    7.0    6.0\nfoo one    4.0    1.0\n    two    NaN    6.0', '>>> table = pd.pivot_table(df, values=\'D\', index=[\'A\', \'B\'],\n...                        columns=[\'C\'], aggfunc=""sum"", fill_value=0)\n>>> table\nC        large  small\nA   B\nbar one      4      5\n    two      7      6\nfoo one      4      1\n    two      0      6', '>>> table = pd.pivot_table(df, values=[\'D\', \'E\'], index=[\'A\', \'C\'],\n...                        aggfunc={\'D\': ""mean"", \'E\': ""mean""})\n>>> table\n                D         E\nA   C\nbar large  5.500000  7.500000\n    small  5.500000  8.500000\nfoo large  2.000000  4.500000\n    small  2.333333  4.333333', '>>> table = pd.pivot_table(df, values=[\'D\', \'E\'], index=[\'A\', \'C\'],\n...                        aggfunc={\'D\': ""mean"",\n...                                 \'E\': [""min"", ""max"", ""mean""]})\n>>> table\n                  D   E\n               mean max      mean  min\nA   C\nbar large  5.500000   9  7.500000    6\n    small  5.500000   9  8.500000    8\nfoo large  2.000000   5  4.500000    4\n    small  2.333333   6  4.333333    2']"
1524,..\pandas\reference\api\pandas.Index.dtype.html,pandas.Index.dtype,Index.dtype[source]# Return the dtype object of the underlying data.,No parameters found,"["">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.dtype\ndtype('int64')""]"
1525,..\pandas\reference\api\pandas.DataFrame.lt.html,pandas.DataFrame.lt,"DataFrame.lt(other, axis='columns', level=None)[source]# Get Less than of dataframe and other, element-wise (binary operator lt). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).","Parameters: otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}, default ‘columns’Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. Returns: DataFrame of boolResult of the comparison.","["">>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300"", '>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df != pd.Series([100, 250], index=[""cost"", ""revenue""])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True', '>>> df.ne(pd.Series([100, 300], index=[""A"", ""D""]), axis=\'index\')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True', '>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False', "">>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False"", "">>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150"", '>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False', "">>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225"", '>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False']"
1526,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_on_offset.html,pandas.tseries.offsets.SemiMonthEnd.is_on_offset,SemiMonthEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1527,..\pandas\reference\api\pandas.Series.set_flags.html,pandas.Series.set_flags,"Series.set_flags(*, copy=False, allows_duplicate_labels=None)[source]# Return a new object with updated flags. Notes This method returns a new object that’s a view on the same data as the input. Mutating the input or the output values will be reflected in the other. This method is intended to be used in method chains. “Flags” differ from “metadata”. Flags reflect properties of the pandas object (the Series or DataFrame). Metadata refer to properties of the dataset, and should be stored in DataFrame.attrs.","Parameters: copybool, default FalseSpecify if a copy of the object should be made. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True allows_duplicate_labelsbool, optionalWhether the returned object allows duplicate labels. Returns: Series or DataFrameThe same type as the caller.","['>>> df = pd.DataFrame({""A"": [1, 2]})\n>>> df.flags.allows_duplicate_labels\nTrue\n>>> df2 = df.set_flags(allows_duplicate_labels=False)\n>>> df2.flags.allows_duplicate_labels\nFalse']"
1528,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.kwds.html,pandas.tseries.offsets.BQuarterEnd.kwds,BQuarterEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1529,..\pandas\reference\api\pandas.tseries.offsets.FY5253.copy.html,pandas.tseries.offsets.FY5253.copy,FY5253.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1530,..\pandas\reference\api\pandas.plotting.andrews_curves.html,pandas.plotting.andrews_curves,"pandas.plotting.andrews_curves(frame, class_column, ax=None, samples=200, color=None, colormap=None, **kwargs)[source]# Generate a matplotlib plot for visualizing clusters of multivariate data. Andrews curves have the functional form: \[f(t) = \frac{x_1}{\sqrt{2}} + x_2 \sin(t) + x_3 \cos(t) + x_4 \sin(2t) + x_5 \cos(2t) + \cdots\] Where \(x\) coefficients correspond to the values of each dimension and \(t\) is linearly spaced between \(-\pi\) and \(+\pi\). Each row of frame then corresponds to a single curve.","Parameters: frameDataFrameData to be plotted, preferably normalized to (0.0, 1.0). class_columnlabelName of the column containing class names. axaxes object, default NoneAxes to use. samplesintNumber of points to plot in each curve. colorstr, list[str] or tuple[str], optionalColors to use for the different classes. Colors can be strings or 3-element floating point RGB values. colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If a string, load colormap with that name from matplotlib. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes","["">>> df = pd.read_csv(\n...     'https://raw.githubusercontent.com/pandas-dev/'\n...     'pandas/main/pandas/tests/io/data/csv/iris.csv'\n... )\n>>> pd.plotting.andrews_curves(df, 'Name')""]"
1531,..\pandas\reference\api\pandas.Index.duplicated.html,pandas.Index.duplicated,"Index.duplicated(keep='first')[source]# Indicate duplicate index values. Duplicated values are indicated as True values in the resulting array. Either all duplicates, all except the first, or all except the last occurrence of duplicates can be indicated.","Parameters: keep{‘first’, ‘last’, False}, default ‘first’The value or values in a set of duplicates to mark as missing. ‘first’ : Mark duplicates as True except for the first occurrence. ‘last’ : Mark duplicates as True except for the last occurrence. False : Mark all duplicates as True. Returns: np.ndarray[bool]","["">>> idx = pd.Index(['lama', 'cow', 'lama', 'beetle', 'lama'])\n>>> idx.duplicated()\narray([False, False,  True, False,  True])"", "">>> idx.duplicated(keep='first')\narray([False, False,  True, False,  True])"", "">>> idx.duplicated(keep='last')\narray([ True, False,  True, False, False])"", '>>> idx.duplicated(keep=False)\narray([ True, False,  True, False,  True])']"
1532,..\pandas\reference\api\pandas.DataFrame.map.html,pandas.DataFrame.map,"DataFrame.map(func, na_action=None, **kwargs)[source]# Apply a function to a Dataframe elementwise. Added in version 2.1.0: DataFrame.applymap was deprecated and renamed to DataFrame.map. This method applies a function that accepts and returns a scalar to every element of a DataFrame.","Parameters: funccallablePython function, returns a single value from a single value. na_action{None, ‘ignore’}, default NoneIf ‘ignore’, propagate NaN values, without passing them to func. **kwargsAdditional keyword arguments to pass as keywords arguments to func. Returns: DataFrameTransformed DataFrame.","['>>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n>>> df\n       0      1\n0  1.000  2.120\n1  3.356  4.567', '>>> df.map(lambda x: len(str(x)))\n   0  1\n0  3  4\n1  5  5', "">>> df_copy = df.copy()\n>>> df_copy.iloc[0, 0] = pd.NA\n>>> df_copy.map(lambda x: len(str(x)), na_action='ignore')\n     0  1\n0  NaN  4\n1  5.0  5"", '>>> df.map(round, ndigits=1)\n     0    1\n0  1.0  2.1\n1  3.4  4.6', '>>> df.map(lambda x: x**2)\n           0          1\n0   1.000000   4.494400\n1  11.262736  20.857489', '>>> df ** 2\n           0          1\n0   1.000000   4.494400\n1  11.262736  20.857489']"
1533,..\pandas\reference\api\pandas.plotting.autocorrelation_plot.html,pandas.plotting.autocorrelation_plot,"pandas.plotting.autocorrelation_plot(series, ax=None, **kwargs)[source]# Autocorrelation plot for time series.","Parameters: seriesSeriesThe time series to visualize. axMatplotlib axis object, optionalThe matplotlib axis object to use. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes","['>>> spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)\n>>> s = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))\n>>> pd.plotting.autocorrelation_plot(s)']"
1534,..\pandas\reference\api\pandas.Index.empty.html,pandas.Index.empty,property Index.empty[source]#,No parameters found,[]
1535,..\pandas\reference\api\pandas.Series.shape.html,pandas.Series.shape,property Series.shape[source]# Return a tuple of the shape of the underlying data.,No parameters found,"['>>> s = pd.Series([1, 2, 3])\n>>> s.shape\n(3,)']"
1536,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.n.html,pandas.tseries.offsets.BQuarterEnd.n,BQuarterEnd.n#,No parameters found,[]
1537,..\pandas\reference\api\pandas.tseries.offsets.FY5253.freqstr.html,pandas.tseries.offsets.FY5253.freqstr,FY5253.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1538,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_quarter_end.html,pandas.tseries.offsets.SemiMonthEnd.is_quarter_end,SemiMonthEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1539,..\pandas\reference\api\pandas.DataFrame.mask.html,pandas.DataFrame.mask,"DataFrame.mask(cond, other=<no_default>, *, inplace=False, axis=None, level=None)[source]# Replace values where the condition is True. Notes The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used. If the axis of other does not align with axis of cond Series/DataFrame, the misaligned index positions will be filled with True. The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2). For further details and examples see the mask documentation in indexing. The dtype of the object takes precedence. The fill value is casted to the object’s dtype, if this can be done losslessly.","Parameters: condbool Series/DataFrame, array-like, or callableWhere cond is False, keep the original value. Where True, replace with corresponding value from other. If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn’t check it). otherscalar, Series/DataFrame, or callableEntries where cond is True are replaced with corresponding value from other. If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn’t check it). If not specified, entries will be filled with the corresponding NULL value (np.nan for numpy dtypes, pd.NA for extension dtypes). inplacebool, default FalseWhether to perform the operation in place on the data. axisint, default NoneAlignment axis if needed. For Series this parameter is unused and defaults to 0. levelint, default NoneAlignment level if needed. Returns: Same type as caller or None if inplace=True.","['>>> s = pd.Series(range(5))\n>>> s.where(s > 0)\n0    NaN\n1    1.0\n2    2.0\n3    3.0\n4    4.0\ndtype: float64\n>>> s.mask(s > 0)\n0    0.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64', '>>> s = pd.Series(range(5))\n>>> t = pd.Series([True, False])\n>>> s.where(t, 99)\n0     0\n1    99\n2    99\n3    99\n4    99\ndtype: int64\n>>> s.mask(t, 99)\n0    99\n1     1\n2    99\n3    99\n4    99\ndtype: int64', '>>> s.where(s > 1, 10)\n0    10\n1    10\n2    2\n3    3\n4    4\ndtype: int64\n>>> s.mask(s > 1, 10)\n0     0\n1     1\n2    10\n3    10\n4    10\ndtype: int64', "">>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n>>> df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n>>> m = df % 3 == 0\n>>> df.where(m, -df)\n   A  B\n0  0 -1\n1 -2  3\n2 -4 -5\n3  6 -7\n4 -8  9\n>>> df.where(m, -df) == np.where(m, df, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n>>> df.where(m, -df) == df.mask(~m, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True""]"
1540,..\pandas\reference\api\pandas.plotting.bootstrap_plot.html,pandas.plotting.bootstrap_plot,"pandas.plotting.bootstrap_plot(series, fig=None, size=50, samples=500, **kwds)[source]# Bootstrap plot on mean, median and mid-range statistics. The bootstrap plot is used to estimate the uncertainty of a statistic by relying on random sampling with replacement [1]. This function will generate bootstrapping plots for mean, median and mid-range statistics for the given number of samples of the given size. [1] “Bootstrapping (statistics)” in     https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29","Parameters: seriespandas.SeriesSeries from where to get the samplings for the bootstrapping. figmatplotlib.figure.Figure, default NoneIf given, it will use the fig reference for plotting instead of creating a new one with default parameters. sizeint, default 50Number of data points to consider during each sampling. It must be less than or equal to the length of the series. samplesint, default 500Number of times the bootstrap procedure is performed. **kwdsOptions to pass to matplotlib plotting method. Returns: matplotlib.figure.FigureMatplotlib figure.",['>>> s = pd.Series(np.random.uniform(size=100))\n>>> pd.plotting.bootstrap_plot(s)  \n<Figure size 640x480 with 6 Axes>']
1541,..\pandas\reference\api\pandas.Series.shift.html,pandas.Series.shift,"Series.shift(periods=1, freq=None, axis=0, fill_value=<no_default>, suffix=None)[source]# Shift index by desired number of periods with an optional time freq. When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError), the index will be increased using the periods and the freq. freq can be inferred when specified as “infer” as long as either freq or inferred_freq attribute is set in the index.","Parameters: periodsint or SequenceNumber of periods to shift. Can be positive or negative. If an iterable of ints, the data will be shifted once by each int. This is equivalent to shifting by one value at a time and concatenating all resulting frames. The resulting columns will have the shift suffixed to their column names. For multiple periods, axis must not be 1. freqDateOffset, tseries.offsets, timedelta, or str, optionalOffset to use from the tseries module or time rule (e.g. ‘EOM’). If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as “infer” then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneShift direction. For Series this parameter is unused and defaults to 0. fill_valueobject, optionalThe scalar value to use for newly introduced missing values. the default depends on the dtype of self. For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. NaT is used. For extension dtypes, self.dtype.na_value is used. suffixstr, optionalIf str and periods is an iterable, this is added after the column name and before the shift value for each shifted column name. Returns: Series/DataFrameCopy of input object, shifted.","['>>> df = pd.DataFrame({""Col1"": [10, 20, 15, 30, 45],\n...                    ""Col2"": [13, 23, 18, 33, 48],\n...                    ""Col3"": [17, 27, 22, 37, 52]},\n...                   index=pd.date_range(""2020-01-01"", ""2020-01-05""))\n>>> df\n            Col1  Col2  Col3\n2020-01-01    10    13    17\n2020-01-02    20    23    27\n2020-01-03    15    18    22\n2020-01-04    30    33    37\n2020-01-05    45    48    52', '>>> df.shift(periods=3)\n            Col1  Col2  Col3\n2020-01-01   NaN   NaN   NaN\n2020-01-02   NaN   NaN   NaN\n2020-01-03   NaN   NaN   NaN\n2020-01-04  10.0  13.0  17.0\n2020-01-05  20.0  23.0  27.0', '>>> df.shift(periods=1, axis=""columns"")\n            Col1  Col2  Col3\n2020-01-01   NaN    10    13\n2020-01-02   NaN    20    23\n2020-01-03   NaN    15    18\n2020-01-04   NaN    30    33\n2020-01-05   NaN    45    48', '>>> df.shift(periods=3, fill_value=0)\n            Col1  Col2  Col3\n2020-01-01     0     0     0\n2020-01-02     0     0     0\n2020-01-03     0     0     0\n2020-01-04    10    13    17\n2020-01-05    20    23    27', '>>> df.shift(periods=3, freq=""D"")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52', '>>> df.shift(periods=3, freq=""infer"")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52', "">>> df['Col1'].shift(periods=[0, 1, 2])\n            Col1_0  Col1_1  Col1_2\n2020-01-01      10     NaN     NaN\n2020-01-02      20    10.0     NaN\n2020-01-03      15    20.0    10.0\n2020-01-04      30    15.0    20.0\n2020-01-05      45    30.0    15.0""]"
1542,..\pandas\reference\api\pandas.DataFrame.max.html,pandas.DataFrame.max,"DataFrame.max(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the maximum of the values over the requested axis. If you want the index of the maximum, use idxmax. This is the equivalent of the numpy.ndarray method argmax.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","["">>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64"", '>>> s.max()\n8']"
1543,..\pandas\reference\api\pandas.Index.equals.html,pandas.Index.equals,Index.equals(other)[source]# Determine if two Index object are equal. The things that are being compared are: The elements inside the Index object. The order of the elements inside the Index object.,Parameters: otherAnyThe other object to compare against. Returns: boolTrue if “other” is an Index and it has the same elements and order as the calling index; False otherwise.,"["">>> idx1 = pd.Index([1, 2, 3])\n>>> idx1\nIndex([1, 2, 3], dtype='int64')\n>>> idx1.equals(pd.Index([1, 2, 3]))\nTrue"", '>>> idx2 = pd.Index([""1"", ""2"", ""3""])\n>>> idx2\nIndex([\'1\', \'2\', \'3\'], dtype=\'object\')', '>>> idx1.equals(idx2)\nFalse', "">>> ascending_idx = pd.Index([1, 2, 3])\n>>> ascending_idx\nIndex([1, 2, 3], dtype='int64')\n>>> descending_idx = pd.Index([3, 2, 1])\n>>> descending_idx\nIndex([3, 2, 1], dtype='int64')\n>>> ascending_idx.equals(descending_idx)\nFalse"", "">>> int64_idx = pd.Index([1, 2, 3], dtype='int64')\n>>> int64_idx\nIndex([1, 2, 3], dtype='int64')\n>>> uint64_idx = pd.Index([1, 2, 3], dtype='uint64')\n>>> uint64_idx\nIndex([1, 2, 3], dtype='uint64')\n>>> int64_idx.equals(uint64_idx)\nTrue""]"
1544,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_quarter_start.html,pandas.tseries.offsets.SemiMonthEnd.is_quarter_start,SemiMonthEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1545,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.name.html,pandas.tseries.offsets.BQuarterEnd.name,BQuarterEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1546,..\pandas\reference\api\pandas.tseries.offsets.FY5253.get_rule_code_suffix.html,pandas.tseries.offsets.FY5253.get_rule_code_suffix,FY5253.get_rule_code_suffix()#,No parameters found,[]
1547,..\pandas\reference\api\pandas.plotting.boxplot.html,pandas.plotting.boxplot,"pandas.plotting.boxplot(data, column=None, by=None, ax=None, fontsize=None, rot=0, grid=True, figsize=None, layout=None, return_type=None, **kwargs)[source]# Make a box plot from DataFrame columns. Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots. For further details see Wikipedia’s entry for boxplot. Notes The return type depends on the return_type parameter: ‘axes’ : object of class matplotlib.axes.Axes ‘dict’ : dict of matplotlib.lines.Line2D objects ‘both’ : a namedtuple with structure (ax, lines) For data grouped with by, return a Series of the above or a numpy array: Series array (for return_type = None) Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned.","Parameters: dataDataFrameThe data to visualize. columnstr or list of str, optionalColumn name or list of names, or vector. Can be any valid input to pandas.DataFrame.groupby(). bystr or array-like, optionalColumn in the DataFrame to pandas.DataFrame.groupby(). One box-plot will be done per value of columns in by. axobject of class matplotlib.axes.Axes, optionalThe matplotlib axes to be used by boxplot. fontsizefloat or strTick label font size in points or as a string (e.g., large). rotfloat, default 0The rotation angle of labels (in degrees) with respect to the screen coordinate system. gridbool, default TrueSetting this to True will show the grid. figsizeA tuple (width, height) in inchesThe size of the figure to create in matplotlib. layouttuple (rows, columns), optionalFor example, (3, 5) will display the subplots using 3 rows and 5 columns, starting from the top-left. return_type{‘axes’, ‘dict’, ‘both’} or None, default ‘axes’The kind of object to return. The default is axes. ‘axes’ returns the matplotlib axes the boxplot is drawn on. ‘dict’ returns a dictionary whose values are the matplotlib Lines of the boxplot. ‘both’ returns a namedtuple with the axes and dict. when grouping with by, a Series mapping columns to return_type is returned. If return_type is None, a NumPy array of axes with the same shape as layout is returned. **kwargsAll other plotting keyword arguments to be passed to matplotlib.pyplot.boxplot(). Returns: resultSee Notes.","["">>> np.random.seed(1234)\n>>> df = pd.DataFrame(np.random.randn(10, 4),\n...                   columns=['Col1', 'Col2', 'Col3', 'Col4'])\n>>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])"", "">>> df = pd.DataFrame(np.random.randn(10, 2),\n...                   columns=['Col1', 'Col2'])\n>>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n...                      'B', 'B', 'B', 'B', 'B'])\n>>> boxplot = df.boxplot(by='X')"", "">>> df = pd.DataFrame(np.random.randn(10, 3),\n...                   columns=['Col1', 'Col2', 'Col3'])\n>>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A',\n...                      'B', 'B', 'B', 'B', 'B'])\n>>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A',\n...                      'B', 'A', 'B', 'A', 'B'])\n>>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y'])"", "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      layout=(2, 1))"", '>>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15)', "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], return_type='axes')\n>>> type(boxplot)\n<class 'matplotlib.axes._axes.Axes'>"", "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      return_type='axes')\n>>> type(boxplot)\n<class 'pandas.core.series.Series'>"", "">>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X',\n...                      return_type=None)\n>>> type(boxplot)\n<class 'numpy.ndarray'>""]"
1548,..\pandas\reference\api\pandas.DataFrame.mean.html,pandas.DataFrame.mean,"DataFrame.mean(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the mean of the values over the requested axis.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","['>>> s = pd.Series([1, 2, 3])\n>>> s.mean()\n2.0', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n>>> df\n       a   b\ntiger  1   2\nzebra  2   3\n>>> df.mean()\na   1.5\nb   2.5\ndtype: float64"", '>>> df.mean(axis=1)\ntiger   1.5\nzebra   2.5\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n...                   index=['tiger', 'zebra'])\n>>> df.mean(numeric_only=True)\na   1.5\ndtype: float64""]"
1549,..\pandas\reference\api\pandas.Series.size.html,pandas.Series.size,property Series.size[source]# Return the number of elements in the underlying data.,No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.size\n3"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.size\n3""]"
1550,..\pandas\reference\api\pandas.Index.factorize.html,pandas.Index.factorize,"Index.factorize(sort=False, use_na_sentinel=True)[source]# Encode the object as an enumerated type or categorical variable. This method is useful for obtaining a numeric representation of an array when all that matters is identifying distinct values. factorize is available as both a top-level function pandas.factorize(), and as a method Series.factorize() and Index.factorize(). Notes Reference the user guide for more examples.","Parameters: sortbool, default FalseSort uniques and shuffle codes to maintain the relationship. use_na_sentinelbool, default TrueIf True, the sentinel -1 will be used for NaN values. If False, NaN values will be encoded as non-negative integers and will not drop the NaN from the uniques of the values. Added in version 1.5.0. Returns: codesndarrayAn integer ndarray that’s an indexer into uniques. uniques.take(codes) will have the same values as values. uniquesndarray, Index, or CategoricalThe unique valid values. When values is Categorical, uniques is a Categorical. When values is some other pandas object, an Index is returned. Otherwise, a 1-D ndarray is returned. Note Even if there’s a missing value in values, uniques will not contain an entry for it.","['>>> codes, uniques = pd.factorize(np.array([\'b\', \'b\', \'a\', \'c\', \'b\'], dtype=""O""))\n>>> codes\narray([0, 0, 1, 2, 0])\n>>> uniques\narray([\'b\', \'a\', \'c\'], dtype=object)', '>>> codes, uniques = pd.factorize(np.array([\'b\', \'b\', \'a\', \'c\', \'b\'], dtype=""O""),\n...                               sort=True)\n>>> codes\narray([1, 1, 0, 2, 1])\n>>> uniques\narray([\'a\', \'b\', \'c\'], dtype=object)', '>>> codes, uniques = pd.factorize(np.array([\'b\', None, \'a\', \'c\', \'b\'], dtype=""O""))\n>>> codes\narray([ 0, -1,  1,  2,  0])\n>>> uniques\narray([\'b\', \'a\', \'c\'], dtype=object)', "">>> cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])\n>>> codes, uniques = pd.factorize(cat)\n>>> codes\narray([0, 0, 1])\n>>> uniques\n['a', 'c']\nCategories (3, object): ['a', 'b', 'c']"", "">>> cat = pd.Series(['a', 'a', 'c'])\n>>> codes, uniques = pd.factorize(cat)\n>>> codes\narray([0, 0, 1])\n>>> uniques\nIndex(['a', 'c'], dtype='object')"", '>>> values = np.array([1, 2, 1, np.nan])\n>>> codes, uniques = pd.factorize(values)  # default: use_na_sentinel=True\n>>> codes\narray([ 0,  1,  0, -1])\n>>> uniques\narray([1., 2.])', '>>> codes, uniques = pd.factorize(values, use_na_sentinel=False)\n>>> codes\narray([0, 1, 0, 2])\n>>> uniques\narray([ 1.,  2., nan])']"
1551,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_year_end.html,pandas.tseries.offsets.SemiMonthEnd.is_year_end,SemiMonthEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1552,..\pandas\reference\api\pandas.tseries.offsets.FY5253.get_year_end.html,pandas.tseries.offsets.FY5253.get_year_end,FY5253.get_year_end(dt)#,No parameters found,[]
1553,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.nanos.html,pandas.tseries.offsets.BQuarterEnd.nanos,BQuarterEnd.nanos#,No parameters found,[]
1554,..\pandas\reference\api\pandas.plotting.deregister_matplotlib_converters.html,pandas.plotting.deregister_matplotlib_converters,"pandas.plotting.deregister_matplotlib_converters()[source]# Remove pandas formatters and converters. Removes the custom converters added by register(). This attempts to set the state of the registry back to the state before pandas registered its own units. Converters for pandas’ own types like Timestamp and Period are removed completely. Converters for types pandas overwrites, like datetime.datetime, are restored to their original value.",No parameters found,"['>>> pd.plotting.register_matplotlib_converters()', "">>> df = pd.DataFrame({'ts': pd.period_range('2020', periods=2, freq='M'),\n...                    'y': [1, 2]\n...                    })\n>>> plot = df.plot.line(x='ts', y='y')"", '>>> pd.set_option(""plotting.matplotlib.register_converters"",\n...               False)  \n>>> df.plot.line(x=\'ts\', y=\'y\')  \nTraceback (most recent call last):\nTypeError: float() argument must be a string or a real number, not \'Period\'']"
1555,..\pandas\reference\api\pandas.DataFrame.median.html,pandas.DataFrame.median,"DataFrame.median(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the median of the values over the requested axis.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","['>>> s = pd.Series([1, 2, 3])\n>>> s.median()\n2.0', "">>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n>>> df\n       a   b\ntiger  1   2\nzebra  2   3\n>>> df.median()\na   1.5\nb   2.5\ndtype: float64"", '>>> df.median(axis=1)\ntiger   1.5\nzebra   2.5\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n...                   index=['tiger', 'zebra'])\n>>> df.median(numeric_only=True)\na   1.5\ndtype: float64""]"
1556,..\pandas\reference\api\pandas.Series.skew.html,pandas.Series.skew,"Series.skew(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return unbiased skew over requested axis. Normalized by N-1.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","['>>> s = pd.Series([1, 2, 3])\n>>> s.skew()\n0.0', "">>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4], 'c': [1, 3, 5]},\n...                   index=['tiger', 'zebra', 'cow'])\n>>> df\n        a   b   c\ntiger   1   2   1\nzebra   2   3   3\ncow     3   4   5\n>>> df.skew()\na   0.0\nb   0.0\nc   0.0\ndtype: float64"", '>>> df.skew(axis=1)\ntiger   1.732051\nzebra  -1.732051\ncow     0.000000\ndtype: float64', "">>> df = pd.DataFrame({'a': [1, 2, 3], 'b': ['T', 'Z', 'X']},\n...                   index=['tiger', 'zebra', 'cow'])\n>>> df.skew(numeric_only=True)\na   0.0\ndtype: float64""]"
1557,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.is_year_start.html,pandas.tseries.offsets.SemiMonthEnd.is_year_start,SemiMonthEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1558,..\pandas\reference\api\pandas.Index.fillna.html,pandas.Index.fillna,"Index.fillna(value=None, downcast=<no_default>)[source]# Fill NA/NaN values with the specified value.","Parameters: valuescalarScalar value to use to fill holes (e.g. 0). This value cannot be a list-likes. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.1.0. Returns: Index","["">>> idx = pd.Index([np.nan, np.nan, 3])\n>>> idx.fillna(0)\nIndex([0.0, 0.0, 3.0], dtype='float64')""]"
1559,..\pandas\reference\api\pandas.tseries.offsets.FY5253.html,pandas.tseries.offsets.FY5253,"class pandas.tseries.offsets.FY5253# Describes 52-53 week fiscal year. This is also known as a 4-4-5 calendar. It is used by companies that desire that their fiscal year always end on the same day of the week. It is a method of managing accounting periods. It is a common calendar structure for some industries, such as retail, manufacturing and parking industry. For more information see: https://en.wikipedia.org/wiki/4-4-5_calendar The year may either: end on the last X day of the Y month. end on the last X day closest to the last day of the Y month. X is a specific day of the week. Y is a certain month of the year Examples In the example below the default parameters give the next 52-53 week fiscal year. By the parameter startingMonth we can specify the month in which fiscal years end. 52-53 week fiscal year can be specified by weekday and variation parameters. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code startingMonth variation weekday","Parameters: nintThe number of fiscal years represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekdayint {0, 1, …, 6}, default 0A specific integer for the day of the week. 0 is Monday 1 is Tuesday 2 is Wednesday 3 is Thursday 4 is Friday 5 is Saturday 6 is Sunday. startingMonthint {1, 2, … 12}, default 1The month in which the fiscal year ends. variationstr, default “nearest”Method of employing 4-4-5 calendar. There are two options: “nearest” means year end is weekday closest to last day of month in year. “last” means year end is final weekday of the final month in fiscal year.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.FY5253()\nTimestamp('2022-01-31 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.FY5253(startingMonth=3)\nTimestamp('2022-03-28 00:00:00')"", '>>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.FY5253(weekday=5, startingMonth=12, variation=""last"")\nTimestamp(\'2022-12-31 00:00:00\')']"
1560,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.normalize.html,pandas.tseries.offsets.BQuarterEnd.normalize,BQuarterEnd.normalize#,No parameters found,[]
1561,..\pandas\reference\api\pandas.plotting.lag_plot.html,pandas.plotting.lag_plot,"pandas.plotting.lag_plot(series, lag=1, ax=None, **kwds)[source]# Lag plot for time series.","Parameters: seriesSeriesThe time series to visualize. lagint, default 1Lag length of the scatter plot. axMatplotlib axis object, optionalThe matplotlib axis object to use. **kwdsMatplotlib scatter method keyword arguments. Returns: matplotlib.axes.Axes","['>>> np.random.seed(5)\n>>> x = np.cumsum(np.random.normal(loc=1, scale=5, size=50))\n>>> s = pd.Series(x)\n>>> s.plot()', "">>> pd.plotting.lag_plot(s, lag=1)\n<Axes: xlabel='y(t)', ylabel='y(t + 1)'>""]"
1562,..\pandas\reference\api\pandas.Index.get_indexer.html,pandas.Index.get_indexer,"final Index.get_indexer(target, method=None, limit=None, tolerance=None)[source]# Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index. Notes Returns -1 for unmatched values, for further explanation see the example below.","Parameters: targetIndex method{None, ‘pad’/’ffill’, ‘backfill’/’bfill’, ‘nearest’}, optional default: exact matches only. pad / ffill: find the PREVIOUS index value if no exact match. backfill / bfill: use NEXT index value if no exact match nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value. limitint, optionalMaximum number of consecutive labels in target to match for inexact matches. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: np.ndarray[np.intp]Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1.","["">>> index = pd.Index(['c', 'a', 'b'])\n>>> index.get_indexer(['a', 'b', 'x'])\narray([ 1,  2, -1])""]"
1563,..\pandas\reference\api\pandas.Series.sort_index.html,pandas.Series.sort_index,"Series.sort_index(*, axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, ignore_index=False, key=None)[source]# Sort Series by index labels. Returns a new Series sorted by label if inplace argument is False, otherwise updates the original series and returns None.","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. levelint, optionalIf not None, sort on values in specified index level(s). ascendingbool or list-like of bools, default TrueSort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually. inplacebool, default FalseIf True, perform operation in-place. kind{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, default ‘quicksort’Choice of sorting algorithm. See also numpy.sort() for more information. ‘mergesort’ and ‘stable’ are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label. na_position{‘first’, ‘last’}, default ‘last’If ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end. Not implemented for MultiIndex. sort_remainingbool, default TrueIf True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. keycallable, optionalIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape. Returns: Series or NoneThe original Series sorted by the labels or None if inplace=True.","["">>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, 4])\n>>> s.sort_index()\n1    c\n2    b\n3    a\n4    d\ndtype: object"", '>>> s.sort_index(ascending=False)\n4    d\n3    a\n2    b\n1    c\ndtype: object', "">>> s = pd.Series(['a', 'b', 'c', 'd'], index=[3, 2, 1, np.nan])\n>>> s.sort_index(na_position='first')\nNaN     d\n 1.0    c\n 2.0    b\n 3.0    a\ndtype: object"", "">>> arrays = [np.array(['qux', 'qux', 'foo', 'foo',\n...                     'baz', 'baz', 'bar', 'bar']),\n...           np.array(['two', 'one', 'two', 'one',\n...                     'two', 'one', 'two', 'one'])]\n>>> s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8], index=arrays)\n>>> s.sort_index(level=1)\nbar  one    8\nbaz  one    6\nfoo  one    4\nqux  one    2\nbar  two    7\nbaz  two    5\nfoo  two    3\nqux  two    1\ndtype: int64"", '>>> s.sort_index(level=1, sort_remaining=False)\nqux  one    2\nfoo  one    4\nbaz  one    6\nbar  one    8\nqux  two    1\nfoo  two    3\nbaz  two    5\nbar  two    7\ndtype: int64', "">>> s = pd.Series([1, 2, 3, 4], index=['A', 'b', 'C', 'd'])\n>>> s.sort_index(key=lambda x : x.str.lower())\nA    1\nb    2\nC    3\nd    4\ndtype: int64""]"
1564,..\pandas\reference\api\pandas.DataFrame.melt.html,pandas.DataFrame.melt,"DataFrame.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)[source]# Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’. Notes Reference the user guide for more examples.","Parameters: id_varsscalar, tuple, list, or ndarray, optionalColumn(s) to use as identifier variables. value_varsscalar, tuple, list, or ndarray, optionalColumn(s) to unpivot. If not specified, uses all columns that are not set as id_vars. var_namescalar, default NoneName to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’. value_namescalar, default ‘value’Name to use for the ‘value’ column, can’t be an existing column label. col_levelscalar, optionalIf columns are a MultiIndex then use this level to melt. ignore_indexbool, default TrueIf True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary. Returns: DataFrameUnpivoted DataFrame.","["">>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n>>> df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6"", "">>> df.melt(id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5"", "">>> df.melt(id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6"", "">>> df.melt(id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5"", "">>> df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6"", "">>> df.columns = [list('ABC'), list('DEF')]\n>>> df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6"", "">>> df.melt(col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5"", "">>> df.melt(id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5""]"
1565,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.kwds.html,pandas.tseries.offsets.SemiMonthEnd.kwds,SemiMonthEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1566,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.rule_code.html,pandas.tseries.offsets.BQuarterEnd.rule_code,BQuarterEnd.rule_code#,No parameters found,[]
1567,..\pandas\reference\api\pandas.plotting.parallel_coordinates.html,pandas.plotting.parallel_coordinates,"pandas.plotting.parallel_coordinates(frame, class_column, cols=None, ax=None, color=None, use_columns=False, xticks=None, colormap=None, axvlines=True, axvlines_kwds=None, sort_labels=False, **kwargs)[source]# Parallel coordinates plotting.","Parameters: frameDataFrame class_columnstrColumn name containing class names. colslist, optionalA list of column names to use. axmatplotlib.axis, optionalMatplotlib axis object. colorlist or tuple, optionalColors to use for the different classes. use_columnsbool, optionalIf true, columns will be used as xticks. xtickslist or tuple, optionalA list of values to use for xticks. colormapstr or matplotlib colormap, default NoneColormap to use for line colors. axvlinesbool, optionalIf true, vertical lines will be added at each xtick. axvlines_kwdskeywords, optionalOptions to be passed to axvline method for vertical lines. sort_labelsbool, default FalseSort class_column labels, useful when assigning colors. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes","["">>> df = pd.read_csv(\n...     'https://raw.githubusercontent.com/pandas-dev/'\n...     'pandas/main/pandas/tests/io/data/csv/iris.csv'\n... )\n>>> pd.plotting.parallel_coordinates(\n...     df, 'Name', color=('#556270', '#4ECDC4', '#C7F464')\n... )""]"
1568,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_anchored.html,pandas.tseries.offsets.FY5253.is_anchored,FY5253.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1569,..\pandas\reference\api\pandas.Index.get_indexer_for.html,pandas.Index.get_indexer_for,final Index.get_indexer_for(target)[source]# Guaranteed return of an indexer even when non-unique. This dispatches to get_indexer or get_indexer_non_unique as appropriate.,Returns: np.ndarray[np.intp]List of indices.,"["">>> idx = pd.Index([np.nan, 'var1', np.nan])\n>>> idx.get_indexer_for([np.nan])\narray([0, 2])""]"
1570,..\pandas\reference\api\pandas.DataFrame.memory_usage.html,pandas.DataFrame.memory_usage,"DataFrame.memory_usage(index=True, deep=False)[source]# Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of object dtype. This value is displayed in DataFrame.info by default. This can be suppressed by setting pandas.options.display.memory_usage to False. Notes See the Frequently Asked Questions for more details.","Parameters: indexbool, default TrueSpecifies whether to include the memory usage of the DataFrame’s index in returned Series. If index=True, the memory usage of the index is the first item in the output. deepbool, default FalseIf True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values. Returns: SeriesA Series whose index is the original column names and whose values is the memory usage of each column in bytes.","["">>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool']\n>>> data = dict([(t, np.ones(shape=5000, dtype=int).astype(t))\n...              for t in dtypes])\n>>> df = pd.DataFrame(data)\n>>> df.head()\n   int64  float64            complex128  object  bool\n0      1      1.0              1.0+0.0j       1  True\n1      1      1.0              1.0+0.0j       1  True\n2      1      1.0              1.0+0.0j       1  True\n3      1      1.0              1.0+0.0j       1  True\n4      1      1.0              1.0+0.0j       1  True"", '>>> df.memory_usage()\nIndex           128\nint64         40000\nfloat64       40000\ncomplex128    80000\nobject        40000\nbool           5000\ndtype: int64', '>>> df.memory_usage(index=False)\nint64         40000\nfloat64       40000\ncomplex128    80000\nobject        40000\nbool           5000\ndtype: int64', '>>> df.memory_usage(deep=True)\nIndex            128\nint64          40000\nfloat64        40000\ncomplex128     80000\nobject        180000\nbool            5000\ndtype: int64', "">>> df['object'].astype('category').memory_usage(deep=True)\n5244""]"
1571,..\pandas\reference\api\pandas.plotting.plot_params.html,pandas.plotting.plot_params,"pandas.plotting.plot_params = {'xaxis.compat': False}# Stores pandas plotting options. Allows for parameter aliasing so you can just use parameter names that are the same as the plot function parameters, but is stored in a canonical format that makes it easy to breakdown into groups later.",No parameters found,"['>>> np.random.seed(42)\n>>> df = pd.DataFrame({\'A\': np.random.randn(10),\n...                   \'B\': np.random.randn(10)},\n...                   index=pd.date_range(""1/1/2000"",\n...                   freq=\'4MS\', periods=10))\n>>> with pd.plotting.plot_params.use(""x_compat"", True):\n...     _ = df[""A""].plot(color=""r"")\n...     _ = df[""B""].plot(color=""g"")']"
1572,..\pandas\reference\api\pandas.Series.sort_values.html,pandas.Series.sort_values,"Series.sort_values(*, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]# Sort by the values. Sort a Series in ascending or descending order by some criterion.","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. ascendingbool or list of bools, default TrueIf True, sort values in ascending order, otherwise descending. inplacebool, default FalseIf True, perform operation in-place. kind{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}, default ‘quicksort’Choice of sorting algorithm. See also numpy.sort() for more information. ‘mergesort’ and ‘stable’ are the only stable  algorithms. na_position{‘first’ or ‘last’}, default ‘last’Argument ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end. ignore_indexbool, default FalseIf True, the resulting axis will be labeled 0, 1, …, n - 1. keycallable, optionalIf not None, apply the key function to the series values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return an array-like. Returns: Series or NoneSeries ordered by values or None if inplace=True.","['>>> s = pd.Series([np.nan, 1, 3, 10, 5])\n>>> s\n0     NaN\n1     1.0\n2     3.0\n3     10.0\n4     5.0\ndtype: float64', '>>> s.sort_values(ascending=True)\n1     1.0\n2     3.0\n4     5.0\n3    10.0\n0     NaN\ndtype: float64', '>>> s.sort_values(ascending=False)\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64', "">>> s.sort_values(na_position='first')\n0     NaN\n1     1.0\n2     3.0\n4     5.0\n3    10.0\ndtype: float64"", "">>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n>>> s\n0    z\n1    b\n2    d\n3    a\n4    c\ndtype: object"", '>>> s.sort_values()\n3    a\n1    b\n4    c\n2    d\n0    z\ndtype: object', "">>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n>>> s.sort_values()\n1    B\n3    D\n0    a\n2    c\n4    e\ndtype: object\n>>> s.sort_values(key=lambda x: x.str.lower())\n0    a\n1    B\n2    c\n3    D\n4    e\ndtype: object"", '>>> s = pd.Series([-4, -2, 0, 2, 4])\n>>> s.sort_values(key=np.sin)\n1   -2\n4    4\n2    0\n0   -4\n3    2\ndtype: int64', '>>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n0   -4\n3    2\n4    4\n1   -2\n2    0\ndtype: int64']"
1573,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.n.html,pandas.tseries.offsets.SemiMonthEnd.n,SemiMonthEnd.n#,No parameters found,[]
1574,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_month_end.html,pandas.tseries.offsets.FY5253.is_month_end,FY5253.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1575,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.agg.html,pandas.core.groupby.SeriesGroupBy.agg,"SeriesGroupBy.agg(func=None, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.","Parameters: funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] None, in which case **kwargs are used with Named Aggregation. Here the output has one column for each element in **kwargs. The name of the column is keyword, whereas the value determines the aggregation used to compute the values in the column. Can also accept a Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine. If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group’s index will be passed to the user defined function and optionally available for use. Deprecated since version 2.1.0: Passing a dictionary is deprecated and will raise in a future version of pandas. Pass a list of aggregations instead. *argsPositional arguments to pass to func. enginestr, default None 'cython' : Runs the function through C-extensions from cython. 'numba' : Runs the function through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function **kwargs If func is None, **kwargs are used to define the output names and aggregations via Named Aggregation. See func entry. Otherwise, keyword arguments to be passed into func. Returns: Series","['>>> s = pd.Series([1, 2, 3, 4])', '>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', '>>> s.groupby([1, 1, 2, 2]).min()\n1    1\n2    3\ndtype: int64', "">>> s.groupby([1, 1, 2, 2]).agg('min')\n1    1\n2    3\ndtype: int64"", "">>> s.groupby([1, 1, 2, 2]).agg(['min', 'max'])\n   min  max\n1    1    2\n2    3    4"", "">>> s.groupby([1, 1, 2, 2]).agg(\n...     minimum='min',\n...     maximum='max',\n... )\n   minimum  maximum\n1        1        2\n2        3        4"", '>>> s.groupby([1, 1, 2, 2]).agg(lambda x: x.astype(float).min())\n1    1.0\n2    3.0\ndtype: float64']"
1576,..\pandas\reference\api\pandas.Index.get_indexer_non_unique.html,pandas.Index.get_indexer_non_unique,Index.get_indexer_non_unique(target)[source]# Compute indexer and mask for new index given the current index. The indexer should be then used as an input to ndarray.take to align the current data to the new index.,Parameters: targetIndex Returns: indexernp.ndarray[np.intp]Integers from 0 to n - 1 indicating that the index at these positions matches the corresponding target values. Missing values in the target are marked by -1. missingnp.ndarray[np.intp]An indexer into the target of the values not found. These correspond to the -1 in the indexer array.,"["">>> index = pd.Index(['c', 'b', 'a', 'b', 'b'])\n>>> index.get_indexer_non_unique(['b', 'b'])\n(array([1, 3, 4, 1, 3, 4]), array([], dtype=int64))"", "">>> index = pd.Index(['c', 'b', 'a', 'b', 'b'])\n>>> index.get_indexer_non_unique(['q', 'r', 't'])\n(array([-1, -1, -1]), array([0, 1, 2]))"", "">>> index = pd.Index(['c', 'b', 'a', 'b', 'b'])\n>>> index.get_indexer_non_unique(['f', 'b', 's'])\n(array([-1,  1,  3,  4, -1]), array([0, 2]))""]"
1577,..\pandas\reference\api\pandas.tseries.offsets.BQuarterEnd.startingMonth.html,pandas.tseries.offsets.BQuarterEnd.startingMonth,BQuarterEnd.startingMonth#,No parameters found,[]
1578,..\pandas\reference\api\pandas.DataFrame.merge.html,pandas.DataFrame.merge,"DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=None, indicator=False, validate=None)[source]# Merge DataFrame or named Series objects with a database-style join. A named Series object is treated as a DataFrame with a single named column. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed. Warning If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.","Parameters: rightDataFrame or named SeriesObject to merge with. how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’Type of merge to be performed. left: use only keys from left frame, similar to a SQL left outer join; preserve key order. right: use only keys from right frame, similar to a SQL right outer join; preserve key order. outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. cross: creates the cartesian product from both frames, preserves the order of the left keys. onlabel or listColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. left_onlabel or list, or array-likeColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_onlabel or list, or array-likeColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. left_indexbool, default FalseUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels. right_indexbool, default FalseUse the index from the right DataFrame as the join key. Same caveats as left_index. sortbool, default FalseSort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword). suffixeslist-like, default is (“_x”, “_y”)A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. copybool, default TrueIf False, avoid copy if possible. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True indicatorbool or str, default FalseIf True, adds a column to the output DataFrame called “_merge” with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of “left_only” for observations whose merge key only appears in the left DataFrame, “right_only” for observations whose merge key only appears in the right DataFrame, and “both” if the observation’s merge key is found in both DataFrames. validatestr, optionalIf specified, checks if merge is of specified type. “one_to_one” or “1:1”: check if merge keys are unique in both left and right datasets. “one_to_many” or “1:m”: check if merge keys are unique in left dataset. “many_to_one” or “m:1”: check if merge keys are unique in right dataset. “many_to_many” or “m:m”: allowed, but does not result in checks. Returns: DataFrameA DataFrame of the two merged objects.","["">>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8"", "">>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  bar        2  bar        6\n3  baz        3  baz        7\n4  foo        5  foo        5\n5  foo        5  foo        8"", "">>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  bar           2  bar            6\n3  baz           3  baz            7\n4  foo           5  foo            5\n5  foo           5  foo            8"", "">>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')"", "">>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4"", "">>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3"", "">>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN"", "">>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8"", "">>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8""]"
1579,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.name.html,pandas.tseries.offsets.SemiMonthEnd.name,SemiMonthEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1580,..\pandas\reference\api\pandas.Index.get_level_values.html,pandas.Index.get_level_values,"Index.get_level_values(level)[source]# Return an Index of values for requested level. This is primarily useful to get an individual level of values from a MultiIndex, but is provided on Index as well for compatibility. Notes For Index, level should be 0, since there are no multiple levels.","Parameters: levelint or strIt is either the integer position or the name of the level. Returns: IndexCalling object, as there is only one level in the Index.","["">>> idx = pd.Index(list('abc'))\n>>> idx\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> idx.get_level_values(0)\nIndex(['a', 'b', 'c'], dtype='object')""]"
1581,..\pandas\reference\api\pandas.plotting.radviz.html,pandas.plotting.radviz,"pandas.plotting.radviz(frame, class_column, ax=None, color=None, colormap=None, **kwds)[source]# Plot a multidimensional dataset in 2D. Each Series in the DataFrame is represented as a evenly distributed slice on a circle. Each data point is rendered in the circle according to the value on each Series. Highly correlated Series in the DataFrame are placed closer on the unit circle. RadViz allow to project a N-dimensional data set into a 2D space where the influence of each dimension can be interpreted as a balance between the influence of all dimensions. More info available at the original article describing RadViz.","Parameters: frameDataFrameObject holding the data. class_columnstrColumn name containing the name of the data point category. axmatplotlib.axes.Axes, optionalA plot instance to which to add the information. colorlist[str] or tuple[str], optionalAssign a color to each category. Example: [‘blue’, ‘green’]. colormapstr or matplotlib.colors.Colormap, default NoneColormap to select colors from. If string, load colormap with that name from matplotlib. **kwdsOptions to pass to matplotlib scatter plotting method. Returns: matplotlib.axes.Axes","["">>> df = pd.DataFrame(\n...     {\n...         'SepalLength': [6.5, 7.7, 5.1, 5.8, 7.6, 5.0, 5.4, 4.6, 6.7, 4.6],\n...         'SepalWidth': [3.0, 3.8, 3.8, 2.7, 3.0, 2.3, 3.0, 3.2, 3.3, 3.6],\n...         'PetalLength': [5.5, 6.7, 1.9, 5.1, 6.6, 3.3, 4.5, 1.4, 5.7, 1.0],\n...         'PetalWidth': [1.8, 2.2, 0.4, 1.9, 2.1, 1.0, 1.5, 0.2, 2.1, 0.2],\n...         'Category': [\n...             'virginica',\n...             'virginica',\n...             'setosa',\n...             'virginica',\n...             'virginica',\n...             'versicolor',\n...             'versicolor',\n...             'setosa',\n...             'virginica',\n...             'setosa'\n...         ]\n...     }\n... )\n>>> pd.plotting.radviz(df, 'Category')""]"
1582,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_month_start.html,pandas.tseries.offsets.FY5253.is_month_start,FY5253.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1583,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.aggregate.html,pandas.core.groupby.SeriesGroupBy.aggregate,"SeriesGroupBy.aggregate(func=None, *args, engine=None, engine_kwargs=None, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below.","Parameters: funcfunction, str, list, dict or NoneFunction to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] None, in which case **kwargs are used with Named Aggregation. Here the output has one column for each element in **kwargs. The name of the column is keyword, whereas the value determines the aggregation used to compute the values in the column. Can also accept a Numba JIT function with engine='numba' specified. Only passing a single function is supported with this engine. If the 'numba' engine is chosen, the function must be a user defined function with values and index as the first and second arguments respectively in the function signature. Each group’s index will be passed to the user defined function and optionally available for use. Deprecated since version 2.1.0: Passing a dictionary is deprecated and will raise in a future version of pandas. Pass a list of aggregations instead. *argsPositional arguments to pass to func. enginestr, default None 'cython' : Runs the function through C-extensions from cython. 'numba' : Runs the function through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to the function **kwargs If func is None, **kwargs are used to define the output names and aggregations via Named Aggregation. See func entry. Otherwise, keyword arguments to be passed into func. Returns: Series","['>>> s = pd.Series([1, 2, 3, 4])', '>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', '>>> s.groupby([1, 1, 2, 2]).min()\n1    1\n2    3\ndtype: int64', "">>> s.groupby([1, 1, 2, 2]).agg('min')\n1    1\n2    3\ndtype: int64"", "">>> s.groupby([1, 1, 2, 2]).agg(['min', 'max'])\n   min  max\n1    1    2\n2    3    4"", "">>> s.groupby([1, 1, 2, 2]).agg(\n...     minimum='min',\n...     maximum='max',\n... )\n   minimum  maximum\n1        1        2\n2        3        4"", '>>> s.groupby([1, 1, 2, 2]).agg(lambda x: x.astype(float).min())\n1    1.0\n2    3.0\ndtype: float64']"
1584,..\pandas\reference\api\pandas.Series.sparse.density.html,pandas.Series.sparse.density,"Series.sparse.density[source]# The percent of non- fill_value points, as decimal.",No parameters found,"['>>> from pandas.arrays import SparseArray\n>>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n>>> s.density\n0.6']"
1585,..\pandas\reference\api\pandas.DataFrame.min.html,pandas.DataFrame.min,"DataFrame.min(axis=0, skipna=True, numeric_only=False, **kwargs)[source]# Return the minimum of the values over the requested axis. If you want the index of the minimum, use idxmin. This is the equivalent of the numpy.ndarray method argmin.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. For DataFrames, specifying axis=None will apply the aggregation across both axes. Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","["">>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64"", '>>> s.min()\n0']"
1586,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.calendar.html,pandas.tseries.offsets.BusinessDay.calendar,BusinessDay.calendar#,No parameters found,[]
1587,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.all.html,pandas.core.groupby.SeriesGroupBy.all,"SeriesGroupBy.all(skipna=True)[source]# Return True if all values in the group are truthful, else False.","Parameters: skipnabool, default TrueFlag to ignore nan values during truth testing. Returns: Series or DataFrameDataFrame or Series of boolean values, where a value is True if all elements are True within its respective group, False otherwise.","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 0], index=lst)\n>>> ser\na    1\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).all()\na     True\nb    False\ndtype: bool"", '>>> data = [[1, 0, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""ostrich"", ""penguin"", ""parrot""])\n>>> df\n         a  b  c\nostrich  1  0  3\npenguin  1  5  6\nparrot   7  8  9\n>>> df.groupby(by=[""a""]).all()\n       b      c\na\n1  False   True\n7   True   True']"
1588,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.nanos.html,pandas.tseries.offsets.SemiMonthEnd.nanos,SemiMonthEnd.nanos#,No parameters found,[]
1589,..\pandas\reference\api\pandas.Series.sparse.fill_value.html,pandas.Series.sparse.fill_value,"Series.sparse.fill_value[source]# Elements in data that are fill_value are not stored. For memory savings, this should be the most common value in the array.",No parameters found,"['>>> ser = pd.Series([0, 0, 2, 2, 2], dtype=""Sparse[int]"")\n>>> ser.sparse.fill_value\n0\n>>> spa_dtype = pd.SparseDtype(dtype=np.int32, fill_value=2)\n>>> ser = pd.Series([0, 0, 2, 2, 2], dtype=spa_dtype)\n>>> ser.sparse.fill_value\n2']"
1590,..\pandas\reference\api\pandas.DataFrame.mod.html,pandas.DataFrame.mod,"DataFrame.mod(other, axis='columns', level=None, fill_value=None)[source]# Get Modulo of dataframe and other, element-wise (binary operator mod). Equivalent to dataframe % other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmod. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1591,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_on_offset.html,pandas.tseries.offsets.FY5253.is_on_offset,FY5253.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1592,..\pandas\reference\api\pandas.Index.get_loc.html,pandas.Index.get_loc,"Index.get_loc(key)[source]# Get integer location, slice or boolean mask for requested label.","Parameters: keylabel Returns: int if unique index, slice if monotonic index, else mask","["">>> unique_index = pd.Index(list('abc'))\n>>> unique_index.get_loc('b')\n1"", "">>> monotonic_index = pd.Index(list('abbc'))\n>>> monotonic_index.get_loc('b')\nslice(1, 3, None)"", "">>> non_monotonic_index = pd.Index(list('abcb'))\n>>> non_monotonic_index.get_loc('b')\narray([False,  True, False,  True])""]"
1593,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.copy.html,pandas.tseries.offsets.BusinessDay.copy,BusinessDay.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1594,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.any.html,pandas.core.groupby.SeriesGroupBy.any,"SeriesGroupBy.any(skipna=True)[source]# Return True if any value in the group is truthful, else False.","Parameters: skipnabool, default TrueFlag to ignore nan values during truth testing. Returns: Series or DataFrameDataFrame or Series of boolean values, where a value is True if any element is True within its respective group, False otherwise.","["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 0], index=lst)\n>>> ser\na    1\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).any()\na     True\nb    False\ndtype: bool"", '>>> data = [[1, 0, 3], [1, 0, 6], [7, 1, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""ostrich"", ""penguin"", ""parrot""])\n>>> df\n         a  b  c\nostrich  1  0  3\npenguin  1  0  6\nparrot   7  1  9\n>>> df.groupby(by=[""a""]).any()\n       b      c\na\n1  False   True\n7   True   True']"
1595,..\pandas\reference\api\pandas.plotting.register_matplotlib_converters.html,pandas.plotting.register_matplotlib_converters,pandas.plotting.register_matplotlib_converters()[source]# Register pandas formatters and converters with matplotlib. This function modifies the global matplotlib.units.registry dictionary. pandas adds custom converters for pd.Timestamp pd.Period np.datetime64 datetime.datetime datetime.date datetime.time,No parameters found,"['>>> pd.plotting.register_matplotlib_converters()', "">>> df = pd.DataFrame({'ts': pd.period_range('2020', periods=2, freq='M'),\n...                    'y': [1, 2]\n...                    })\n>>> plot = df.plot.line(x='ts', y='y')"", '>>> pd.set_option(""plotting.matplotlib.register_converters"",\n...               False)  \n>>> df.plot.line(x=\'ts\', y=\'y\')  \nTraceback (most recent call last):\nTypeError: float() argument must be a string or a real number, not \'Period\'']"
1596,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.normalize.html,pandas.tseries.offsets.SemiMonthEnd.normalize,SemiMonthEnd.normalize#,No parameters found,[]
1597,..\pandas\reference\api\pandas.Series.sparse.from_coo.html,pandas.Series.sparse.from_coo,"classmethod Series.sparse.from_coo(A, dense_index=False)[source]# Create a Series with sparse values from a scipy.sparse.coo_matrix.","Parameters: Ascipy.sparse.coo_matrix dense_indexbool, default FalseIf False (default), the index consists of only the coords of the non-null entries of the original coo_matrix. If True, the index consists of the full sorted (row, col) coordinates of the coo_matrix. Returns: sSeriesA Series with sparse values.","['>>> from scipy import sparse', "">>> A = sparse.coo_matrix(\n...     ([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4)\n... )\n>>> A\n<COOrdinate sparse matrix of dtype 'float64'\n    with 3 stored elements and shape (3, 4)>"", '>>> A.todense()\nmatrix([[0., 0., 1., 2.],\n[3., 0., 0., 0.],\n[0., 0., 0., 0.]])', '>>> ss = pd.Series.sparse.from_coo(A)\n>>> ss\n0  2    1.0\n   3    2.0\n1  0    3.0\ndtype: Sparse[float64, nan]']"
1598,..\pandas\reference\api\pandas.DataFrame.mode.html,pandas.DataFrame.mode,"DataFrame.mode(axis=0, numeric_only=False, dropna=True)[source]# Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to iterate over while searching for the mode: 0 or ‘index’ : get mode of each column 1 or ‘columns’ : get mode of each row. numeric_onlybool, default FalseIf True, only apply to numeric columns. dropnabool, default TrueDon’t consider counts of NaN/NaT. Returns: DataFrameThe modes of each column or row.","["">>> df = pd.DataFrame([('bird', 2, 2),\n...                    ('mammal', 4, np.nan),\n...                    ('arthropod', 8, 0),\n...                    ('bird', 2, np.nan)],\n...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n...                   columns=('species', 'legs', 'wings'))\n>>> df\n           species  legs  wings\nfalcon        bird     2    2.0\nhorse       mammal     4    NaN\nspider   arthropod     8    0.0\nostrich       bird     2    NaN"", '>>> df.mode()\n  species  legs  wings\n0    bird   2.0    0.0\n1     NaN   NaN    2.0', '>>> df.mode(dropna=False)\n  species  legs  wings\n0    bird     2    NaN', '>>> df.mode(numeric_only=True)\n   legs  wings\n0   2.0    0.0\n1   NaN    2.0', "">>> df.mode(axis='columns', numeric_only=True)\n           0    1\nfalcon   2.0  NaN\nhorse    4.0  NaN\nspider   0.0  8.0\nostrich  2.0  NaN""]"
1599,..\pandas\reference\api\pandas.Series.sparse.html,pandas.Series.sparse,Series.sparse()[source]# Accessor for SparseSparse from other sparse matrix data types.,No parameters found,"['>>> ser = pd.Series([0, 0, 2, 2, 2], dtype=""Sparse[int]"")\n>>> ser.sparse.density\n0.6\n>>> ser.sparse.sp_values\narray([2, 2, 2])']"
1600,..\pandas\reference\api\pandas.Index.get_slice_bound.html,pandas.Index.get_slice_bound,"Index.get_slice_bound(label, side)[source]# Calculate slice bound that corresponds to given label. Returns leftmost (one-past-the-rightmost if side=='right') position of given label.","Parameters: labelobject side{‘left’, ‘right’} Returns: intIndex of label.","["">>> idx = pd.RangeIndex(5)\n>>> idx.get_slice_bound(3, 'left')\n3"", "">>> idx.get_slice_bound(3, 'right')\n4"", "">>> idx_duplicate = pd.Index(['a', 'b', 'a', 'c', 'd'])\n>>> idx_duplicate.get_slice_bound('a', 'left')\nTraceback (most recent call last):\nKeyError: Cannot get left slice bound for non-unique label: 'a'""]"
1601,..\pandas\reference\api\pandas.plotting.scatter_matrix.html,pandas.plotting.scatter_matrix,"pandas.plotting.scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False, diagonal='hist', marker='.', density_kwds=None, hist_kwds=None, range_padding=0.05, **kwargs)[source]# Draw a matrix of scatter plots.","Parameters: frameDataFrame alphafloat, optionalAmount of transparency applied. figsize(float,float), optionalA tuple (width, height) in inches. axMatplotlib axis object, optional gridbool, optionalSetting this to True will show the grid. diagonal{‘hist’, ‘kde’}Pick between ‘kde’ and ‘hist’ for either Kernel Density Estimation or Histogram plot in the diagonal. markerstr, optionalMatplotlib marker type, default ‘.’. density_kwdskeywordsKeyword arguments to be passed to kernel density estimate plot. hist_kwdskeywordsKeyword arguments to be passed to hist function. range_paddingfloat, default 0.05Relative extension of axis range in x and y with respect to (x_max - x_min) or (y_max - y_min). **kwargsKeyword arguments to be passed to scatter function. Returns: numpy.ndarrayA matrix of scatter plots.","["">>> df = pd.DataFrame(np.random.randn(1000, 4), columns=['A','B','C','D'])\n>>> pd.plotting.scatter_matrix(df, alpha=0.2)\narray([[<Axes: xlabel='A', ylabel='A'>, <Axes: xlabel='B', ylabel='A'>,\n        <Axes: xlabel='C', ylabel='A'>, <Axes: xlabel='D', ylabel='A'>],\n       [<Axes: xlabel='A', ylabel='B'>, <Axes: xlabel='B', ylabel='B'>,\n        <Axes: xlabel='C', ylabel='B'>, <Axes: xlabel='D', ylabel='B'>],\n       [<Axes: xlabel='A', ylabel='C'>, <Axes: xlabel='B', ylabel='C'>,\n        <Axes: xlabel='C', ylabel='C'>, <Axes: xlabel='D', ylabel='C'>],\n       [<Axes: xlabel='A', ylabel='D'>, <Axes: xlabel='B', ylabel='D'>,\n        <Axes: xlabel='C', ylabel='D'>, <Axes: xlabel='D', ylabel='D'>]],\n      dtype=object)""]"
1602,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.freqstr.html,pandas.tseries.offsets.BusinessDay.freqstr,BusinessDay.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1603,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_quarter_end.html,pandas.tseries.offsets.FY5253.is_quarter_end,FY5253.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1604,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.apply.html,pandas.core.groupby.SeriesGroupBy.apply,"SeriesGroupBy.apply(func, *args, **kwargs)[source]# Apply function func group-wise and combine the results together. The function passed to apply must take a series as its first argument and return a DataFrame, Series or scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method. While apply is a very flexible method, its downside is that using it can be quite a bit slower than using more specific methods like agg or transform. Pandas offers a wide range of method that will be much faster than using apply for their specific purposes, so try to use them before reaching for apply. Notes Changed in version 1.3.0: The resulting dtype will reflect the return value of the passed func, see the examples below. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funccallableA callable that takes a series as its first argument, and returns a dataframe, a series or a scalar. In addition the callable may take positional and keyword arguments. include_groupsbool, default TrueWhen True, will attempt to apply func to the groupings in the case that they are columns of the DataFrame. If this raises a TypeError, the result will be computed with the groupings excluded. When False, the groupings will be excluded when applying func. Added in version 2.2.0. Deprecated since version 2.2.0: Setting include_groups to True is deprecated. Only the value False will be allowed in a future version of pandas. args, kwargstuple and dictOptional positional and keyword arguments to pass to func. Returns: Series or DataFrame","["">>> s = pd.Series([0, 1, 2], index='a a b'.split())\n>>> g1 = s.groupby(s.index, group_keys=False)\n>>> g2 = s.groupby(s.index, group_keys=True)"", "">>> g1.apply(lambda x: x * 2 if x.name == 'a' else x / 2)\na    0.0\na    2.0\nb    1.0\ndtype: float64"", "">>> g2.apply(lambda x: x * 2 if x.name == 'a' else x / 2)\na  a    0.0\n   a    2.0\nb  b    1.0\ndtype: float64"", '>>> g1.apply(lambda x: x.max() - x.min())\na    1\nb    0\ndtype: int64', '>>> g2.apply(lambda x: x.max() - x.min())\na    1\nb    0\ndtype: int64']"
1605,..\pandas\reference\api\pandas.tseries.offsets.SemiMonthEnd.rule_code.html,pandas.tseries.offsets.SemiMonthEnd.rule_code,SemiMonthEnd.rule_code#,No parameters found,[]
1606,..\pandas\reference\api\pandas.DataFrame.mul.html,pandas.DataFrame.mul,"DataFrame.mul(other, axis='columns', level=None, fill_value=None)[source]# Get Multiplication of dataframe and other, element-wise (binary operator mul). Equivalent to dataframe * other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1607,..\pandas\reference\api\pandas.Series.sparse.npoints.html,pandas.Series.sparse.npoints,Series.sparse.npoints[source]# The number of non- fill_value points.,No parameters found,"['>>> from pandas.arrays import SparseArray\n>>> s = SparseArray([0, 0, 1, 1, 1], fill_value=0)\n>>> s.npoints\n3']"
1608,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.bfill.html,pandas.core.groupby.SeriesGroupBy.bfill,SeriesGroupBy.bfill(limit=None)[source]# Backward fill the values.,"Parameters: limitint, optionalLimit of how many values to fill. Returns: Series or DataFrameObject with missing values filled.","["">>> index = ['Falcon', 'Falcon', 'Parrot', 'Parrot', 'Parrot']\n>>> s = pd.Series([None, 1, None, None, 3], index=index)\n>>> s\nFalcon    NaN\nFalcon    1.0\nParrot    NaN\nParrot    NaN\nParrot    3.0\ndtype: float64\n>>> s.groupby(level=0).bfill()\nFalcon    1.0\nFalcon    1.0\nParrot    3.0\nParrot    3.0\nParrot    3.0\ndtype: float64\n>>> s.groupby(level=0).bfill(limit=1)\nFalcon    1.0\nFalcon    1.0\nParrot    NaN\nParrot    3.0\nParrot    3.0\ndtype: float64"", "">>> df = pd.DataFrame({'A': [1, None, None, None, 4],\n...                    'B': [None, None, 5, None, 7]}, index=index)\n>>> df\n          A         B\nFalcon  1.0       NaN\nFalcon  NaN       NaN\nParrot  NaN       5.0\nParrot  NaN       NaN\nParrot  4.0       7.0\n>>> df.groupby(level=0).bfill()\n          A         B\nFalcon  1.0       NaN\nFalcon  NaN       NaN\nParrot  4.0       5.0\nParrot  4.0       7.0\nParrot  4.0       7.0\n>>> df.groupby(level=0).bfill(limit=1)\n          A         B\nFalcon  1.0       NaN\nFalcon  NaN       NaN\nParrot  NaN       5.0\nParrot  4.0       7.0\nParrot  4.0       7.0""]"
1609,..\pandas\reference\api\pandas.Index.hasnans.html,pandas.Index.hasnans,Index.hasnans[source]# Return True if there are any NaNs. Enables various performance speedups.,Returns: bool,"["">>> s = pd.Series([1, 2, 3], index=['a', 'b', None])\n>>> s\na    1\nb    2\nNone 3\ndtype: int64\n>>> s.index.hasnans\nTrue""]"
1610,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.holidays.html,pandas.tseries.offsets.BusinessDay.holidays,BusinessDay.holidays#,No parameters found,[]
1611,..\pandas\reference\api\pandas.plotting.table.html,pandas.plotting.table,"pandas.plotting.table(ax, data, **kwargs)[source]# Helper function to convert DataFrame and Series to matplotlib.table.","Parameters: axMatplotlib axes object dataDataFrame or SeriesData for table contents. **kwargsKeyword arguments to be passed to matplotlib.table.table. If rowLabels or colLabels is not specified, data index or column name will be used. Returns: matplotlib table object","["">>> import matplotlib.pyplot as plt\n>>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n>>> fix, ax = plt.subplots()\n>>> ax.axis('off')\n(0.0, 1.0, 0.0, 1.0)\n>>> table = pd.plotting.table(ax, df, loc='center',\n...                           cellLoc='center', colWidths=list([.2, .2]))""]"
1612,..\pandas\reference\api\pandas.DataFrame.ndim.html,pandas.DataFrame.ndim,property DataFrame.ndim[source]# Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame.,No parameters found,"["">>> s = pd.Series({'a': 1, 'b': 2, 'c': 3})\n>>> s.ndim\n1"", "">>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n>>> df.ndim\n2""]"
1613,..\pandas\reference\api\pandas.tseries.offsets.Tick.copy.html,pandas.tseries.offsets.Tick.copy,Tick.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1614,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_quarter_start.html,pandas.tseries.offsets.FY5253.is_quarter_start,FY5253.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1615,..\pandas\reference\api\pandas.Series.sparse.sp_values.html,pandas.Series.sparse.sp_values,Series.sparse.sp_values[source]# An ndarray containing the non- fill_value values.,No parameters found,"['>>> from pandas.arrays import SparseArray\n>>> s = SparseArray([0, 0, 1, 0, 2], fill_value=0)\n>>> s.sp_values\narray([1, 2])']"
1616,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.corr.html,pandas.core.groupby.SeriesGroupBy.corr,"SeriesGroupBy.corr(other, method='pearson', min_periods=None)[source]# Compute correlation with other Series, excluding missing values. The two Series objects are not required to be the same length and will be aligned internally before the correlation function is applied. Notes Pearson, Kendall and Spearman correlation are currently computed using pairwise complete observations. Pearson correlation coefficient Kendall rank correlation coefficient Spearman’s rank correlation coefficient Automatic data alignment: as with all pandas operations, automatic data alignment is performed for this method. corr() automatically considers values with matching indices.","Parameters: otherSeriesSeries with which to compute the correlation. method{‘pearson’, ‘kendall’, ‘spearman’} or callableMethod used to compute correlation: pearson : Standard correlation coefficient kendall : Kendall Tau correlation coefficient spearman : Spearman rank correlation callable: Callable with input two 1d ndarrays and returning a float. Warning Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable’s behavior. min_periodsint, optionalMinimum number of observations needed to have a valid result. Returns: floatCorrelation with other.","['>>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> s1 = pd.Series([.2, .0, .6, .2])\n>>> s2 = pd.Series([.3, .6, .0, .1])\n>>> s1.corr(s2, method=histogram_intersection)\n0.3', '>>> s1 = pd.Series([1, 2, 3], index=[0, 1, 2])\n>>> s2 = pd.Series([1, 2, 3], index=[2, 1, 0])\n>>> s1.corr(s2)\n-1.0']"
1617,..\pandas\reference\api\pandas.Index.has_duplicates.html,pandas.Index.has_duplicates,property Index.has_duplicates[source]# Check if the Index has duplicate values.,Returns: boolWhether or not the Index has duplicate values.,"['>>> idx = pd.Index([1, 5, 7, 7])\n>>> idx.has_duplicates\nTrue', '>>> idx = pd.Index([1, 5, 7])\n>>> idx.has_duplicates\nFalse', '>>> idx = pd.Index([""Watermelon"", ""Orange"", ""Apple"",\n...                 ""Watermelon""]).astype(""category"")\n>>> idx.has_duplicates\nTrue', '>>> idx = pd.Index([""Orange"", ""Apple"",\n...                 ""Watermelon""]).astype(""category"")\n>>> idx.has_duplicates\nFalse']"
1618,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.html,pandas.tseries.offsets.BusinessDay,"class pandas.tseries.offsets.BusinessDay# DateOffset subclass representing possibly n business days. Examples You can use the parameter n to represent a shift of n business days. Passing the parameter normalize equal to True, you shift the start of the next business day to midnight. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. calendar freqstr Return a string representing the frequency. holidays kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize offset Alias for self._offset. rule_code weekmask","Parameters: nint, default 1The number of days represented. normalizebool, default FalseNormalize start/end dates to midnight. offsettimedelta, default timedelta(0)Time offset to apply.","["">>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts.strftime('%a %d %b %Y %H:%M')\n'Fri 09 Dec 2022 15:00'\n>>> (ts + pd.offsets.BusinessDay(n=5)).strftime('%a %d %b %Y %H:%M')\n'Fri 16 Dec 2022 15:00'"", "">>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts + pd.offsets.BusinessDay(normalize=True)\nTimestamp('2022-12-12 00:00:00')""]"
1619,..\pandas\reference\api\pandas.qcut.html,pandas.qcut,"pandas.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')[source]# Quantile-based discretization function. Discretize variable into equal-sized buckets based on rank or based on sample quantiles. For example 1000 values for 10 quantiles would produce a Categorical object indicating quantile membership for each data point. Notes Out of bounds values will be NA in the resulting Categorical object","Parameters: x1d ndarray or Series qint or list-like of floatNumber of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles. labelsarray or False, default NoneUsed as labels for the resulting bins. Must be of the same length as the resulting bins. If False, return only integer indicators of the bins. If True, raises an error. retbinsbool, optionalWhether to return the (bins, labels) or not. Can be useful if bins is given as a scalar. precisionint, optionalThe precision at which to store and display the bins labels. duplicates{default ‘raise’, ‘drop’}, optionalIf bin edges are not unique, raise ValueError or drop non-uniques. Returns: outCategorical or Series or array of integers if labels is FalseThe return type (Categorical or Series) depends on the input: a Series of type category if input is a Series else Categorical. Bins are represented as categories when categorical data is returned. binsndarray of floatsReturned only if retbins is True.","['>>> pd.qcut(range(5), 4)\n... \n[(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]]\nCategories (4, interval[float64, right]): [(-0.001, 1.0] < (1.0, 2.0] ...', '>>> pd.qcut(range(5), 3, labels=[""good"", ""medium"", ""bad""])\n... \n[good, good, medium, bad, bad]\nCategories (3, object): [good < medium < bad]', '>>> pd.qcut(range(5), 4, labels=False)\narray([0, 0, 1, 2, 3])']"
1620,..\pandas\reference\api\pandas.Index.html,pandas.Index,"class pandas.Index(data=None, dtype=None, copy=False, name=None, tupleize_cols=True)[source]# Immutable sequence used for indexing and alignment. The basic object storing axis labels for all pandas objects. Changed in version 2.0.0: Index can hold all numpy numeric dtypes (except float16). Previously only int64/uint64/float64 dtypes were accepted. Notes An Index instance can only contain hashable objects. An Index instance can not hold numpy float16 dtype. Examples Attributes T Return the transpose, which is by definition self. array The ExtensionArray of the data backing this Series or Index. dtype Return the dtype object of the underlying data. empty has_duplicates Check if the Index has duplicate values. hasnans Return True if there are any NaNs. inferred_type Return a string of the type inferred from the values. is_monotonic_decreasing Return a boolean if the values are equal or decreasing. is_monotonic_increasing Return a boolean if the values are equal or increasing. is_unique Return if the index has unique values. name Return Index or MultiIndex name. names nbytes Return the number of bytes in the underlying data. ndim Number of dimensions of the underlying data, by definition 1. nlevels Number of levels. shape Return a tuple of the shape of the underlying data. size Return the number of elements in the underlying data. values Return an array representing the data in the Index.","Parameters: dataarray-like (1-dimensional) dtypestr, numpy.dtype, or ExtensionDtype, optionalData type for the output Index. If not specified, this will be inferred from data. See the user guide for more usages. copybool, default FalseCopy input data. nameobjectName to be stored in the index. tupleize_colsbool (default: True)When True, attempt to create a MultiIndex if possible.","["">>> pd.Index([1, 2, 3])\nIndex([1, 2, 3], dtype='int64')"", "">>> pd.Index(list('abc'))\nIndex(['a', 'b', 'c'], dtype='object')"", '>>> pd.Index([1, 2, 3], dtype=""uint8"")\nIndex([1, 2, 3], dtype=\'uint8\')']"
1621,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.count.html,pandas.core.groupby.SeriesGroupBy.count,"SeriesGroupBy.count()[source]# Compute count of group, excluding missing values.",Returns: Series or DataFrameCount of values within each group.,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, np.nan], index=lst)\n>>> ser\na    1.0\na    2.0\nb    NaN\ndtype: float64\n>>> ser.groupby(level=0).count()\na    2\nb    0\ndtype: int64"", '>>> data = [[1, np.nan, 3], [1, np.nan, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a         b     c\ncow     1       NaN     3\nhorse   1       NaN     6\nbull    7       8.0     9\n>>> df.groupby(""a"").count()\n    b   c\na\n1   0   2\n7   1   1', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').count()\n2023-01-01    2\n2023-02-01    2\nFreq: MS, dtype: int64""]"
1622,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_year_end.html,pandas.tseries.offsets.FY5253.is_year_end,FY5253.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1623,..\pandas\reference\api\pandas.tseries.offsets.Tick.delta.html,pandas.tseries.offsets.Tick.delta,Tick.delta#,No parameters found,[]
1624,..\pandas\reference\api\pandas.DataFrame.ne.html,pandas.DataFrame.ne,"DataFrame.ne(other, axis='columns', level=None)[source]# Get Not equal to of dataframe and other, element-wise (binary operator ne). Among flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators. Equivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or columns) and level for comparison. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN).","Parameters: otherscalar, sequence, Series, or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}, default ‘columns’Whether to compare by the index (0 or ‘index’) or columns (1 or ‘columns’). levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. Returns: DataFrame of boolResult of the comparison.","["">>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300"", '>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False', '>>> df != pd.Series([100, 250], index=[""cost"", ""revenue""])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True', '>>> df.ne(pd.Series([100, 300], index=[""A"", ""D""]), axis=\'index\')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True', '>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False', "">>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False"", "">>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150"", '>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False', "">>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225"", '>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False']"
1625,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_anchored.html,pandas.tseries.offsets.BusinessDay.is_anchored,BusinessDay.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1626,..\pandas\reference\api\pandas.Series.sparse.to_coo.html,pandas.Series.sparse.to_coo,"Series.sparse.to_coo(row_levels=(0,), column_levels=(1,), sort_labels=False)[source]# Create a scipy.sparse.coo_matrix from a Series with MultiIndex. Use row_levels and column_levels to determine the row and column coordinates respectively. row_levels and column_levels are the names (labels) or numbers of the levels. {row_levels, column_levels} must be a partition of the MultiIndex level names (or numbers).","Parameters: row_levelstuple/list column_levelstuple/list sort_labelsbool, default FalseSort the row and column labels before forming the sparse matrix. When row_levels and/or column_levels refer to a single level, set to True for a faster execution. Returns: yscipy.sparse.coo_matrix rowslist (row labels) columnslist (column labels)","['>>> s = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan])\n>>> s.index = pd.MultiIndex.from_tuples(\n...     [\n...         (1, 2, ""a"", 0),\n...         (1, 2, ""a"", 1),\n...         (1, 1, ""b"", 0),\n...         (1, 1, ""b"", 1),\n...         (2, 1, ""b"", 0),\n...         (2, 1, ""b"", 1)\n...     ],\n...     names=[""A"", ""B"", ""C"", ""D""],\n... )\n>>> s\nA  B  C  D\n1  2  a  0    3.0\n         1    NaN\n   1  b  0    1.0\n         1    3.0\n2  1  b  0    NaN\n         1    NaN\ndtype: float64', '>>> ss = s.astype(""Sparse"")\n>>> ss\nA  B  C  D\n1  2  a  0    3.0\n         1    NaN\n   1  b  0    1.0\n         1    3.0\n2  1  b  0    NaN\n         1    NaN\ndtype: Sparse[float64, nan]', '>>> A, rows, columns = ss.sparse.to_coo(\n...     row_levels=[""A"", ""B""], column_levels=[""C"", ""D""], sort_labels=True\n... )\n>>> A\n<COOrdinate sparse matrix of dtype \'float64\'\n    with 3 stored elements and shape (3, 4)>\n>>> A.todense()\nmatrix([[0., 0., 1., 3.],\n[3., 0., 0., 0.],\n[0., 0., 0., 0.]])', "">>> rows\n[(1, 1), (1, 2), (2, 1)]\n>>> columns\n[('a', 0), ('a', 1), ('b', 0), ('b', 1)]""]"
1627,..\pandas\reference\api\pandas.RangeIndex.from_range.html,pandas.RangeIndex.from_range,"classmethod RangeIndex.from_range(data, name=None, dtype=None)[source]# Create pandas.RangeIndex from a range object.",Returns: RangeIndex,"['>>> pd.RangeIndex.from_range(range(5))\nRangeIndex(start=0, stop=5, step=1)', '>>> pd.RangeIndex.from_range(range(2, -10, -3))\nRangeIndex(start=2, stop=-10, step=-3)']"
1628,..\pandas\reference\api\pandas.Index.identical.html,pandas.Index.identical,"final Index.identical(other)[source]# Similar to equals, but checks that object attributes and types are also equal.","Returns: boolIf two Index objects have equal elements and same type True, otherwise False.","["">>> idx1 = pd.Index(['1', '2', '3'])\n>>> idx2 = pd.Index(['1', '2', '3'])\n>>> idx2.identical(idx1)\nTrue"", '>>> idx1 = pd.Index([\'1\', \'2\', \'3\'], name=""A"")\n>>> idx2 = pd.Index([\'1\', \'2\', \'3\'], name=""B"")\n>>> idx2.identical(idx1)\nFalse']"
1629,..\pandas\reference\api\pandas.DataFrame.nlargest.html,pandas.DataFrame.nlargest,"DataFrame.nlargest(n, columns, keep='first')[source]# Return the first n rows ordered by columns in descending order. Return the first n rows with the largest values in columns, in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=False).head(n), but more performant. Notes This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised.","Parameters: nintNumber of rows to return. columnslabel or list of labelsColumn label(s) to order by. keep{‘first’, ‘last’, ‘all’}, default ‘first’Where there are duplicate values: first : prioritize the first occurrence(s) last : prioritize the last occurrence(s) all : keep all the ties of the smallest item even if it means selecting more than n items. Returns: DataFrameThe first n rows ordered by the given columns in descending order.","['>>> df = pd.DataFrame({\'population\': [59000000, 65000000, 434000,\n...                                   434000, 434000, 337000, 11300,\n...                                   11300, 11300],\n...                    \'GDP\': [1937894, 2583560 , 12011, 4520, 12128,\n...                            17036, 182, 38, 311],\n...                    \'alpha-2\': [""IT"", ""FR"", ""MT"", ""MV"", ""BN"",\n...                                ""IS"", ""NR"", ""TV"", ""AI""]},\n...                   index=[""Italy"", ""France"", ""Malta"",\n...                          ""Maldives"", ""Brunei"", ""Iceland"",\n...                          ""Nauru"", ""Tuvalu"", ""Anguilla""])\n>>> df\n          population      GDP alpha-2\nItaly       59000000  1937894      IT\nFrance      65000000  2583560      FR\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN\nIceland       337000    17036      IS\nNauru          11300      182      NR\nTuvalu         11300       38      TV\nAnguilla       11300      311      AI', "">>> df.nlargest(3, 'population')\n        population      GDP alpha-2\nFrance    65000000  2583560      FR\nItaly     59000000  1937894      IT\nMalta       434000    12011      MT"", "">>> df.nlargest(3, 'population', keep='last')\n        population      GDP alpha-2\nFrance    65000000  2583560      FR\nItaly     59000000  1937894      IT\nBrunei      434000    12128      BN"", "">>> df.nlargest(3, 'population', keep='all')\n          population      GDP alpha-2\nFrance      65000000  2583560      FR\nItaly       59000000  1937894      IT\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN"", "">>> df.nlargest(5, 'population', keep='all')\n          population      GDP alpha-2\nFrance      65000000  2583560      FR\nItaly       59000000  1937894      IT\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN"", "">>> df.nlargest(3, ['population', 'GDP'])\n        population      GDP alpha-2\nFrance    65000000  2583560      FR\nItaly     59000000  1937894      IT\nBrunei      434000    12128      BN""]"
1630,..\pandas\reference\api\pandas.RangeIndex.html,pandas.RangeIndex,"class pandas.RangeIndex(start=None, stop=None, step=None, dtype=None, copy=False, name=None)[source]# Immutable Index implementing a monotonic integer range. RangeIndex is a memory-saving special case of an Index limited to representing monotonic ranges with a 64-bit dtype. Using RangeIndex may in some instances improve computing speed. This is the default index type used by DataFrame and Series when no explicit index is provided by the user. Attributes start The value of the start parameter (0 if this was not supplied). stop The value of the stop parameter. step The value of the step parameter (1 if this was not supplied). Methods from_range(data[, name, dtype]) Create pandas.RangeIndex from a range object.","Parameters: startint (default: 0), range, or other RangeIndex instanceIf int and “stop” is not given, interpreted as “stop” instead. stopint (default: 0) stepint (default: 1) dtypenp.int64Unused, accepted for homogeneity with other index types. copybool, default FalseUnused, accepted for homogeneity with other index types. nameobject, optionalName to be stored in the index.","['>>> list(pd.RangeIndex(5))\n[0, 1, 2, 3, 4]', '>>> list(pd.RangeIndex(-2, 4))\n[-2, -1, 0, 1, 2, 3]', '>>> list(pd.RangeIndex(0, 10, 2))\n[0, 2, 4, 6, 8]', '>>> list(pd.RangeIndex(2, -10, -3))\n[2, -1, -4, -7]', '>>> list(pd.RangeIndex(0))\n[]', '>>> list(pd.RangeIndex(1, 0))\n[]']"
1631,..\pandas\reference\api\pandas.tseries.offsets.FY5253.is_year_start.html,pandas.tseries.offsets.FY5253.is_year_start,FY5253.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1632,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.cov.html,pandas.core.groupby.SeriesGroupBy.cov,"SeriesGroupBy.cov(other, min_periods=None, ddof=1)[source]# Compute covariance with Series, excluding missing values. The two Series objects are not required to be the same length and will be aligned internally before the covariance is calculated.","Parameters: otherSeriesSeries with which to compute the covariance. min_periodsint, optionalMinimum number of observations needed to have a valid result. ddofint, default 1Delta degrees of freedom.  The divisor used in calculations is N - ddof, where N represents the number of elements. Returns: floatCovariance between Series and other normalized by N-1 (unbiased estimator).","['>>> s1 = pd.Series([0.90010907, 0.13484424, 0.62036035])\n>>> s2 = pd.Series([0.12528585, 0.26962463, 0.51111198])\n>>> s1.cov(s2)\n-0.01685762652715874']"
1633,..\pandas\reference\api\pandas.Series.squeeze.html,pandas.Series.squeeze,"Series.squeeze(axis=None)[source]# Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don’t know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series.","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneA specific axis to squeeze. By default, all length-1 axes are squeezed. For Series this parameter is unused and defaults to None. Returns: DataFrame, Series, or scalarThe projection after squeezing axis or all the axes.","['>>> primes = pd.Series([2, 3, 5, 7])', '>>> even_primes = primes[primes % 2 == 0]\n>>> even_primes\n0    2\ndtype: int64', '>>> even_primes.squeeze()\n2', '>>> odd_primes = primes[primes % 2 == 1]\n>>> odd_primes\n1    3\n2    5\n3    7\ndtype: int64', '>>> odd_primes.squeeze()\n1    3\n2    5\n3    7\ndtype: int64', "">>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n>>> df\n   a  b\n0  1  2\n1  3  4"", "">>> df_a = df[['a']]\n>>> df_a\n   a\n0  1\n1  3"", "">>> df_a.squeeze('columns')\n0    1\n1    3\nName: a, dtype: int64"", "">>> df_0a = df.loc[df.index < 1, ['a']]\n>>> df_0a\n   a\n0  1"", "">>> df_0a.squeeze('rows')\na    1\nName: 0, dtype: int64"", '>>> df_0a.squeeze()\n1']"
1634,..\pandas\reference\api\pandas.tseries.offsets.Tick.freqstr.html,pandas.tseries.offsets.Tick.freqstr,Tick.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1635,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_month_end.html,pandas.tseries.offsets.BusinessDay.is_month_end,BusinessDay.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1636,..\pandas\reference\api\pandas.Index.inferred_type.html,pandas.Index.inferred_type,Index.inferred_type[source]# Return a string of the type inferred from the values.,No parameters found,"["">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.inferred_type\n'integer'""]"
1637,..\pandas\reference\api\pandas.DataFrame.notna.html,pandas.DataFrame.notna,"DataFrame.notna()[source]# Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.",Returns: DataFrameMask of bool values for each element in DataFrame that indicates whether an element is not an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.notna()\n     age   born  name    toy\n0   True  False  True  False\n1   True   True  True   True\n2  False   True  True   True', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.notna()\n0     True\n1     True\n2    False\ndtype: bool']"
1638,..\pandas\reference\api\pandas.RangeIndex.start.html,pandas.RangeIndex.start,property RangeIndex.start[source]# The value of the start parameter (0 if this was not supplied).,No parameters found,"['>>> idx = pd.RangeIndex(5)\n>>> idx.start\n0', '>>> idx = pd.RangeIndex(2, -10, -3)\n>>> idx.start\n2']"
1639,..\pandas\reference\api\pandas.tseries.offsets.FY5253.kwds.html,pandas.tseries.offsets.FY5253.kwds,FY5253.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1640,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.cumcount.html,pandas.core.groupby.SeriesGroupBy.cumcount,"SeriesGroupBy.cumcount(ascending=True)[source]# Number each item in each group from 0 to the length of that group - 1. Essentially this is equivalent to self.apply(lambda x: pd.Series(np.arange(len(x)), x.index))","Parameters: ascendingbool, default TrueIf False, number in reverse, from length of group - 1 to 0. Returns: SeriesSequence number of each element within each group.","["">>> df = pd.DataFrame([['a'], ['a'], ['a'], ['b'], ['b'], ['a']],\n...                   columns=['A'])\n>>> df\n   A\n0  a\n1  a\n2  a\n3  b\n4  b\n5  a\n>>> df.groupby('A').cumcount()\n0    0\n1    1\n2    2\n3    0\n4    1\n5    3\ndtype: int64\n>>> df.groupby('A').cumcount(ascending=False)\n0    3\n1    2\n2    1\n3    1\n4    0\n5    0\ndtype: int64""]"
1641,..\pandas\reference\api\pandas.Series.std.html,pandas.Series.std,"Series.std(axis=None, skipna=True, ddof=1, numeric_only=False, **kwargs)[source]# Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument. Notes To have the same behaviour as numpy.std, use ddof=0 (instead of the default ddof=1)","Parameters: axis{index (0)}For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.std with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. ddofint, default 1Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. Returns: scalar or Series (if level specified)","["">>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                    'age': [21, 25, 62, 43],\n...                    'height': [1.61, 1.87, 1.49, 2.01]}\n...                   ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01"", '>>> df.std()\nage       18.786076\nheight     0.237417\ndtype: float64', '>>> df.std(ddof=0)\nage       16.269219\nheight     0.205609\ndtype: float64']"
1642,..\pandas\reference\api\pandas.tseries.offsets.Tick.html,pandas.tseries.offsets.Tick,class pandas.tseries.offsets.Tick# Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,No parameters found,[]
1643,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_month_start.html,pandas.tseries.offsets.BusinessDay.is_month_start,BusinessDay.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1644,..\pandas\reference\api\pandas.Index.insert.html,pandas.Index.insert,"Index.insert(loc, item)[source]# Make new Index inserting new item at location. Follows Python numpy.insert semantics for negative values.",Parameters: locint itemobject Returns: Index,"["">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.insert(1, 'x')\nIndex(['a', 'x', 'b', 'c'], dtype='object')""]"
1645,..\pandas\reference\api\pandas.DataFrame.notnull.html,pandas.DataFrame.notnull,"DataFrame.notnull()[source]# DataFrame.notnull is an alias for DataFrame.notna. Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True). NA values, such as None or numpy.NaN, get mapped to False values.",Returns: DataFrameMask of bool values for each element in DataFrame that indicates whether an element is not an NA value.,"["">>> df = pd.DataFrame(dict(age=[5, 6, np.nan],\n...                        born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                              pd.Timestamp('1940-04-25')],\n...                        name=['Alfred', 'Batman', ''],\n...                        toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker"", '>>> df.notna()\n     age   born  name    toy\n0   True  False  True  False\n1   True   True  True   True\n2  False   True  True   True', '>>> ser = pd.Series([5, 6, np.nan])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64', '>>> ser.notna()\n0     True\n1     True\n2    False\ndtype: bool']"
1646,..\pandas\reference\api\pandas.RangeIndex.step.html,pandas.RangeIndex.step,property RangeIndex.step[source]# The value of the step parameter (1 if this was not supplied).,No parameters found,"['>>> idx = pd.RangeIndex(5)\n>>> idx.step\n1', '>>> idx = pd.RangeIndex(2, -10, -3)\n>>> idx.step\n-3', '>>> idx = pd.RangeIndex(1, 0)\n>>> idx.step\n1']"
1647,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.cummax.html,pandas.core.groupby.SeriesGroupBy.cummax,"SeriesGroupBy.cummax(axis=<no_default>, numeric_only=False, **kwargs)[source]# Cumulative max for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 6, 2, 3, 1, 4], index=lst)\n>>> ser\na    1\na    6\na    2\nb    3\nb    1\nb    4\ndtype: int64\n>>> ser.groupby(level=0).cummax()\na    1\na    6\na    6\nb    3\nb    3\nb    4\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 1, 0], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a   b   c\ncow     1   8   2\nhorse   1   1   0\nbull    2   6   9\n>>> df.groupby(""a"").groups\n{1: [\'cow\', \'horse\'], 2: [\'bull\']}\n>>> df.groupby(""a"").cummax()\n        b   c\ncow     8   2\nhorse   8   2\nbull    6   9']"
1648,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_anchored.html,pandas.tseries.offsets.Tick.is_anchored,Tick.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
1649,..\pandas\reference\api\pandas.tseries.offsets.FY5253.n.html,pandas.tseries.offsets.FY5253.n,FY5253.n#,No parameters found,[]
1650,..\pandas\reference\api\pandas.Index.intersection.html,pandas.Index.intersection,"final Index.intersection(other, sort=False)[source]# Form the intersection of two Index objects. This returns a new Index with elements common to the index and other.","Parameters: otherIndex or array-like sortTrue, False or None, default FalseWhether to sort the resulting index. None : sort the result, except when self and other are equal or when the values cannot be compared. False : do not sort the result. True : Sort the result (which may raise TypeError). Returns: Index","["">>> idx1 = pd.Index([1, 2, 3, 4])\n>>> idx2 = pd.Index([3, 4, 5, 6])\n>>> idx1.intersection(idx2)\nIndex([3, 4], dtype='int64')""]"
1651,..\pandas\reference\api\pandas.Series.str.capitalize.html,pandas.Series.str.capitalize,Series.str.capitalize()[source]# Convert strings in the Series/Index to be capitalized. Equivalent to str.capitalize().,Returns: Series or Index of object,"["">>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object"", '>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object', '>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object', '>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object', '>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object', '>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object']"
1652,..\pandas\reference\api\pandas.DataFrame.nsmallest.html,pandas.DataFrame.nsmallest,"DataFrame.nsmallest(n, columns, keep='first')[source]# Return the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns, in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n), but more performant.","Parameters: nintNumber of items to retrieve. columnslist or strColumn name or names to order by. keep{‘first’, ‘last’, ‘all’}, default ‘first’Where there are duplicate values: first : take the first occurrence. last : take the last occurrence. all : keep all the ties of the largest item even if it means selecting more than n items. Returns: DataFrame","['>>> df = pd.DataFrame({\'population\': [59000000, 65000000, 434000,\n...                                   434000, 434000, 337000, 337000,\n...                                   11300, 11300],\n...                    \'GDP\': [1937894, 2583560 , 12011, 4520, 12128,\n...                            17036, 182, 38, 311],\n...                    \'alpha-2\': [""IT"", ""FR"", ""MT"", ""MV"", ""BN"",\n...                                ""IS"", ""NR"", ""TV"", ""AI""]},\n...                   index=[""Italy"", ""France"", ""Malta"",\n...                          ""Maldives"", ""Brunei"", ""Iceland"",\n...                          ""Nauru"", ""Tuvalu"", ""Anguilla""])\n>>> df\n          population      GDP alpha-2\nItaly       59000000  1937894      IT\nFrance      65000000  2583560      FR\nMalta         434000    12011      MT\nMaldives      434000     4520      MV\nBrunei        434000    12128      BN\nIceland       337000    17036      IS\nNauru         337000      182      NR\nTuvalu         11300       38      TV\nAnguilla       11300      311      AI', "">>> df.nsmallest(3, 'population')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS"", "">>> df.nsmallest(3, 'population', keep='last')\n          population  GDP alpha-2\nAnguilla       11300  311      AI\nTuvalu         11300   38      TV\nNauru         337000  182      NR"", "">>> df.nsmallest(3, 'population', keep='all')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS\nNauru         337000    182      NR"", "">>> df.nsmallest(4, 'population', keep='all')\n          population    GDP alpha-2\nTuvalu         11300     38      TV\nAnguilla       11300    311      AI\nIceland       337000  17036      IS\nNauru         337000    182      NR"", "">>> df.nsmallest(3, ['population', 'GDP'])\n          population  GDP alpha-2\nTuvalu         11300   38      TV\nAnguilla       11300  311      AI\nNauru         337000  182      NR""]"
1653,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_on_offset.html,pandas.tseries.offsets.BusinessDay.is_on_offset,BusinessDay.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1654,..\pandas\reference\api\pandas.RangeIndex.stop.html,pandas.RangeIndex.stop,property RangeIndex.stop[source]# The value of the stop parameter.,No parameters found,"['>>> idx = pd.RangeIndex(5)\n>>> idx.stop\n5', '>>> idx = pd.RangeIndex(2, -10, -3)\n>>> idx.stop\n-10']"
1655,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.cummin.html,pandas.core.groupby.SeriesGroupBy.cummin,"SeriesGroupBy.cummin(axis=<no_default>, numeric_only=False, **kwargs)[source]# Cumulative min for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 6, 2, 3, 0, 4], index=lst)\n>>> ser\na    1\na    6\na    2\nb    3\nb    0\nb    4\ndtype: int64\n>>> ser.groupby(level=0).cummin()\na    1\na    1\na    1\nb    3\nb    0\nb    0\ndtype: int64"", '>>> data = [[1, 0, 2], [1, 1, 5], [6, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""snake"", ""rabbit"", ""turtle""])\n>>> df\n        a   b   c\nsnake   1   0   2\nrabbit  1   1   5\nturtle  6   6   9\n>>> df.groupby(""a"").groups\n{1: [\'snake\', \'rabbit\'], 6: [\'turtle\']}\n>>> df.groupby(""a"").cummin()\n        b   c\nsnake   0   2\nrabbit  0   2\nturtle  6   9']"
1656,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_month_end.html,pandas.tseries.offsets.Tick.is_month_end,Tick.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1657,..\pandas\reference\api\pandas.DataFrame.nunique.html,pandas.DataFrame.nunique,"DataFrame.nunique(axis=0, dropna=True)[source]# Count number of distinct elements in specified axis. Return Series with number of distinct elements. Can ignore NaN values.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to use. 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. dropnabool, default TrueDon’t include NaN in the counts. Returns: Series","["">>> df = pd.DataFrame({'A': [4, 5, 6], 'B': [4, 1, 1]})\n>>> df.nunique()\nA    3\nB    2\ndtype: int64"", '>>> df.nunique(axis=1)\n0    1\n1    2\n2    2\ndtype: int64']"
1658,..\pandas\reference\api\pandas.Series.str.casefold.html,pandas.Series.str.casefold,Series.str.casefold()[source]# Convert strings in the Series/Index to be casefolded. Equivalent to str.casefold().,Returns: Series or Index of object,"["">>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object"", '>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object', '>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object', '>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object', '>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object', '>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object']"
1659,..\pandas\reference\api\pandas.Index.isin.html,pandas.Index.isin,"Index.isin(values, level=None)[source]# Return a boolean array where the index values are in values. Compute boolean array of whether each index value is found in the passed set of values. The length of the returned boolean array matches the length of the index. Notes In the case of MultiIndex you must either specify values as a list-like object containing tuples that are the same length as the number of levels, or specify level. Otherwise it will raise a ValueError. If level is specified: if it is the name of one and only one index level, use that level; otherwise it should be a number indicating level position.","Parameters: valuesset or list-likeSought values. levelstr or int, optionalName or position of the index level to use (if the index is a MultiIndex). Returns: np.ndarray[bool]NumPy array of boolean values.","["">>> idx = pd.Index([1,2,3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')"", '>>> idx.isin([1, 4])\narray([ True, False, False])', "">>> midx = pd.MultiIndex.from_arrays([[1,2,3],\n...                                  ['red', 'blue', 'green']],\n...                                  names=('number', 'color'))\n>>> midx\nMultiIndex([(1,   'red'),\n            (2,  'blue'),\n            (3, 'green')],\n           names=['number', 'color'])"", "">>> midx.isin(['red', 'orange', 'yellow'], level='color')\narray([ True, False, False])"", "">>> midx.isin([(1, 'red'), (3, 'red')])\narray([ True, False, False])""]"
1660,..\pandas\reference\api\pandas.read_clipboard.html,pandas.read_clipboard,"pandas.read_clipboard(sep='\\s+', dtype_backend=<no_default>, **kwargs)[source]# Read text from clipboard and pass to read_csv(). Parses clipboard contents similar to how CSV files are parsed using read_csv().","Parameters: sepstr, default ‘\s+’A string or regex delimiter. The default of '\\s+' denotes one or more whitespace characters. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. **kwargsSee read_csv() for the full argument list. Returns: DataFrameA parsed DataFrame object.","["">>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])\n>>> df.to_clipboard()  \n>>> pd.read_clipboard()  \n     A  B  C\n0    1  2  3\n1    4  5  6""]"
1661,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_quarter_end.html,pandas.tseries.offsets.BusinessDay.is_quarter_end,BusinessDay.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1662,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.cumprod.html,pandas.core.groupby.SeriesGroupBy.cumprod,"SeriesGroupBy.cumprod(axis=<no_default>, *args, **kwargs)[source]# Cumulative product for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([6, 2, 0], index=lst)\n>>> ser\na    6\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).cumprod()\na    6\na   12\nb    0\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""cow"", ""horse"", ""bull""])\n>>> df\n        a   b   c\ncow     1   8   2\nhorse   1   2   5\nbull    2   6   9\n>>> df.groupby(""a"").groups\n{1: [\'cow\', \'horse\'], 2: [\'bull\']}\n>>> df.groupby(""a"").cumprod()\n        b   c\ncow     8   2\nhorse  16  10\nbull    6   9']"
1663,..\pandas\reference\api\pandas.tseries.offsets.FY5253.name.html,pandas.tseries.offsets.FY5253.name,FY5253.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1664,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_month_start.html,pandas.tseries.offsets.Tick.is_month_start,Tick.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1665,..\pandas\reference\api\pandas.Index.isna.html,pandas.Index.isna,"final Index.isna()[source]# Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None, numpy.NaN or pd.NaT, get mapped to True values. Everything else get mapped to False values. Characters such as empty strings ‘’ or numpy.inf are not considered NA values.",Returns: numpy.ndarray[bool]A boolean array of whether my values are NA.,"["">>> idx = pd.Index([5.2, 6.0, np.nan])\n>>> idx\nIndex([5.2, 6.0, nan], dtype='float64')\n>>> idx.isna()\narray([False, False,  True])"", "">>> idx = pd.Index(['black', '', 'red', None])\n>>> idx\nIndex(['black', '', 'red', None], dtype='object')\n>>> idx.isna()\narray([False, False, False,  True])"", "">>> idx = pd.DatetimeIndex([pd.Timestamp('1940-04-25'),\n...                         pd.Timestamp(''), None, pd.NaT])\n>>> idx\nDatetimeIndex(['1940-04-25', 'NaT', 'NaT', 'NaT'],\n              dtype='datetime64[ns]', freq=None)\n>>> idx.isna()\narray([False,  True,  True,  True])""]"
1666,..\pandas\reference\api\pandas.tseries.offsets.FY5253.nanos.html,pandas.tseries.offsets.FY5253.nanos,FY5253.nanos#,No parameters found,[]
1667,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.cumsum.html,pandas.core.groupby.SeriesGroupBy.cumsum,"SeriesGroupBy.cumsum(axis=<no_default>, *args, **kwargs)[source]# Cumulative sum for each group.",Returns: Series or DataFrame,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([6, 2, 0], index=lst)\n>>> ser\na    6\na    2\nb    0\ndtype: int64\n>>> ser.groupby(level=0).cumsum()\na    6\na    8\nb    0\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""fox"", ""gorilla"", ""lion""])\n>>> df\n          a   b   c\nfox       1   8   2\ngorilla   1   2   5\nlion      2   6   9\n>>> df.groupby(""a"").groups\n{1: [\'fox\', \'gorilla\'], 2: [\'lion\']}\n>>> df.groupby(""a"").cumsum()\n          b   c\nfox       8   2\ngorilla  10   7\nlion      6   9']"
1668,..\pandas\reference\api\pandas.DataFrame.pad.html,pandas.DataFrame.pad,"DataFrame.pad(*, axis=None, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values by propagating the last valid observation to next valid. Deprecated since version 2.0: Series/DataFrame.pad is deprecated. Use Series/DataFrame.ffill instead.",Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.,[]
1669,..\pandas\reference\api\pandas.read_csv.html,pandas.read_csv,"pandas.read_csv(filepath_or_buffer, *, sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=<no_default>, skip_blank_lines=True, parse_dates=None, infer_datetime_format=<no_default>, keep_date_col=<no_default>, date_parser=<no_default>, date_format=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal='.', lineterminator=None, quotechar='""', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors='strict', dialect=None, on_bad_lines='error', delim_whitespace=<no_default>, low_memory=True, memory_map=False, float_precision=None, storage_options=None, dtype_backend=<no_default>)[source]# Read a comma-separated values (csv) file into DataFrame. Also supports optionally iterating or breaking of the file into chunks. Additional help can be found in the online docs for IO Tools.","Parameters: filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO. sepstr, default ‘,’Character or regex pattern to treat as the delimiter. If sep=None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator from only the first valid row of the file by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\r\t'. delimiterstr, optionalAlias for sep. headerint, Sequence of int, ‘infer’ or None, default ‘infer’Row number(s) containing column labels and marking the start of the data (zero-indexed). Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly to names then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a MultiIndex on the columns e.g. [0, 1, 3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file. namesSequence of Hashable, optionalSequence of column labels to apply. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed. index_colHashable, Sequence of Hashable or False, optionalColumn(s) to use as row label(s), denoted either by column labels or column indices.  If a sequence of labels or indices is given, MultiIndex will be formed for the row labels. Note: index_col=False can be used to force pandas to not use the first column as the index, e.g., when you have a malformed file with delimiters at the end of each line. usecolsSequence of Hashable or Callable, optionalSubset of columns to select, denoted either by column labels or column indices. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in ['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage. dtypedtype or dict of {Hashabledtype}, optionalData type(s) to apply to either the whole dataset or individual columns. E.g., {'a': np.float64, 'b': np.int32, 'c': 'Int64'} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. Added in version 1.5.0: Support for defaultdict was added. Specify a defaultdict as input where the default determines the dtype of the columns which are not explicitly listed. engine{‘c’, ‘python’, ‘pyarrow’}, optionalParser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine. Added in version 1.4.0: The ‘pyarrow’ engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine. convertersdict of {HashableCallable}, optionalFunctions for converting values in specified columns. Keys can either be column labels or column indices. true_valueslist, optionalValues to consider as True in addition to case-insensitive variants of ‘True’. false_valueslist, optionalValues to consider as False in addition to case-insensitive variants of ‘False’. skipinitialspacebool, default FalseSkip spaces after delimiter. skiprowsint, list of int or Callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2]. skipfooterint, default 0Number of lines at bottom of file to skip (Unsupported with engine='c'). nrowsint, optionalNumber of rows of file to read. Useful for reading pieces of large files. na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optionalAdditional strings to recognize as NA/NaN. If dict passed, specific per-column NA values.  By default the following values are interpreted as NaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null “. keep_default_nabool, default TrueWhether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows: If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN. Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored. na_filterbool, default TrueDetect missing value markers (empty strings and the value of na_values). In data without any NA values, passing na_filter=False can improve the performance of reading a large file. verbosebool, default FalseIndicate number of NA values placed in non-numeric columns. Deprecated since version 2.2.0. skip_blank_linesbool, default TrueIf True, skip over blank lines rather than interpreting as NaN values. parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default FalseThe behavior is as follows: bool. If True -> try parsing the index. Note: Automatically set to True if date_format or date_parser arguments have been passed. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of list. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. Values are joined with a space before parsing. dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’. Values are joined with a space before parsing. If a column or index cannot be represented as an array of datetime, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use to_datetime() after read_csv(). Note: A fast-path exists for iso8601-formatted dates. infer_datetime_formatbool, default FalseIf True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x. Deprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect. keep_date_colbool, default FalseIf True and parse_dates specifies combining multiple columns then keep the original columns. date_parserCallable, optionalFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. Deprecated since version 2.0.0: Use date_format instead, or read in as object and then apply to_datetime() as-needed. date_formatstr or dict of column -> format, optionalFormat to use for parsing dates when used in conjunction with parse_dates. The strftime to parse time, e.g. ""%d/%m/%Y"". See strftime documentation for more information on choices, though note that ""%f"" will parse all the way up to nanoseconds. You can also pass: “ISO8601”, to parse any ISO8601time string (not necessarily in exactly the same format); “mixed”, to infer the format for each element individually. This is risky,and you should probably use it along with dayfirst. Added in version 2.0.0. dayfirstbool, default FalseDD/MM format dates, international and European format. cache_datesbool, default TrueIf True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets. iteratorbool, default FalseReturn TextFileReader object for iteration or getting chunks with get_chunk(). chunksizeint, optionalNumber of lines to read from the file per chunk. Passing a value will cause the function to return a TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. thousandsstr (length 1), optionalCharacter acting as the thousands separator in numerical values. decimalstr (length 1), default ‘.’Character to recognize as decimal point (e.g., use ‘,’ for European data). lineterminatorstr (length 1), optionalCharacter used to denote a line break. Only valid with C parser. quotecharstr (length 1), optionalCharacter used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored. quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMALControl field quoting behavior per csv.QUOTE_* constants. Default is csv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special characters are quoted (e.g., characters defined in quotechar, delimiter, or lineterminator. doublequotebool, default TrueWhen quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element. escapecharstr (length 1), optionalCharacter used to escape other characters. commentstr (length 1), optionalCharacter indicating that the remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\na,b,c\n1,2,3 with header=0 will result in 'a,b,c' being treated as the header. encodingstr, optional, default ‘utf-8’Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python standard encodings . encoding_errorsstr, optional, default ‘strict’How encoding errors are treated. List of possible values . Added in version 1.3.0. dialectstr or csv.Dialect, optionalIf provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details. on_bad_lines{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are : 'error', raise an Exception when a bad line is encountered. 'warn', raise a warning when a bad line is encountered and skip that line. 'skip', skip bad lines without raising or warning when they are encountered. Added in version 1.3.0. Added in version 1.4.0:  Callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None, the bad line will be ignored. If the function returns a new list of strings with more elements than expected, a ParserWarning will be emitted while dropping extra elements. Only supported when engine='python' Changed in version 2.2.0:  Callable, function with signature as described in pyarrow documentation when engine='pyarrow' delim_whitespacebool, default FalseSpecifies whether or not whitespace (e.g. ' ' or '\t') will be used as the sep delimiter. Equivalent to setting sep='\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter. Deprecated since version 2.2.0: Use sep=""\s+"" instead. low_memorybool, default TrueInternally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference.  To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser). memory_mapbool, default FalseIf a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead. float_precision{‘high’, ‘legacy’, ‘round_trip’}, optionalSpecifies which converter the C engine should use for floating-point values. The options are None or 'high' for the ordinary converter, 'legacy' for the original lower precision pandas converter, and 'round_trip' for the round-trip converter. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.","["">>> pd.read_csv('data.csv')""]"
1670,..\pandas\reference\api\pandas.Series.str.cat.html,pandas.Series.str.cat,"Series.str.cat(others=None, sep=None, na_rep=None, join='left')[source]# Concatenate strings in the Series/Index with given separator. If others is specified, this function concatenates the Series/Index and elements of others element-wise. If others is not passed, then all values in the Series/Index are concatenated into a single string with a given sep.","Parameters: othersSeries, Index, DataFrame, np.ndarray or list-likeSeries, Index, DataFrame, np.ndarray (one- or two-dimensional) and other list-likes of strings must have the same length as the calling Series/Index, with the exception of indexed objects (i.e. Series/Index/DataFrame) if join is not None. If others is a list-like that contains a combination of Series, Index or np.ndarray (1-dim), then all elements will be unpacked and must satisfy the above criteria individually. If others is None, the method returns the concatenation of all strings in the calling Series/Index. sepstr, default ‘’The separator between the different elements/columns. By default the empty string ‘’ is used. na_repstr or None, default NoneRepresentation that is inserted for all missing values: If na_rep is None, and others is None, missing values in the Series/Index are omitted from the result. If na_rep is None, and others is not None, a row containing a missing value in any of the columns (before concatenation) will have a missing value in the result. join{‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘left’Determines the join-style between the calling Series/Index and any Series/Index/DataFrame in others (objects without an index need to match the length of the calling Series/Index). To disable alignment, use .values on any Series/Index/DataFrame in others. Returns: str, Series or IndexIf others is None, str is returned, otherwise a Series/Index (same type as caller) of objects is returned.","["">>> s = pd.Series(['a', 'b', np.nan, 'd'])\n>>> s.str.cat(sep=' ')\n'a b d'"", "">>> s.str.cat(sep=' ', na_rep='?')\n'a b ? d'"", "">>> s.str.cat(['A', 'B', 'C', 'D'], sep=',')\n0    a,A\n1    b,B\n2    NaN\n3    d,D\ndtype: object"", "">>> s.str.cat(['A', 'B', 'C', 'D'], sep=',', na_rep='-')\n0    a,A\n1    b,B\n2    -,C\n3    d,D\ndtype: object"", "">>> s.str.cat(['A', 'B', 'C', 'D'], na_rep='-')\n0    aA\n1    bB\n2    -C\n3    dD\ndtype: object"", "">>> t = pd.Series(['d', 'a', 'e', 'c'], index=[3, 0, 4, 2])\n>>> s.str.cat(t, join='left', na_rep='-')\n0    aa\n1    b-\n2    -c\n3    dd\ndtype: object\n>>>\n>>> s.str.cat(t, join='outer', na_rep='-')\n0    aa\n1    b-\n2    -c\n3    dd\n4    -e\ndtype: object\n>>>\n>>> s.str.cat(t, join='inner', na_rep='-')\n0    aa\n2    -c\n3    dd\ndtype: object\n>>>\n>>> s.str.cat(t, join='right', na_rep='-')\n3    dd\n0    aa\n4    -e\n2    -c\ndtype: object""]"
1671,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_quarter_start.html,pandas.tseries.offsets.BusinessDay.is_quarter_start,BusinessDay.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1672,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_on_offset.html,pandas.tseries.offsets.Tick.is_on_offset,Tick.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1673,..\pandas\reference\api\pandas.Index.is_.html,pandas.Index.is_,"final Index.is_(other)[source]# More flexible, faster check like is but that works through views. Note: this is not the same as Index.identical(), which checks that metadata is also the same.","Parameters: otherobjectOther object to compare against. Returns: boolTrue if both have same underlying data, False otherwise.","["">>> idx1 = pd.Index(['1', '2', '3'])\n>>> idx1.is_(idx1.view())\nTrue"", '>>> idx1.is_(idx1.copy())\nFalse']"
1674,..\pandas\reference\api\pandas.Series.str.center.html,pandas.Series.str.center,"Series.str.center(width, fillchar=' ')[source]# Pad left and right side of strings in the Series/Index. Equivalent to str.center().","Parameters: widthintMinimum width of resulting string; additional characters will be filled with fillchar. fillcharstrAdditional character for filling, default is whitespace. Returns: Series/Index of objects.","["">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.center(8, fillchar='.')\n0   ..dog...\n1   ..bird..\n2   .mouse..\ndtype: object"", "">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.ljust(8, fillchar='.')\n0   dog.....\n1   bird....\n2   mouse...\ndtype: object"", "">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.rjust(8, fillchar='.')\n0   .....dog\n1   ....bird\n2   ...mouse\ndtype: object""]"
1675,..\pandas\reference\api\pandas.read_excel.html,pandas.read_excel,"pandas.read_excel(io, sheet_name=0, *, header=0, names=None, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, parse_dates=False, date_parser=<no_default>, date_format=None, thousands=None, decimal='.', comment=None, skipfooter=0, storage_options=None, dtype_backend=<no_default>, engine_kwargs=None)[source]# Read an Excel file into a pandas DataFrame. Supports xls, xlsx, xlsm, xlsb, odf, ods and odt file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets. Notes For specific information on the methods used for each Excel engine, refer to the pandas user guide","Parameters: iostr, bytes, ExcelFile, xlrd.Book, path object, or file-like objectAny valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.xlsx. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO. Deprecated since version 2.1.0: Passing byte strings is deprecated. To read from a byte string, wrap it in a BytesIO object. sheet_namestr, int, list, or None, default 0Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets. Available cases: Defaults to 0: 1st sheet as a DataFrame 1: 2nd sheet as a DataFrame ""Sheet1"": Load sheet with name “Sheet1” [0, 1, ""Sheet5""]: Load first, second and sheet named “Sheet5” as a dict of DataFrame None: All worksheets. headerint, list of int, default 0Row (0-indexed) to use for the column labels of the parsed DataFrame. If a list of integers is passed those row positions will be combined into a MultiIndex. Use None if there is no header. namesarray-like, default NoneList of column names to use. If file contains no header row, then you should explicitly pass header=None. index_colint, str, list of int, default NoneColumn (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column.  If a list is passed, those columns will be combined into a MultiIndex.  If a subset of data is selected with usecols, index_col is based on the subset. Missing values will be forward filled to allow roundtripping with to_excel for merged_cells=True. To avoid forward filling the missing values use set_index after reading the data instead of index_col. usecolsstr, list-like, or callable, default None If None, then parse all columns. If str, then indicates comma separated list of Excel column letters and column ranges (e.g. “A:E” or “A,C,E:F”). Ranges are inclusive of both sides. If list of int, then indicates list of column numbers to be parsed (0-indexed). If list of string, then indicates list of column names to be parsed. If callable, then evaluate each column name against it and parse the column if the callable returns True. Returns a subset of the columns according to behavior above. dtypeType name or dict of column -> type, default NoneData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32} Use object to preserve data as stored in Excel and not interpret dtype, which will necessarily result in object dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. If you use None, it will infer the dtype of each column based on the data. engine{‘openpyxl’, ‘calamine’, ‘odf’, ‘pyxlsb’, ‘xlrd’}, default NoneIf io is not a buffer or path, this must be set to identify io. Engine compatibility : openpyxl supports newer Excel file formats. calamine supports Excel (.xls, .xlsx, .xlsm, .xlsb) and OpenDocument (.ods) file formats. odf supports OpenDocument file formats (.odf, .ods, .odt). pyxlsb supports Binary Excel files. xlrd supports old-style Excel files (.xls). When engine=None, the following logic will be used to determine the engine: If path_or_buffer is an OpenDocument format (.odf, .ods, .odt), then odf will be used. Otherwise if path_or_buffer is an xls format, xlrd will be used. Otherwise if path_or_buffer is in xlsb format, pyxlsb will be used. Otherwise openpyxl will be used. convertersdict, default NoneDict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the Excel cell content, and return the transformed content. true_valueslist, default NoneValues to consider as True. false_valueslist, default NoneValues to consider as False. skiprowslist-like, int, or callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2]. nrowsint, default NoneNumber of rows to parse. na_valuesscalar, str, list-like, or dict, default NoneAdditional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘None’, ‘n/a’, ‘nan’, ‘null’. keep_default_nabool, default TrueWhether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows: If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN. Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored. na_filterbool, default TrueDetect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file. verbosebool, default FalseIndicate number of NA values placed in non-numeric columns. parse_datesbool, list-like, or dict, default FalseThe behavior is as follows: bool. If True -> try parsing the index. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. dict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’ If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don`t want to parse some cells as date just change their type in Excel to “Text”. For non-standard datetime parsing, use pd.to_datetime after pd.read_excel. Note: A fast-path exists for iso8601-formatted dates. date_parserfunction, optionalFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. Pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. Deprecated since version 2.0.0: Use date_format instead, or read in as object and then apply to_datetime() as-needed. date_formatstr or dict of column -> format, default NoneIf used in conjunction with parse_dates, will parse dates according to this format. For anything more complex, please read in as object and then apply to_datetime() as-needed. Added in version 2.0.0. thousandsstr, default NoneThousands separator for parsing string columns to numeric.  Note that this parameter is only necessary for columns stored as TEXT in Excel, any numeric columns will automatically be parsed, regardless of display format. decimalstr, default ‘.’Character to recognize as decimal point for parsing string columns to numeric. Note that this parameter is only necessary for columns stored as TEXT in Excel, any numeric columns will automatically be parsed, regardless of display format.(e.g. use ‘,’ for European data). Added in version 1.4.0. commentstr, default NoneComments out remainder of line. Pass a character or characters to this argument to indicate comments in the input file. Any data between the comment string and the end of the current line is ignored. skipfooterint, default 0Rows at the end to skip (0-indexed). storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. engine_kwargsdict, optionalArbitrary keyword arguments passed to excel engine. Returns: DataFrame or dict of DataFramesDataFrame from the passed in Excel file. See notes in sheet_name argument for more information on when a dict of DataFrames is returned.","["">>> pd.read_excel('tmp.xlsx', index_col=0)  \n       Name  Value\n0   string1      1\n1   string2      2\n2  #Comment      3"", "">>> pd.read_excel(open('tmp.xlsx', 'rb'),\n...               sheet_name='Sheet3')  \n   Unnamed: 0      Name  Value\n0           0   string1      1\n1           1   string2      2\n2           2  #Comment      3"", "">>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  \n     0         1      2\n0  NaN      Name  Value\n1  0.0   string1      1\n2  1.0   string2      2\n3  2.0  #Comment      3"", "">>> pd.read_excel('tmp.xlsx', index_col=0,\n...               dtype={'Name': str, 'Value': float})  \n       Name  Value\n0   string1    1.0\n1   string2    2.0\n2  #Comment    3.0"", "">>> pd.read_excel('tmp.xlsx', index_col=0,\n...               na_values=['string1', 'string2'])  \n       Name  Value\n0       NaN      1\n1       NaN      2\n2  #Comment      3"", "">>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  \n      Name  Value\n0  string1    1.0\n1  string2    2.0\n2     None    NaN""]"
1676,..\pandas\reference\api\pandas.tseries.offsets.FY5253.normalize.html,pandas.tseries.offsets.FY5253.normalize,FY5253.normalize#,No parameters found,[]
1677,..\pandas\reference\api\pandas.DataFrame.pct_change.html,pandas.DataFrame.pct_change,"DataFrame.pct_change(periods=1, fill_method=<no_default>, limit=<no_default>, freq=None, **kwargs)[source]# Fractional change between the current and a prior element. Computes the fractional change from the immediately previous row by default. This is useful in comparing the fraction of change in a time series of elements. Note Despite the name of this method, it calculates fractional change (also known as per unit change or relative change) and not percentage change. If you need the percentage change, multiply these values by 100.","Parameters: periodsint, default 1Periods to shift for forming percent change. fill_method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default ‘pad’How to handle NAs before computing percent changes. Deprecated since version 2.1: All options of fill_method are deprecated except fill_method=None. limitint, default NoneThe number of consecutive NAs to fill before stopping. Deprecated since version 2.1. freqDateOffset, timedelta, or str, optionalIncrement to use from time series API (e.g. ‘ME’ or BDay()). **kwargsAdditional keyword arguments are passed into DataFrame.shift or Series.shift. Returns: Series or DataFrameThe same type as the calling object.","['>>> s = pd.Series([90, 91, 85])\n>>> s\n0    90\n1    91\n2    85\ndtype: int64', '>>> s.pct_change()\n0         NaN\n1    0.011111\n2   -0.065934\ndtype: float64', '>>> s.pct_change(periods=2)\n0         NaN\n1         NaN\n2   -0.055556\ndtype: float64', '>>> s = pd.Series([90, 91, None, 85])\n>>> s\n0    90.0\n1    91.0\n2     NaN\n3    85.0\ndtype: float64', '>>> s.ffill().pct_change()\n0         NaN\n1    0.011111\n2    0.000000\n3   -0.065934\ndtype: float64', "">>> df = pd.DataFrame({\n...     'FR': [4.0405, 4.0963, 4.3149],\n...     'GR': [1.7246, 1.7482, 1.8519],\n...     'IT': [804.74, 810.01, 860.13]},\n...     index=['1980-01-01', '1980-02-01', '1980-03-01'])\n>>> df\n                FR      GR      IT\n1980-01-01  4.0405  1.7246  804.74\n1980-02-01  4.0963  1.7482  810.01\n1980-03-01  4.3149  1.8519  860.13"", '>>> df.pct_change()\n                  FR        GR        IT\n1980-01-01       NaN       NaN       NaN\n1980-02-01  0.013810  0.013684  0.006549\n1980-03-01  0.053365  0.059318  0.061876', "">>> df = pd.DataFrame({\n...     '2016': [1769950, 30586265],\n...     '2015': [1500923, 40912316],\n...     '2014': [1371819, 41403351]},\n...     index=['GOOG', 'APPL'])\n>>> df\n          2016      2015      2014\nGOOG   1769950   1500923   1371819\nAPPL  30586265  40912316  41403351"", "">>> df.pct_change(axis='columns', periods=-1)\n          2016      2015  2014\nGOOG  0.179241  0.094112   NaN\nAPPL -0.252395 -0.011860   NaN""]"
1678,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_quarter_end.html,pandas.tseries.offsets.Tick.is_quarter_end,Tick.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1679,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.describe.html,pandas.core.groupby.SeriesGroupBy.describe,"SeriesGroupBy.describe(percentiles=None, include=None, exclude=None)[source]# Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Notes For numeric data, the result’s index will include count, mean, std, min, max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75. The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. The top is the most common value. The freq is the most common value’s frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series.","Parameters: percentileslist-like of numbers, optionalThe percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles. include‘all’, list-like of dtypes or None (default), optionalA white list of data types to include in the result. Ignored for Series. Here are the options: ‘all’ : All columns of the input will be included in the output. A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit numpy.number. To limit it instead to object columns submit the numpy.object data type. Strings can also be used in the style of select_dtypes (e.g. df.describe(include=['O'])). To select pandas categorical columns, use 'category' None (default) : The result will include all numeric columns. excludelist-like of dtypes or None (default), optional,A black list of data types to omit from the result. Ignored for Series. Here are the options: A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit numpy.number. To exclude object columns submit the data type numpy.object. Strings can also be used in the style of select_dtypes (e.g. df.describe(exclude=['O'])). To exclude pandas categorical columns, use 'category' None (default) : The result will exclude nothing. Returns: Series or DataFrameSummary statistics of the Series or Dataframe provided.","['>>> s = pd.Series([1, 2, 3])\n>>> s.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\ndtype: float64', "">>> s = pd.Series(['a', 'a', 'b', 'c'])\n>>> s.describe()\ncount     4\nunique    3\ntop       a\nfreq      2\ndtype: object"", '>>> s = pd.Series([\n...     np.datetime64(""2000-01-01""),\n...     np.datetime64(""2010-01-01""),\n...     np.datetime64(""2010-01-01"")\n... ])\n>>> s.describe()\ncount                      3\nmean     2006-09-01 08:00:00\nmin      2000-01-01 00:00:00\n25%      2004-12-31 12:00:00\n50%      2010-01-01 00:00:00\n75%      2010-01-01 00:00:00\nmax      2010-01-01 00:00:00\ndtype: object', "">>> df = pd.DataFrame({'categorical': pd.Categorical(['d', 'e', 'f']),\n...                    'numeric': [1, 2, 3],\n...                    'object': ['a', 'b', 'c']\n...                    })\n>>> df.describe()\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0"", "">>> df.describe(include='all')  \n       categorical  numeric object\ncount            3      3.0      3\nunique           3      NaN      3\ntop              f      NaN      a\nfreq             1      NaN      1\nmean           NaN      2.0    NaN\nstd            NaN      1.0    NaN\nmin            NaN      1.0    NaN\n25%            NaN      1.5    NaN\n50%            NaN      2.0    NaN\n75%            NaN      2.5    NaN\nmax            NaN      3.0    NaN"", '>>> df.numeric.describe()\ncount    3.0\nmean     2.0\nstd      1.0\nmin      1.0\n25%      1.5\n50%      2.0\n75%      2.5\nmax      3.0\nName: numeric, dtype: float64', '>>> df.describe(include=[np.number])\n       numeric\ncount      3.0\nmean       2.0\nstd        1.0\nmin        1.0\n25%        1.5\n50%        2.0\n75%        2.5\nmax        3.0', '>>> df.describe(include=[object])  \n       object\ncount       3\nunique      3\ntop         a\nfreq        1', "">>> df.describe(include=['category'])\n       categorical\ncount            3\nunique           3\ntop              d\nfreq             1"", '>>> df.describe(exclude=[np.number])  \n       categorical object\ncount            3      3\nunique           3      3\ntop              f      a\nfreq             1      1', '>>> df.describe(exclude=[object])  \n       categorical  numeric\ncount            3      3.0\nunique           3      NaN\ntop              f      NaN\nfreq             1      NaN\nmean           NaN      2.0\nstd            NaN      1.0\nmin            NaN      1.0\n25%            NaN      1.5\n50%            NaN      2.0\n75%            NaN      2.5\nmax            NaN      3.0']"
1680,..\pandas\reference\api\pandas.Index.is_boolean.html,pandas.Index.is_boolean,final Index.is_boolean()[source]# Check if the Index only consists of booleans. Deprecated since version 2.0.0: Use pandas.api.types.is_bool_dtype instead.,Returns: boolWhether or not the Index only consists of booleans.,"['>>> idx = pd.Index([True, False, True])\n>>> idx.is_boolean()  \nTrue', '>>> idx = pd.Index([""True"", ""False"", ""True""])\n>>> idx.is_boolean()  \nFalse', '>>> idx = pd.Index([True, False, ""True""])\n>>> idx.is_boolean()  \nFalse']"
1681,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_year_end.html,pandas.tseries.offsets.BusinessDay.is_year_end,BusinessDay.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1682,..\pandas\reference\api\pandas.Series.str.contains.html,pandas.Series.str.contains,"Series.str.contains(pat, case=True, flags=0, na=None, regex=True)[source]# Test if pattern or regex is contained within a string of a Series or Index. Return boolean Series or Index based on whether a given pattern or regex is contained within a string of a Series or Index.","Parameters: patstrCharacter sequence or regular expression. casebool, default TrueIf True, case sensitive. flagsint, default 0 (no flags)Flags to pass through to the re module, e.g. re.IGNORECASE. nascalar, optionalFill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used. regexbool, default TrueIf True, assumes the pat is a regular expression. If False, treats the pat as a literal string. Returns: Series or Index of boolean valuesA Series or Index of boolean values indicating whether the given pattern is contained within the string of each element of the Series or Index.","["">>> s1 = pd.Series(['Mouse', 'dog', 'house and parrot', '23', np.nan])\n>>> s1.str.contains('og', regex=False)\n0    False\n1     True\n2    False\n3    False\n4      NaN\ndtype: object"", "">>> ind = pd.Index(['Mouse', 'dog', 'house and parrot', '23.0', np.nan])\n>>> ind.str.contains('23', regex=False)\nIndex([False, False, False, True, nan], dtype='object')"", "">>> s1.str.contains('oG', case=True, regex=True)\n0    False\n1    False\n2    False\n3    False\n4      NaN\ndtype: object"", "">>> s1.str.contains('og', na=False, regex=True)\n0    False\n1     True\n2    False\n3    False\n4    False\ndtype: bool"", "">>> s1.str.contains('house|dog', regex=True)\n0    False\n1     True\n2     True\n3    False\n4      NaN\ndtype: object"", "">>> import re\n>>> s1.str.contains('PARROT', flags=re.IGNORECASE, regex=True)\n0    False\n1    False\n2     True\n3    False\n4      NaN\ndtype: object"", "">>> s1.str.contains('\\\\d', regex=True)\n0    False\n1    False\n2    False\n3     True\n4      NaN\ndtype: object"", "">>> s2 = pd.Series(['40', '40.0', '41', '41.0', '35'])\n>>> s2.str.contains('.0', regex=True)\n0     True\n1     True\n2    False\n3     True\n4    False\ndtype: bool""]"
1683,..\pandas\reference\api\pandas.Index.is_categorical.html,pandas.Index.is_categorical,"final Index.is_categorical()[source]# Check if the Index holds categorical data. Deprecated since version 2.0.0: Use isinstance(index.dtype, pd.CategoricalDtype) instead.",Returns: boolTrue if the Index is categorical.,"['>>> idx = pd.Index([""Watermelon"", ""Orange"", ""Apple"",\n...                 ""Watermelon""]).astype(""category"")\n>>> idx.is_categorical()  \nTrue', '>>> idx = pd.Index([1, 3, 5, 7])\n>>> idx.is_categorical()  \nFalse', '>>> s = pd.Series([""Peter"", ""Victor"", ""Elisabeth"", ""Mar""])\n>>> s\n0        Peter\n1       Victor\n2    Elisabeth\n3          Mar\ndtype: object\n>>> s.index.is_categorical()  \nFalse']"
1684,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.diff.html,pandas.core.groupby.SeriesGroupBy.diff,"SeriesGroupBy.diff(periods=1, axis=<no_default>)[source]# First discrete difference of element. Calculates the difference of each element compared with another element in the group (default is element in previous row).","Parameters: periodsint, default 1Periods to shift for calculating difference, accepts negative values. axisaxis to shift, default 0Take difference over rows (0) or columns (1). Deprecated since version 2.1.0: For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. Returns: Series or DataFrameFirst differences.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).diff()\na    NaN\na   -5.0\na    6.0\nb    NaN\nb   -1.0\nb    0.0\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).diff()\n         a    b\n  dog  NaN  NaN\n  dog  2.0  3.0\n  dog  2.0  4.0\nmouse  NaN  NaN\nmouse  0.0  0.0\nmouse  1.0 -2.0\nmouse -5.0 -1.0""]"
1685,..\pandas\reference\api\pandas.Series.str.count.html,pandas.Series.str.count,"Series.str.count(pat, flags=0)[source]# Count occurrences of pattern in each string of the Series/Index. This function is used to count the number of times a particular regex pattern is repeated in each of the string elements of the Series. Notes Some characters need to be escaped when passing in pat. eg. '$' has a special meaning in regex and must be escaped when finding this literal character.","Parameters: patstrValid regular expression. flagsint, default 0, meaning no flagsFlags for the re module. For a complete list, see here. **kwargsFor compatibility with other string methods. Not used. Returns: Series or IndexSame type as the calling object containing the integer counts.","["">>> s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n>>> s.str.count('a')\n0    0.0\n1    0.0\n2    2.0\n3    2.0\n4    NaN\n5    0.0\n6    1.0\ndtype: float64"", "">>> s = pd.Series(['$', 'B', 'Aab$', '$$ca', 'C$B$', 'cat'])\n>>> s.str.count('\\\\$')\n0    1\n1    0\n2    1\n3    2\n4    2\n5    0\ndtype: int64"", "">>> pd.Index(['A', 'A', 'Aaba', 'cat']).str.count('a')\nIndex([0, 0, 2, 1], dtype='int64')""]"
1686,..\pandas\reference\api\pandas.DataFrame.pipe.html,pandas.DataFrame.pipe,"DataFrame.pipe(func, *args, **kwargs)[source]# Apply chainable functions that expect Series or DataFrames. Notes Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects.","Parameters: funcfunctionFunction to apply to the Series/DataFrame. args, and kwargs are passed into func. Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame. *argsiterable, optionalPositional arguments passed into func. **kwargsmapping, optionalA dictionary of keyword arguments passed into func. Returns: the return type of func.","["">>> data = [[8000, 1000], [9500, np.nan], [5000, 2000]]\n>>> df = pd.DataFrame(data, columns=['Salary', 'Others'])\n>>> df\n   Salary  Others\n0    8000  1000.0\n1    9500     NaN\n2    5000  2000.0"", '>>> def subtract_federal_tax(df):\n...     return df * 0.9\n>>> def subtract_state_tax(df, rate):\n...     return df * (1 - rate)\n>>> def subtract_national_insurance(df, rate, rate_increase):\n...     new_rate = rate + rate_increase\n...     return df * (1 - new_rate)', '>>> subtract_national_insurance(\n...     subtract_state_tax(subtract_federal_tax(df), rate=0.12),\n...     rate=0.05,\n...     rate_increase=0.02)', '>>> (\n...     df.pipe(subtract_federal_tax)\n...     .pipe(subtract_state_tax, rate=0.12)\n...     .pipe(subtract_national_insurance, rate=0.05, rate_increase=0.02)\n... )\n    Salary   Others\n0  5892.48   736.56\n1  6997.32      NaN\n2  3682.80  1473.12', "">>> def subtract_national_insurance(rate, df, rate_increase):\n...     new_rate = rate + rate_increase\n...     return df * (1 - new_rate)\n>>> (\n...     df.pipe(subtract_federal_tax)\n...     .pipe(subtract_state_tax, rate=0.12)\n...     .pipe(\n...         (subtract_national_insurance, 'df'),\n...         rate=0.05,\n...         rate_increase=0.02\n...     )\n... )\n    Salary   Others\n0  5892.48   736.56\n1  6997.32      NaN\n2  3682.80  1473.12""]"
1687,..\pandas\reference\api\pandas.read_feather.html,pandas.read_feather,"pandas.read_feather(path, columns=None, use_threads=True, storage_options=None, dtype_backend=<no_default>)[source]# Load a feather-format object from the file path.","Parameters: pathstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.feather. columnssequence, default NoneIf not provided, all columns are read. use_threadsbool, default TrueWhether to parallelize reading using multiple threads. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: type of object stored in file","['>>> df = pd.read_feather(""path/to/file.feather"")']"
1688,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_quarter_start.html,pandas.tseries.offsets.Tick.is_quarter_start,Tick.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1689,..\pandas\reference\api\pandas.tseries.offsets.FY5253.rule_code.html,pandas.tseries.offsets.FY5253.rule_code,FY5253.rule_code#,No parameters found,[]
1690,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.is_year_start.html,pandas.tseries.offsets.BusinessDay.is_year_start,BusinessDay.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1691,..\pandas\reference\api\pandas.Index.is_floating.html,pandas.Index.is_floating,"final Index.is_floating()[source]# Check if the Index is a floating type. Deprecated since version 2.0.0: Use pandas.api.types.is_float_dtype instead The Index may consist of only floats, NaNs, or a mix of floats, integers, or NaNs.","Returns: boolWhether or not the Index only consists of only consists of floats, NaNs, or a mix of floats, integers, or NaNs.","['>>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n>>> idx.is_floating()  \nTrue', '>>> idx = pd.Index([1.0, 2.0, np.nan, 4.0])\n>>> idx.is_floating()  \nTrue', '>>> idx = pd.Index([1, 2, 3, 4, np.nan])\n>>> idx.is_floating()  \nTrue', '>>> idx = pd.Index([1, 2, 3, 4])\n>>> idx.is_floating()  \nFalse']"
1692,..\pandas\reference\api\pandas.tseries.offsets.FY5253.startingMonth.html,pandas.tseries.offsets.FY5253.startingMonth,FY5253.startingMonth#,No parameters found,[]
1693,..\pandas\reference\api\pandas.DataFrame.pivot.html,pandas.DataFrame.pivot,"DataFrame.pivot(*, columns, index=<no_default>, values=<no_default>)[source]# Return reshaped DataFrame organized by given index / column values. Reshape data (produce a “pivot” table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the User Guide for more on reshaping. Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods. Reference the user guide for more examples.","Parameters: columnsstr or object or a list of strColumn to use to make new frame’s columns. indexstr or object or a list of str, optionalColumn to use to make new frame’s index. If not given, uses existing index. valuesstr, object or a list of the previous, optionalColumn(s) to use for populating new frame’s values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns. Returns: DataFrameReturns reshaped DataFrame. Raises: ValueError:When there are any index, columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.","["">>> df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two',\n...                            'two'],\n...                    'bar': ['A', 'B', 'C', 'A', 'B', 'C'],\n...                    'baz': [1, 2, 3, 4, 5, 6],\n...                    'zoo': ['x', 'y', 'z', 'q', 'w', 't']})\n>>> df\n    foo   bar  baz  zoo\n0   one   A    1    x\n1   one   B    2    y\n2   one   C    3    z\n3   two   A    4    q\n4   two   B    5    w\n5   two   C    6    t"", "">>> df.pivot(index='foo', columns='bar', values='baz')\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6"", "">>> df.pivot(index='foo', columns='bar')['baz']\nbar  A   B   C\nfoo\none  1   2   3\ntwo  4   5   6"", "">>> df.pivot(index='foo', columns='bar', values=['baz', 'zoo'])\n      baz       zoo\nbar   A  B  C   A  B  C\nfoo\none   1  2  3   x  y  z\ntwo   4  5  6   q  w  t"", '>>> df = pd.DataFrame({\n...        ""lev1"": [1, 1, 1, 2, 2, 2],\n...        ""lev2"": [1, 1, 2, 1, 1, 2],\n...        ""lev3"": [1, 2, 1, 2, 1, 2],\n...        ""lev4"": [1, 2, 3, 4, 5, 6],\n...        ""values"": [0, 1, 2, 3, 4, 5]})\n>>> df\n    lev1 lev2 lev3 lev4 values\n0   1    1    1    1    0\n1   1    1    2    2    1\n2   1    2    1    3    2\n3   2    1    2    4    3\n4   2    1    1    5    4\n5   2    2    2    6    5', '>>> df.pivot(index=""lev1"", columns=[""lev2"", ""lev3""], values=""values"")\nlev2    1         2\nlev3    1    2    1    2\nlev1\n1     0.0  1.0  2.0  NaN\n2     4.0  3.0  NaN  5.0', '>>> df.pivot(index=[""lev1"", ""lev2""], columns=[""lev3""], values=""values"")\n      lev3    1    2\nlev1  lev2\n   1     1  0.0  1.0\n         2  2.0  NaN\n   2     1  4.0  3.0\n         2  NaN  5.0', '>>> df = pd.DataFrame({""foo"": [\'one\', \'one\', \'two\', \'two\'],\n...                    ""bar"": [\'A\', \'A\', \'B\', \'C\'],\n...                    ""baz"": [1, 2, 3, 4]})\n>>> df\n   foo bar  baz\n0  one   A    1\n1  one   A    2\n2  two   B    3\n3  two   C    4', "">>> df.pivot(index='foo', columns='bar', values='baz')\nTraceback (most recent call last):\n   ...\nValueError: Index contains duplicate entries, cannot reshape""]"
1694,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_year_end.html,pandas.tseries.offsets.Tick.is_year_end,Tick.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1695,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.ffill.html,pandas.core.groupby.SeriesGroupBy.ffill,SeriesGroupBy.ffill(limit=None)[source]# Forward fill the values.,"Parameters: limitint, optionalLimit of how many values to fill. Returns: Series or DataFrameObject with missing values filled.","['>>> key = [0, 0, 1, 1]\n>>> ser = pd.Series([np.nan, 2, 3, np.nan], index=key)\n>>> ser\n0    NaN\n0    2.0\n1    3.0\n1    NaN\ndtype: float64\n>>> ser.groupby(level=0).ffill()\n0    NaN\n0    2.0\n1    3.0\n1    3.0\ndtype: float64', '>>> df = pd.DataFrame(\n...     {\n...         ""key"": [0, 0, 1, 1, 1],\n...         ""A"": [np.nan, 2, np.nan, 3, np.nan],\n...         ""B"": [2, 3, np.nan, np.nan, np.nan],\n...         ""C"": [np.nan, np.nan, 2, np.nan, np.nan],\n...     }\n... )\n>>> df\n   key    A    B   C\n0    0  NaN  2.0 NaN\n1    0  2.0  3.0 NaN\n2    1  NaN  NaN 2.0\n3    1  3.0  NaN NaN\n4    1  NaN  NaN NaN', '>>> df.groupby(""key"").ffill()\n     A    B   C\n0  NaN  2.0 NaN\n1  2.0  3.0 NaN\n2  NaN  NaN 2.0\n3  3.0  NaN 2.0\n4  3.0  NaN 2.0', '>>> df.T.groupby(np.array([0, 0, 1, 1])).ffill().T\n   key    A    B    C\n0  0.0  0.0  2.0  2.0\n1  0.0  2.0  3.0  3.0\n2  1.0  1.0  NaN  2.0\n3  1.0  3.0  NaN  NaN\n4  1.0  1.0  NaN  NaN', '>>> df.groupby(""key"").ffill(limit=1)\n     A    B    C\n0  NaN  2.0  NaN\n1  2.0  3.0  NaN\n2  NaN  NaN  2.0\n3  3.0  NaN  2.0\n4  3.0  NaN  NaN']"
1696,..\pandas\reference\api\pandas.read_fwf.html,pandas.read_fwf,"pandas.read_fwf(filepath_or_buffer, *, colspecs='infer', widths=None, infer_nrows=100, dtype_backend=<no_default>, iterator=False, chunksize=None, **kwds)[source]# Read a table of fixed-width formatted lines into DataFrame. Also supports optionally iterating or breaking of the file into chunks. Additional help can be found in the online docs for IO Tools.","Parameters: filepath_or_bufferstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a text read() function.The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. colspecslist of tuple (int, int) or ‘infer’. optionalA list of tuples giving the extents of the fixed-width fields of each line as half-open intervals (i.e.,  [from, to[ ). String value ‘infer’ can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data which are not being skipped via skiprows (default=’infer’). widthslist of int, optionalA list of field widths which can be used instead of ‘colspecs’ if the intervals are contiguous. infer_nrowsint, default 100The number of rows to consider when letting the parser determine the colspecs. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. **kwdsoptionalOptional keyword arguments can be passed to TextFileReader. Returns: DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.","["">>> pd.read_fwf('data.csv')""]"
1697,..\pandas\reference\api\pandas.Series.str.decode.html,pandas.Series.str.decode,"Series.str.decode(encoding, errors='strict')[source]# Decode character string in the Series/Index using indicated encoding. Equivalent to str.decode() in python2 and bytes.decode() in python3.","Parameters: encodingstr errorsstr, optional Returns: Series or Index","["">>> ser = pd.Series([b'cow', b'123', b'()'])\n>>> ser.str.decode('ascii')\n0   cow\n1   123\n2   ()\ndtype: object""]"
1698,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.kwds.html,pandas.tseries.offsets.BusinessDay.kwds,BusinessDay.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1699,..\pandas\reference\api\pandas.Index.is_integer.html,pandas.Index.is_integer,final Index.is_integer()[source]# Check if the Index only consists of integers. Deprecated since version 2.0.0: Use pandas.api.types.is_integer_dtype instead.,Returns: boolWhether or not the Index only consists of integers.,"['>>> idx = pd.Index([1, 2, 3, 4])\n>>> idx.is_integer()  \nTrue', '>>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n>>> idx.is_integer()  \nFalse', '>>> idx = pd.Index([""Apple"", ""Mango"", ""Watermelon""])\n>>> idx.is_integer()  \nFalse']"
1700,..\pandas\reference\api\pandas.tseries.offsets.FY5253.variation.html,pandas.tseries.offsets.FY5253.variation,FY5253.variation#,No parameters found,[]
1701,..\pandas\reference\api\pandas.Index.is_interval.html,pandas.Index.is_interval,"final Index.is_interval()[source]# Check if the Index holds Interval objects. Deprecated since version 2.0.0: Use isinstance(index.dtype, pd.IntervalDtype) instead.",Returns: boolWhether or not the Index holds Interval objects.,"['>>> idx = pd.Index([pd.Interval(left=0, right=5),\n...                 pd.Interval(left=5, right=10)])\n>>> idx.is_interval()  \nTrue', '>>> idx = pd.Index([1, 3, 5, 7])\n>>> idx.is_interval()  \nFalse']"
1702,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.n.html,pandas.tseries.offsets.BusinessDay.n,BusinessDay.n#,No parameters found,[]
1703,..\pandas\reference\api\pandas.read_gbq.html,pandas.read_gbq,"pandas.read_gbq(query, project_id=None, index_col=None, col_order=None, reauth=False, auth_local_webserver=True, dialect=None, location=None, configuration=None, credentials=None, use_bqstorage_api=None, max_results=None, progress_bar_type=None)[source]# Load data from Google BigQuery. Deprecated since version 2.2.0: Please use pandas_gbq.read_gbq instead. This function requires the pandas-gbq package. See the How to authenticate with Google BigQuery guide for authentication instructions.","Parameters: querystrSQL-Like Query to return data values. project_idstr, optionalGoogle BigQuery Account project ID. Optional when available from the environment. index_colstr, optionalName of result column to use for index in results DataFrame. col_orderlist(str), optionalList of BigQuery column names in the desired order for results DataFrame. reauthbool, default FalseForce Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used. auth_local_webserverbool, default TrueUse the local webserver flow instead of the console flow when getting user credentials. New in version 0.2.0 of pandas-gbq. Changed in version 1.5.0: Default value is changed to True. Google has deprecated the auth_local_webserver = False “out of band” (copy-paste) flow. dialectstr, default ‘legacy’Note: The default value is changing to ‘standard’ in a future version. SQL syntax dialect to use. Value can be one of: 'legacy'Use BigQuery’s legacy SQL dialect. For more information see BigQuery Legacy SQL Reference. 'standard'Use BigQuery’s standard SQL, which is compliant with the SQL 2011 standard. For more information see BigQuery Standard SQL Reference. locationstr, optionalLocation where the query job should run. See the BigQuery locations documentation for a list of available locations. The location must match that of any datasets used in the query. New in version 0.5.0 of pandas-gbq. configurationdict, optionalQuery config parameters for job processing. For example: configuration = {‘query’: {‘useQueryCache’: False}} For more information see BigQuery REST API Reference. credentialsgoogle.auth.credentials.Credentials, optionalCredentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine google.auth.compute_engine.Credentials or Service Account google.oauth2.service_account.Credentials directly. New in version 0.8.0 of pandas-gbq. use_bqstorage_apibool, default FalseUse the BigQuery Storage API to download query results quickly, but at an increased cost. To use this API, first enable it in the Cloud Console. You must also have the bigquery.readsessions.create permission on the project you are billing queries to. This feature requires version 0.10.0 or later of the pandas-gbq package. It also requires the google-cloud-bigquery-storage and fastavro packages. max_resultsint, optionalIf set, limit the maximum number of rows to fetch from the query results. progress_bar_typeOptional, strIf set, use the tqdm library to display a progress bar while the data downloads. Install the tqdm package to use this feature. Possible values of progress_bar_type include: NoneNo progress bar. 'tqdm'Use the tqdm.tqdm() function to print a progress bar to sys.stderr. 'tqdm_notebook'Use the tqdm.tqdm_notebook() function to display a progress bar as a Jupyter notebook widget. 'tqdm_gui'Use the tqdm.tqdm_gui() function to display a progress bar as a graphical dialog box. Returns: df: DataFrameDataFrame representing results of query.","['>>> sql = ""SELECT name FROM table_name WHERE state = \'TX\' LIMIT 100;""\n>>> df = pd.read_gbq(sql, dialect=""standard"")  \n>>> project_id = ""your-project-id""  \n>>> df = pd.read_gbq(sql,\n...                  project_id=project_id,\n...                  dialect=""standard""\n...                  )']"
1704,..\pandas\reference\api\pandas.tseries.offsets.Tick.is_year_start.html,pandas.tseries.offsets.Tick.is_year_start,Tick.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1705,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.fillna.html,pandas.core.groupby.SeriesGroupBy.fillna,"SeriesGroupBy.fillna(value=None, method=None, axis=<no_default>, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values using the specified method within groups. Deprecated since version 2.2.0: This method is deprecated and will be removed in a future version. Use the SeriesGroupBy.ffill() or SeriesGroupBy.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna() instead.","Parameters: valuescalar, dict, Series, or DataFrameValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame).  Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. Users wanting to use the value argument and not method should prefer Series.fillna() as this will produce the same result and be more performant. method{{‘bfill’, ‘ffill’, None}}, default NoneMethod to use for filling holes. 'ffill' will propagate the last valid observation forward within a group. 'bfill' will use next valid observation to fill the gap. axis{0 or ‘index’, 1 or ‘columns’}Unused, only for compatibility with DataFrameGroupBy.fillna(). inplacebool, default FalseBroken. Do not set to True. limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill within a group. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Returns: SeriesObject with missing values filled within groups.","["">>> lst = ['cat', 'cat', 'cat', 'mouse', 'mouse']\n>>> ser = pd.Series([1, None, None, 2, None], index=lst)\n>>> ser\ncat    1.0\ncat    NaN\ncat    NaN\nmouse  2.0\nmouse  NaN\ndtype: float64\n>>> ser.groupby(level=0).fillna(0, limit=1)\ncat    1.0\ncat    0.0\ncat    NaN\nmouse  2.0\nmouse  0.0\ndtype: float64""]"
1706,..\pandas\reference\api\pandas.DataFrame.pivot_table.html,pandas.DataFrame.pivot_table,"DataFrame.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All', observed=<no_default>, sort=True)[source]# Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame. Notes Reference the user guide for more examples.","Parameters: valueslist-like or scalar, optionalColumn or columns to aggregate. indexcolumn, Grouper, array, or list of the previousKeys to group by on the pivot table index. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. columnscolumn, Grouper, array, or list of the previousKeys to group by on the pivot table column. If a list is passed, it can contain any of the other types (except list). If an array is passed, it must be the same length as the data and will be used in the same manner as column values. aggfuncfunction, list of functions, dict, default “mean”If a list of functions is passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves). If a dict is passed, the key is column to aggregate and the value is function or list of functions. If margin=True, aggfunc will be used to calculate the partial aggregates. fill_valuescalar, default NoneValue to replace missing values with (in the resulting pivot table, after aggregation). marginsbool, default FalseIf margins=True, special All columns and rows will be added with partial group aggregates across the categories on the rows and columns. dropnabool, default TrueDo not include columns whose entries are all NaN. If True, rows with a NaN value in any column will be omitted before computing margins. margins_namestr, default ‘All’Name of the row / column that will contain the totals when margins is True. observedbool, default FalseThis only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. Deprecated since version 2.2.0: The default value of False is deprecated and will change to True in a future version of pandas. sortbool, default TrueSpecifies if the result should be sorted. Added in version 1.3.0. Returns: DataFrameAn Excel style pivot table.","['>>> df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",\n...                          ""bar"", ""bar"", ""bar"", ""bar""],\n...                    ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",\n...                          ""one"", ""one"", ""two"", ""two""],\n...                    ""C"": [""small"", ""large"", ""large"", ""small"",\n...                          ""small"", ""large"", ""small"", ""small"",\n...                          ""large""],\n...                    ""D"": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n...                    ""E"": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n>>> df\n     A    B      C  D  E\n0  foo  one  small  1  2\n1  foo  one  large  2  4\n2  foo  one  large  2  5\n3  foo  two  small  3  5\n4  foo  two  small  3  6\n5  bar  one  large  4  6\n6  bar  one  small  5  8\n7  bar  two  small  6  9\n8  bar  two  large  7  9', '>>> table = pd.pivot_table(df, values=\'D\', index=[\'A\', \'B\'],\n...                        columns=[\'C\'], aggfunc=""sum"")\n>>> table\nC        large  small\nA   B\nbar one    4.0    5.0\n    two    7.0    6.0\nfoo one    4.0    1.0\n    two    NaN    6.0', '>>> table = pd.pivot_table(df, values=\'D\', index=[\'A\', \'B\'],\n...                        columns=[\'C\'], aggfunc=""sum"", fill_value=0)\n>>> table\nC        large  small\nA   B\nbar one      4      5\n    two      7      6\nfoo one      4      1\n    two      0      6', '>>> table = pd.pivot_table(df, values=[\'D\', \'E\'], index=[\'A\', \'C\'],\n...                        aggfunc={\'D\': ""mean"", \'E\': ""mean""})\n>>> table\n                D         E\nA   C\nbar large  5.500000  7.500000\n    small  5.500000  8.500000\nfoo large  2.000000  4.500000\n    small  2.333333  4.333333', '>>> table = pd.pivot_table(df, values=[\'D\', \'E\'], index=[\'A\', \'C\'],\n...                        aggfunc={\'D\': ""mean"",\n...                                 \'E\': [""min"", ""max"", ""mean""]})\n>>> table\n                  D   E\n               mean max      mean  min\nA   C\nbar large  5.500000   9  7.500000    6\n    small  5.500000   9  8.500000    8\nfoo large  2.000000   5  4.500000    4\n    small  2.333333   6  4.333333    2']"
1707,..\pandas\reference\api\pandas.Series.str.encode.html,pandas.Series.str.encode,"Series.str.encode(encoding, errors='strict')[source]# Encode character string in the Series/Index using indicated encoding. Equivalent to str.encode().","Parameters: encodingstr errorsstr, optional Returns: Series/Index of objects","["">>> ser = pd.Series(['cow', '123', '()'])\n>>> ser.str.encode(encoding='ascii')\n0     b'cow'\n1     b'123'\n2      b'()'\ndtype: object""]"
1708,..\pandas\reference\api\pandas.tseries.offsets.FY5253.weekday.html,pandas.tseries.offsets.FY5253.weekday,FY5253.weekday#,No parameters found,[]
1709,..\pandas\reference\api\pandas.Index.is_monotonic_decreasing.html,pandas.Index.is_monotonic_decreasing,property Index.is_monotonic_decreasing[source]# Return a boolean if the values are equal or decreasing.,Returns: bool,"['>>> pd.Index([3, 2, 1]).is_monotonic_decreasing\nTrue\n>>> pd.Index([3, 2, 2]).is_monotonic_decreasing\nTrue\n>>> pd.Index([3, 1, 2]).is_monotonic_decreasing\nFalse']"
1710,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.name.html,pandas.tseries.offsets.BusinessDay.name,BusinessDay.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1711,..\pandas\reference\api\pandas.read_hdf.html,pandas.read_hdf,"pandas.read_hdf(path_or_buf, key=None, mode='r', errors='strict', where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, **kwargs)[source]# Read from the store, close it if we opened it. Retrieve pandas object stored in file, optionally based on where criteria. Warning Pandas uses PyTables for reading and writing HDF5 files, which allows serializing object-dtype data with pickle when using the “fixed” format. Loading pickled data received from untrusted sources can be unsafe. See: https://docs.python.org/3/library/pickle.html for more.","Parameters: path_or_bufstr, path object, pandas.HDFStoreAny valid string path is acceptable. Only supports the local file system, remote URLs and file-like objects are not supported. If you want to pass in a path object, pandas accepts any os.PathLike. Alternatively, pandas accepts an open pandas.HDFStore object. keyobject, optionalThe group identifier in the store. Can be omitted if the HDF file contains a single pandas object. mode{‘r’, ‘r+’, ‘a’}, default ‘r’Mode to use when opening the file. Ignored if path_or_buf is a pandas.HDFStore. Default is ‘r’. errorsstr, default ‘strict’Specifies how encoding and decoding errors are to be handled. See the errors argument for open() for a full list of options. wherelist, optionalA list of Term (or convertible) objects. startint, optionalRow number to start selection. stopint, optionalRow number to stop selection. columnslist, optionalA list of columns names to return. iteratorbool, optionalReturn an iterator object. chunksizeint, optionalNumber of rows to include in an iteration when using an iterator. **kwargsAdditional keyword arguments passed to HDFStore. Returns: objectThe selected object. Return type depends on the object stored.","["">>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  \n>>> df.to_hdf('./store.h5', 'data')  \n>>> reread = pd.read_hdf('./store.h5')""]"
1712,..\pandas\reference\api\pandas.tseries.offsets.Tick.kwds.html,pandas.tseries.offsets.Tick.kwds,Tick.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1713,..\pandas\reference\api\pandas.DataFrame.plot.area.html,pandas.DataFrame.plot.area,"DataFrame.plot.area(x=None, y=None, stacked=True, **kwargs)[source]# Draw a stacked area plot. An area plot displays quantitative data visually. This function wraps the matplotlib area function.","Parameters: xlabel or position, optionalCoordinates for the X axis. By default uses the index. ylabel or position, optionalColumn to plot. By default uses all columns. stackedbool, default TrueArea plots are stacked by default. Set to False to create a unstacked plot. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarrayArea plot, or array of area plots if subplots is True.","["">>> df = pd.DataFrame({\n...     'sales': [3, 2, 3, 9, 10, 6],\n...     'signups': [5, 5, 6, 12, 14, 13],\n...     'visits': [20, 42, 28, 62, 81, 50],\n... }, index=pd.date_range(start='2018/01/01', end='2018/07/01',\n...                        freq='ME'))\n>>> ax = df.plot.area()"", '>>> ax = df.plot.area(stacked=False)', "">>> ax = df.plot.area(y='sales')"", "">>> df = pd.DataFrame({\n...     'sales': [3, 2, 3],\n...     'visits': [20, 42, 28],\n...     'day': [1, 2, 3],\n... })\n>>> ax = df.plot.area(x='day')""]"
1714,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.filter.html,pandas.core.groupby.SeriesGroupBy.filter,"SeriesGroupBy.filter(func, dropna=True, *args, **kwargs)[source]# Filter elements from groups that don’t satisfy a criterion. Elements from groups are filtered if they do not satisfy the boolean criterion specified by func. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funcfunctionCriterion to apply to each group. Should return True or False. dropnaboolDrop groups that do not pass the filter. True by default; if False, groups that evaluate False are filled with NaNs. Returns: Series","["">>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : [1, 2, 3, 4, 5, 6],\n...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')\n>>> df.groupby('A').B.filter(lambda x: x.mean() > 3.)\n1    2\n3    4\n5    6\nName: B, dtype: int64""]"
1715,..\pandas\reference\api\pandas.read_html.html,pandas.read_html,"pandas.read_html(io, *, match='.+', flavor=None, header=None, index_col=None, skiprows=None, attrs=None, parse_dates=False, thousands=', ', encoding=None, decimal='.', converters=None, na_values=None, keep_default_na=True, displayed_only=True, extract_links=None, dtype_backend=<no_default>, storage_options=None)[source]# Read HTML tables into a list of DataFrame objects. Notes Before using this function you should read the gotchas about the HTML parsing libraries. Expect to do some cleanup after you call this function. For example, you might need to manually assign column names if the column names are converted to NaN when you pass the header=0 argument. We try to assume as little as possible about the structure of the table and push the idiosyncrasies of the HTML contained in the table to the user. This function searches for <table> elements and only for <tr> and <th> rows and <td> elements within each <tr> or <th> element in the table. <td> stands for “table data”. This function attempts to properly handle colspan and rowspan attributes. If the function has a <thead> argument, it is used to construct the header, otherwise the function attempts to find the header within the body (by putting rows with only <th> elements into the header). Similar to read_csv() the header argument is applied after skiprows is applied. This function will always return a list of DataFrame or it will fail, e.g., it will not return an empty list.","Parameters: iostr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a string read() function. The string can represent a URL or the HTML itself. Note that lxml only accepts the http, ftp and file url protocols. If you have a URL that starts with 'https' you might try removing the 's'. Deprecated since version 2.1.0: Passing html literal strings is deprecated. Wrap literal string/bytes input in io.StringIO/io.BytesIO instead. matchstr or compiled regular expression, optionalThe set of tables containing text matching this regex or string will be returned. Unless the HTML is extremely simple you will probably need to pass a non-empty string here. Defaults to ‘.+’ (match any non-empty string). The default value will return all tables contained on a page. This value is converted to a regular expression so that there is consistent behavior between Beautiful Soup and lxml. flavor{“lxml”, “html5lib”, “bs4”} or list-like, optionalThe parsing engine (or list of parsing engines) to use. ‘bs4’ and ‘html5lib’ are synonymous with each other, they are both there for backwards compatibility. The default of None tries to use lxml to parse and if that fails it falls back on bs4 + html5lib. headerint or list-like, optionalThe row (or list of rows for a MultiIndex) to use to make the columns headers. index_colint or list-like, optionalThe column (or list of columns) to use to create the index. skiprowsint, list-like or slice, optionalNumber of rows to skip after parsing the column integer. 0-based. If a sequence of integers or a slice is given, will skip the rows indexed by that sequence.  Note that a single element sequence means ‘skip the nth row’ whereas an integer means ‘skip n rows’. attrsdict, optionalThis is a dictionary of attributes that you can pass to use to identify the table in the HTML. These are not checked for validity before being passed to lxml or Beautiful Soup. However, these attributes must be valid HTML table attributes to work correctly. For example, attrs = {'id': 'table'} is a valid attribute dictionary because the ‘id’ HTML tag attribute is a valid HTML attribute for any HTML tag as per this document. attrs = {'asdf': 'table'} is not a valid attribute dictionary because ‘asdf’ is not a valid HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01 table attributes can be found here. A working draft of the HTML 5 spec can be found here. It contains the latest information on table attributes for the modern web. parse_datesbool, optionalSee read_csv() for more details. thousandsstr, optionalSeparator to use to parse thousands. Defaults to ','. encodingstr, optionalThe encoding used to decode the web page. Defaults to None.``None`` preserves the previous encoding behavior, which depends on the underlying parser library (e.g., the parser library will try to use the encoding provided by the document). decimalstr, default ‘.’Character to recognize as decimal point (e.g. use ‘,’ for European data). convertersdict, default NoneDict of functions for converting values in certain columns. Keys can either be integers or column labels, values are functions that take one input argument, the cell (not column) content, and return the transformed content. na_valuesiterable, default NoneCustom NA values. keep_default_nabool, default TrueIf na_values are specified and keep_default_na is False the default NaN values are overridden, otherwise they’re appended to. displayed_onlybool, default TrueWhether elements with “display: none” should be parsed. extract_links{None, “all”, “header”, “body”, “footer”}Table elements in the specified section(s) with <a> tags will have their href extracted. Added in version 1.5.0. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Added in version 2.1.0. Returns: dfsA list of DataFrames.",[]
1716,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.nanos.html,pandas.tseries.offsets.BusinessDay.nanos,BusinessDay.nanos#,No parameters found,[]
1717,..\pandas\reference\api\pandas.Index.is_monotonic_increasing.html,pandas.Index.is_monotonic_increasing,property Index.is_monotonic_increasing[source]# Return a boolean if the values are equal or increasing.,Returns: bool,"['>>> pd.Index([1, 2, 3]).is_monotonic_increasing\nTrue\n>>> pd.Index([1, 2, 2]).is_monotonic_increasing\nTrue\n>>> pd.Index([1, 3, 2]).is_monotonic_increasing\nFalse']"
1718,..\pandas\reference\api\pandas.tseries.offsets.Tick.n.html,pandas.tseries.offsets.Tick.n,Tick.n#,No parameters found,[]
1719,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.copy.html,pandas.tseries.offsets.FY5253Quarter.copy,FY5253Quarter.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1720,..\pandas\reference\api\pandas.Series.str.endswith.html,pandas.Series.str.endswith,"Series.str.endswith(pat, na=None)[source]# Test if the end of each string element matches a pattern. Equivalent to str.endswith().","Parameters: patstr or tuple[str, …]Character sequence or tuple of strings. Regular expressions are not accepted. naobject, default NaNObject shown if element tested is not a string. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used. Returns: Series or Index of boolA Series of booleans indicating whether the given pattern matches the end of each string element.","["">>> s = pd.Series(['bat', 'bear', 'caT', np.nan])\n>>> s\n0     bat\n1    bear\n2     caT\n3     NaN\ndtype: object"", "">>> s.str.endswith('t')\n0     True\n1    False\n2    False\n3      NaN\ndtype: object"", "">>> s.str.endswith(('t', 'T'))\n0     True\n1    False\n2     True\n3      NaN\ndtype: object"", "">>> s.str.endswith('t', na=False)\n0     True\n1    False\n2    False\n3    False\ndtype: bool""]"
1721,..\pandas\reference\api\pandas.DataFrame.plot.bar.html,pandas.DataFrame.plot.bar,"DataFrame.plot.bar(x=None, y=None, **kwargs)[source]# Vertical bar plot. A bar plot is a plot that presents categorical data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.","Parameters: xlabel or position, optionalAllows plotting of one column versus another. If not specified, the index of the DataFrame is used. ylabel or position, optionalAllows plotting of one column versus another. If not specified, all numerical columns are used. colorstr, array-like, or dict, optionalThe color for each of the DataFrame’s columns. Possible values are: A single color string referred to by name, RGB or RGBA code,for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used. A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.","["">>> df = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n>>> ax = df.plot.bar(x='lab', y='val', rot=0)"", "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.bar(rot=0)"", '>>> ax = df.plot.bar(stacked=True)', '>>> axes = df.plot.bar(rot=0, subplots=True)\n>>> axes[1].legend(loc=2)', '>>> axes = df.plot.bar(\n...     rot=0, subplots=True, color={""speed"": ""red"", ""lifespan"": ""green""}\n... )\n>>> axes[1].legend(loc=2)', "">>> ax = df.plot.bar(y='speed', rot=0)"", "">>> ax = df.plot.bar(x='lifespan', rot=0)""]"
1722,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.first.html,pandas.core.groupby.SeriesGroupBy.first,"SeriesGroupBy.first(numeric_only=False, min_count=-1, skipna=True)[source]# Compute the first entry of each column within each group. Defaults to skipping NA elements.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count valid values are present the result will be NA. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Added in version 2.2.1. Returns: Series or DataFrameFirst values within each group.","['>>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[None, 5, 6], C=[1, 2, 3],\n...                        D=[\'3/11/2000\', \'3/12/2000\', \'3/13/2000\']))\n>>> df[\'D\'] = pd.to_datetime(df[\'D\'])\n>>> df.groupby(""A"").first()\n     B  C          D\nA\n1  5.0  1 2000-03-11\n3  6.0  3 2000-03-13\n>>> df.groupby(""A"").first(min_count=2)\n    B    C          D\nA\n1 NaN  1.0 2000-03-11\n3 NaN  NaN        NaT\n>>> df.groupby(""A"").first(numeric_only=True)\n     B  C\nA\n1  5.0  1\n3  6.0  3']"
1723,..\pandas\reference\api\pandas.read_json.html,pandas.read_json,"pandas.read_json(path_or_buf, *, orient=None, typ='frame', dtype=None, convert_axes=None, convert_dates=True, keep_default_dates=True, precise_float=False, date_unit=None, encoding=None, encoding_errors='strict', lines=False, chunksize=None, compression='infer', nrows=None, storage_options=None, dtype_backend=<no_default>, engine='ujson')[source]# Convert a JSON string to pandas object. Notes Specific to orient='table', if a DataFrame with a literal Index name of index gets written with to_json(), the subsequent read operation will incorrectly set the Index name to None. This is because index is also used by DataFrame.to_json() to denote a missing Index name, and the subsequent read_json() operation cannot distinguish between the two. The same limitation is encountered with a MultiIndex and any names beginning with 'level_'.","Parameters: path_or_bufa valid JSON str, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.json. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO. Deprecated since version 2.1.0: Passing json literal strings is deprecated. orientstr, optionalIndication of expected JSON string format. Compatible JSON strings can be produced by to_json() with a corresponding orient value. The set of possible orients is: 'split' : dict like {index -> [index], columns -> [columns], data -> [values]} 'records' : list like [{column -> value}, ... , {column -> value}] 'index' : dict like {index -> {column -> value}} 'columns' : dict like {column -> {index -> value}} 'values' : just the values array 'table' : dict like {'schema': {schema}, 'data': {data}} The allowed and default values depend on the value of the typ parameter. when typ == 'series', allowed orients are {'split','records','index'} default is 'index' The Series index must be unique for orient 'index'. when typ == 'frame', allowed orients are {'split','records','index', 'columns','values', 'table'} default is 'columns' The DataFrame index must be unique for orients 'index' and 'columns'. The DataFrame columns must be unique for orients 'index', 'columns', and 'records'. typ{‘frame’, ‘series’}, default ‘frame’The type of object to recover. dtypebool or dict, default NoneIf True, infer dtypes; if a dict of column to dtype, then use those; if False, then don’t infer dtypes at all, applies only to the data. For all orient values except 'table', default is True. convert_axesbool, default NoneTry to convert the axes to the proper dtypes. For all orient values except 'table', default is True. convert_datesbool or list of str, default TrueIf True then default datelike columns may be converted (depending on keep_default_dates). If False, no dates will be converted. If a list of column names, then those columns will be converted and default datelike columns may also be converted (depending on keep_default_dates). keep_default_datesbool, default TrueIf parsing dates (convert_dates is not False), then try to parse the default datelike columns. A column label is datelike if it ends with '_at', it ends with '_time', it begins with 'timestamp', it is 'modified', or it is 'date'. precise_floatbool, default FalseSet to enable usage of higher precision (strtod) function when decoding string to double values. Default (False) is to use fast but less precise builtin functionality. date_unitstr, default NoneThe timestamp unit to detect if converting dates. The default behaviour is to try and detect the correct precision, but if this is not desired then pass one of ‘s’, ‘ms’, ‘us’ or ‘ns’ to force parsing only seconds, milliseconds, microseconds or nanoseconds respectively. encodingstr, default is ‘utf-8’The encoding to use to decode py3 bytes. encoding_errorsstr, optional, default “strict”How encoding errors are treated. List of possible values . Added in version 1.3.0. linesbool, default FalseRead the file as a json object per line. chunksizeint, optionalReturn JsonReader object for iteration. See the line-delimited json docs for more information on chunksize. This can only be passed if lines=True. If this is None, the file will be read into memory all at once. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buf’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. nrowsint, optionalThe number of lines from the line-delimited jsonfile that has to be read. This can only be passed if lines=True. If this is None, all the rows will be returned. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. engine{“ujson”, “pyarrow”}, default “ujson”Parser engine to use. The ""pyarrow"" engine is only available when lines=True. Added in version 2.0. Returns: Series, DataFrame, or pandas.api.typing.JsonReaderA JsonReader is returned when chunksize is not 0 or None. Otherwise, the type returned depends on the value of typ.","["">>> from io import StringIO\n>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n...                   index=['row 1', 'row 2'],\n...                   columns=['col 1', 'col 2'])"", '>>> df.to_json(orient=\'split\')\n    \'{""columns"":[""col 1"",""col 2""],""index"":[""row 1"",""row 2""],""data"":[[""a"",""b""],[""c"",""d""]]}\'\n>>> pd.read_json(StringIO(_), orient=\'split\')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d', '>>> df.to_json(orient=\'index\')\n\'{""row 1"":{""col 1"":""a"",""col 2"":""b""},""row 2"":{""col 1"":""c"",""col 2"":""d""}}\'', "">>> pd.read_json(StringIO(_), orient='index')\n      col 1 col 2\nrow 1     a     b\nrow 2     c     d"", '>>> df.to_json(orient=\'records\')\n\'[{""col 1"":""a"",""col 2"":""b""},{""col 1"":""c"",""col 2"":""d""}]\'\n>>> pd.read_json(StringIO(_), orient=\'records\')\n  col 1 col 2\n0     a     b\n1     c     d', '>>> df.to_json(orient=\'table\')\n    \'{""schema"":{""fields"":[{""name"":""index"",""type"":""string""},{""name"":""col 1"",""type"":""string""},{""name"":""col 2"",""type"":""string""}],""primaryKey"":[""index""],""pandas_version"":""1.4.0""},""data"":[{""index"":""row 1"",""col 1"":""a"",""col 2"":""b""},{""index"":""row 2"",""col 1"":""c"",""col 2"":""d""}]}\'', '>>> data = \'\'\'{""index"": {""0"": 0, ""1"": 1},\n...        ""a"": {""0"": 1, ""1"": null},\n...        ""b"": {""0"": 2.5, ""1"": 4.5},\n...        ""c"": {""0"": true, ""1"": false},\n...        ""d"": {""0"": ""a"", ""1"": ""b""},\n...        ""e"": {""0"": 1577.2, ""1"": 1577.1}}\'\'\'\n>>> pd.read_json(StringIO(data), dtype_backend=""numpy_nullable"")\n   index     a    b      c  d       e\n0      0     1  2.5   True  a  1577.2\n1      1  <NA>  4.5  False  b  1577.1']"
1724,..\pandas\reference\api\pandas.Index.is_numeric.html,pandas.Index.is_numeric,final Index.is_numeric()[source]# Check if the Index only consists of numeric data. Deprecated since version 2.0.0: Use pandas.api.types.is_numeric_dtype instead.,Returns: boolWhether or not the Index only consists of numeric data.,"['>>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n>>> idx.is_numeric()  \nTrue', '>>> idx = pd.Index([1, 2, 3, 4.0])\n>>> idx.is_numeric()  \nTrue', '>>> idx = pd.Index([1, 2, 3, 4])\n>>> idx.is_numeric()  \nTrue', '>>> idx = pd.Index([1, 2, 3, 4.0, np.nan])\n>>> idx.is_numeric()  \nTrue', '>>> idx = pd.Index([1, 2, 3, 4.0, np.nan, ""Apple""])\n>>> idx.is_numeric()  \nFalse']"
1725,..\pandas\reference\api\pandas.Series.str.extract.html,pandas.Series.str.extract,"Series.str.extract(pat, flags=0, expand=True)[source]# Extract capture groups in the regex pat as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression pat.","Parameters: patstrRegular expression pattern with capturing groups. flagsint, default 0 (no flags)Flags from the re module, e.g. re.IGNORECASE, that modify regular expression matching for things like case, spaces, etc. For more details, see re. expandbool, default TrueIf True, return DataFrame with one column per capture group. If False, return a Series/Index if there is one capture group or DataFrame if there are multiple capture groups. Returns: DataFrame or Series or IndexA DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If expand=False and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index).","["">>> s = pd.Series(['a1', 'b2', 'c3'])\n>>> s.str.extract(r'([ab])(\\d)')\n    0    1\n0    a    1\n1    b    2\n2  NaN  NaN"", "">>> s.str.extract(r'([ab])?(\\d)')\n    0  1\n0    a  1\n1    b  2\n2  NaN  3"", "">>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\nletter digit\n0      a     1\n1      b     2\n2    NaN   NaN"", "">>> s.str.extract(r'[ab](\\d)', expand=True)\n    0\n0    1\n1    2\n2  NaN"", "">>> s.str.extract(r'[ab](\\d)', expand=False)\n0      1\n1      2\n2    NaN\ndtype: object""]"
1726,..\pandas\reference\api\pandas.tseries.offsets.Tick.name.html,pandas.tseries.offsets.Tick.name,Tick.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1727,..\pandas\reference\api\pandas.read_orc.html,pandas.read_orc,"pandas.read_orc(path, columns=None, dtype_backend=<no_default>, filesystem=None, **kwargs)[source]# Load an ORC object from the file path, returning a DataFrame. Notes Before using this function you should read the user guide about ORC and install optional dependencies. If path is a URI scheme pointing to a local or remote file (e.g. “s3://”), a pyarrow.fs filesystem will be attempted to read the file. You can also pass a pyarrow or fsspec filesystem object into the filesystem keyword to override this behavior.","Parameters: pathstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.orc. columnslist, default NoneIf not None, only these columns will be read from the file. Output always follows the ordering of the file and not the columns list. This mirrors the original behaviour of pyarrow.orc.ORCFile.read(). dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file. Added in version 2.1.0. **kwargsAny additional kwargs are passed to pyarrow. Returns: DataFrame","['>>> result = pd.read_orc(""example_pa.orc"")']"
1728,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.freqstr.html,pandas.tseries.offsets.FY5253Quarter.freqstr,FY5253Quarter.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1729,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.normalize.html,pandas.tseries.offsets.BusinessDay.normalize,BusinessDay.normalize#,No parameters found,[]
1730,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.get_group.html,pandas.core.groupby.SeriesGroupBy.get_group,"SeriesGroupBy.get_group(name, obj=None)[source]# Construct DataFrame from group with provided name.","Parameters: nameobjectThe name of the group to get as a DataFrame. objDataFrame, default NoneThe DataFrame to take the DataFrame out of.  If it is None, the object groupby was called on will be used. Deprecated since version 2.1.0: The obj is deprecated and will be removed in a future version. Do df.iloc[gb.indices.get(name)] instead of gb.get_group(name, obj=df). Returns: same type as obj","['>>> lst = [\'a\', \'a\', \'b\']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).get_group(""a"")\na    1\na    2\ndtype: int64', '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(by=[""a""]).get_group((1,))\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').get_group('2023-01-01')\n2023-01-01    1\n2023-01-15    2\ndtype: int64""]"
1731,..\pandas\reference\api\pandas.DataFrame.plot.barh.html,pandas.DataFrame.plot.barh,"DataFrame.plot.barh(x=None, y=None, **kwargs)[source]# Make a horizontal bar plot. A horizontal bar plot is a plot that presents quantitative data with rectangular bars with lengths proportional to the values that they represent. A bar plot shows comparisons among discrete categories. One axis of the plot shows the specific categories being compared, and the other axis represents a measured value.","Parameters: xlabel or position, optionalAllows plotting of one column versus another. If not specified, the index of the DataFrame is used. ylabel or position, optionalAllows plotting of one column versus another. If not specified, all numerical columns are used. colorstr, array-like, or dict, optionalThe color for each of the DataFrame’s columns. Possible values are: A single color string referred to by name, RGB or RGBA code,for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s bar will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used. A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color bars for column a in green and bars for column b in red. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.","["">>> df = pd.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n>>> ax = df.plot.barh(x='lab', y='val')"", "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh()"", '>>> ax = df.plot.barh(stacked=True)', '>>> ax = df.plot.barh(color={""speed"": ""red"", ""lifespan"": ""green""})', "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(y='speed')"", "">>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n>>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n>>> index = ['snail', 'pig', 'elephant',\n...          'rabbit', 'giraffe', 'coyote', 'horse']\n>>> df = pd.DataFrame({'speed': speed,\n...                    'lifespan': lifespan}, index=index)\n>>> ax = df.plot.barh(x='lifespan')""]"
1732,..\pandas\reference\api\pandas.Index.is_object.html,pandas.Index.is_object,final Index.is_object()[source]# Check if the Index is of the object dtype. Deprecated since version 2.0.0: Use pandas.api.types.is_object_dtype instead.,Returns: boolWhether or not the Index is of the object dtype.,"['>>> idx = pd.Index([""Apple"", ""Mango"", ""Watermelon""])\n>>> idx.is_object()  \nTrue', '>>> idx = pd.Index([""Apple"", ""Mango"", 2.0])\n>>> idx.is_object()  \nTrue', '>>> idx = pd.Index([""Watermelon"", ""Orange"", ""Apple"",\n...                 ""Watermelon""]).astype(""category"")\n>>> idx.is_object()  \nFalse', '>>> idx = pd.Index([1.0, 2.0, 3.0, 4.0])\n>>> idx.is_object()  \nFalse']"
1733,..\pandas\reference\api\pandas.Series.str.extractall.html,pandas.Series.str.extractall,"Series.str.extractall(pat, flags=0)[source]# Extract capture groups in the regex pat as columns in DataFrame. For each subject string in the Series, extract groups from all matches of regular expression pat. When each subject string in the Series has exactly one match, extractall(pat).xs(0, level=’match’) is the same as extract(pat).","Parameters: patstrRegular expression pattern with capturing groups. flagsint, default 0 (no flags)A re module flag, for example re.IGNORECASE. These allow to modify regular expression matching for things like case, spaces, etc. Multiple flags can be combined with the bitwise OR operator, for example re.IGNORECASE | re.MULTILINE. Returns: DataFrameA DataFrame with one row for each match, and one column for each group. Its rows have a MultiIndex with first levels that come from the subject Series. The last level is named ‘match’ and indexes the matches in each item of the Series. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used.","['>>> s = pd.Series([""a1a2"", ""b1"", ""c1""], index=[""A"", ""B"", ""C""])\n>>> s.str.extractall(r""[ab](\\d)"")\n        0\nmatch\nA 0      1\n  1      2\nB 0      1', '>>> s.str.extractall(r""[ab](?P<digit>\\d)"")\n        digit\nmatch\nA 0         1\n  1         2\nB 0         1', '>>> s.str.extractall(r""(?P<letter>[ab])(?P<digit>\\d)"")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1', '>>> s.str.extractall(r""(?P<letter>[ab])?(?P<digit>\\d)"")\n        letter digit\nmatch\nA 0          a     1\n  1          a     2\nB 0          b     1\nC 0        NaN     1']"
1734,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.groups.html,pandas.core.groupby.SeriesGroupBy.groups,property SeriesGroupBy.groups[source]# Dict {group name -> group labels}.,No parameters found,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).groups\n{'a': ['a', 'a'], 'b': ['b']}"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""])\n>>> df\n   a  b  c\n0  1  2  3\n1  1  5  6\n2  7  8  9\n>>> df.groupby(by=[""a""]).groups\n{1: [0, 1], 7: [2]}', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').groups\n{Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}""]"
1735,..\pandas\reference\api\pandas.Index.is_unique.html,pandas.Index.is_unique,Index.is_unique[source]# Return if the index has unique values.,Returns: bool,"['>>> idx = pd.Index([1, 5, 7, 7])\n>>> idx.is_unique\nFalse', '>>> idx = pd.Index([1, 5, 7])\n>>> idx.is_unique\nTrue', '>>> idx = pd.Index([""Watermelon"", ""Orange"", ""Apple"",\n...                 ""Watermelon""]).astype(""category"")\n>>> idx.is_unique\nFalse', '>>> idx = pd.Index([""Orange"", ""Apple"",\n...                 ""Watermelon""]).astype(""category"")\n>>> idx.is_unique\nTrue']"
1736,..\pandas\reference\api\pandas.tseries.offsets.Tick.nanos.html,pandas.tseries.offsets.Tick.nanos,Tick.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
1737,..\pandas\reference\api\pandas.DataFrame.plot.box.html,pandas.DataFrame.plot.box,"DataFrame.plot.box(by=None, **kwargs)[source]# Make a box plot of the DataFrame columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers. For further details see Wikipedia’s entry for boxplot. A consideration when using this chart is that the box and the whiskers can overlap, which is very common when plotting small sets of data.","Parameters: bystr or sequenceColumn in the DataFrame to group by. Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings **kwargsAdditional keywords are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","["">>> data = np.random.randn(25, 4)\n>>> df = pd.DataFrame(data, columns=list('ABCD'))\n>>> ax = df.plot.box()"", '>>> age_list = [8, 10, 12, 14, 72, 74, 76, 78, 20, 25, 30, 35, 60, 85]\n>>> df = pd.DataFrame({""gender"": list(""MMMMMMMMFFFFFF""), ""age"": age_list})\n>>> ax = df.plot.box(column=""age"", by=""gender"", figsize=(10, 8))']"
1738,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.rule_code.html,pandas.tseries.offsets.BusinessDay.rule_code,BusinessDay.rule_code#,No parameters found,[]
1739,..\pandas\reference\api\pandas.read_parquet.html,pandas.read_parquet,"pandas.read_parquet(path, engine='auto', columns=None, storage_options=None, use_nullable_dtypes=<no_default>, dtype_backend=<no_default>, filesystem=None, filters=None, **kwargs)[source]# Load a parquet object from the file path, returning a DataFrame.","Parameters: pathstr, path object or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.parquet. A file URL can also be a path to a directory that contains multiple partitioned parquet files. Both pyarrow and fastparquet support paths to directories as well as file URLs. A directory path could be: file://localhost/path/to/tables or s3://bucket/partition_dir. engine{‘auto’, ‘pyarrow’, ‘fastparquet’}, default ‘auto’Parquet library to use. If ‘auto’, then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if ‘pyarrow’ is unavailable. When using the 'pyarrow' engine and no storage options are provided and a filesystem is implemented by both pyarrow.fs and fsspec (e.g. “s3://”), then the pyarrow.fs filesystem is attempted first. Use the filesystem keyword with an instantiated fsspec filesystem if you wish to use its implementation. columnslist, default=NoneIf not None, only these columns will be read from the file. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Added in version 1.3.0. use_nullable_dtypesbool, default FalseIf True, use dtypes that use pd.NA as missing value indicator for the resulting DataFrame. (only applicable for the pyarrow engine) As new dtypes are added that support pd.NA in the future, the output with this option will change to use those dtypes. Note: this is an experimental option, and behaviour (e.g. additional support dtypes) may change without notice. Deprecated since version 2.0. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. filesystemfsspec or pyarrow filesystem, default NoneFilesystem object to use when reading the parquet file. Only implemented for engine=""pyarrow"". Added in version 2.1.0. filtersList[Tuple] or List[List[Tuple]], default NoneTo filter out data. Filter syntax: [[(column, op, val), …],…] where op is [==, =, >, >=, <, <=, !=, in, not in] The innermost tuples are transposed into a set of filters applied through an AND operation. The outer list combines these sets of filters through an OR operation. A single list of tuples can also be used, meaning that no OR operation between set of filters is to be conducted. Using this argument will NOT result in row-wise filtering of the final partitions unless engine=""pyarrow"" is also specified.  For other engines, filtering is only performed at the partition level, that is, to prevent the loading of some row-groups and/or files. Added in version 2.1.0. **kwargsAny additional kwargs are passed to the engine. Returns: DataFrame","['>>> original_df = pd.DataFrame(\n...     {""foo"": range(5), ""bar"": range(5, 10)}\n...    )\n>>> original_df\n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n>>> df_parquet_bytes = original_df.to_parquet()\n>>> from io import BytesIO\n>>> restored_df = pd.read_parquet(BytesIO(df_parquet_bytes))\n>>> restored_df\n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n>>> restored_df.equals(original_df)\nTrue\n>>> restored_bar = pd.read_parquet(BytesIO(df_parquet_bytes), columns=[""bar""])\n>>> restored_bar\n    bar\n0    5\n1    6\n2    7\n3    8\n4    9\n>>> restored_bar.equals(original_df[[\'bar\']])\nTrue', '>>> sel = [(""foo"", "">"", 2)]\n>>> restored_part = pd.read_parquet(BytesIO(df_parquet_bytes), filters=sel)\n>>> restored_part\n    foo  bar\n0    3    8\n1    4    9']"
1740,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.get_rule_code_suffix.html,pandas.tseries.offsets.FY5253Quarter.get_rule_code_suffix,FY5253Quarter.get_rule_code_suffix()#,No parameters found,[]
1741,..\pandas\reference\api\pandas.read_pickle.html,pandas.read_pickle,"pandas.read_pickle(filepath_or_buffer, compression='infer', storage_options=None)[source]# Load pickled pandas object (or any object) from file. Warning Loading pickled data received from untrusted sources can be unsafe. See here. Notes read_pickle is only guaranteed to be backwards compatible to pandas 0.20.3 provided the object was serialized with to_pickle.","Parameters: filepath_or_bufferstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary readlines() function. Also accepts URL. URL is not limited to S3 and GCS. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Returns: same type as object stored in file","['>>> original_df = pd.DataFrame(\n...     {""foo"": range(5), ""bar"": range(5, 10)}\n...    )  \n>>> original_df  \n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9\n>>> pd.to_pickle(original_df, ""./dummy.pkl"")', '>>> unpickled_df = pd.read_pickle(""./dummy.pkl"")  \n>>> unpickled_df  \n   foo  bar\n0    0    5\n1    1    6\n2    2    7\n3    3    8\n4    4    9']"
1742,..\pandas\reference\api\pandas.Series.str.find.html,pandas.Series.str.find,"Series.str.find(sub, start=0, end=None)[source]# Return lowest indexes in each strings in the Series/Index. Each of returned indexes corresponds to the position where the substring is fully contained between [start:end]. Return -1 on failure. Equivalent to standard str.find().",Parameters: substrSubstring being searched. startintLeft edge index. endintRight edge index. Returns: Series or Index of int.,"['>>> ser = pd.Series([""cow_"", ""duck_"", ""do_ve""])\n>>> ser.str.find(""_"")\n0   3\n1   4\n2   2\ndtype: int64', '>>> ser = pd.Series([""_cow_"", ""duck_"", ""do_v_e""])\n>>> ser.str.rfind(""_"")\n0   4\n1   4\n2   4\ndtype: int64']"
1743,..\pandas\reference\api\pandas.DataFrame.plot.density.html,pandas.DataFrame.plot.density,"DataFrame.plot.density(bw_method=None, ind=None, **kwargs)[source]# Generate Kernel Density Estimate plot using Gaussian kernels. In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.","Parameters: bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be ‘scott’, ‘silverman’, a scalar constant or a callable. If None (default), ‘scott’ is used. See scipy.stats.gaussian_kde for more information. indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","['>>> s = pd.Series([1, 2, 2.5, 3, 3.5, 4, 5])\n>>> ax = s.plot.kde()', '>>> ax = s.plot.kde(bw_method=0.3)', '>>> ax = s.plot.kde(bw_method=3)', '>>> ax = s.plot.kde(ind=[1, 2, 3, 4, 5])', "">>> df = pd.DataFrame({\n...     'x': [1, 2, 2.5, 3, 3.5, 4, 5],\n...     'y': [4, 4, 4.5, 5, 5.5, 6, 6],\n... })\n>>> ax = df.plot.kde()"", '>>> ax = df.plot.kde(bw_method=0.3)', '>>> ax = df.plot.kde(bw_method=3)', '>>> ax = df.plot.kde(ind=[1, 2, 3, 4, 5, 6])']"
1744,..\pandas\reference\api\pandas.tseries.offsets.Tick.normalize.html,pandas.tseries.offsets.Tick.normalize,Tick.normalize#,No parameters found,[]
1745,..\pandas\reference\api\pandas.tseries.offsets.BusinessDay.weekmask.html,pandas.tseries.offsets.BusinessDay.weekmask,BusinessDay.weekmask#,No parameters found,[]
1746,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.head.html,pandas.core.groupby.SeriesGroupBy.head,"SeriesGroupBy.head(n=5)[source]# Return first n rows of each group. Similar to .apply(lambda x: x.head(n)), but it returns a subset of rows from the original DataFrame with original index and order preserved (as_index flag is ignored).",Parameters: nintIf positive: number of entries to include from start of each group. If negative: number of entries to exclude from end of each group. Returns: Series or DataFrameSubset of original Series or DataFrame as determined by n.,"["">>> df = pd.DataFrame([[1, 2], [1, 4], [5, 6]],\n...                   columns=['A', 'B'])\n>>> df.groupby('A').head(1)\n   A  B\n0  1  2\n2  5  6\n>>> df.groupby('A').head(-1)\n   A  B\n0  1  2""]"
1747,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.get_weeks.html,pandas.tseries.offsets.FY5253Quarter.get_weeks,FY5253Quarter.get_weeks(dt)#,No parameters found,[]
1748,..\pandas\reference\api\pandas.read_sas.html,pandas.read_sas,"pandas.read_sas(filepath_or_buffer, *, format=None, index=None, encoding=None, chunksize=None, iterator=False, compression='infer')[source]# Read SAS files stored as either XPORT or SAS7BDAT format files.","Parameters: filepath_or_bufferstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a binary read() function. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.sas7bdat. formatstr {‘xport’, ‘sas7bdat’} or NoneIf None, file format is inferred from file extension. If ‘xport’ or ‘sas7bdat’, uses the corresponding format. indexidentifier of index column, defaults to NoneIdentifier of column that should be used as index of the DataFrame. encodingstr, default is NoneEncoding for text data.  If None, text data are stored as raw bytes. chunksizeintRead file chunksize lines at a time, returns iterator. iteratorbool, defaults to FalseIf True, returns an iterator for reading the file incrementally. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. Returns: DataFrame if iterator=False and chunksize=None, else SAS7BDATReader or XportReader","['>>> df = pd.read_sas(""sas_data.sas7bdat"")']"
1749,..\pandas\reference\api\pandas.Index.item.html,pandas.Index.item,Index.item()[source]# Return the first element of the underlying data as a Python scalar.,Returns: scalarThe first element of Series or Index. Raises: ValueErrorIf the data is not length = 1.,"['>>> s = pd.Series([1])\n>>> s.item()\n1', "">>> s = pd.Series([1], index=['a'])\n>>> s.index.item()\n'a'""]"
1750,..\pandas\reference\api\pandas.Series.str.findall.html,pandas.Series.str.findall,"Series.str.findall(pat, flags=0)[source]# Find all occurrences of pattern or regular expression in the Series/Index. Equivalent to applying re.findall() to all the elements in the Series/Index.","Parameters: patstrPattern or regular expression. flagsint, default 0Flags from re module, e.g. re.IGNORECASE (default is 0, which means no flags). Returns: Series/Index of lists of stringsAll non-overlapping matches of pattern or regular expression in each string of this Series/Index.","["">>> s = pd.Series(['Lion', 'Monkey', 'Rabbit'])"", "">>> s.str.findall('Monkey')\n0          []\n1    [Monkey]\n2          []\ndtype: object"", "">>> s.str.findall('MONKEY')\n0    []\n1    []\n2    []\ndtype: object"", "">>> import re\n>>> s.str.findall('MONKEY', flags=re.IGNORECASE)\n0          []\n1    [Monkey]\n2          []\ndtype: object"", "">>> s.str.findall('on')\n0    [on]\n1    [on]\n2      []\ndtype: object"", "">>> s.str.findall('on$')\n0    [on]\n1      []\n2      []\ndtype: object"", "">>> s.str.findall('b')\n0        []\n1        []\n2    [b, b]\ndtype: object""]"
1751,..\pandas\reference\api\pandas.Index.join.html,pandas.Index.join,"final Index.join(other, *, how='left', level=None, return_indexers=False, sort=False)[source]# Compute join_index and indexers to conform data structures to the new index.","Parameters: otherIndex how{‘left’, ‘right’, ‘inner’, ‘outer’} levelint or level name, default None return_indexersbool, default False sortbool, default FalseSort the join keys lexicographically in the result Index. If False, the order of the join keys depends on the join type (how keyword). Returns: join_index, (left_indexer, right_indexer)","["">>> idx1 = pd.Index([1, 2, 3])\n>>> idx2 = pd.Index([4, 5, 6])\n>>> idx1.join(idx2, how='outer')\nIndex([1, 2, 3, 4, 5, 6], dtype='int64')""]"
1752,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.calendar.html,pandas.tseries.offsets.BusinessHour.calendar,BusinessHour.calendar#,No parameters found,[]
1753,..\pandas\reference\api\pandas.read_spss.html,pandas.read_spss,"pandas.read_spss(path, usecols=None, convert_categoricals=True, dtype_backend=<no_default>)[source]# Load an SPSS file from the file path, returning a DataFrame.","Parameters: pathstr or PathFile path. usecolslist-like, optionalReturn a subset of the columns. If None, return all columns. convert_categoricalsbool, default is TrueConvert categorical columns into pd.Categorical. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: DataFrame","['>>> df = pd.read_spss(""spss_data.sav"")']"
1754,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.html,pandas.tseries.offsets.FY5253Quarter,"class pandas.tseries.offsets.FY5253Quarter# DateOffset increments between business quarter dates for 52-53 week fiscal year. Also known as a 4-4-5 calendar. It is used by companies that desire that their fiscal year always end on the same day of the week. It is a method of managing accounting periods. It is a common calendar structure for some industries, such as retail, manufacturing and parking industry. For more information see: https://en.wikipedia.org/wiki/4-4-5_calendar The year may either: end on the last X day of the Y month. end on the last X day closest to the last day of the Y month. X is a specific day of the week. Y is a certain month of the year startingMonth = 1 corresponds to dates like 1/31/2007, 4/30/2007, … startingMonth = 2 corresponds to dates like 2/28/2007, 5/31/2007, … startingMonth = 3 corresponds to dates like 3/30/2007, 6/29/2007, … Examples In the example below the default parameters give the next business quarter for 52-53 week fiscal year. By the parameter startingMonth we can specify the month in which fiscal years end. Business quarters for 52-53 week fiscal year can be specified by weekday and variation parameters. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize qtr_with_extra_week rule_code startingMonth variation weekday","Parameters: nintThe number of business quarters represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekdayint {0, 1, …, 6}, default 0A specific integer for the day of the week. 0 is Monday 1 is Tuesday 2 is Wednesday 3 is Thursday 4 is Friday 5 is Saturday 6 is Sunday. startingMonthint {1, 2, …, 12}, default 1The month in which fiscal years end. qtr_with_extra_weekint {1, 2, 3, 4}, default 1The quarter number that has the leap or 14 week when needed. variationstr, default “nearest”Method of employing 4-4-5 calendar. There are two options: “nearest” means year end is weekday closest to last day of month in year. “last” means year end is final weekday of the final month in fiscal year.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.FY5253Quarter()\nTimestamp('2022-01-31 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.FY5253Quarter(startingMonth=3)\nTimestamp('2022-03-28 00:00:00')"", '>>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.FY5253Quarter(weekday=5, startingMonth=12, variation=""last"")\nTimestamp(\'2022-04-02 00:00:00\')']"
1755,..\pandas\reference\api\pandas.DataFrame.plot.hexbin.html,pandas.DataFrame.plot.hexbin,"DataFrame.plot.hexbin(x, y, C=None, reduce_C_function=None, gridsize=None, **kwargs)[source]# Generate a hexagonal binning plot. Generate a hexagonal binning plot of x versus y. If C is None (the default), this is a histogram of the number of occurrences of the observations at (x[i], y[i]). If C is specified, specifies values at given coordinates (x[i], y[i]). These values are accumulated for each hexagonal bin and then reduced according to reduce_C_function, having as default the NumPy’s mean function (numpy.mean()). (If C is specified, it must also be a 1-D sequence of the same length as x and y, or a column label.)","Parameters: xint or strThe column label or position for x points. yint or strThe column label or position for y points. Cint or str, optionalThe column label or position for the value of (x, y) point. reduce_C_functioncallable, default np.meanFunction of one argument that reduces all the values in a bin to a single number (e.g. np.mean, np.max, np.sum, np.std). gridsizeint or tuple of (int, int), default 100The number of hexagons in the x-direction. The corresponding number of hexagons in the y-direction is chosen in a way that the hexagons are approximately regular. Alternatively, gridsize can be a tuple with two elements specifying the number of hexagons in the x-direction and the y-direction. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.AxesSubplotThe matplotlib Axes on which the hexbin is plotted.","["">>> n = 10000\n>>> df = pd.DataFrame({'x': np.random.randn(n),\n...                    'y': np.random.randn(n)})\n>>> ax = df.plot.hexbin(x='x', y='y', gridsize=20)"", '>>> n = 500\n>>> df = pd.DataFrame({\n...     \'coord_x\': np.random.uniform(-3, 3, size=n),\n...     \'coord_y\': np.random.uniform(30, 50, size=n),\n...     \'observations\': np.random.randint(1,5, size=n)\n...     })\n>>> ax = df.plot.hexbin(x=\'coord_x\',\n...                     y=\'coord_y\',\n...                     C=\'observations\',\n...                     reduce_C_function=np.sum,\n...                     gridsize=10,\n...                     cmap=""viridis"")']"
1756,..\pandas\reference\api\pandas.tseries.offsets.Tick.rule_code.html,pandas.tseries.offsets.Tick.rule_code,Tick.rule_code#,No parameters found,[]
1757,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.hist.html,pandas.core.groupby.SeriesGroupBy.hist,"SeriesGroupBy.hist(by=None, ax=None, grid=True, xlabelsize=None, xrot=None, ylabelsize=None, yrot=None, figsize=None, bins=10, backend=None, legend=False, **kwargs)[source]# Draw histogram of the input series using matplotlib.","Parameters: byobject, optionalIf passed, then used to form histograms for separate groups. axmatplotlib axis objectIf not passed, uses gca(). gridbool, default TrueWhether to show axis grid lines. xlabelsizeint, default NoneIf specified changes the x-axis label size. xrotfloat, default NoneRotation of x axis labels. ylabelsizeint, default NoneIf specified changes the y-axis label size. yrotfloat, default NoneRotation of y axis labels. figsizetuple, default NoneFigure size in inches by default. binsint or sequence, default 10Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. legendbool, default FalseWhether to show the legend. **kwargsTo be passed to the actual plotting function. Returns: matplotlib.AxesSubplotA histogram plot.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> hist = ser.hist()"", "">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> hist = ser.groupby(level=0).hist()""]"
1758,..\pandas\reference\api\pandas.Series.str.fullmatch.html,pandas.Series.str.fullmatch,"Series.str.fullmatch(pat, case=True, flags=0, na=None)[source]# Determine if each string entirely matches a regular expression.","Parameters: patstrCharacter sequence or regular expression. casebool, default TrueIf True, case sensitive. flagsint, default 0 (no flags)Regex module flags, e.g. re.IGNORECASE. nascalar, optionalFill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used. Returns: Series/Index/array of boolean values","['>>> ser = pd.Series([""cat"", ""duck"", ""dove""])\n>>> ser.str.fullmatch(r\'d.+\')\n0   False\n1    True\n2    True\ndtype: bool']"
1759,..\pandas\reference\api\pandas.Index.map.html,pandas.Index.map,"Index.map(mapper, na_action=None)[source]# Map values using an input mapping or function.","Parameters: mapperfunction, dict, or SeriesMapping correspondence. na_action{None, ‘ignore’}If ‘ignore’, propagate NA values, without passing them to the mapping correspondence. Returns: Union[Index, MultiIndex]The output of the mapping function applied to the index. If the function returns a tuple with more than one element a MultiIndex will be returned.","["">>> idx = pd.Index([1, 2, 3])\n>>> idx.map({1: 'a', 2: 'b', 3: 'c'})\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx.map('I am a {}'.format)\nIndex(['I am a 1', 'I am a 2', 'I am a 3'], dtype='object')"", "">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.map(lambda x: x.upper())\nIndex(['A', 'B', 'C'], dtype='object')""]"
1760,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.copy.html,pandas.tseries.offsets.BusinessHour.copy,BusinessHour.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1761,..\pandas\reference\api\pandas.read_sql.html,pandas.read_sql,"pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None, dtype_backend=<no_default>, dtype=None)[source]# Read SQL query or database table into a DataFrame. This function is a convenience wrapper around read_sql_table and read_sql_query (for backward compatibility). It will delegate to the specific function depending on the provided input. A SQL query will be routed to read_sql_query, while a database table name will be routed to read_sql_table. Note that the delegated function might have more specific notes about their functionality not listed here.","Parameters: sqlstr or SQLAlchemy Selectable (select or text object)SQL query to be executed or a table name. conADBC Connection, SQLAlchemy connectable, str, or sqlite3 connectionADBC provides high performance I/O with native type support, where available. Using SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible for engine disposal and connection closure for the ADBC connection and SQLAlchemy connectable; str connections are closed automatically. See here. index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex). coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. paramslist, tuple or dict, optional, default: NoneList of parameters to pass to execute method.  The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249’s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}. parse_dateslist or dict, default: None List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite. columnslist, default: NoneList of column names to select from SQL table (only used when reading a table). chunksizeint, default NoneIf specified, return an iterator where chunksize is the number of rows to include in each chunk. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}. The argument is ignored if a table is passed instead of a query. Added in version 2.0.0. Returns: DataFrame or Iterator[DataFrame]","["">>> from sqlite3 import connect\n>>> conn = connect(':memory:')\n>>> df = pd.DataFrame(data=[[0, '10/11/12'], [1, '12/11/10']],\n...                   columns=['int_column', 'date_column'])\n>>> df.to_sql(name='test_data', con=conn)\n2"", "">>> pd.read_sql('SELECT int_column, date_column FROM test_data', conn)\n   int_column date_column\n0           0    10/11/12\n1           1    12/11/10"", "">>> pd.read_sql('test_data', 'postgres:///db_name')"", '>>> pd.read_sql(\'SELECT int_column, date_column FROM test_data\',\n...             conn,\n...             parse_dates={""date_column"": {""format"": ""%d/%m/%y""}})\n   int_column date_column\n0           0  2012-11-10\n1           1  2010-11-12', "">>> from adbc_driver_postgresql import dbapi  \n>>> with dbapi.connect('postgres:///db_name') as conn:  \n...     pd.read_sql('SELECT int_column FROM test_data', conn)\n   int_column\n0           0\n1           1""]"
1762,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.idxmax.html,pandas.core.groupby.SeriesGroupBy.idxmax,"SeriesGroupBy.idxmax(axis=<no_default>, skipna=True)[source]# Return the row label of the maximum value. If multiple values equal the maximum, the first row label with that value is returned. Notes This method is the Series version of ndarray.argmax. This method returns the label of the maximum, while ndarray.argmax returns the position. To get the position, use series.values.argmax().","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values. If the entire Series is NA, the result will be NA. *args, **kwargsAdditional arguments and keywords have no effect but might be accepted for compatibility with NumPy. Returns: IndexLabel of the maximum value. Raises: ValueErrorIf the Series is empty.","["">>> s = pd.Series(data=[1, None, 4, 3, 4],\n...               index=['A', 'B', 'C', 'D', 'E'])\n>>> s\nA    1.0\nB    NaN\nC    4.0\nD    3.0\nE    4.0\ndtype: float64"", "">>> s.idxmax()\n'C'"", '>>> s.idxmax(skipna=False)\nnan']"
1763,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_anchored.html,pandas.tseries.offsets.FY5253Quarter.is_anchored,FY5253Quarter.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1764,..\pandas\reference\api\pandas.DataFrame.plot.hist.html,pandas.DataFrame.plot.hist,"DataFrame.plot.hist(by=None, bins=10, **kwargs)[source]# Draw one histogram of the DataFrame’s columns. A histogram is a representation of the distribution of data. This function groups the values of all given Series in the DataFrame into bins and draws all bins in one matplotlib.axes.Axes. This is useful when the DataFrame’s Series are in a similar scale.","Parameters: bystr or sequence, optionalColumn in the DataFrame to group by. Changed in version 1.4.0: Previously, by is silently ignore and makes no groupings binsint, default 10Number of histogram bins to be used. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: class:matplotlib.AxesSubplotReturn a histogram plot.","["">>> df = pd.DataFrame(np.random.randint(1, 7, 6000), columns=['one'])\n>>> df['two'] = df['one'] + np.random.randint(1, 7, 6000)\n>>> ax = df.plot.hist(bins=12, alpha=0.5)"", '>>> age_list = [8, 10, 12, 14, 72, 74, 76, 78, 20, 25, 30, 35, 60, 85]\n>>> df = pd.DataFrame({""gender"": list(""MMMMMMMMFFFFFF""), ""age"": age_list})\n>>> ax = df.plot.hist(column=[""age""], by=""gender"", figsize=(10, 8))']"
1765,..\pandas\reference\api\pandas.tseries.offsets.Week.copy.html,pandas.tseries.offsets.Week.copy,Week.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1766,..\pandas\reference\api\pandas.Series.str.get.html,pandas.Series.str.get,"Series.str.get(i)[source]# Extract element from each component at specified position or with specified key. Extract element from lists, tuples, dict, or strings in each element in the Series/Index.",Parameters: iint or hashable dict labelPosition or key of element to extract. Returns: Series or Index,"['>>> s = pd.Series([""String"",\n...               (1, 2, 3),\n...               [""a"", ""b"", ""c""],\n...               123,\n...               -456,\n...               {1: ""Hello"", ""2"": ""World""}])\n>>> s\n0                        String\n1                     (1, 2, 3)\n2                     [a, b, c]\n3                           123\n4                          -456\n5    {1: \'Hello\', \'2\': \'World\'}\ndtype: object', '>>> s.str.get(1)\n0        t\n1        2\n2        b\n3      NaN\n4      NaN\n5    Hello\ndtype: object', '>>> s.str.get(-1)\n0      g\n1      3\n2      c\n3    NaN\n4    NaN\n5    None\ndtype: object', '>>> s = pd.Series([{""name"": ""Hello"", ""value"": ""World""},\n...               {""name"": ""Goodbye"", ""value"": ""Planet""}])\n>>> s.str.get(\'name\')\n0      Hello\n1    Goodbye\ndtype: object']"
1767,..\pandas\reference\api\pandas.read_sql_query.html,pandas.read_sql_query,"pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, chunksize=None, dtype=None, dtype_backend=<no_default>)[source]# Read SQL query into a DataFrame. Returns a DataFrame corresponding to the result set of the query string. Optionally provide an index_col parameter to use one of the columns as the index, otherwise default integer index will be used. Notes Any datetime values with time zone information parsed via the parse_dates parameter will be converted to UTC.","Parameters: sqlstr SQL query or SQLAlchemy Selectable (select or text object)SQL query to be executed. conSQLAlchemy connectable, str, or sqlite3 connectionUsing SQLAlchemy makes it possible to use any DB supported by that library. If a DBAPI2 object, only sqlite3 is supported. index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex). coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Useful for SQL result sets. paramslist, tuple or mapping, optional, default: NoneList of parameters to pass to execute method.  The syntax used to pass parameters is database driver dependent. Check your database driver documentation for which of the five syntax styles, described in PEP 249’s paramstyle, is supported. Eg. for psycopg2, uses %(name)s so use params={‘name’ : ‘value’}. parse_dateslist or dict, default: None List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times, or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite. chunksizeint, default NoneIf specified, return an iterator where chunksize is the number of rows to include in each chunk. dtypeType name or dict of columnsData type for data or columns. E.g. np.float64 or {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’}. Added in version 1.3.0. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: DataFrame or Iterator[DataFrame]","['>>> from sqlalchemy import create_engine  \n>>> engine = create_engine(""sqlite:///database.db"")  \n>>> with engine.connect() as conn, conn.begin():  \n...     data = pd.read_sql_table(""data"", conn)']"
1768,..\pandas\reference\api\pandas.tseries.offsets.Week.freqstr.html,pandas.tseries.offsets.Week.freqstr,Week.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1769,..\pandas\reference\api\pandas.Index.max.html,pandas.Index.max,"Index.max(axis=None, skipna=True, *args, **kwargs)[source]# Return the maximum value of the Index.","Parameters: axisint, optionalFor compatibility with NumPy. Only 0 or None are allowed. skipnabool, default TrueExclude NA/null values when showing the result. *args, **kwargsAdditional arguments and keywords for compatibility with NumPy. Returns: scalarMaximum value.","['>>> idx = pd.Index([3, 2, 1])\n>>> idx.max()\n3', "">>> idx = pd.Index(['c', 'b', 'a'])\n>>> idx.max()\n'c'"", "">>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\n>>> idx.max()\n('b', 2)""]"
1770,..\pandas\reference\api\pandas.DataFrame.plot.html,pandas.DataFrame.plot,"DataFrame.plot(*args, **kwargs)[source]# Make plots of Series or DataFrame. Uses the backend specified by the option plotting.backend. By default, matplotlib is used. Notes See matplotlib documentation online for more on this subject If kind = ‘bar’ or ‘barh’, you can specify relative alignments for bar plot layout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)","Parameters: dataSeries or DataFrameThe object for which the method is called. xlabel or position, default NoneOnly used if data is a DataFrame. ylabel, position or list of label, positions, default NoneAllows plotting of one column versus another. Only used if data is a DataFrame. kindstrThe kind of plot to produce: ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plot ‘scatter’ : scatter plot (DataFrame only) ‘hexbin’ : hexbin plot (DataFrame only) axmatplotlib axes object, default NoneAn axes of the current figure. subplotsbool or sequence of iterables, default FalseWhether to group columns into subplots: False : No subplots will be used True : Make separate subplots for each column. sequence of iterables of column labels: Create a subplot for each group of columns. For example [(‘a’, ‘c’), (‘b’, ‘d’)] will create 2 subplots: one with columns ‘a’ and ‘c’, and one with columns ‘b’ and ‘d’. Remaining columns that aren’t specified will be plotted in additional subplots (one per column). Added in version 1.5.0. sharexbool, default True if ax is None else FalseIn case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure. shareybool, default FalseIn case subplots=True, share y axis and set some y axis labels to invisible. layouttuple, optional(rows, columns) for the layout of subplots. figsizea tuple (width, height) in inchesSize of a figure object. use_indexbool, default TrueUse index as ticks for x axis. titlestr or listTitle to use for the plot. If a string is passed, print the string at the top of the figure. If a list is passed and subplots is True, print each item in the list above the corresponding subplot. gridbool, default None (matlab style default)Axis grid lines. legendbool or {‘reverse’}Place legend on axis subplots. stylelist or dictThe matplotlib line style per column. logxbool or ‘sym’, default FalseUse log scaling or symlog scaling on x axis. logybool or ‘sym’ default FalseUse log scaling or symlog scaling on y axis. loglogbool or ‘sym’, default FalseUse log scaling or symlog scaling on both x and y axes. xtickssequenceValues to use for the xticks. ytickssequenceValues to use for the yticks. xlim2-tuple/listSet the x limits of the current axes. ylim2-tuple/listSet the y limits of the current axes. xlabellabel, optionalName to use for the xlabel on x-axis. Default uses index name as xlabel, or the x-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. ylabellabel, optionalName to use for the ylabel on y-axis. Default will show no ylabel, or the y-column name for planar plots. Changed in version 2.0.0: Now applicable to histograms. rotfloat, default NoneRotation for ticks (xticks for vertical, yticks for horizontal plots). fontsizefloat, default NoneFont size for xticks and yticks. colormapstr or matplotlib colormap object, default NoneColormap to select colors from. If string, load colormap with that name from matplotlib. colorbarbool, optionalIf True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots). positionfloatSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center). tablebool, Series or DataFrame, default FalseIf True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table. yerrDataFrame, Series, array-like, dict and strSee Plotting with Error Bars for detail. xerrDataFrame, Series, array-like, dict and strEquivalent to yerr. stackedbool, default False in line and bar plots, and True in area plotIf True, create stacked plot. secondary_ybool or sequence, default FalseWhether to plot on the secondary y-axis if a list/tuple, which columns to plot on secondary y-axis. mark_rightbool, default TrueWhen using a secondary_y axis, automatically mark the column labels with “(right)” in the legend. include_boolbool, default is FalseIf True, boolean values can be plotted. backendstr, default NoneBackend to use instead of the backend specified in the option plotting.backend. For instance, ‘matplotlib’. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend. **kwargsOptions to pass to matplotlib plotting method. Returns: matplotlib.axes.Axes or numpy.ndarray of themIf the backend is not the default matplotlib one, the return value will be the object returned by the backend.","['>>> ser = pd.Series([1, 2, 3, 3])\n>>> plot = ser.plot(kind=\'hist\', title=""My plot"")', '>>> df = pd.DataFrame({\'length\': [1.5, 0.5, 1.2, 0.9, 3],\n...                   \'width\': [0.7, 0.2, 0.15, 0.2, 1.1]},\n...                   index=[\'pig\', \'rabbit\', \'duck\', \'chicken\', \'horse\'])\n>>> plot = df.plot(title=""DataFrame Plot"")', '>>> lst = [-1, -2, -3, 1, 2, 3]\n>>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)\n>>> plot = ser.groupby(lambda x: x > 0).plot(title=""SeriesGroupBy Plot"")', '>>> df = pd.DataFrame({""col1"" : [1, 2, 3, 4],\n...                   ""col2"" : [""A"", ""B"", ""A"", ""B""]})\n>>> plot = df.groupby(""col2"").plot(kind=""bar"", title=""DataFrameGroupBy Plot"")']"
1771,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.idxmin.html,pandas.core.groupby.SeriesGroupBy.idxmin,"SeriesGroupBy.idxmin(axis=<no_default>, skipna=True)[source]# Return the row label of the minimum value. If multiple values equal the minimum, the first row label with that value is returned. Notes This method is the Series version of ndarray.argmin. This method returns the label of the minimum, while ndarray.argmin returns the position. To get the position, use series.values.argmin().","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values. If the entire Series is NA, the result will be NA. *args, **kwargsAdditional arguments and keywords have no effect but might be accepted for compatibility with NumPy. Returns: IndexLabel of the minimum value. Raises: ValueErrorIf the Series is empty.","["">>> s = pd.Series(data=[1, None, 4, 1],\n...               index=['A', 'B', 'C', 'D'])\n>>> s\nA    1.0\nB    NaN\nC    4.0\nD    1.0\ndtype: float64"", "">>> s.idxmin()\n'A'"", '>>> s.idxmin(skipna=False)\nnan']"
1772,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_month_end.html,pandas.tseries.offsets.FY5253Quarter.is_month_end,FY5253Quarter.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1773,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.end.html,pandas.tseries.offsets.BusinessHour.end,BusinessHour.end#,No parameters found,[]
1774,..\pandas\reference\api\pandas.Series.str.get_dummies.html,pandas.Series.str.get_dummies,Series.str.get_dummies(sep='|')[source]# Return DataFrame of dummy/indicator variables for Series. Each string in Series is split by sep and returned as a DataFrame of dummy/indicator variables.,"Parameters: sepstr, default “|”String to split on. Returns: DataFrameDummy variables corresponding to values of the Series.","["">>> pd.Series(['a|b', 'a', 'a|c']).str.get_dummies()\n   a  b  c\n0  1  1  0\n1  1  0  0\n2  1  0  1"", "">>> pd.Series(['a|b', np.nan, 'a|c']).str.get_dummies()\n   a  b  c\n0  1  1  0\n1  0  0  0\n2  1  0  1""]"
1775,..\pandas\reference\api\pandas.read_sql_table.html,pandas.read_sql_table,"pandas.read_sql_table(table_name, con, schema=None, index_col=None, coerce_float=True, parse_dates=None, columns=None, chunksize=None, dtype_backend=<no_default>)[source]# Read SQL database table into a DataFrame. Given a table name and a SQLAlchemy connectable, returns a DataFrame. This function does not support DBAPI connections. Notes Any datetime values with time zone information will be converted to UTC.","Parameters: table_namestrName of SQL table in database. conSQLAlchemy connectable or strA database URI could be provided as str. SQLite DBAPI connection mode not supported. schemastr, default NoneName of SQL schema in database to query (if database flavor supports this). Uses default schema if None (default). index_colstr or list of str, optional, default: NoneColumn(s) to set as index(MultiIndex). coerce_floatbool, default TrueAttempts to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point. Can result in loss of Precision. parse_dateslist or dict, default None List of column names to parse as dates. Dict of {column_name: format string} where format string is strftime compatible in case of parsing string times or is one of (D, s, ns, ms, us) in case of parsing integer timestamps. Dict of {column_name: arg dict}, where the arg dict corresponds to the keyword arguments of pandas.to_datetime() Especially useful with databases without native Datetime support, such as SQLite. columnslist, default NoneList of column names to select from SQL table. chunksizeint, default NoneIf specified, returns an iterator where chunksize is the number of rows to include in each chunk. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: DataFrame or Iterator[DataFrame]A SQL table is returned as two-dimensional data structure with labeled axes.","["">>> pd.read_sql_table('table_name', 'postgres:///db_name')""]"
1776,..\pandas\reference\api\pandas.tseries.offsets.Week.html,pandas.tseries.offsets.Week,class pandas.tseries.offsets.Week# Weekly offset. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code weekday,"Parameters: nint, default 1The number of weeks represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekdayint or None, default NoneAlways generate specific day of week. 0 for Monday and 6 for Sunday.","['>>> date_object = pd.Timestamp(""2023-01-13"")\n>>> date_object\nTimestamp(\'2023-01-13 00:00:00\')', "">>> date_plus_one_week = date_object + pd.tseries.offsets.Week(n=1)\n>>> date_plus_one_week\nTimestamp('2023-01-20 00:00:00')"", "">>> date_next_monday = date_object + pd.tseries.offsets.Week(weekday=0)\n>>> date_next_monday\nTimestamp('2023-01-16 00:00:00')"", "">>> date_next_sunday = date_object + pd.tseries.offsets.Week(weekday=6)\n>>> date_next_sunday\nTimestamp('2023-01-15 00:00:00')""]"
1777,..\pandas\reference\api\pandas.Index.memory_usage.html,pandas.Index.memory_usage,Index.memory_usage(deep=False)[source]# Memory usage of the values. Notes Memory usage does not include memory consumed by elements that are not components of the array if deep=False or if used on PyPy,"Parameters: deepbool, default FalseIntrospect the data deeply, interrogate object dtypes for system-level memory consumption. Returns: bytes used","['>>> idx = pd.Index([1, 2, 3])\n>>> idx.memory_usage()\n24']"
1778,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.indices.html,pandas.core.groupby.SeriesGroupBy.indices,property SeriesGroupBy.indices[source]# Dict {group name -> group indices}.,No parameters found,"["">>> lst = ['a', 'a', 'b']\n>>> ser = pd.Series([1, 2, 3], index=lst)\n>>> ser\na    1\na    2\nb    3\ndtype: int64\n>>> ser.groupby(level=0).indices\n{'a': array([0, 1]), 'b': array([2])}"", '>>> data = [[1, 2, 3], [1, 5, 6], [7, 8, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""owl"", ""toucan"", ""eagle""])\n>>> df\n        a  b  c\nowl     1  2  3\ntoucan  1  5  6\neagle   7  8  9\n>>> df.groupby(by=[""a""]).indices\n{1: array([0, 1]), 7: array([2])}', "">>> ser = pd.Series([1, 2, 3, 4], index=pd.DatetimeIndex(\n...                 ['2023-01-01', '2023-01-15', '2023-02-01', '2023-02-15']))\n>>> ser\n2023-01-01    1\n2023-01-15    2\n2023-02-01    3\n2023-02-15    4\ndtype: int64\n>>> ser.resample('MS').indices\ndefaultdict(<class 'list'>, {Timestamp('2023-01-01 00:00:00'): [0, 1],\nTimestamp('2023-02-01 00:00:00'): [2, 3]})""]"
1779,..\pandas\reference\api\pandas.Series.str.html,pandas.Series.str,"Series.str()[source]# Vectorized string functions for Series and Index. NAs stay NA unless handled otherwise by a particular method. Patterned after Python’s string methods, with some inspiration from R’s stringr package.",No parameters found,"['>>> s = pd.Series([""A_Str_Series""])\n>>> s\n0    A_Str_Series\ndtype: object', '>>> s.str.split(""_"")\n0    [A, Str, Series]\ndtype: object', '>>> s.str.replace(""_"", """")\n0    AStrSeries\ndtype: object']"
1780,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.freqstr.html,pandas.tseries.offsets.BusinessHour.freqstr,BusinessHour.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1781,..\pandas\reference\api\pandas.DataFrame.plot.kde.html,pandas.DataFrame.plot.kde,"DataFrame.plot.kde(bw_method=None, ind=None, **kwargs)[source]# Generate Kernel Density Estimate plot using Gaussian kernels. In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.","Parameters: bw_methodstr, scalar or callable, optionalThe method used to calculate the estimator bandwidth. This can be ‘scott’, ‘silverman’, a scalar constant or a callable. If None (default), ‘scott’ is used. See scipy.stats.gaussian_kde for more information. indNumPy array or int, optionalEvaluation points for the estimated PDF. If None (default), 1000 equally spaced points are used. If ind is a NumPy array, the KDE is evaluated at the points passed. If ind is an integer, ind number of equally spaced points are used. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","['>>> s = pd.Series([1, 2, 2.5, 3, 3.5, 4, 5])\n>>> ax = s.plot.kde()', '>>> ax = s.plot.kde(bw_method=0.3)', '>>> ax = s.plot.kde(bw_method=3)', '>>> ax = s.plot.kde(ind=[1, 2, 3, 4, 5])', "">>> df = pd.DataFrame({\n...     'x': [1, 2, 2.5, 3, 3.5, 4, 5],\n...     'y': [4, 4, 4.5, 5, 5.5, 6, 6],\n... })\n>>> ax = df.plot.kde()"", '>>> ax = df.plot.kde(bw_method=0.3)', '>>> ax = df.plot.kde(bw_method=3)', '>>> ax = df.plot.kde(ind=[1, 2, 3, 4, 5, 6])']"
1782,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_month_start.html,pandas.tseries.offsets.FY5253Quarter.is_month_start,FY5253Quarter.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1783,..\pandas\reference\api\pandas.read_stata.html,pandas.read_stata,"pandas.read_stata(filepath_or_buffer, *, convert_dates=True, convert_categoricals=True, index_col=None, convert_missing=False, preserve_dtypes=True, columns=None, order_categoricals=True, chunksize=None, iterator=False, compression='infer', storage_options=None)[source]# Read Stata file into DataFrame. Notes Categorical variables read through an iterator may not have the same categories and dtype. This occurs when  a variable stored in a DTA file is associated to an incomplete set of value labels that only label a strict subset of the values.","Parameters: filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.dta. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO. convert_datesbool, default TrueConvert date variables to DataFrame time values. convert_categoricalsbool, default TrueRead value labels and convert columns to Categorical/Factor variables. index_colstr, optionalColumn to set as index. convert_missingbool, default FalseFlag indicating whether to convert missing values to their Stata representations.  If False, missing values are replaced with nan. If True, columns containing missing values are returned with object data types and missing values are represented by StataMissingValue objects. preserve_dtypesbool, default TruePreserve Stata datatypes. If False, numeric data are upcast to pandas default types for foreign data (float64 or int64). columnslist or NoneColumns to retain.  Columns will be returned in the given order.  None returns all columns. order_categoricalsbool, default TrueFlag indicating whether converted categorical data are ordered. chunksizeint, default NoneReturn StataReader object for iterations, returns chunks with given number of lines. iteratorbool, default FalseReturn StataReader object. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. Returns: DataFrame or pandas.api.typing.StataReader","["">>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon', 'parrot'],\n...                     'speed': [350, 18, 361, 15]})  \n>>> df.to_stata('animals.dta')"", "">>> df = pd.read_stata('animals.dta')"", '>>> values = np.random.randint(0, 10, size=(20_000, 1), dtype=""uint8"")  \n>>> df = pd.DataFrame(values, columns=[""i""])  \n>>> df.to_stata(\'filename.dta\')', "">>> with pd.read_stata('filename.dta', chunksize=10000) as itr: \n>>>     for chunk in itr:\n...         # Operate on a single chunk, e.g., chunk.mean()\n...         pass""]"
1784,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing.html,pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing,property SeriesGroupBy.is_monotonic_decreasing[source]# Return whether each group’s values are monotonically decreasing.,Returns: Series,"["">>> s = pd.Series([2, 1, 3, 4], index=['Falcon', 'Falcon', 'Parrot', 'Parrot'])\n>>> s.groupby(level=0).is_monotonic_decreasing\nFalcon     True\nParrot    False\ndtype: bool""]"
1785,..\pandas\reference\api\pandas.read_table.html,pandas.read_table,"pandas.read_table(filepath_or_buffer, *, sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=<no_default>, skip_blank_lines=True, parse_dates=False, infer_datetime_format=<no_default>, keep_date_col=<no_default>, date_parser=<no_default>, date_format=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal='.', lineterminator=None, quotechar='""', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors='strict', dialect=None, on_bad_lines='error', delim_whitespace=<no_default>, low_memory=True, memory_map=False, float_precision=None, storage_options=None, dtype_backend=<no_default>)[source]# Read general delimited file into DataFrame. Also supports optionally iterating or breaking of the file into chunks. Additional help can be found in the online docs for IO Tools.","Parameters: filepath_or_bufferstr, path object or file-like objectAny valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handle (e.g. via builtin open function) or StringIO. sepstr, default ‘\t’ (tab-stop)Character or regex pattern to treat as the delimiter. If sep=None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator from only the first valid row of the file by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\r\t'. delimiterstr, optionalAlias for sep. headerint, Sequence of int, ‘infer’ or None, default ‘infer’Row number(s) containing column labels and marking the start of the data (zero-indexed). Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly to names then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names. The header can be a list of integers that specify row locations for a MultiIndex on the columns e.g. [0, 1, 3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file. namesSequence of Hashable, optionalSequence of column labels to apply. If the file contains a header row, then you should explicitly pass header=0 to override the column names. Duplicates in this list are not allowed. index_colHashable, Sequence of Hashable or False, optionalColumn(s) to use as row label(s), denoted either by column labels or column indices.  If a sequence of labels or indices is given, MultiIndex will be formed for the row labels. Note: index_col=False can be used to force pandas to not use the first column as the index, e.g., when you have a malformed file with delimiters at the end of each line. usecolsSequence of Hashable or Callable, optionalSubset of columns to select, denoted either by column labels or column indices. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). If names are given, the document header row(s) are not taken into account. For example, a valid list-like usecols parameter would be [0, 1, 2] or ['foo', 'bar', 'baz']. Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']] for ['bar', 'foo'] order. If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True. An example of a valid callable argument would be lambda x: x.upper() in ['AAA', 'BBB', 'DDD']. Using this parameter results in much faster parsing time and lower memory usage. dtypedtype or dict of {Hashabledtype}, optionalData type(s) to apply to either the whole dataset or individual columns. E.g., {'a': np.float64, 'b': np.int32, 'c': 'Int64'} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. Added in version 1.5.0: Support for defaultdict was added. Specify a defaultdict as input where the default determines the dtype of the columns which are not explicitly listed. engine{‘c’, ‘python’, ‘pyarrow’}, optionalParser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine. Added in version 1.4.0: The ‘pyarrow’ engine was added as an experimental engine, and some features are unsupported, or may not work correctly, with this engine. convertersdict of {HashableCallable}, optionalFunctions for converting values in specified columns. Keys can either be column labels or column indices. true_valueslist, optionalValues to consider as True in addition to case-insensitive variants of ‘True’. false_valueslist, optionalValues to consider as False in addition to case-insensitive variants of ‘False’. skipinitialspacebool, default FalseSkip spaces after delimiter. skiprowsint, list of int or Callable, optionalLine numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be lambda x: x in [0, 2]. skipfooterint, default 0Number of lines at bottom of file to skip (Unsupported with engine='c'). nrowsint, optionalNumber of rows of file to read. Useful for reading pieces of large files. na_valuesHashable, Iterable of Hashable or dict of {HashableIterable}, optionalAdditional strings to recognize as NA/NaN. If dict passed, specific per-column NA values.  By default the following values are interpreted as NaN: “ “, “#N/A”, “#N/A N/A”, “#NA”, “-1.#IND”, “-1.#QNAN”, “-NaN”, “-nan”, “1.#IND”, “1.#QNAN”, “<NA>”, “N/A”, “NA”, “NULL”, “NaN”, “None”, “n/a”, “nan”, “null “. keep_default_nabool, default TrueWhether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows: If keep_default_na is True, and na_values are specified, na_values is appended to the default NaN values used for parsing. If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing. If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing. If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN. Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will be ignored. na_filterbool, default TrueDetect missing value markers (empty strings and the value of na_values). In data without any NA values, passing na_filter=False can improve the performance of reading a large file. verbosebool, default FalseIndicate number of NA values placed in non-numeric columns. Deprecated since version 2.2.0. skip_blank_linesbool, default TrueIf True, skip over blank lines rather than interpreting as NaN values. parse_datesbool, list of Hashable, list of lists or dict of {Hashablelist}, default FalseThe behavior is as follows: bool. If True -> try parsing the index. Note: Automatically set to True if date_format or date_parser arguments have been passed. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of list. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. Values are joined with a space before parsing. dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’. Values are joined with a space before parsing. If a column or index cannot be represented as an array of datetime, say because of an unparsable value or a mixture of timezones, the column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use to_datetime() after read_csv(). Note: A fast-path exists for iso8601-formatted dates. infer_datetime_formatbool, default FalseIf True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x. Deprecated since version 2.0.0: A strict version of this argument is now the default, passing it has no effect. keep_date_colbool, default FalseIf True and parse_dates specifies combining multiple columns then keep the original columns. date_parserCallable, optionalFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments. Deprecated since version 2.0.0: Use date_format instead, or read in as object and then apply to_datetime() as-needed. date_formatstr or dict of column -> format, optionalFormat to use for parsing dates when used in conjunction with parse_dates. The strftime to parse time, e.g. ""%d/%m/%Y"". See strftime documentation for more information on choices, though note that ""%f"" will parse all the way up to nanoseconds. You can also pass: “ISO8601”, to parse any ISO8601time string (not necessarily in exactly the same format); “mixed”, to infer the format for each element individually. This is risky,and you should probably use it along with dayfirst. Added in version 2.0.0. dayfirstbool, default FalseDD/MM format dates, international and European format. cache_datesbool, default TrueIf True, use a cache of unique, converted dates to apply the datetime conversion. May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets. iteratorbool, default FalseReturn TextFileReader object for iteration or getting chunks with get_chunk(). chunksizeint, optionalNumber of lines to read from the file per chunk. Passing a value will cause the function to return a TextFileReader object for iteration. See the IO Tools docs for more information on iterator and chunksize. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘filepath_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. thousandsstr (length 1), optionalCharacter acting as the thousands separator in numerical values. decimalstr (length 1), default ‘.’Character to recognize as decimal point (e.g., use ‘,’ for European data). lineterminatorstr (length 1), optionalCharacter used to denote a line break. Only valid with C parser. quotecharstr (length 1), optionalCharacter used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored. quoting{0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMALControl field quoting behavior per csv.QUOTE_* constants. Default is csv.QUOTE_MINIMAL (i.e., 0) which implies that only fields containing special characters are quoted (e.g., characters defined in quotechar, delimiter, or lineterminator. doublequotebool, default TrueWhen quotechar is specified and quoting is not QUOTE_NONE, indicate whether or not to interpret two consecutive quotechar elements INSIDE a field as a single quotechar element. escapecharstr (length 1), optionalCharacter used to escape other characters. commentstr (length 1), optionalCharacter indicating that the remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing #empty\na,b,c\n1,2,3 with header=0 will result in 'a,b,c' being treated as the header. encodingstr, optional, default ‘utf-8’Encoding to use for UTF when reading/writing (ex. 'utf-8'). List of Python standard encodings . encoding_errorsstr, optional, default ‘strict’How encoding errors are treated. List of possible values . Added in version 1.3.0. dialectstr or csv.Dialect, optionalIf provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv.Dialect documentation for more details. on_bad_lines{‘error’, ‘warn’, ‘skip’} or Callable, default ‘error’Specifies what to do upon encountering a bad line (a line with too many fields). Allowed values are : 'error', raise an Exception when a bad line is encountered. 'warn', raise a warning when a bad line is encountered and skip that line. 'skip', skip bad lines without raising or warning when they are encountered. Added in version 1.3.0. Added in version 1.4.0:  Callable, function with signature (bad_line: list[str]) -> list[str] | None that will process a single bad line. bad_line is a list of strings split by the sep. If the function returns None, the bad line will be ignored. If the function returns a new list of strings with more elements than expected, a ParserWarning will be emitted while dropping extra elements. Only supported when engine='python' Changed in version 2.2.0:  Callable, function with signature as described in pyarrow documentation when engine='pyarrow' delim_whitespacebool, default FalseSpecifies whether or not whitespace (e.g. ' ' or '\t') will be used as the sep delimiter. Equivalent to setting sep='\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter. Deprecated since version 2.2.0: Use sep=""\s+"" instead. low_memorybool, default TrueInternally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference.  To ensure no mixed types either set False, or specify the type with the dtype parameter. Note that the entire file is read into a single DataFrame regardless, use the chunksize or iterator parameter to return the data in chunks. (Only valid with C parser). memory_mapbool, default FalseIf a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead. float_precision{‘high’, ‘legacy’, ‘round_trip’}, optionalSpecifies which converter the C engine should use for floating-point values. The options are None or 'high' for the ordinary converter, 'legacy' for the original lower precision pandas converter, and 'round_trip' for the round-trip converter. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: DataFrame or TextFileReaderA comma-separated values (csv) file is returned as two-dimensional data structure with labeled axes.","["">>> pd.read_table('data.csv')""]"
1786,..\pandas\reference\api\pandas.DataFrame.plot.line.html,pandas.DataFrame.plot.line,"DataFrame.plot.line(x=None, y=None, **kwargs)[source]# Plot Series or DataFrame as lines. This function is useful to plot lines using DataFrame’s values as coordinates.","Parameters: xlabel or position, optionalAllows plotting of one column versus another. If not specified, the index of the DataFrame is used. ylabel or position, optionalAllows plotting of one column versus another. If not specified, all numerical columns are used. colorstr, array-like, or dict, optionalThe color for each of the DataFrame’s columns. Possible values are: A single color string referred to by name, RGB or RGBA code,for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBAcode, which will be used for each column recursively. For instance [‘green’,’yellow’] each column’s line will be filled in green or yellow, alternatively. If there is only a single column to be plotted, then only the first color from the color list will be used. A dict of the form {column namecolor}, so that each column will becolored accordingly. For example, if your columns are called a and b, then passing {‘a’: ‘green’, ‘b’: ‘red’} will color lines for column a in green and lines for column b in red. **kwargsAdditional keyword arguments are documented in DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themAn ndarray is returned with one matplotlib.axes.Axes per column when subplots=True.","['>>> s = pd.Series([1, 3, 2])\n>>> s.plot.line()', "">>> df = pd.DataFrame({\n...    'pig': [20, 18, 489, 675, 1776],\n...    'horse': [4, 25, 281, 600, 1900]\n...    }, index=[1990, 1997, 2003, 2009, 2014])\n>>> lines = df.plot.line()"", "">>> axes = df.plot.line(subplots=True)\n>>> type(axes)\n<class 'numpy.ndarray'>"", '>>> axes = df.plot.line(\n...     subplots=True, color={""pig"": ""pink"", ""horse"": ""#742802""}\n... )', "">>> lines = df.plot.line(x='pig', y='horse')""]"
1787,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_on_offset.html,pandas.tseries.offsets.FY5253Quarter.is_on_offset,FY5253Quarter.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1788,..\pandas\reference\api\pandas.tseries.offsets.Week.is_anchored.html,pandas.tseries.offsets.Week.is_anchored,Week.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1789,..\pandas\reference\api\pandas.Series.str.index.html,pandas.Series.str.index,"Series.str.index(sub, start=0, end=None)[source]# Return lowest indexes in each string in Series/Index. Each of the returned indexes corresponds to the position where the substring is fully contained between [start:end]. This is the same as str.find except instead of returning -1, it raises a ValueError when the substring is not found. Equivalent to standard str.index.",Parameters: substrSubstring being searched. startintLeft edge index. endintRight edge index. Returns: Series or Index of object,"['>>> ser = pd.Series([""horse"", ""eagle"", ""donkey""])\n>>> ser.str.index(""e"")\n0   4\n1   0\n2   4\ndtype: int64', '>>> ser = pd.Series([""Deer"", ""eagle"", ""Sheep""])\n>>> ser.str.rindex(""e"")\n0   2\n1   4\n2   3\ndtype: int64']"
1790,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing.html,pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing,property SeriesGroupBy.is_monotonic_increasing[source]# Return whether each group’s values are monotonically increasing.,Returns: Series,"["">>> s = pd.Series([2, 1, 3, 4], index=['Falcon', 'Falcon', 'Parrot', 'Parrot'])\n>>> s.groupby(level=0).is_monotonic_increasing\nFalcon    False\nParrot     True\ndtype: bool""]"
1791,..\pandas\reference\api\pandas.Index.min.html,pandas.Index.min,"Index.min(axis=None, skipna=True, *args, **kwargs)[source]# Return the minimum value of the Index.","Parameters: axis{None}Dummy argument for consistency with Series. skipnabool, default TrueExclude NA/null values when showing the result. *args, **kwargsAdditional arguments and keywords for compatibility with NumPy. Returns: scalarMinimum value.","['>>> idx = pd.Index([3, 2, 1])\n>>> idx.min()\n1', "">>> idx = pd.Index(['c', 'b', 'a'])\n>>> idx.min()\n'a'"", "">>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\n>>> idx.min()\n('a', 1)""]"
1792,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.holidays.html,pandas.tseries.offsets.BusinessHour.holidays,BusinessHour.holidays#,No parameters found,[]
1793,..\pandas\reference\api\pandas.read_xml.html,pandas.read_xml,"pandas.read_xml(path_or_buffer, *, xpath='./*', namespaces=None, elems_only=False, attrs_only=False, names=None, dtype=None, converters=None, parse_dates=None, encoding='utf-8', parser='lxml', stylesheet=None, iterparse=None, compression='infer', storage_options=None, dtype_backend=<no_default>)[source]# Read XML document into a DataFrame object. Added in version 1.3.0. Notes This method is best designed to import shallow XML documents in following format which is the ideal fit for the two-dimensions of a DataFrame (row by column). <root>     <row>       <column1>data</column1>       <column2>data</column2>       <column3>data</column3>       ...    </row>    <row>       ...    </row>    ... </root> As a file format, XML documents can be designed any way including layout of elements and attributes as long as it conforms to W3C specifications. Therefore, this method is a convenience handler for a specific flatter design and not all possible XML structures. However, for more complex XML documents, stylesheet allows you to temporarily redesign original document with XSLT (a special purpose language) for a flatter version for migration to a DataFrame. This function will always return a single DataFrame or raise exceptions due to issues with XML document, xpath, or other parameters. See the read_xml documentation in the IO section of the docs for more information in using this method to parse XML files to DataFrames.","Parameters: path_or_bufferstr, path object, or file-like objectString, path object (implementing os.PathLike[str]), or file-like object implementing a read() function. The string can be any valid XML string or a path. The string can further be a URL. Valid URL schemes include http, ftp, s3, and file. Deprecated since version 2.1.0: Passing xml literal strings is deprecated. Wrap literal xml input in io.StringIO or io.BytesIO instead. xpathstr, optional, default ‘./*’The XPath to parse required set of nodes for migration to DataFrame.``XPath`` should return a collection of elements and not a single element. Note: The etree parser supports limited XPath expressions. For more complex XPath, use lxml which requires installation. namespacesdict, optionalThe namespaces defined in XML document as dicts with key being namespace prefix and value the URI. There is no need to include all namespaces in XML, only the ones used in xpath expression. Note: if XML document uses default namespace denoted as xmlns=’<URI>’ without a prefix, you must assign any temporary namespace prefix such as ‘doc’ to the URI in order to parse underlying nodes and/or attributes. For example, namespaces = {""doc"": ""https://example.com""} elems_onlybool, optional, default FalseParse only the child elements at the specified xpath. By default, all child elements and non-empty text nodes are returned. attrs_onlybool, optional, default FalseParse only the attributes at the specified xpath. By default, all attributes are returned. nameslist-like, optionalColumn names for DataFrame of parsed XML data. Use this parameter to rename original element names and distinguish same named elements and attributes. dtypeType name or dict of column -> type, optionalData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32, ‘c’: ‘Int64’} Use str or object together with suitable na_values settings to preserve and not interpret dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. Added in version 1.5.0. convertersdict, optionalDict of functions for converting values in certain columns. Keys can either be integers or column labels. Added in version 1.5.0. parse_datesbool or list of int or names or list of lists or dict, default FalseIdentifiers to parse index or columns to datetime. The behavior is as follows: boolean. If True -> try parsing the index. list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column. list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. dict, e.g. {‘foo’ : [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’ Added in version 1.5.0. encodingstr, optional, default ‘utf-8’Encoding of XML document. parser{‘lxml’,’etree’}, default ‘lxml’Parser module to use for retrieval of data. Only ‘lxml’ and ‘etree’ are supported. With ‘lxml’ more complex XPath searches and ability to use XSLT stylesheet are supported. stylesheetstr, path object or file-like objectA URL, file-like object, or a raw string containing an XSLT script. This stylesheet should flatten complex, deeply nested XML documents for easier parsing. To use this feature you must have lxml module installed and specify ‘lxml’ as parser. The xpath must reference nodes of transformed XML document generated after XSLT transformation and not the original XML document. Only XSLT 1.0 scripts and not later versions is currently supported. iterparsedict, optionalThe nodes or attributes to retrieve in iterparsing of XML document as a dict with key being the name of repeating element and value being list of elements or attribute names that are descendants of the repeated element. Note: If this option is used, it will replace xpath parsing and unlike xpath, descendants do not need to relate to each other but can exist any where in document under the repeating element. This memory- efficient method should be used for very large XML files (500MB, 1GB, or 5GB+). For example, iterparse = {""row_element"": [""child_elem"", ""attr"", ""grandchild_elem""]} Added in version 1.5.0. compressionstr or dict, default ‘infer’For on-the-fly decompression of on-disk data. If ‘infer’ and ‘path_or_buffer’ is path-like, then detect compression from the following extensions: ‘.gz’, ‘.bz2’, ‘.zip’, ‘.xz’, ‘.zst’, ‘.tar’, ‘.tar.gz’, ‘.tar.xz’ or ‘.tar.bz2’ (otherwise no compression). If using ‘zip’ or ‘tar’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2', 'zstd', 'xz', 'tar'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, bz2.BZ2File, zstandard.ZstdDecompressor, lzma.LZMAFile or tarfile.TarFile, respectively. As an example, the following could be passed for Zstandard decompression using a custom compression dictionary: compression={'method': 'zstd', 'dict_data': my_compression_dict}. Added in version 1.5.0: Added support for .tar files. Changed in version 1.4.0: Zstandard support. storage_optionsdict, optionalExtra options that make sense for a particular storage connection, e.g. host, port, username, password, etc. For HTTP(S) URLs the key-value pairs are forwarded to urllib.request.Request as header options. For other URLs (e.g. starting with “s3://”, and “gcs://”) the key-value pairs are forwarded to fsspec.open. Please see fsspec and urllib for more details, and for more examples on storage options refer here. dtype_backend{‘numpy_nullable’, ‘pyarrow’}, default ‘numpy_nullable’Back-end data type applied to the resultant DataFrame (still experimental). Behaviour is as follows: ""numpy_nullable"": returns nullable-dtype-backed DataFrame (default). ""pyarrow"": returns pyarrow-backed nullable ArrowDtype DataFrame. Added in version 2.0. Returns: dfA DataFrame.","['>>> from io import StringIO\n>>> xml = \'\'\'<?xml version=\'1.0\' encoding=\'utf-8\'?>\n... <data xmlns=""http://example.com"">\n...  <row>\n...    <shape>square</shape>\n...    <degrees>360</degrees>\n...    <sides>4.0</sides>\n...  </row>\n...  <row>\n...    <shape>circle</shape>\n...    <degrees>360</degrees>\n...    <sides/>\n...  </row>\n...  <row>\n...    <shape>triangle</shape>\n...    <degrees>180</degrees>\n...    <sides>3.0</sides>\n...  </row>\n... </data>\'\'\'', '>>> df = pd.read_xml(StringIO(xml))\n>>> df\n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0', '>>> xml = \'\'\'<?xml version=\'1.0\' encoding=\'utf-8\'?>\n... <data>\n...   <row shape=""square"" degrees=""360"" sides=""4.0""/>\n...   <row shape=""circle"" degrees=""360""/>\n...   <row shape=""triangle"" degrees=""180"" sides=""3.0""/>\n... </data>\'\'\'', '>>> df = pd.read_xml(StringIO(xml), xpath="".//row"")\n>>> df\n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0', '>>> xml = \'\'\'<?xml version=\'1.0\' encoding=\'utf-8\'?>\n... <doc:data xmlns:doc=""https://example.com"">\n...   <doc:row>\n...     <doc:shape>square</doc:shape>\n...     <doc:degrees>360</doc:degrees>\n...     <doc:sides>4.0</doc:sides>\n...   </doc:row>\n...   <doc:row>\n...     <doc:shape>circle</doc:shape>\n...     <doc:degrees>360</doc:degrees>\n...     <doc:sides/>\n...   </doc:row>\n...   <doc:row>\n...     <doc:shape>triangle</doc:shape>\n...     <doc:degrees>180</doc:degrees>\n...     <doc:sides>3.0</doc:sides>\n...   </doc:row>\n... </doc:data>\'\'\'', '>>> df = pd.read_xml(StringIO(xml),\n...                  xpath=""//doc:row"",\n...                  namespaces={""doc"": ""https://example.com""})\n>>> df\n      shape  degrees  sides\n0    square      360    4.0\n1    circle      360    NaN\n2  triangle      180    3.0', "">>> xml_data = '''\n...         <data>\n...            <row>\n...               <index>0</index>\n...               <a>1</a>\n...               <b>2.5</b>\n...               <c>True</c>\n...               <d>a</d>\n...               <e>2019-12-31 00:00:00</e>\n...            </row>\n...            <row>\n...               <index>1</index>\n...               <b>4.5</b>\n...               <c>False</c>\n...               <d>b</d>\n...               <e>2019-12-31 00:00:00</e>\n...            </row>\n...         </data>\n...         '''"", '>>> df = pd.read_xml(StringIO(xml_data),\n...                  dtype_backend=""numpy_nullable"",\n...                  parse_dates=[""e""])\n>>> df\n   index     a    b      c  d          e\n0      0     1  2.5   True  a 2019-12-31\n1      1  <NA>  4.5  False  b 2019-12-31']"
1794,..\pandas\reference\api\pandas.DataFrame.plot.pie.html,pandas.DataFrame.plot.pie,DataFrame.plot.pie(**kwargs)[source]# Generate a pie plot. A pie plot is a proportional representation of the numerical data in a column. This function wraps matplotlib.pyplot.pie() for the specified column. If no column reference is passed and subplots=True a pie plot is drawn for each numerical column independently.,"Parameters: yint or label, optionalLabel or position of the column to plot. If not provided, subplots=True argument must be passed. **kwargsKeyword arguments to pass on to DataFrame.plot(). Returns: matplotlib.axes.Axes or np.ndarray of themA NumPy array is returned when subplots is True.","["">>> df = pd.DataFrame({'mass': [0.330, 4.87 , 5.97],\n...                    'radius': [2439.7, 6051.8, 6378.1]},\n...                   index=['Mercury', 'Venus', 'Earth'])\n>>> plot = df.plot.pie(y='mass', figsize=(5, 5))"", '>>> plot = df.plot.pie(subplots=True, figsize=(11, 6))']"
1795,..\pandas\reference\api\pandas.Index.name.html,pandas.Index.name,property Index.name[source]# Return Index or MultiIndex name.,No parameters found,"["">>> idx = pd.Index([1, 2, 3], name='x')\n>>> idx\nIndex([1, 2, 3], dtype='int64',  name='x')\n>>> idx.name\n'x'""]"
1796,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_quarter_end.html,pandas.tseries.offsets.FY5253Quarter.is_quarter_end,FY5253Quarter.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1797,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.html,pandas.tseries.offsets.BusinessHour,"class pandas.tseries.offsets.BusinessHour# DateOffset subclass representing possibly n business hours. Examples You can use the parameter n to represent a shift of n hours. You can also change the start and the end of business hours. Passing the parameter normalize equal to True, you shift the start of the next business hour to midnight. You can divide your business day hours into several parts. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. calendar end freqstr Return a string representing the frequency. holidays kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos next_bday Used for moving to next business day. normalize offset Alias for self._offset. rule_code start weekmask","Parameters: nint, default 1The number of hours represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. startstr, time, or list of str/time, default “09:00”Start time of your custom business hour in 24h format. endstr, time, or list of str/time, default: “17:00”End time of your custom business hour in 24h format. offsettimedelta, default timedelta(0)Time offset to apply.","["">>> ts = pd.Timestamp(2022, 12, 9, 8)\n>>> ts + pd.offsets.BusinessHour(n=5)\nTimestamp('2022-12-09 14:00:00')"", '>>> ts = pd.Timestamp(2022, 8, 5, 16)\n>>> ts + pd.offsets.BusinessHour(start=""11:00"")\nTimestamp(\'2022-08-08 11:00:00\')', "">>> from datetime import time as dt_time\n>>> ts = pd.Timestamp(2022, 8, 5, 22)\n>>> ts + pd.offsets.BusinessHour(end=dt_time(19, 0))\nTimestamp('2022-08-08 10:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 9, 8)\n>>> ts + pd.offsets.BusinessHour(normalize=True)\nTimestamp('2022-12-09 00:00:00')"", '>>> import datetime as dt\n>>> freq = pd.offsets.BusinessHour(start=[""06:00"", ""10:00"", ""15:00""],\n...                                end=[""08:00"", ""12:00"", ""17:00""])\n>>> pd.date_range(dt.datetime(2022, 12, 9), dt.datetime(2022, 12, 13), freq=freq)\nDatetimeIndex([\'2022-12-09 06:00:00\', \'2022-12-09 07:00:00\',\n               \'2022-12-09 10:00:00\', \'2022-12-09 11:00:00\',\n               \'2022-12-09 15:00:00\', \'2022-12-09 16:00:00\',\n               \'2022-12-12 06:00:00\', \'2022-12-12 07:00:00\',\n               \'2022-12-12 10:00:00\', \'2022-12-12 11:00:00\',\n               \'2022-12-12 15:00:00\', \'2022-12-12 16:00:00\'],\n               dtype=\'datetime64[ns]\', freq=\'bh\')']"
1798,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.last.html,pandas.core.groupby.SeriesGroupBy.last,"SeriesGroupBy.last(numeric_only=False, min_count=-1, skipna=True)[source]# Compute the last entry of each column within each group. Defaults to skipping NA elements.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count valid values are present the result will be NA. skipnabool, default TrueExclude NA/null values. If an entire row/column is NA, the result will be NA. Added in version 2.2.1. Returns: Series or DataFrameLast of values within each group.","['>>> df = pd.DataFrame(dict(A=[1, 1, 3], B=[5, None, 6], C=[1, 2, 3]))\n>>> df.groupby(""A"").last()\n     B  C\nA\n1  5.0  2\n3  6.0  3']"
1799,..\pandas\reference\api\pandas.reset_option.html,pandas.reset_option,"pandas.reset_option(pat) = <pandas._config.config.CallableDynamicDoc object># Reset one or more options to their default value. Pass “all” as argument to reset all options. Available options: compute.[use_bottleneck, use_numba, use_numexpr] display.[chop_threshold, colheader_justify, date_dayfirst, date_yearfirst, encoding, expand_frame_repr, float_format] display.html.[border, table_schema, use_mathjax] display.[large_repr, max_categories, max_columns, max_colwidth, max_dir_items, max_info_columns, max_info_rows, max_rows, max_seq_items, memory_usage, min_rows, multi_sparse, notebook_repr_html, pprint_nest_depth, precision, show_dimensions] display.unicode.[ambiguous_as_wide, east_asian_width] display.[width] future.[infer_string, no_silent_downcasting] io.excel.ods.[reader, writer] io.excel.xls.[reader] io.excel.xlsb.[reader] io.excel.xlsm.[reader, writer] io.excel.xlsx.[reader, writer] io.hdf.[default_format, dropna_table] io.parquet.[engine] io.sql.[engine] mode.[chained_assignment, copy_on_write, data_manager, sim_interactive, string_storage, use_inf_as_na] plotting.[backend] plotting.matplotlib.[register_converters] styler.format.[decimal, escape, formatter, na_rep, precision, thousands] styler.html.[mathjax] styler.latex.[environment, hrules, multicol_align, multirow_align] styler.render.[encoding, max_columns, max_elements, max_rows, repr] styler.sparse.[columns, index] Notes Please reference the User Guide for more information. The available options with its descriptions: compute.use_bottleneckboolUse the bottleneck library to accelerate if it is installed, the default is True Valid values: False,True [default: True] [currently: True] compute.use_numbaboolUse the numba engine option for select operations if it is installed, the default is False Valid values: False,True [default: False] [currently: False] compute.use_numexprboolUse the numexpr library to accelerate computation if it is installed, the default is True Valid values: False,True [default: True] [currently: True] display.chop_thresholdfloat or Noneif set to a float value, all float values smaller than the given threshold will be displayed as exactly 0 by repr and friends. [default: None] [currently: None] display.colheader_justify‘left’/’right’Controls the justification of column headers. used by DataFrameFormatter. [default: right] [currently: right] display.date_dayfirstbooleanWhen True, prints and parses dates with the day first, eg 20/01/2005 [default: False] [currently: False] display.date_yearfirstbooleanWhen True, prints and parses dates with the year first, eg 2005/01/20 [default: False] [currently: False] display.encodingstr/unicodeDefaults to the detected encoding of the console. Specifies the encoding to be used for strings returned by to_string, these are generally strings meant to be displayed on the console. [default: utf-8] [currently: utf-8] display.expand_frame_reprbooleanWhether to print out the full DataFrame repr for wide DataFrames across multiple lines, max_columns is still respected, but the output will wrap-around across multiple “pages” if its width exceeds display.width. [default: True] [currently: True] display.float_formatcallableThe callable should accept a floating point number and return a string with the desired format of the number. This is used in some places like SeriesFormatter. See formats.format.EngFormatter for an example. [default: None] [currently: None] display.html.borderintA border=value attribute is inserted in the <table> tag for the DataFrame HTML repr. [default: 1] [currently: 1] display.html.table_schemabooleanWhether to publish a Table Schema representation for frontends that support it. (default: False) [default: False] [currently: False] display.html.use_mathjaxbooleanWhen True, Jupyter notebook will process table contents using MathJax, rendering mathematical expressions enclosed by the dollar symbol. (default: True) [default: True] [currently: True] display.large_repr‘truncate’/’info’For DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show a truncated table, or switch to the view from df.info() (the behaviour in earlier versions of pandas). [default: truncate] [currently: truncate] display.max_categoriesintThis sets the maximum number of categories pandas should output when printing out a Categorical or a Series of dtype “category”. [default: 8] [currently: 8] display.max_columnsintIf max_cols is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 or None and pandas will auto-detect the width of the terminal and print a truncated object which fits the screen width. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection and defaults to 20. [default: 0] [currently: 0] display.max_colwidthint or NoneThe maximum width in characters of a column in the repr of a pandas data structure. When the column overflows, a “…” placeholder is embedded in the output. A ‘None’ value means unlimited. [default: 50] [currently: 50] display.max_dir_itemsintThe number of items that will be added to dir(…). ‘None’ value means unlimited. Because dir is cached, changing this option will not immediately affect already existing dataframes until a column is deleted or added. This is for instance used to suggest columns from a dataframe to tab completion. [default: 100] [currently: 100] display.max_info_columnsintmax_info_columns is used in DataFrame.info method to decide if per column information will be printed. [default: 100] [currently: 100] display.max_info_rowsintdf.info() will usually show null-counts for each column. For large frames this can be quite slow. max_info_rows and max_info_cols limit this null check only to frames with smaller dimensions than specified. [default: 1690785] [currently: 1690785] display.max_rowsintIf max_rows is exceeded, switch to truncate view. Depending on large_repr, objects are either centrally truncated or printed as a summary view. ‘None’ value means unlimited. In case python/IPython is running in a terminal and large_repr equals ‘truncate’ this can be set to 0 and pandas will auto-detect the height of the terminal and print a truncated object which fits the screen height. The IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to do correct auto-detection. [default: 60] [currently: 60] display.max_seq_itemsint or NoneWhen pretty-printing a long sequence, no more then max_seq_items will be printed. If items are omitted, they will be denoted by the addition of “…” to the resulting string. If set to None, the number of items to be printed is unlimited. [default: 100] [currently: 100] display.memory_usagebool, string or NoneThis specifies if the memory usage of a DataFrame should be displayed when df.info() is called. Valid values True,False,’deep’ [default: True] [currently: True] display.min_rowsintThe numbers of rows to show in a truncated view (when max_rows is exceeded). Ignored when max_rows is set to None or 0. When set to None, follows the value of max_rows. [default: 10] [currently: 10] display.multi_sparseboolean“sparsify” MultiIndex display (don’t display repeated elements in outer levels within groups) [default: True] [currently: True] display.notebook_repr_htmlbooleanWhen True, IPython notebook will use html representation for pandas objects (if it is available). [default: True] [currently: True] display.pprint_nest_depthintControls the number of nested levels to process when pretty-printing [default: 3] [currently: 3] display.precisionintFloating point output precision in terms of number of places after the decimal, for regular formatting as well as scientific notation. Similar to precision in numpy.set_printoptions(). [default: 6] [currently: 6] display.show_dimensionsboolean or ‘truncate’Whether to print out dimensions at the end of DataFrame repr. If ‘truncate’ is specified, only print out the dimensions if the frame is truncated (e.g. not display all rows and/or columns) [default: truncate] [currently: truncate] display.unicode.ambiguous_as_widebooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.unicode.east_asian_widthbooleanWhether to use the Unicode East Asian Width to calculate the display text width. Enabling this may affect to the performance (default: False) [default: False] [currently: False] display.widthintWidth of the display in characters. In case python/IPython is running in a terminal this can be set to None and pandas will correctly auto-detect the width. Note that the IPython notebook, IPython qtconsole, or IDLE do not run in a terminal and hence it is not possible to correctly detect the width. [default: 80] [currently: 80] future.infer_string Whether to infer sequence of str objects as pyarrow string dtype, which will be the default in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] future.no_silent_downcasting Whether to opt-in to the future behavior which will not silently downcast results from Series and DataFrame where, mask, and clip methods. Silent downcasting will be removed in pandas 3.0 (at which point this option will be deprecated).[default: False] [currently: False] io.excel.ods.readerstringThe default Excel reader engine for ‘ods’ files. Available options: auto, odf, calamine. [default: auto] [currently: auto] io.excel.ods.writerstringThe default Excel writer engine for ‘ods’ files. Available options: auto, odf. [default: auto] [currently: auto] io.excel.xls.readerstringThe default Excel reader engine for ‘xls’ files. Available options: auto, xlrd, calamine. [default: auto] [currently: auto] io.excel.xlsb.readerstringThe default Excel reader engine for ‘xlsb’ files. Available options: auto, pyxlsb, calamine. [default: auto] [currently: auto] io.excel.xlsm.readerstringThe default Excel reader engine for ‘xlsm’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsm.writerstringThe default Excel writer engine for ‘xlsm’ files. Available options: auto, openpyxl. [default: auto] [currently: auto] io.excel.xlsx.readerstringThe default Excel reader engine for ‘xlsx’ files. Available options: auto, xlrd, openpyxl, calamine. [default: auto] [currently: auto] io.excel.xlsx.writerstringThe default Excel writer engine for ‘xlsx’ files. Available options: auto, openpyxl, xlsxwriter. [default: auto] [currently: auto] io.hdf.default_formatformatdefault format writing format, if None, then put will default to ‘fixed’ and append will default to ‘table’ [default: None] [currently: None] io.hdf.dropna_tablebooleandrop ALL nan rows when appending to a table [default: False] [currently: False] io.parquet.enginestringThe default parquet reader/writer engine. Available options: ‘auto’, ‘pyarrow’, ‘fastparquet’, the default is ‘auto’ [default: auto] [currently: auto] io.sql.enginestringThe default sql reader/writer engine. Available options: ‘auto’, ‘sqlalchemy’, the default is ‘auto’ [default: auto] [currently: auto] mode.chained_assignmentstringRaise an exception, warn, or no action if trying to use chained assignment, The default is warn [default: warn] [currently: warn] mode.copy_on_writeboolUse new copy-view behaviour using Copy-on-Write. Defaults to False, unless overridden by the ‘PANDAS_COPY_ON_WRITE’ environment variable (if set to “1” for True, needs to be set before pandas is imported). [default: False] [currently: False] mode.data_managerstringInternal data manager type; can be “block” or “array”. Defaults to “block”, unless overridden by the ‘PANDAS_DATA_MANAGER’ environment variable (needs to be set before pandas is imported). [default: block] [currently: block] (Deprecated, use `` instead.) mode.sim_interactivebooleanWhether to simulate interactive mode for purposes of testing [default: False] [currently: False] mode.string_storagestringThe default storage for StringDtype. This option is ignored if future.infer_string is set to True. [default: python] [currently: python] mode.use_inf_as_nabooleanTrue means treat None, NaN, INF, -INF as NA (old way), False means None and NaN are null, but INF, -INF are not NA (new way). This option is deprecated in pandas 2.1.0 and will be removed in 3.0. [default: False] [currently: False] (Deprecated, use `` instead.) plotting.backendstrThe plotting backend to use. The default value is “matplotlib”, the backend provided with pandas. Other backends can be specified by providing the name of the module that implements the backend. [default: matplotlib] [currently: matplotlib] plotting.matplotlib.register_convertersbool or ‘auto’.Whether to register converters with matplotlib’s units registry for dates, times, datetimes, and Periods. Toggling to False will remove the converters, restoring any converters that pandas overwrote. [default: auto] [currently: auto] styler.format.decimalstrThe character representation for the decimal separator for floats and complex. [default: .] [currently: .] styler.format.escapestr, optionalWhether to escape certain characters according to the given context; html or latex. [default: None] [currently: None] styler.format.formatterstr, callable, dict, optionalA formatter object to be used as default within Styler.format. [default: None] [currently: None] styler.format.na_repstr, optionalThe string representation for values identified as missing. [default: None] [currently: None] styler.format.precisionintThe precision for floats and complex numbers. [default: 6] [currently: 6] styler.format.thousandsstr, optionalThe character representation for thousands separator for floats, int and complex. [default: None] [currently: None] styler.html.mathjaxboolIf False will render special CSS classes to table attributes that indicate Mathjax will not be used in Jupyter Notebook. [default: True] [currently: True] styler.latex.environmentstrThe environment to replace \begin{table}. If “longtable” is used results in a specific longtable environment format. [default: None] [currently: None] styler.latex.hrulesboolWhether to add horizontal rules on top and bottom and below the headers. [default: False] [currently: False] styler.latex.multicol_align{“r”, “c”, “l”, “naive-l”, “naive-r”}The specifier for horizontal alignment of sparsified LaTeX multicolumns. Pipe decorators can also be added to non-naive values to draw vertical rules, e.g. “|r” will draw a rule on the left side of right aligned merged cells. [default: r] [currently: r] styler.latex.multirow_align{“c”, “t”, “b”}The specifier for vertical alignment of sparsified LaTeX multirows. [default: c] [currently: c] styler.render.encodingstrThe encoding used for output HTML and LaTeX files. [default: utf-8] [currently: utf-8] styler.render.max_columnsint, optionalThe maximum number of columns that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.max_elementsintThe maximum number of data-cell (<td>) elements that will be rendered before trimming will occur over columns, rows or both if needed. [default: 262144] [currently: 262144] styler.render.max_rowsint, optionalThe maximum number of rows that will be rendered. May still be reduced to satisfy max_elements, which takes precedence. [default: None] [currently: None] styler.render.reprstrDetermine which output to use in Jupyter Notebook in {“html”, “latex”}. [default: html] [currently: html] styler.sparse.columnsboolWhether to sparsify the display of hierarchical columns. Setting to False will display each explicit level element in a hierarchical key for each column. [default: True] [currently: True] styler.sparse.indexboolWhether to sparsify the display of a hierarchical index. Setting to False will display each explicit level element in a hierarchical key for each row. [default: True] [currently: True]","Parameters: patstr/regexIf specified only options matching prefix* will be reset. Note: partial matches are supported for convenience, but unless you use the full option name (e.g. x.y.z.option_name), your code may break in future versions if new options with similar names are introduced. Returns: None","["">>> pd.reset_option('display.max_columns')""]"
1800,..\pandas\reference\api\pandas.tseries.offsets.Week.is_month_end.html,pandas.tseries.offsets.Week.is_month_end,Week.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1801,..\pandas\reference\api\pandas.DataFrame.plot.scatter.html,pandas.DataFrame.plot.scatter,"DataFrame.plot.scatter(x, y, s=None, c=None, **kwargs)[source]# Create a scatter plot with varying marker point size and color. The coordinates of each point are defined by two dataframe columns and filled circles are used to represent each point. This kind of plot is useful to see complex correlations between two variables. Points could be for instance natural 2D coordinates like longitude and latitude in a map or, in general, any pair of metrics that can be plotted against each other.","Parameters: xint or strThe column name or column position to be used as horizontal coordinates for each point. yint or strThe column name or column position to be used as vertical coordinates for each point. sstr, scalar or array-like, optionalThe size of each point. Possible values are: A string with the name of the column to be used for marker’s size. A single scalar so all points have the same size. A sequence of scalars, which will be used for each point’s size recursively. For instance, when passing [2,14] all points size will be either 2 or 14, alternatively. cstr, int or array-like, optionalThe color of each point. Possible values are: A single color string referred to by name, RGB or RGBA code, for instance ‘red’ or ‘#a98d19’. A sequence of color strings referred to by name, RGB or RGBA code, which will be used for each point’s color recursively. For instance [‘green’,’yellow’] all points will be filled in green or yellow, alternatively. A column name or position whose values will be used to color the marker points according to a colormap. **kwargsKeyword arguments to pass on to DataFrame.plot(). Returns: matplotlib.axes.Axes or numpy.ndarray of them","["">>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\n...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\n...                   columns=['length', 'width', 'species'])\n>>> ax1 = df.plot.scatter(x='length',\n...                       y='width',\n...                       c='DarkBlue')"", "">>> ax2 = df.plot.scatter(x='length',\n...                       y='width',\n...                       c='species',\n...                       colormap='viridis')""]"
1802,..\pandas\reference\api\pandas.Series.str.isalnum.html,pandas.Series.str.isalnum,"Series.str.isalnum()[source]# Check whether all characters in each string are alphanumeric. This is equivalent to running the Python string method str.isalnum() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1803,..\pandas\reference\api\pandas.Index.names.html,pandas.Index.names,property Index.names[source]#,No parameters found,[]
1804,..\pandas\reference\api\pandas.DataFrame.pop.html,pandas.DataFrame.pop,DataFrame.pop(item)[source]# Return item and drop from frame. Raise KeyError if not found.,Parameters: itemlabelLabel of column to be popped. Returns: Series,"["">>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n...                    ('parrot', 'bird', 24.0),\n...                    ('lion', 'mammal', 80.5),\n...                    ('monkey', 'mammal', np.nan)],\n...                   columns=('name', 'class', 'max_speed'))\n>>> df\n     name   class  max_speed\n0  falcon    bird      389.0\n1  parrot    bird       24.0\n2    lion  mammal       80.5\n3  monkey  mammal        NaN"", "">>> df.pop('class')\n0      bird\n1      bird\n2    mammal\n3    mammal\nName: class, dtype: object"", '>>> df\n     name  max_speed\n0  falcon      389.0\n1  parrot       24.0\n2    lion       80.5\n3  monkey        NaN']"
1805,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_quarter_start.html,pandas.tseries.offsets.FY5253Quarter.is_quarter_start,FY5253Quarter.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1806,..\pandas\reference\api\pandas.tseries.offsets.Week.is_month_start.html,pandas.tseries.offsets.Week.is_month_start,Week.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1807,..\pandas\reference\api\pandas.Series.str.isalpha.html,pandas.Series.str.isalpha,"Series.str.isalpha()[source]# Check whether all characters in each string are alphabetic. This is equivalent to running the Python string method str.isalpha() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1808,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.max.html,pandas.core.groupby.SeriesGroupBy.max,"SeriesGroupBy.max(numeric_only=False, min_count=-1, engine=None, engine_kwargs=None)[source]# Compute max of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. enginestr, default None None 'cython' : Runs rolling apply through C-extensions from cython. 'numba'Runs rolling apply through JIT compiled code from numba.Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogiland parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply groupby aggregation. Returns: Series or DataFrameComputed max of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).max()\na    2\nb    4\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").max()\n    b  c\na\n1   8  5\n2   6  9']"
1809,..\pandas\reference\api\pandas.Series.abs.html,pandas.Series.abs,"Series.abs()[source]# Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Notes For complex inputs, 1.2 + 1j, the absolute value is \(\sqrt{ a^2 + b^2 }\).",Returns: absSeries/DataFrame containing the absolute value of each element.,"['>>> s = pd.Series([-1.10, 2, -3.33, 4])\n>>> s.abs()\n0    1.10\n1    2.00\n2    3.33\n3    4.00\ndtype: float64', '>>> s = pd.Series([1.2 + 1j])\n>>> s.abs()\n0    1.56205\ndtype: float64', "">>> s = pd.Series([pd.Timedelta('1 days')])\n>>> s.abs()\n0   1 days\ndtype: timedelta64[ns]"", "">>> df = pd.DataFrame({\n...     'a': [4, 5, 6, 7],\n...     'b': [10, 20, 30, 40],\n...     'c': [100, 50, -30, -50]\n... })\n>>> df\n     a    b    c\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n>>> df.loc[(df.c - 43).abs().argsort()]\n     a    b    c\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50""]"
1810,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_anchored.html,pandas.tseries.offsets.BusinessHour.is_anchored,BusinessHour.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1811,..\pandas\reference\api\pandas.Index.nbytes.html,pandas.Index.nbytes,property Index.nbytes[source]# Return the number of bytes in the underlying data.,No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.nbytes\n24"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.nbytes\n24""]"
1812,..\pandas\reference\api\pandas.DataFrame.pow.html,pandas.DataFrame.pow,"DataFrame.pow(other, axis='columns', level=None, fill_value=None)[source]# Get Exponential power of dataframe and other, element-wise (binary operator pow). Equivalent to dataframe ** other, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rpow. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1813,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.mean.html,pandas.core.groupby.SeriesGroupBy.mean,"SeriesGroupBy.mean(numeric_only=False, engine=None, engine_kwargs=None)[source]# Compute mean of groups, excluding missing values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None and defaults to False. enginestr, default None 'cython' : Runs the operation through C-extensions from cython. 'numba' : Runs the operation through JIT compiled code from numba. None : Defaults to 'cython' or globally setting compute.use_numba Added in version 1.4.0. engine_kwargsdict, default None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogil and parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {{'nopython': True, 'nogil': False, 'parallel': False}} Added in version 1.4.0. Returns: pandas.Series or pandas.DataFrame","["">>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n...                    'B': [np.nan, 2, 3, 4, 5],\n...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])"", "">>> df.groupby('A').mean()\n     B         C\nA\n1  3.0  1.333333\n2  4.0  1.500000"", "">>> df.groupby(['A', 'B']).mean()\n         C\nA B\n1 2.0  2.0\n  4.0  1.0\n2 3.0  1.0\n  5.0  2.0"", "">>> df.groupby('A')['B'].mean()\nA\n1    3.0\n2    4.0\nName: B, dtype: float64""]"
1814,..\pandas\reference\api\pandas.Series.str.isdecimal.html,pandas.Series.str.isdecimal,"Series.str.isdecimal()[source]# Check whether all characters in each string are decimal. This is equivalent to running the Python string method str.isdecimal() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1815,..\pandas\reference\api\pandas.tseries.offsets.Week.is_on_offset.html,pandas.tseries.offsets.Week.is_on_offset,Week.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1816,..\pandas\reference\api\pandas.Series.add.html,pandas.Series.add,"Series.add(other, level=None, fill_value=None, axis=0)[source]# Return Addition of series and other, element-wise (binary operator add). Equivalent to series + other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.add(b, fill_value=0)\na    2.0\nb    1.0\nc    1.0\nd    1.0\ne    NaN\ndtype: float64""]"
1817,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_year_end.html,pandas.tseries.offsets.FY5253Quarter.is_year_end,FY5253Quarter.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1818,..\pandas\reference\api\pandas.Index.ndim.html,pandas.Index.ndim,"property Index.ndim[source]# Number of dimensions of the underlying data, by definition 1.",No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.ndim\n1"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.ndim\n1""]"
1819,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_month_end.html,pandas.tseries.offsets.BusinessHour.is_month_end,BusinessHour.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1820,..\pandas\reference\api\pandas.DataFrame.prod.html,pandas.DataFrame.prod,"DataFrame.prod(axis=0, skipna=True, numeric_only=False, min_count=0, **kwargs)[source]# Return the product of the values over the requested axis.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","['>>> pd.Series([], dtype=""float64"").prod()\n1.0', '>>> pd.Series([], dtype=""float64"").prod(min_count=1)\nnan', '>>> pd.Series([np.nan]).prod()\n1.0', '>>> pd.Series([np.nan]).prod(min_count=1)\nnan']"
1821,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.median.html,pandas.core.groupby.SeriesGroupBy.median,"SeriesGroupBy.median(numeric_only=False)[source]# Compute median of groups, excluding missing values. For multiple groupings, the result index will be a MultiIndex","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None and defaults to False. Returns: Series or DataFrameMedian of values within each group.","["">>> lst = ['a', 'a', 'a', 'b', 'b', 'b']\n>>> ser = pd.Series([7, 2, 8, 4, 3, 3], index=lst)\n>>> ser\na     7\na     2\na     8\nb     4\nb     3\nb     3\ndtype: int64\n>>> ser.groupby(level=0).median()\na    7.0\nb    3.0\ndtype: float64"", "">>> data = {'a': [1, 3, 5, 7, 7, 8, 3], 'b': [1, 4, 8, 4, 4, 2, 1]}\n>>> df = pd.DataFrame(data, index=['dog', 'dog', 'dog',\n...                   'mouse', 'mouse', 'mouse', 'mouse'])\n>>> df\n         a  b\n  dog    1  1\n  dog    3  4\n  dog    5  8\nmouse    7  4\nmouse    7  4\nmouse    8  2\nmouse    3  1\n>>> df.groupby(level=0).median()\n         a    b\ndog    3.0  4.0\nmouse  7.0  3.0"", "">>> ser = pd.Series([1, 2, 3, 3, 4, 5],\n...                 index=pd.DatetimeIndex(['2023-01-01',\n...                                         '2023-01-10',\n...                                         '2023-01-15',\n...                                         '2023-02-01',\n...                                         '2023-02-10',\n...                                         '2023-02-15']))\n>>> ser.resample('MS').median()\n2023-01-01    2.0\n2023-02-01    4.0\nFreq: MS, dtype: float64""]"
1822,..\pandas\reference\api\pandas.Index.notna.html,pandas.Index.notna,"final Index.notna()[source]# Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or numpy.inf are not considered NA values. NA values, such as None or numpy.NaN, get mapped to False values.",Returns: numpy.ndarray[bool]Boolean array to indicate which entries are not NA.,"["">>> idx = pd.Index([5.2, 6.0, np.nan])\n>>> idx\nIndex([5.2, 6.0, nan], dtype='float64')\n>>> idx.notna()\narray([ True,  True, False])"", "">>> idx = pd.Index(['black', '', 'red', None])\n>>> idx\nIndex(['black', '', 'red', None], dtype='object')\n>>> idx.notna()\narray([ True,  True,  True, False])""]"
1823,..\pandas\reference\api\pandas.DataFrame.product.html,pandas.DataFrame.product,"DataFrame.product(axis=0, skipna=True, numeric_only=False, min_count=0, **kwargs)[source]# Return the product of the values over the requested axis.","Parameters: axis{index (0), columns (1)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.prod with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargsAdditional keyword arguments to be passed to the function. Returns: Series or scalar","['>>> pd.Series([], dtype=""float64"").prod()\n1.0', '>>> pd.Series([], dtype=""float64"").prod(min_count=1)\nnan', '>>> pd.Series([np.nan]).prod()\n1.0', '>>> pd.Series([np.nan]).prod(min_count=1)\nnan']"
1824,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.is_year_start.html,pandas.tseries.offsets.FY5253Quarter.is_year_start,FY5253Quarter.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1825,..\pandas\reference\api\pandas.Series.add_prefix.html,pandas.Series.add_prefix,"Series.add_prefix(prefix, axis=None)[source]# Prefix labels with string prefix. For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed.","Parameters: prefixstrThe string to add before each label. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneAxis to add prefix on Added in version 2.0.0. Returns: Series or DataFrameNew Series or DataFrame with updated labels.","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', "">>> s.add_prefix('item_')\nitem_0    1\nitem_1    2\nitem_2    3\nitem_3    4\ndtype: int64"", "">>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6"", "">>> df.add_prefix('col_')\n     col_A  col_B\n0       1       3\n1       2       4\n2       3       5\n3       4       6""]"
1826,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_month_start.html,pandas.tseries.offsets.BusinessHour.is_month_start,BusinessHour.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1827,..\pandas\reference\api\pandas.tseries.offsets.Week.is_quarter_end.html,pandas.tseries.offsets.Week.is_quarter_end,Week.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1828,..\pandas\reference\api\pandas.core.groupby.SeriesGroupBy.min.html,pandas.core.groupby.SeriesGroupBy.min,"SeriesGroupBy.min(numeric_only=False, min_count=-1, engine=None, engine_kwargs=None)[source]# Compute min of group values.","Parameters: numeric_onlybool, default FalseInclude only float, int, boolean columns. Changed in version 2.0.0: numeric_only no longer accepts None. min_countint, default -1The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. enginestr, default None None 'cython' : Runs rolling apply through C-extensions from cython. 'numba'Runs rolling apply through JIT compiled code from numba.Only available when raw is set to True. None : Defaults to 'cython' or globally setting compute.use_numba engine_kwargsdict, default None None For 'cython' engine, there are no accepted engine_kwargs For 'numba' engine, the engine can accept nopython, nogiland parallel dictionary keys. The values must either be True or False. The default engine_kwargs for the 'numba' engine is {'nopython': True, 'nogil': False, 'parallel': False} and will be applied to both the func and the apply groupby aggregation. Returns: Series or DataFrameComputed min of values within each group.","["">>> lst = ['a', 'a', 'b', 'b']\n>>> ser = pd.Series([1, 2, 3, 4], index=lst)\n>>> ser\na    1\na    2\nb    3\nb    4\ndtype: int64\n>>> ser.groupby(level=0).min()\na    1\nb    3\ndtype: int64"", '>>> data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n>>> df = pd.DataFrame(data, columns=[""a"", ""b"", ""c""],\n...                   index=[""tiger"", ""leopard"", ""cheetah"", ""lion""])\n>>> df\n          a  b  c\n  tiger   1  8  2\nleopard   1  2  5\ncheetah   2  5  8\n   lion   2  6  9\n>>> df.groupby(""a"").min()\n    b  c\na\n1   2  2\n2   5  8']"
1829,..\pandas\reference\api\pandas.Series.str.isdigit.html,pandas.Series.str.isdigit,"Series.str.isdigit()[source]# Check whether all characters in each string are digits. This is equivalent to running the Python string method str.isdigit() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1830,..\pandas\reference\api\pandas.Series.add_suffix.html,pandas.Series.add_suffix,"Series.add_suffix(suffix, axis=None)[source]# Suffix labels with string suffix. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.","Parameters: suffixstrThe string to add after each label. axis{0 or ‘index’, 1 or ‘columns’, None}, default NoneAxis to add suffix on Added in version 2.0.0. Returns: Series or DataFrameNew Series or DataFrame with updated labels.","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', "">>> s.add_suffix('_item')\n0_item    1\n1_item    2\n2_item    3\n3_item    4\ndtype: int64"", "">>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6"", "">>> df.add_suffix('_col')\n     A_col  B_col\n0       1       3\n1       2       4\n2       3       5\n3       4       6""]"
1831,..\pandas\reference\api\pandas.DataFrame.quantile.html,pandas.DataFrame.quantile,"DataFrame.quantile(q=0.5, axis=0, numeric_only=False, interpolation='linear', method='single')[source]# Return values at the given quantile over requested axis.","Parameters: qfloat or array-like, default 0.5 (50% quantile)Value between 0 <= q <= 1, the quantile(s) to compute. axis{0 or ‘index’, 1 or ‘columns’}, default 0Equals 0 or ‘index’ for row-wise, 1 or ‘columns’ for column-wise. numeric_onlybool, default FalseInclude only float, int or boolean data. Changed in version 2.0.0: The default value of numeric_only is now False. interpolation{‘linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j: linear: i + (j - i) * fraction, where fraction is the fractional part of the index surrounded by i and j. lower: i. higher: j. nearest: i or j whichever is nearest. midpoint: (i + j) / 2. method{‘single’, ‘table’}, default ‘single’Whether to compute quantiles per-column (‘single’) or over all columns (‘table’). When ‘table’, the only allowed interpolation methods are ‘nearest’, ‘lower’, and ‘higher’. Returns: Series or DataFrame If q is an array, a DataFrame will be returned where theindex is q, the columns are the columns of self, and the values are the quantiles. If q is a float, a Series will be returned where theindex is the columns of self and the values are the quantiles.","["">>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\n...                   columns=['a', 'b'])\n>>> df.quantile(.1)\na    1.3\nb    3.7\nName: 0.1, dtype: float64\n>>> df.quantile([.1, .5])\n       a     b\n0.1  1.3   3.7\n0.5  2.5  55.0"", '>>> df.quantile(.1, method=""table"", interpolation=""nearest"")\na    1\nb    1\nName: 0.1, dtype: int64\n>>> df.quantile([.1, .5], method=""table"", interpolation=""nearest"")\n     a    b\n0.1  1    1\n0.5  3  100', "">>> df = pd.DataFrame({'A': [1, 2],\n...                    'B': [pd.Timestamp('2010'),\n...                          pd.Timestamp('2011')],\n...                    'C': [pd.Timedelta('1 days'),\n...                          pd.Timedelta('2 days')]})\n>>> df.quantile(0.5, numeric_only=False)\nA                    1.5\nB    2010-07-02 12:00:00\nC        1 days 12:00:00\nName: 0.5, dtype: object""]"
1832,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.kwds.html,pandas.tseries.offsets.FY5253Quarter.kwds,FY5253Quarter.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1833,..\pandas\reference\api\pandas.tseries.offsets.Week.is_quarter_start.html,pandas.tseries.offsets.Week.is_quarter_start,Week.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1834,..\pandas\reference\api\pandas.Series.str.islower.html,pandas.Series.str.islower,"Series.str.islower()[source]# Check whether all characters in each string are lowercase. This is equivalent to running the Python string method str.islower() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1835,..\pandas\reference\api\pandas.Index.nunique.html,pandas.Index.nunique,Index.nunique(dropna=True)[source]# Return number of unique elements in the object. Excludes NA values by default.,"Parameters: dropnabool, default TrueDon’t include NaN in the count. Returns: int","['>>> s = pd.Series([1, 3, 5, 7, 7])\n>>> s\n0    1\n1    3\n2    5\n3    7\n4    7\ndtype: int64', '>>> s.nunique()\n4']"
1836,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_on_offset.html,pandas.tseries.offsets.BusinessHour.is_on_offset,BusinessHour.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1837,..\pandas\reference\api\pandas.Series.agg.html,pandas.Series.agg,"Series.agg(func=None, axis=0, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', "">>> s.agg('min')\n1"", "">>> s.agg(['min', 'max'])\nmin   1\nmax   4\ndtype: int64""]"
1838,..\pandas\reference\api\pandas.DataFrame.query.html,pandas.DataFrame.query,"DataFrame.query(expr, *, inplace=False, **kwargs)[source]# Query the columns of a DataFrame with a boolean expression. Notes The result of the evaluation of this expression is first passed to DataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to DataFrame.__getitem__(). This method uses the top-level eval() function to evaluate the passed query. The query() method uses a slightly modified Python syntax by default. For example, the & and | (bitwise) operators have the precedence of their boolean cousins, and and or. This is syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument parser='python'. This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python' to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using numexpr as the engine. The DataFrame.index and DataFrame.columns attributes of the DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the query documentation in indexing. Backtick quoted variables Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, `it's` > `that's` will raise an error, as it forms a quoted string ('s > `that') with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in pandas.core.computation.parsing.","Parameters: exprstrThe query string to evaluate. You can refer to variables in the environment by prefixing them with an ‘@’ character like @a + b. You can refer to column names that are not valid Python variable names by surrounding them in backticks. Thus, column names containing spaces or punctuations (besides underscores) or starting with digits must be surrounded by backticks. (For example, a column named “Area (cm^2)” would be referenced as `Area (cm^2)`). Column names which are Python keywords (like “list”, “for”, “import”, etc) cannot be used. For example, if one of your columns is called a a and you want to sum it with b, your query should be `a a` + b. inplaceboolWhether to modify the DataFrame rather than creating a new one. **kwargsSee the documentation for eval() for complete details on the keyword arguments accepted by DataFrame.query(). Returns: DataFrame or NoneDataFrame resulting from the provided query expression or None if inplace=True.","["">>> df = pd.DataFrame({'A': range(1, 6),\n...                    'B': range(10, 0, -2),\n...                    'C C': range(10, 5, -1)})\n>>> df\n   A   B  C C\n0  1  10   10\n1  2   8    9\n2  3   6    8\n3  4   4    7\n4  5   2    6\n>>> df.query('A > B')\n   A  B  C C\n4  5  2    6"", '>>> df[df.A > df.B]\n   A  B  C C\n4  5  2    6', "">>> df.query('B == `C C`')\n   A   B  C C\n0  1  10   10"", "">>> df[df.B == df['C C']]\n   A   B  C C\n0  1  10   10""]"
1839,..\pandas\reference\api\pandas.Series.aggregate.html,pandas.Series.aggregate,"Series.aggregate(func=None, axis=0, *args, **kwargs)[source]# Aggregate using one or more operations over the specified axis. Notes The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions (mean, median, prod, sum, std, var), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0). agg is an alias for aggregate. Use the alias. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details. A passed user-defined-function will be passed a Series for evaluation.","Parameters: funcfunction, str, list or dictFunction to use for aggregating the data. If a function, must either work when passed a Series or when passed to Series.apply. Accepted combinations are: function string function name list of functions and/or function names, e.g. [np.sum, 'mean'] dict of axis labels -> functions, function names or list of such. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. *argsPositional arguments to pass to func. **kwargsKeyword arguments to pass to func. Returns: scalar, Series or DataFrameThe return can be: scalar : when Series.agg is called with single function Series : when DataFrame.agg is called with a single function DataFrame : when DataFrame.agg is called with several functions","['>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64', "">>> s.agg('min')\n1"", "">>> s.agg(['min', 'max'])\nmin   1\nmax   4\ndtype: int64""]"
1840,..\pandas\reference\api\pandas.Series.str.isnumeric.html,pandas.Series.str.isnumeric,"Series.str.isnumeric()[source]# Check whether all characters in each string are numeric. This is equivalent to running the Python string method str.isnumeric() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1841,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.n.html,pandas.tseries.offsets.FY5253Quarter.n,FY5253Quarter.n#,No parameters found,[]
1842,..\pandas\reference\api\pandas.Index.putmask.html,pandas.Index.putmask,"Index.putmask(mask, value)[source]# Return a new Index of the values set with the mask.",Returns: Index,"["">>> idx1 = pd.Index([1, 2, 3])\n>>> idx2 = pd.Index([5, 6, 7])\n>>> idx1.putmask([True, False, False], idx2)\nIndex([5, 2, 3], dtype='int64')""]"
1843,..\pandas\reference\api\pandas.tseries.offsets.Week.is_year_end.html,pandas.tseries.offsets.Week.is_year_end,Week.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1844,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_quarter_end.html,pandas.tseries.offsets.BusinessHour.is_quarter_end,BusinessHour.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1845,..\pandas\reference\api\pandas.DataFrame.radd.html,pandas.DataFrame.radd,"DataFrame.radd(other, axis='columns', level=None, fill_value=None)[source]# Get Addition of dataframe and other, element-wise (binary operator radd). Equivalent to other + dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, add. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1846,..\pandas\reference\api\pandas.Series.align.html,pandas.Series.align,"Series.align(other, join='outer', axis=None, level=None, copy=None, fill_value=None, method=<no_default>, limit=<no_default>, fill_axis=<no_default>, broadcast_axis=<no_default>)[source]# Align two objects on their axes with the specified join method. Join method is specified for each axis Index.","Parameters: otherDataFrame or Series join{‘outer’, ‘inner’, ‘left’, ‘right’}, default ‘outer’Type of alignment to be performed. left: use only keys from left frame, preserve key order. right: use only keys from right frame, preserve key order. outer: use union of keys from both frames, sort keys lexicographically. inner: use intersection of keys from both frames, preserve the order of the left keys. axisallowed axis of the other object, default NoneAlign on index (0), columns (1), or both (None). levelint or level name, default NoneBroadcast across a level, matching Index values on the passed MultiIndex level. copybool, default TrueAlways returns new objects. If copy=False and no reindexing is required then original objects are returned. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any “compatible” value. method{‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default NoneMethod to use for filling holes in reindexed Series: pad / ffill: propagate last valid observation forward to next valid. backfill / bfill: use NEXT valid observation to fill gap. Deprecated since version 2.1. limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. Deprecated since version 2.1. fill_axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrame, default 0Filling axis, method and limit. Deprecated since version 2.1. broadcast_axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrame, default NoneBroadcast values along this axis, if aligning two objects of different dimensions. Deprecated since version 2.1. Returns: tuple of (Series/DataFrame, type of other)Aligned objects.","['>>> df = pd.DataFrame(\n...     [[1, 2, 3, 4], [6, 7, 8, 9]], columns=[""D"", ""B"", ""E"", ""A""], index=[1, 2]\n... )\n>>> other = pd.DataFrame(\n...     [[10, 20, 30, 40], [60, 70, 80, 90], [600, 700, 800, 900]],\n...     columns=[""A"", ""B"", ""C"", ""D""],\n...     index=[2, 3, 4],\n... )\n>>> df\n   D  B  E  A\n1  1  2  3  4\n2  6  7  8  9\n>>> other\n    A    B    C    D\n2   10   20   30   40\n3   60   70   80   90\n4  600  700  800  900', '>>> left, right = df.align(other, join=""outer"", axis=1)\n>>> left\n   A  B   C  D  E\n1  4  2 NaN  1  3\n2  9  7 NaN  6  8\n>>> right\n    A    B    C    D   E\n2   10   20   30   40 NaN\n3   60   70   80   90 NaN\n4  600  700  800  900 NaN', '>>> left, right = df.align(other, join=""outer"", axis=0)\n>>> left\n    D    B    E    A\n1  1.0  2.0  3.0  4.0\n2  6.0  7.0  8.0  9.0\n3  NaN  NaN  NaN  NaN\n4  NaN  NaN  NaN  NaN\n>>> right\n    A      B      C      D\n1    NaN    NaN    NaN    NaN\n2   10.0   20.0   30.0   40.0\n3   60.0   70.0   80.0   90.0\n4  600.0  700.0  800.0  900.0', '>>> left, right = df.align(other, join=""outer"", axis=None)\n>>> left\n     A    B   C    D    E\n1  4.0  2.0 NaN  1.0  3.0\n2  9.0  7.0 NaN  6.0  8.0\n3  NaN  NaN NaN  NaN  NaN\n4  NaN  NaN NaN  NaN  NaN\n>>> right\n       A      B      C      D   E\n1    NaN    NaN    NaN    NaN NaN\n2   10.0   20.0   30.0   40.0 NaN\n3   60.0   70.0   80.0   90.0 NaN\n4  600.0  700.0  800.0  900.0 NaN']"
1847,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_quarter_start.html,pandas.tseries.offsets.BusinessHour.is_quarter_start,BusinessHour.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1848,..\pandas\reference\api\pandas.Series.str.isspace.html,pandas.Series.str.isspace,"Series.str.isspace()[source]# Check whether all characters in each string are whitespace. This is equivalent to running the Python string method str.isspace() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1849,..\pandas\reference\api\pandas.Index.ravel.html,pandas.Index.ravel,final Index.ravel(order='C')[source]# Return a view on self.,Returns: Index,"["">>> s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n>>> s.index.ravel()\nIndex(['a', 'b', 'c'], dtype='object')""]"
1850,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.name.html,pandas.tseries.offsets.FY5253Quarter.name,FY5253Quarter.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1851,..\pandas\reference\api\pandas.tseries.offsets.Week.is_year_start.html,pandas.tseries.offsets.Week.is_year_start,Week.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1852,..\pandas\reference\api\pandas.Series.all.html,pandas.Series.all,"Series.all(axis=0, bool_only=False, skipna=True, **kwargs)[source]# Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty).","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Indicate which axis or axes should be reduced. For Series this parameter is unused and defaults to 0. 0 / ‘index’ : reduce the index, return a Series whose index is the original column labels. 1 / ‘columns’ : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series. skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargsany, default NoneAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: scalar or SeriesIf level is specified, then, Series is returned; otherwise, scalar is returned.","['>>> pd.Series([True, True]).all()\nTrue\n>>> pd.Series([True, False]).all()\nFalse\n>>> pd.Series([], dtype=""float64"").all()\nTrue\n>>> pd.Series([np.nan]).all()\nTrue\n>>> pd.Series([np.nan]).all(skipna=False)\nTrue', "">>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n>>> df\n   col1   col2\n0  True   True\n1  True  False"", '>>> df.all()\ncol1     True\ncol2    False\ndtype: bool', "">>> df.all(axis='columns')\n0     True\n1    False\ndtype: bool"", '>>> df.all(axis=None)\nFalse']"
1853,..\pandas\reference\api\pandas.DataFrame.rank.html,pandas.DataFrame.rank,"DataFrame.rank(axis=0, method='average', numeric_only=False, na_option='keep', ascending=True, pct=False)[source]# Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values.","Parameters: axis{0 or ‘index’, 1 or ‘columns’}, default 0Index to direct ranking. For Series this parameter is unused and defaults to 0. method{‘average’, ‘min’, ‘max’, ‘first’, ‘dense’}, default ‘average’How to rank the group of records that have the same value (i.e. ties): average: average rank of the group min: lowest rank in the group max: highest rank in the group first: ranks assigned in order they appear in the array dense: like ‘min’, but rank always increases by 1 between groups. numeric_onlybool, default FalseFor DataFrame objects, rank only numeric columns if set to True. Changed in version 2.0.0: The default value of numeric_only is now False. na_option{‘keep’, ‘top’, ‘bottom’}, default ‘keep’How to rank NaN values: keep: assign NaN rank to NaN values top: assign lowest rank to NaN values bottom: assign highest rank to NaN values ascendingbool, default TrueWhether or not the elements should be ranked in ascending order. pctbool, default FalseWhether or not to display the returned rankings in percentile form. Returns: same type as callerReturn a Series or DataFrame with data ranks as values.","["">>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog',\n...                                    'spider', 'snake'],\n...                         'Number_legs': [4, 2, 4, 8, np.nan]})\n>>> df\n    Animal  Number_legs\n0      cat          4.0\n1  penguin          2.0\n2      dog          4.0\n3   spider          8.0\n4    snake          NaN"", '>>> s = pd.Series(range(5), index=list(""abcde""))\n>>> s[""d""] = s[""b""]\n>>> s.rank()\na    1.0\nb    2.5\nc    4.0\nd    2.5\ne    5.0\ndtype: float64', "">>> df['default_rank'] = df['Number_legs'].rank()\n>>> df['max_rank'] = df['Number_legs'].rank(method='max')\n>>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom')\n>>> df['pct_rank'] = df['Number_legs'].rank(pct=True)\n>>> df\n    Animal  Number_legs  default_rank  max_rank  NA_bottom  pct_rank\n0      cat          4.0           2.5       3.0        2.5     0.625\n1  penguin          2.0           1.0       1.0        1.0     0.250\n2      dog          4.0           2.5       3.0        2.5     0.625\n3   spider          8.0           4.0       4.0        4.0     1.000\n4    snake          NaN           NaN       NaN        5.0       NaN""]"
1854,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_year_end.html,pandas.tseries.offsets.BusinessHour.is_year_end,BusinessHour.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1855,..\pandas\reference\api\pandas.Series.any.html,pandas.Series.any,"Series.any(*, axis=0, bool_only=False, skipna=True, **kwargs)[source]# Return whether any element is True, potentially over an axis. Returns False unless there is at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).","Parameters: axis{0 or ‘index’, 1 or ‘columns’, None}, default 0Indicate which axis or axes should be reduced. For Series this parameter is unused and defaults to 0. 0 / ‘index’ : reduce the index, return a Series whose index is the original column labels. 1 / ‘columns’ : reduce the columns, return a Series whose index is the original index. None : reduce all axes, return a scalar. bool_onlybool, default FalseInclude only boolean columns. Not implemented for Series. skipnabool, default TrueExclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. **kwargsany, default NoneAdditional keywords have no effect but might be accepted for compatibility with NumPy. Returns: scalar or SeriesIf level is specified, then, Series is returned; otherwise, scalar is returned.","['>>> pd.Series([False, False]).any()\nFalse\n>>> pd.Series([True, False]).any()\nTrue\n>>> pd.Series([], dtype=""float64"").any()\nFalse\n>>> pd.Series([np.nan]).any()\nFalse\n>>> pd.Series([np.nan]).any(skipna=False)\nTrue', '>>> df = pd.DataFrame({""A"": [1, 2], ""B"": [0, 2], ""C"": [0, 0]})\n>>> df\n   A  B  C\n0  1  0  0\n1  2  2  0', '>>> df.any()\nA     True\nB     True\nC    False\ndtype: bool', '>>> df = pd.DataFrame({""A"": [True, False], ""B"": [1, 2]})\n>>> df\n       A  B\n0   True  1\n1  False  2', "">>> df.any(axis='columns')\n0    True\n1    True\ndtype: bool"", '>>> df = pd.DataFrame({""A"": [True, False], ""B"": [1, 0]})\n>>> df\n       A  B\n0   True  1\n1  False  0', "">>> df.any(axis='columns')\n0    True\n1    False\ndtype: bool"", '>>> df.any(axis=None)\nTrue', '>>> pd.DataFrame([]).any()\nSeries([], dtype: bool)']"
1856,..\pandas\reference\api\pandas.tseries.offsets.Week.kwds.html,pandas.tseries.offsets.Week.kwds,Week.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1857,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.nanos.html,pandas.tseries.offsets.FY5253Quarter.nanos,FY5253Quarter.nanos#,No parameters found,[]
1858,..\pandas\reference\api\pandas.DataFrame.rdiv.html,pandas.DataFrame.rdiv,"DataFrame.rdiv(other, axis='columns', level=None, fill_value=None)[source]# Get Floating division of dataframe and other, element-wise (binary operator rtruediv). Equivalent to other / dataframe, but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv. Among flexible wrappers (add, sub, mul, div, floordiv, mod, pow) to arithmetic operators: +, -, *, /, //, %, **. Notes Mismatched indices will be unioned together.","Parameters: otherscalar, sequence, Series, dict or DataFrameAny single or multiple element data structure, or list-like object. axis{0 or ‘index’, 1 or ‘columns’}Whether to compare by the index (0 or ‘index’) or columns. (1 or ‘columns’). For Series input, axis to match Series index on. levelint or labelBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuefloat or None, default NoneFill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns: DataFrameResult of the arithmetic operation.","["">>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360"", '>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361', '>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0', '>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778', '>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358', "">>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358"", "">>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359"", "">>> df.mul({'angles': 0, 'degrees': 2})\n            angles  degrees\ncircle           0      720\ntriangle         0      360\nrectangle        0      720"", "">>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles  degrees\ncircle           0        0\ntriangle         6      360\nrectangle       12     1080"", "">>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4"", '>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN', '>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0', "">>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720"", '>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0']"
1859,..\pandas\reference\api\pandas.Index.reindex.html,pandas.Index.reindex,"Index.reindex(target, method=None, level=None, limit=None, tolerance=None)[source]# Create index with target’s values.","Parameters: targetan iterable method{None, ‘pad’/’ffill’, ‘backfill’/’bfill’, ‘nearest’}, optional default: exact matches only. pad / ffill: find the PREVIOUS index value if no exact match. backfill / bfill: use NEXT index value if no exact match nearest: use the NEAREST index value if no exact match. Tied distances are broken by preferring the larger index value. levelint, optionalLevel of multiindex. limitint, optionalMaximum number of consecutive labels in target to match for inexact matches. toleranceint or float, optionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: new_indexpd.IndexResulting index. indexernp.ndarray[np.intp] or NoneIndices of output values in original index. Raises: TypeErrorIf method passed along with level. ValueErrorIf non-unique multi-index ValueErrorIf non-unique index and method or limit passed.","["">>> idx = pd.Index(['car', 'bike', 'train', 'tractor'])\n>>> idx\nIndex(['car', 'bike', 'train', 'tractor'], dtype='object')\n>>> idx.reindex(['car', 'bike'])\n(Index(['car', 'bike'], dtype='object'), array([0, 1]))""]"
1860,..\pandas\reference\api\pandas.Series.str.istitle.html,pandas.Series.str.istitle,"Series.str.istitle()[source]# Check whether all characters in each string are titlecase. This is equivalent to running the Python string method str.istitle() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1861,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.is_year_start.html,pandas.tseries.offsets.BusinessHour.is_year_start,BusinessHour.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1862,..\pandas\reference\api\pandas.Series.apply.html,pandas.Series.apply,"Series.apply(func, convert_dtype=<no_default>, args=(), *, by_row='compat', **kwargs)[source]# Invoke function on values of Series. Can be ufunc (a NumPy function that applies to the entire Series) or a Python function that only works on single values. Notes Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See Mutating with User Defined Function (UDF) methods for more details.","Parameters: funcfunctionPython function or NumPy ufunc to apply. convert_dtypebool, default TrueTry to find better dtype for elementwise function results. If False, leave as dtype=object. Note that the dtype is always preserved for some extension array dtypes, such as Categorical. Deprecated since version 2.1.0: convert_dtype has been deprecated. Do ser.astype(object).apply() instead if you want convert_dtype=False. argstuplePositional arguments passed to func after the series value. by_rowFalse or “compat”, default “compat”If ""compat"" and func is a callable, func will be passed each element of the Series, like Series.map. If func is a list or dict of callables, will first try to translate each func into pandas methods. If that doesn’t work, will try call to apply again with by_row=""compat"" and if that fails, will call apply again with by_row=False (backward compatible). If False, the func will be passed the whole Series at once. by_row has no effect when func is a string. Added in version 2.1.0. **kwargsAdditional keyword arguments passed to func. Returns: Series or DataFrameIf func returns a Series object the result will be a DataFrame.","["">>> s = pd.Series([20, 21, 12],\n...               index=['London', 'New York', 'Helsinki'])\n>>> s\nLondon      20\nNew York    21\nHelsinki    12\ndtype: int64"", '>>> def square(x):\n...     return x ** 2\n>>> s.apply(square)\nLondon      400\nNew York    441\nHelsinki    144\ndtype: int64', '>>> s.apply(lambda x: x ** 2)\nLondon      400\nNew York    441\nHelsinki    144\ndtype: int64', '>>> def subtract_custom_value(x, custom_value):\n...     return x - custom_value', '>>> s.apply(subtract_custom_value, args=(5,))\nLondon      15\nNew York    16\nHelsinki     7\ndtype: int64', '>>> def add_custom_values(x, **kwargs):\n...     for month in kwargs:\n...         x += kwargs[month]\n...     return x', '>>> s.apply(add_custom_values, june=30, july=20, august=25)\nLondon      95\nNew York    96\nHelsinki    87\ndtype: int64', '>>> s.apply(np.log)\nLondon      2.995732\nNew York    3.044522\nHelsinki    2.484907\ndtype: float64']"
1863,..\pandas\reference\api\pandas.tseries.offsets.Week.n.html,pandas.tseries.offsets.Week.n,Week.n#,No parameters found,[]
1864,..\pandas\reference\api\pandas.Series.str.isupper.html,pandas.Series.str.isupper,"Series.str.isupper()[source]# Check whether all characters in each string are uppercase. This is equivalent to running the Python string method str.isupper() for each element of the Series/Index. If a string has zero characters, False is returned for that check.",Returns: Series or Index of boolSeries or Index of boolean values with the same length as the original Series/Index.,"["">>> s1 = pd.Series(['one', 'one1', '1', ''])"", '>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool"", "">>> s3 = pd.Series(['23', '³', '⅕', ''])"", '>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool', '>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool', "">>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool"", "">>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])"", '>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool', '>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool', '>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool']"
1865,..\pandas\reference\api\pandas.Index.rename.html,pandas.Index.rename,"Index.rename(name, *, inplace=False)[source]# Alter Index or MultiIndex name. Able to set new names without level. Defaults to returning new index. Length of names must match number of levels in MultiIndex.","Parameters: namelabel or list of labelsName(s) to set. inplacebool, default FalseModifies the object directly, instead of creating a new Index or MultiIndex. Returns: Index or NoneThe same type as the caller or None if inplace=True.","["">>> idx = pd.Index(['A', 'C', 'A', 'B'], name='score')\n>>> idx.rename('grade')\nIndex(['A', 'C', 'A', 'B'], dtype='object', name='grade')"", "">>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n...                                   [2018, 2019]],\n...                                   names=['kind', 'year'])\n>>> idx\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['kind', 'year'])\n>>> idx.rename(['species', 'year'])\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['species', 'year'])\n>>> idx.rename('species')\nTraceback (most recent call last):\nTypeError: Must pass list-like as `names`.""]"
1866,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.normalize.html,pandas.tseries.offsets.FY5253Quarter.normalize,FY5253Quarter.normalize#,No parameters found,[]
1867,..\pandas\reference\api\pandas.DataFrame.reindex.html,pandas.DataFrame.reindex,"DataFrame.reindex(labels=None, *, index=None, columns=None, axis=None, method=None, copy=None, level=None, fill_value=nan, limit=None, tolerance=None)[source]# Conform DataFrame to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.","Parameters: labelsarray-like, optionalNew labels / index to conform the axis specified by ‘axis’ to. indexarray-like, optionalNew labels for the index. Preferably an Index object to avoid duplicating data. columnsarray-like, optionalNew labels for the columns. Preferably an Index object to avoid duplicating data. axisint or str, optionalAxis to target. Can be either the axis name (‘index’, ‘columns’) or number (0, 1). method{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. None (default): don’t fill gaps pad / ffill: Propagate last valid observation forward to next valid. backfill / bfill: Use next valid observation to fill gap. nearest: Use nearest valid observations to fill gap. copybool, default TrueReturn a new object, even if the passed indexes are the same. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valuescalar, default np.nanValue to use for missing values. Defaults to NaN, but can be any “compatible” value. limitint, default NoneMaximum number of consecutive elements to forward or backward fill. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: DataFrame with changed index.","["">>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n...                   index=index)\n>>> df\n           http_status  response_time\nFirefox            200           0.04\nChrome             200           0.02\nSafari             404           0.07\nIE10               404           0.08\nKonqueror          301           1.00"", "">>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n...              'Chrome']\n>>> df.reindex(new_index)\n               http_status  response_time\nSafari               404.0           0.07\nIceweasel              NaN            NaN\nComodo Dragon          NaN            NaN\nIE10                 404.0           0.08\nChrome               200.0           0.02"", '>>> df.reindex(new_index, fill_value=0)\n               http_status  response_time\nSafari                 404           0.07\nIceweasel                0           0.00\nComodo Dragon            0           0.00\nIE10                   404           0.08\nChrome                 200           0.02', "">>> df.reindex(new_index, fill_value='missing')\n              http_status response_time\nSafari                404          0.07\nIceweasel         missing       missing\nComodo Dragon     missing       missing\nIE10                  404          0.08\nChrome                200          0.02"", "">>> df.reindex(columns=['http_status', 'user_agent'])\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN"", '>>> df.reindex([\'http_status\', \'user_agent\'], axis=""columns"")\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN', '>>> date_index = pd.date_range(\'1/1/2010\', periods=6, freq=\'D\')\n>>> df2 = pd.DataFrame({""prices"": [100, 101, np.nan, 100, 89, 88]},\n...                    index=date_index)\n>>> df2\n            prices\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0', "">>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n>>> df2.reindex(date_index2)\n            prices\n2009-12-29     NaN\n2009-12-30     NaN\n2009-12-31     NaN\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN"", "">>> df2.reindex(date_index2, method='bfill')\n            prices\n2009-12-29   100.0\n2009-12-30   100.0\n2009-12-31   100.0\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN""]"
1868,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.kwds.html,pandas.tseries.offsets.BusinessHour.kwds,BusinessHour.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1869,..\pandas\reference\api\pandas.Series.argmax.html,pandas.Series.argmax,"Series.argmax(axis=None, skipna=True, *args, **kwargs)[source]# Return int position of the largest value in the Series. If the maximum is achieved in multiple locations, the first row position is returned.","Parameters: axis{None}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values when showing the result. *args, **kwargsAdditional arguments and keywords for compatibility with NumPy. Returns: intRow position of the maximum value.","["">>> s = pd.Series({'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0})\n>>> s\nCorn Flakes              100.0\nAlmond Delight           110.0\nCinnamon Toast Crunch    120.0\nCocoa Puff               110.0\ndtype: float64"", '>>> s.argmax()\n2\n>>> s.argmin()\n0']"
1870,..\pandas\reference\api\pandas.Series.str.join.html,pandas.Series.str.join,"Series.str.join(sep)[source]# Join lists contained as elements in the Series/Index with passed delimiter. If the elements of a Series are lists themselves, join the content of these lists using the delimiter passed to the function. This function is an equivalent to str.join(). Notes If any of the list items is not a string object, the result of the join will be NaN.",Parameters: sepstrDelimiter to use between list entries. Returns: Series/Index: objectThe list entries concatenated by intervening occurrences of the delimiter. Raises: AttributeErrorIf the supplied Series contains neither strings nor lists.,"["">>> s = pd.Series([['lion', 'elephant', 'zebra'],\n...                [1.1, 2.2, 3.3],\n...                ['cat', np.nan, 'dog'],\n...                ['cow', 4.5, 'goat'],\n...                ['duck', ['swan', 'fish'], 'guppy']])\n>>> s\n0        [lion, elephant, zebra]\n1                [1.1, 2.2, 3.3]\n2                [cat, nan, dog]\n3               [cow, 4.5, goat]\n4    [duck, [swan, fish], guppy]\ndtype: object"", "">>> s.str.join('-')\n0    lion-elephant-zebra\n1                    NaN\n2                    NaN\n3                    NaN\n4                    NaN\ndtype: object""]"
1871,..\pandas\reference\api\pandas.tseries.offsets.Week.name.html,pandas.tseries.offsets.Week.name,Week.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1872,..\pandas\reference\api\pandas.Series.str.len.html,pandas.Series.str.len,"Series.str.len()[source]# Compute the length of each element in the Series/Index. The element may be a sequence (such as a string, tuple or list) or a collection (such as a dictionary).",Returns: Series or Index of intA Series or Index of integer values indicating the length of each element in the Series or Index.,"["">>> s = pd.Series(['dog',\n...                 '',\n...                 5,\n...                 {'foo' : 'bar'},\n...                 [2, 3, 5, 7],\n...                 ('one', 'two', 'three')])\n>>> s\n0                  dog\n1\n2                    5\n3       {'foo': 'bar'}\n4         [2, 3, 5, 7]\n5    (one, two, three)\ndtype: object\n>>> s.str.len()\n0    3.0\n1    0.0\n2    NaN\n3    1.0\n4    4.0\n5    3.0\ndtype: float64""]"
1873,..\pandas\reference\api\pandas.Index.repeat.html,pandas.Index.repeat,"Index.repeat(repeats, axis=None)[source]# Repeat elements of a Index. Returns a new Index where each element of the current Index is repeated consecutively a given number of times.",Parameters: repeatsint or array of intsThe number of repetitions for each element. This should be a non-negative integer. Repeating 0 times will return an empty Index. axisNoneMust be None. Has no effect but is accepted for compatibility with numpy. Returns: IndexNewly created Index with repeated elements.,"["">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx\nIndex(['a', 'b', 'c'], dtype='object')\n>>> idx.repeat(2)\nIndex(['a', 'a', 'b', 'b', 'c', 'c'], dtype='object')\n>>> idx.repeat([1, 2, 3])\nIndex(['a', 'b', 'b', 'c', 'c', 'c'], dtype='object')""]"
1874,..\pandas\reference\api\pandas.tseries.offsets.Week.nanos.html,pandas.tseries.offsets.Week.nanos,Week.nanos#,No parameters found,[]
1875,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.n.html,pandas.tseries.offsets.BusinessHour.n,BusinessHour.n#,No parameters found,[]
1876,..\pandas\reference\api\pandas.DataFrame.reindex_like.html,pandas.DataFrame.reindex_like,"DataFrame.reindex_like(other, method=None, copy=None, limit=None, tolerance=None)[source]# Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Notes Same as calling .reindex(index=other.index, columns=other.columns,...).","Parameters: otherObject of the same data typeIts row and column indices are used to define the new indices of this object. method{None, ‘backfill’/’bfill’, ‘pad’/’ffill’, ‘nearest’}Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. None (default): don’t fill gaps pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap nearest: use nearest valid observations to fill gap. copybool, default TrueReturn a new object, even if the passed indexes are the same. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True limitint, default NoneMaximum number of consecutive labels to fill for inexact matches. toleranceoptionalMaximum distance between original and new labels for inexact matches. The values of the index at the matching locations must satisfy the equation abs(index[indexer] - target) <= tolerance. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index’s type. Returns: Series or DataFrameSame type as caller, but with changed indices on each axis.","["">>> df1 = pd.DataFrame([[24.3, 75.7, 'high'],\n...                     [31, 87.8, 'high'],\n...                     [22, 71.6, 'medium'],\n...                     [35, 95, 'medium']],\n...                    columns=['temp_celsius', 'temp_fahrenheit',\n...                             'windspeed'],\n...                    index=pd.date_range(start='2014-02-12',\n...                                        end='2014-02-15', freq='D'))"", '>>> df1\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          24.3             75.7      high\n2014-02-13          31.0             87.8      high\n2014-02-14          22.0             71.6    medium\n2014-02-15          35.0             95.0    medium', "">>> df2 = pd.DataFrame([[28, 'low'],\n...                     [30, 'low'],\n...                     [35.1, 'medium']],\n...                    columns=['temp_celsius', 'windspeed'],\n...                    index=pd.DatetimeIndex(['2014-02-12', '2014-02-13',\n...                                            '2014-02-15']))"", '>>> df2\n            temp_celsius windspeed\n2014-02-12          28.0       low\n2014-02-13          30.0       low\n2014-02-15          35.1    medium', '>>> df2.reindex_like(df1)\n            temp_celsius  temp_fahrenheit windspeed\n2014-02-12          28.0              NaN       low\n2014-02-13          30.0              NaN       low\n2014-02-14           NaN              NaN       NaN\n2014-02-15          35.1              NaN    medium']"
1877,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.qtr_with_extra_week.html,pandas.tseries.offsets.FY5253Quarter.qtr_with_extra_week,FY5253Quarter.qtr_with_extra_week#,No parameters found,[]
1878,..\pandas\reference\api\pandas.Series.str.ljust.html,pandas.Series.str.ljust,"Series.str.ljust(width, fillchar=' ')[source]# Pad right side of strings in the Series/Index. Equivalent to str.ljust().","Parameters: widthintMinimum width of resulting string; additional characters will be filled with fillchar. fillcharstrAdditional character for filling, default is whitespace. Returns: Series/Index of objects.","["">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.center(8, fillchar='.')\n0   ..dog...\n1   ..bird..\n2   .mouse..\ndtype: object"", "">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.ljust(8, fillchar='.')\n0   dog.....\n1   bird....\n2   mouse...\ndtype: object"", "">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.rjust(8, fillchar='.')\n0   .....dog\n1   ....bird\n2   ...mouse\ndtype: object""]"
1879,..\pandas\reference\api\pandas.Series.argmin.html,pandas.Series.argmin,"Series.argmin(axis=None, skipna=True, *args, **kwargs)[source]# Return int position of the smallest value in the Series. If the minimum is achieved in multiple locations, the first row position is returned.","Parameters: axis{None}Unused. Parameter needed for compatibility with DataFrame. skipnabool, default TrueExclude NA/null values when showing the result. *args, **kwargsAdditional arguments and keywords for compatibility with NumPy. Returns: intRow position of the minimum value.","["">>> s = pd.Series({'Corn Flakes': 100.0, 'Almond Delight': 110.0,\n...                'Cinnamon Toast Crunch': 120.0, 'Cocoa Puff': 110.0})\n>>> s\nCorn Flakes              100.0\nAlmond Delight           110.0\nCinnamon Toast Crunch    120.0\nCocoa Puff               110.0\ndtype: float64"", '>>> s.argmax()\n2\n>>> s.argmin()\n0']"
1880,..\pandas\reference\api\pandas.Index.searchsorted.html,pandas.Index.searchsorted,"Index.searchsorted(value, side='left', sorter=None)[source]# Find indices where elements should be inserted to maintain order. Find the indices into a sorted Index self such that, if the corresponding elements in value were inserted before the indices, the order of self would be preserved. Note The Index must be monotonically sorted, otherwise wrong locations will likely be returned. Pandas does not check this for you. Notes Binary search is used to find the required insertion points.","Parameters: valuearray-like or scalarValues to insert into self. side{‘left’, ‘right’}, optionalIf ‘left’, the index of the first suitable location found is given. If ‘right’, return the last such index.  If there is no suitable index, return either 0 or N (where N is the length of self). sorter1-D array-like, optionalOptional array of integer indices that sort self into ascending order. They are typically the result of np.argsort. Returns: int or array of intA scalar or array of insertion points with the same shape as value.","['>>> ser = pd.Series([1, 2, 3])\n>>> ser\n0    1\n1    2\n2    3\ndtype: int64', '>>> ser.searchsorted(4)\n3', '>>> ser.searchsorted([0, 4])\narray([0, 3])', "">>> ser.searchsorted([1, 3], side='left')\narray([0, 2])"", "">>> ser.searchsorted([1, 3], side='right')\narray([1, 3])"", "">>> ser = pd.Series(pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000']))\n>>> ser\n0   2000-03-11\n1   2000-03-12\n2   2000-03-13\ndtype: datetime64[ns]"", "">>> ser.searchsorted('3/14/2000')\n3"", "">>> ser = pd.Categorical(\n...     ['apple', 'bread', 'bread', 'cheese', 'milk'], ordered=True\n... )\n>>> ser\n['apple', 'bread', 'bread', 'cheese', 'milk']\nCategories (4, object): ['apple' < 'bread' < 'cheese' < 'milk']"", "">>> ser.searchsorted('bread')\n1"", "">>> ser.searchsorted(['bread'], side='right')\narray([3])"", '>>> ser = pd.Series([2, 1, 3])\n>>> ser\n0    2\n1    1\n2    3\ndtype: int64', '>>> ser.searchsorted(1)  \n0  # wrong result, correct would be 1']"
1881,..\pandas\reference\api\pandas.tseries.offsets.Week.normalize.html,pandas.tseries.offsets.Week.normalize,Week.normalize#,No parameters found,[]
1882,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.name.html,pandas.tseries.offsets.BusinessHour.name,BusinessHour.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1883,..\pandas\reference\api\pandas.DataFrame.rename.html,pandas.DataFrame.rename,"DataFrame.rename(mapper=None, *, index=None, columns=None, axis=None, copy=None, inplace=False, level=None, errors='ignore')[source]# Rename columns or index labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don’t throw an error. See the user guide for more.","Parameters: mapperdict-like or functionDict-like or function transformations to apply to that axis’ values. Use either mapper and axis to specify the axis to target with mapper, or index and columns. indexdict-like or functionAlternative to specifying axis (mapper, axis=0 is equivalent to index=mapper). columnsdict-like or functionAlternative to specifying axis (mapper, axis=1 is equivalent to columns=mapper). axis{0 or ‘index’, 1 or ‘columns’}, default 0Axis to target with mapper. Can be either the axis name (‘index’, ‘columns’) or number (0, 1). The default is ‘index’. copybool, default TrueAlso copy underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True inplacebool, default FalseWhether to modify the DataFrame rather than creating a new one. If True then value of copy is ignored. levelint or level name, default NoneIn case of a MultiIndex, only rename labels in the specified level. errors{‘ignore’, ‘raise’}, default ‘ignore’If ‘raise’, raise a KeyError when a dict-like mapper, index, or columns contains labels that are not present in the Index being transformed. If ‘ignore’, existing keys will be renamed and extra keys will be ignored. Returns: DataFrame or NoneDataFrame with the renamed axis labels or None if inplace=True. Raises: KeyErrorIf any of the labels is not found in the selected axis and “errors=’raise’”.","['>>> df = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})\n>>> df.rename(columns={""A"": ""a"", ""B"": ""c""})\n   a  c\n0  1  4\n1  2  5\n2  3  6', '>>> df.rename(index={0: ""x"", 1: ""y"", 2: ""z""})\n   A  B\nx  1  4\ny  2  5\nz  3  6', "">>> df.index\nRangeIndex(start=0, stop=3, step=1)\n>>> df.rename(index=str).index\nIndex(['0', '1', '2'], dtype='object')"", '>>> df.rename(columns={""A"": ""a"", ""B"": ""b"", ""C"": ""c""}, errors=""raise"")\nTraceback (most recent call last):\nKeyError: [\'C\'] not found in axis', "">>> df.rename(str.lower, axis='columns')\n   a  b\n0  1  4\n1  2  5\n2  3  6"", "">>> df.rename({1: 2, 2: 4}, axis='index')\n   A  B\n0  1  4\n2  2  5\n4  3  6""]"
1884,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.rule_code.html,pandas.tseries.offsets.FY5253Quarter.rule_code,FY5253Quarter.rule_code#,No parameters found,[]
1885,..\pandas\reference\api\pandas.DataFrame.rename_axis.html,pandas.DataFrame.rename_axis,"DataFrame.rename_axis(mapper=<no_default>, *, index=<no_default>, columns=<no_default>, axis=0, copy=None, inplace=False)[source]# Set the name of the axis for the index or columns. Notes DataFrame.rename_axis supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored. The second calling convention will modify the names of the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels. We highly recommend using keyword arguments to clarify your intent.","Parameters: mapperscalar, list-like, optionalValue to set the axis name attribute. index, columnsscalar, list-like, dict-like or function, optionalA scalar, list-like, dict-like or functions transformations to apply to that axis’ values. Note that the columns parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects. Use either mapper and axis to specify the axis to target with mapper, or index and/or columns. axis{0 or ‘index’, 1 or ‘columns’}, default 0The axis to rename. For Series this parameter is unused and defaults to 0. copybool, default NoneAlso copy underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True inplacebool, default FalseModifies the object directly, instead of creating a new Series or DataFrame. Returns: Series, DataFrame, or NoneThe same type as the caller or None if inplace=True.","['>>> s = pd.Series([""dog"", ""cat"", ""monkey""])\n>>> s\n0       dog\n1       cat\n2    monkey\ndtype: object\n>>> s.rename_axis(""animal"")\nanimal\n0    dog\n1    cat\n2    monkey\ndtype: object', '>>> df = pd.DataFrame({""num_legs"": [4, 4, 2],\n...                    ""num_arms"": [0, 0, 2]},\n...                   [""dog"", ""cat"", ""monkey""])\n>>> df\n        num_legs  num_arms\ndog            4         0\ncat            4         0\nmonkey         2         2\n>>> df = df.rename_axis(""animal"")\n>>> df\n        num_legs  num_arms\nanimal\ndog            4         0\ncat            4         0\nmonkey         2         2\n>>> df = df.rename_axis(""limbs"", axis=""columns"")\n>>> df\nlimbs   num_legs  num_arms\nanimal\ndog            4         0\ncat            4         0\nmonkey         2         2', "">>> df.index = pd.MultiIndex.from_product([['mammal'],\n...                                        ['dog', 'cat', 'monkey']],\n...                                       names=['type', 'name'])\n>>> df\nlimbs          num_legs  num_arms\ntype   name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2"", "">>> df.rename_axis(index={'type': 'class'})\nlimbs          num_legs  num_arms\nclass  name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2"", '>>> df.rename_axis(columns=str.upper)\nLIMBS          num_legs  num_arms\ntype   name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2']"
1886,..\pandas\reference\api\pandas.Series.argsort.html,pandas.Series.argsort,"Series.argsort(axis=0, kind='quicksort', order=None, stable=None)[source]# Return the integer indices that would sort the Series values. Override ndarray.argsort. Argsorts the value, omitting NA/null values, and places the result in the same locations as the non-NA values.","Parameters: axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. kind{‘mergesort’, ‘quicksort’, ‘heapsort’, ‘stable’}, default ‘quicksort’Choice of sorting algorithm. See numpy.sort() for more information. ‘mergesort’ and ‘stable’ are the only stable algorithms. orderNoneHas no effect but is accepted for compatibility with numpy. stableNoneHas no effect but is accepted for compatibility with numpy. Returns: Series[np.intp]Positions of values within the sort order with -1 indicating nan values.","['>>> s = pd.Series([3, 2, 1])\n>>> s.argsort()\n0    2\n1    1\n2    0\ndtype: int64']"
1887,..\pandas\reference\api\pandas.Index.set_names.html,pandas.Index.set_names,"Index.set_names(names, *, level=None, inplace=False)[source]# Set Index or MultiIndex name. Able to set new names partially and by level.","Parameters: nameslabel or list of label or dict-like for MultiIndexName(s) to set. Changed in version 1.3.0. levelint, label or list of int or label, optionalIf the index is a MultiIndex and names is not dict-like, level(s) to set (None for all levels). Otherwise level must be None. Changed in version 1.3.0. inplacebool, default FalseModifies the object directly, instead of creating a new Index or MultiIndex. Returns: Index or NoneThe same type as the caller or None if inplace=True.","["">>> idx = pd.Index([1, 2, 3, 4])\n>>> idx\nIndex([1, 2, 3, 4], dtype='int64')\n>>> idx.set_names('quarter')\nIndex([1, 2, 3, 4], dtype='int64', name='quarter')"", "">>> idx = pd.MultiIndex.from_product([['python', 'cobra'],\n...                                   [2018, 2019]])\n>>> idx\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           )\n>>> idx = idx.set_names(['kind', 'year'])\n>>> idx.set_names('species', level=0)\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['species', 'year'])"", "">>> idx.set_names({'kind': 'snake'})\nMultiIndex([('python', 2018),\n            ('python', 2019),\n            ( 'cobra', 2018),\n            ( 'cobra', 2019)],\n           names=['snake', 'year'])""]"
1888,..\pandas\reference\api\pandas.tseries.offsets.Week.rule_code.html,pandas.tseries.offsets.Week.rule_code,Week.rule_code#,No parameters found,[]
1889,..\pandas\reference\api\pandas.Series.str.lower.html,pandas.Series.str.lower,Series.str.lower()[source]# Convert strings in the Series/Index to lowercase. Equivalent to str.lower().,Returns: Series or Index of object,"["">>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object"", '>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object', '>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object', '>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object', '>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object', '>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object']"
1890,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.nanos.html,pandas.tseries.offsets.BusinessHour.nanos,BusinessHour.nanos#,No parameters found,[]
1891,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.startingMonth.html,pandas.tseries.offsets.FY5253Quarter.startingMonth,FY5253Quarter.startingMonth#,No parameters found,[]
1892,..\pandas\reference\api\pandas.DataFrame.reorder_levels.html,pandas.DataFrame.reorder_levels,"DataFrame.reorder_levels(order, axis=0)[source]# Rearrange index levels using input order. May not drop or duplicate levels.","Parameters: orderlist of int or list of strList representing new level order. Reference level by number (position) or by key (label). axis{0 or ‘index’, 1 or ‘columns’}, default 0Where to reorder levels. Returns: DataFrame","['>>> data = {\n...     ""class"": [""Mammals"", ""Mammals"", ""Reptiles""],\n...     ""diet"": [""Omnivore"", ""Carnivore"", ""Carnivore""],\n...     ""species"": [""Humans"", ""Dogs"", ""Snakes""],\n... }\n>>> df = pd.DataFrame(data, columns=[""class"", ""diet"", ""species""])\n>>> df = df.set_index([""class"", ""diet""])\n>>> df\n                                  species\nclass      diet\nMammals    Omnivore                Humans\n           Carnivore                 Dogs\nReptiles   Carnivore               Snakes', '>>> df.reorder_levels([""diet"", ""class""])\n                                  species\ndiet      class\nOmnivore  Mammals                  Humans\nCarnivore Mammals                    Dogs\n          Reptiles                 Snakes']"
1893,..\pandas\reference\api\pandas.Series.array.html,pandas.Series.array,"property Series.array[source]# The ExtensionArray of the data backing this Series or Index. Notes This table lays out the different array types for each extension dtype within pandas. dtype array type category Categorical period PeriodArray interval IntervalArray IntegerNA IntegerArray string StringArray boolean BooleanArray datetime64[ns, tz] DatetimeArray For any 3rd-party extension types, the array type will be an ExtensionArray. For all remaining dtypes .array will be a arrays.NumpyExtensionArray wrapping the actual ndarray stored within. If you absolutely need a NumPy array (possibly with copying / coercing data), then use Series.to_numpy() instead.","Returns: ExtensionArrayAn ExtensionArray of the values stored within. For extension types, this is the actual array. For NumPy native types, this is a thin (no copy) wrapper around numpy.ndarray. .array differs from .values, which may require converting the data to a different form.","['>>> pd.Series([1, 2, 3]).array\n<NumpyExtensionArray>\n[1, 2, 3]\nLength: 3, dtype: int64', "">>> ser = pd.Series(pd.Categorical(['a', 'b', 'a']))\n>>> ser.array\n['a', 'b', 'a']\nCategories (2, object): ['a', 'b']""]"
1894,..\pandas\reference\api\pandas.Series.str.lstrip.html,pandas.Series.str.lstrip,Series.str.lstrip(to_strip=None)[source]# Remove leading characters. Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from left side. Replaces any non-strings in Series with NaNs. Equivalent to str.lstrip().,"Parameters: to_stripstr or None, default NoneSpecifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed. Returns: Series or Index of object","["">>> s = pd.Series(['1. Ant.  ', '2. Bee!\\n', '3. Cat?\\t', np.nan, 10, True])\n>>> s\n0    1. Ant.\n1    2. Bee!\\n\n2    3. Cat?\\t\n3          NaN\n4           10\n5         True\ndtype: object"", '>>> s.str.strip()\n0    1. Ant.\n1    2. Bee!\n2    3. Cat?\n3        NaN\n4        NaN\n5        NaN\ndtype: object', "">>> s.str.lstrip('123.')\n0    Ant.\n1    Bee!\\n\n2    Cat?\\t\n3       NaN\n4       NaN\n5       NaN\ndtype: object"", "">>> s.str.rstrip('.!? \\n\\t')\n0    1. Ant\n1    2. Bee\n2    3. Cat\n3       NaN\n4       NaN\n5       NaN\ndtype: object"", "">>> s.str.strip('123.!? \\n\\t')\n0    Ant\n1    Bee\n2    Cat\n3    NaN\n4    NaN\n5    NaN\ndtype: object""]"
1895,..\pandas\reference\api\pandas.Index.shape.html,pandas.Index.shape,property Index.shape[source]# Return a tuple of the shape of the underlying data.,No parameters found,"["">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.shape\n(3,)""]"
1896,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.variation.html,pandas.tseries.offsets.FY5253Quarter.variation,FY5253Quarter.variation#,No parameters found,[]
1897,..\pandas\reference\api\pandas.DataFrame.replace.html,pandas.DataFrame.replace,"DataFrame.replace(to_replace=None, value=<no_default>, *, inplace=False, limit=None, regex=False, method=<no_default>)[source]# Replace values given in to_replace with value. Values of the Series/DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc, which require you to specify a location to update with some value. Notes Regex substitution is performed under the hood with re.sub. The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.","Parameters: to_replacestr, regex, list, dict, Series, int, float, or NoneHow to find the values that will be replaced. numeric, str or regex: numeric: numeric values equal to to_replace will be replaced with value str: string exactly matching to_replace will be replaced with value regex: regexs matching to_replace will be replaced with value list of str, regex, or numeric: First, if to_replace and value are both lists, they must be the same length. Second, if regex=True then all of the strings in both lists will be interpreted as regexs otherwise they will match directly. This doesn’t matter much for value since there are only a few possible substitution regexes you can use. str, regex and numeric rules apply as above. dict: Dicts can be used to specify different replacement values for different existing values. For example, {'a': 'b', 'y': 'z'} replaces the value ‘a’ with ‘b’ and ‘y’ with ‘z’. To use a dict in this way, the optional value parameter should not be given. For a DataFrame a dict can specify that different values should be replaced in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1 in column ‘a’ and the value ‘z’ in column ‘b’ and replaces these values with whatever is specified in value. The value parameter should not be None in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. For a DataFrame nested dictionaries, e.g., {'a': {'b': np.nan}}, are read as follows: look in column ‘a’ for the value ‘b’ and replace it with NaN. The optional value parameter should not be specified to use a nested dict in this way. You can nest regular expressions as well. Note that column names (the top-level dictionary keys in a nested dictionary) cannot be regular expressions. None: This means that the regex argument must be a string, compiled regular expression, or list, dict, ndarray or Series of such elements. If value is also None then this must be a nested dictionary or Series. See the examples section for examples of each of these. valuescalar, dict, list, str, regex, default NoneValue to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed. inplacebool, default FalseIf True, performs operation inplace and returns None. limitint, default NoneMaximum size gap to forward or backward fill. Deprecated since version 2.1.0. regexbool or same types as to_replace, default FalseWhether to interpret to_replace and/or value as regular expressions. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None. method{‘pad’, ‘ffill’, ‘bfill’}The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None. Deprecated since version 2.1.0. Returns: Series/DataFrameObject after replacement. Raises: AssertionError If regex is not a bool and to_replace is not None. TypeError If to_replace is not a scalar, array-like, dict, or None If to_replace is a dict and value is not a list, dict, ndarray, or Series If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced ValueError If a list or an ndarray is passed to to_replace and value but they are not the same length.","['>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64', "">>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e"", '>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e', '>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e', "">>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64"", '>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e', "">>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e"", "">>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e"", "">>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz"", "">>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz"", "">>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz"", "">>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz"", "">>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz"", "">>> s = pd.Series([10, 'a', 'a', 'b', 'a'])"", "">>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object"", "">>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object"", "">>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object"", "">>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': ['a', 'b', 'c', 'd', 'e'],\n...                    'C': ['f', 'g', 'h', 'i', 'j']})"", "">>> df.replace(to_replace='^[a-g]', value='e', regex=True)\n    A  B  C\n0  0  e  e\n1  1  e  e\n2  2  e  h\n3  3  e  i\n4  4  e  j"", "">>> df.replace(to_replace={'B': '^[a-c]', 'C': '^[h-j]'}, value='e', regex=True)\n    A  B  C\n0  0  e  f\n1  1  e  g\n2  2  e  e\n3  3  d  e\n4  4  e  e""]"
1898,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.normalize.html,pandas.tseries.offsets.BusinessHour.normalize,BusinessHour.normalize#,No parameters found,[]
1899,..\pandas\reference\api\pandas.tseries.offsets.Week.weekday.html,pandas.tseries.offsets.Week.weekday,Week.weekday#,No parameters found,[]
1900,..\pandas\reference\api\pandas.Series.asfreq.html,pandas.Series.asfreq,"Series.asfreq(freq, method=None, how=None, normalize=False, fill_value=None)[source]# Convert time series to specified frequency. Returns the original data conformed to a new index with the specified frequency. If the index of this Series/DataFrame is a PeriodIndex, the new index is the result of transforming the original index with PeriodIndex.asfreq (so the original index will map one-to-one to the new index). Otherwise, the new index will be equivalent to pd.date_range(start, end, freq=freq) where start and end are, respectively, the first and last entries in the original index (see pandas.date_range()). The values corresponding to any timesteps in the new index which were not present in the original index will be null (NaN), unless a method for filling such unknowns is provided (see the method parameter below). The resample() method is more appropriate if an operation on each group of timesteps (such as an aggregate) is necessary to represent the data at the new frequency. Notes To learn more about the frequency strings, please see this link.","Parameters: freqDateOffset or strFrequency DateOffset or string. method{‘backfill’/’bfill’, ‘pad’/’ffill’}, default NoneMethod to use for filling holes in reindexed Series (note this does not fill NaNs that already were present): ‘pad’ / ‘ffill’: propagate last valid observation forward to next valid ‘backfill’ / ‘bfill’: use NEXT valid observation to fill. how{‘start’, ‘end’}, default endFor PeriodIndex only (see PeriodIndex.asfreq). normalizebool, default FalseWhether to reset output index to midnight. fill_valuescalar, optionalValue to use for missing values, applied during upsampling (note this does not fill NaNs that already were present). Returns: Series/DataFrameSeries/DataFrame object reindexed to the specified frequency.","["">>> index = pd.date_range('1/1/2000', periods=4, freq='min')\n>>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n>>> df = pd.DataFrame({'s': series})\n>>> df\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:01:00    NaN\n2000-01-01 00:02:00    2.0\n2000-01-01 00:03:00    3.0"", "">>> df.asfreq(freq='30s')\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    NaN\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    NaN\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    NaN\n2000-01-01 00:03:00    3.0"", "">>> df.asfreq(freq='30s', fill_value=9.0)\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    9.0\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    9.0\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    9.0\n2000-01-01 00:03:00    3.0"", "">>> df.asfreq(freq='30s', method='bfill')\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    NaN\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    2.0\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    3.0\n2000-01-01 00:03:00    3.0""]"
1901,..\pandas\reference\api\pandas.Series.str.match.html,pandas.Series.str.match,"Series.str.match(pat, case=True, flags=0, na=None)[source]# Determine if each string starts with a match of a regular expression.","Parameters: patstrCharacter sequence. casebool, default TrueIf True, case sensitive. flagsint, default 0 (no flags)Regex module flags, e.g. re.IGNORECASE. nascalar, optionalFill value for missing values. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used. Returns: Series/Index/array of boolean values","['>>> ser = pd.Series([""horse"", ""eagle"", ""donkey""])\n>>> ser.str.match(""e"")\n0   False\n1   True\n2   False\ndtype: bool']"
1902,..\pandas\reference\api\pandas.Index.shift.html,pandas.Index.shift,"Index.shift(periods=1, freq=None)[source]# Shift index by desired number of time frequency increments. This method is for shifting the values of datetime-like indexes by a specified time increment a given number of times. Notes This method is only implemented for datetime-like index classes, i.e., DatetimeIndex, PeriodIndex and TimedeltaIndex.","Parameters: periodsint, default 1Number of periods (or increments) to shift by, can be positive or negative. freqpandas.DateOffset, pandas.Timedelta or str, optionalFrequency increment to shift by. If None, the index is shifted by its own freq attribute. Offset aliases are valid strings, e.g., ‘D’, ‘W’, ‘M’ etc. Returns: pandas.IndexShifted index.","["">>> month_starts = pd.date_range('1/1/2011', periods=5, freq='MS')\n>>> month_starts\nDatetimeIndex(['2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01',\n               '2011-05-01'],\n              dtype='datetime64[ns]', freq='MS')"", "">>> month_starts.shift(10, freq='D')\nDatetimeIndex(['2011-01-11', '2011-02-11', '2011-03-11', '2011-04-11',\n               '2011-05-11'],\n              dtype='datetime64[ns]', freq=None)"", "">>> month_starts.shift(10)\nDatetimeIndex(['2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01',\n               '2012-03-01'],\n              dtype='datetime64[ns]', freq='MS')""]"
1903,..\pandas\reference\api\pandas.Series.str.normalize.html,pandas.Series.str.normalize,"Series.str.normalize(form)[source]# Return the Unicode normal form for the strings in the Series/Index. For more information on the forms, see the unicodedata.normalize().","Parameters: form{‘NFC’, ‘NFKC’, ‘NFD’, ‘NFKD’}Unicode form. Returns: Series/Index of objects","["">>> ser = pd.Series(['ñ'])\n>>> ser.str.normalize('NFC') == ser.str.normalize('NFD')\n0   False\ndtype: bool""]"
1904,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.weekday.html,pandas.tseries.offsets.FY5253Quarter.weekday,FY5253Quarter.weekday#,No parameters found,[]
1905,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.rule_code.html,pandas.tseries.offsets.BusinessHour.rule_code,BusinessHour.rule_code#,No parameters found,[]
1906,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.copy.html,pandas.tseries.offsets.WeekOfMonth.copy,WeekOfMonth.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1907,..\pandas\reference\api\pandas.Series.asof.html,pandas.Series.asof,"Series.asof(where, subset=None)[source]# Return the last row(s) without any NaNs before where. The last row (for each element in where, if list) without any NaN is taken. In case of a DataFrame, the last row without NaN considering only the subset of columns (if not None) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Notes Dates are assumed to be sorted. Raises if this is not the case.","Parameters: wheredate or array-like of datesDate(s) before which the last row(s) are returned. subsetstr or array-like of str, default NoneFor DataFrame, if not None, only use these columns to check for NaNs. Returns: scalar, Series, or DataFrameThe return can be: scalar : when self is a Series and where is a scalar Series: when self is a Series and where is an array-like, or when self is a DataFrame and where is a scalar DataFrame : when self is a DataFrame and where is an array-like","['>>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\n>>> s\n10    1.0\n20    2.0\n30    NaN\n40    4.0\ndtype: float64', '>>> s.asof(20)\n2.0', '>>> s.asof([5, 20])\n5     NaN\n20    2.0\ndtype: float64', '>>> s.asof(30)\n2.0', "">>> df = pd.DataFrame({'a': [10., 20., 30., 40., 50.],\n...                    'b': [None, None, None, None, 500]},\n...                   index=pd.DatetimeIndex(['2018-02-27 09:01:00',\n...                                           '2018-02-27 09:02:00',\n...                                           '2018-02-27 09:03:00',\n...                                           '2018-02-27 09:04:00',\n...                                           '2018-02-27 09:05:00']))\n>>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']))\n                      a   b\n2018-02-27 09:03:30 NaN NaN\n2018-02-27 09:04:30 NaN NaN"", "">>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30',\n...                           '2018-02-27 09:04:30']),\n...         subset=['a'])\n                        a   b\n2018-02-27 09:03:30  30.0 NaN\n2018-02-27 09:04:30  40.0 NaN""]"
1908,..\pandas\reference\api\pandas.DataFrame.resample.html,pandas.DataFrame.resample,"DataFrame.resample(rule, axis=<no_default>, closed=None, label=None, convention=<no_default>, kind=<no_default>, on=None, level=None, origin='start_day', offset=None, group_keys=False)[source]# Resample time-series data. Convenience method for frequency conversion and resampling of time series. The object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or the caller must pass the label of a datetime-like series/index to the on/level keyword parameter. Notes See the user guide for more. To learn more about the offset strings, please see this link.","Parameters: ruleDateOffset, Timedelta or strThe offset string or object representing target conversion. axis{0 or ‘index’, 1 or ‘columns’}, default 0Which axis to use for up- or down-sampling. For Series this parameter is unused and defaults to 0. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex. Deprecated since version 2.0.0: Use frame.T.resample(…) instead. closed{‘right’, ‘left’}, default NoneWhich side of bin interval is closed. The default is ‘left’ for all frequency offsets except for ‘ME’, ‘YE’, ‘QE’, ‘BME’, ‘BA’, ‘BQE’, and ‘W’ which all have a default of ‘right’. label{‘right’, ‘left’}, default NoneWhich bin edge label to label bucket with. The default is ‘left’ for all frequency offsets except for ‘ME’, ‘YE’, ‘QE’, ‘BME’, ‘BA’, ‘BQE’, and ‘W’ which all have a default of ‘right’. convention{‘start’, ‘end’, ‘s’, ‘e’}, default ‘start’For PeriodIndex only, controls whether to use the start or end of rule. Deprecated since version 2.2.0: Convert PeriodIndex to DatetimeIndex before resampling instead. kind{‘timestamp’, ‘period’}, optional, default NonePass ‘timestamp’ to convert the resulting index to a DateTimeIndex or ‘period’ to convert it to a PeriodIndex. By default the input representation is retained. Deprecated since version 2.2.0: Convert index to desired type explicitly instead. onstr, optionalFor a DataFrame, column to use instead of index for resampling. Column must be datetime-like. levelstr or int, optionalFor a MultiIndex, level (name or number) to use for resampling. level must be datetime-like. originTimestamp or str, default ‘start_day’The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If string, must be one of the following: ‘epoch’: origin is 1970-01-01 ‘start’: origin is the first value of the timeseries ‘start_day’: origin is the first day at midnight of the timeseries ‘end’: origin is the last value of the timeseries ‘end_day’: origin is the ceiling midnight of the last day Added in version 1.3.0. Note Only takes effect for Tick-frequencies (i.e. fixed frequencies like days, hours, and minutes, rather than months or quarters). offsetTimedelta or str, default is NoneAn offset timedelta added to the origin. group_keysbool, default FalseWhether to include the group keys in the result index when using .apply() on the resampled object. Added in version 1.5.0: Not specifying group_keys will retain values-dependent behavior from pandas 1.4 and earlier (see pandas 1.5.0 Release notes for examples). Changed in version 2.0.0: group_keys now defaults to False. Returns: pandas.api.typing.ResamplerResampler object.","["">>> index = pd.date_range('1/1/2000', periods=9, freq='min')\n>>> series = pd.Series(range(9), index=index)\n>>> series\n2000-01-01 00:00:00    0\n2000-01-01 00:01:00    1\n2000-01-01 00:02:00    2\n2000-01-01 00:03:00    3\n2000-01-01 00:04:00    4\n2000-01-01 00:05:00    5\n2000-01-01 00:06:00    6\n2000-01-01 00:07:00    7\n2000-01-01 00:08:00    8\nFreq: min, dtype: int64"", "">>> series.resample('3min').sum()\n2000-01-01 00:00:00     3\n2000-01-01 00:03:00    12\n2000-01-01 00:06:00    21\nFreq: 3min, dtype: int64"", "">>> series.resample('3min', label='right').sum()\n2000-01-01 00:03:00     3\n2000-01-01 00:06:00    12\n2000-01-01 00:09:00    21\nFreq: 3min, dtype: int64"", "">>> series.resample('3min', label='right', closed='right').sum()\n2000-01-01 00:00:00     0\n2000-01-01 00:03:00     6\n2000-01-01 00:06:00    15\n2000-01-01 00:09:00    15\nFreq: 3min, dtype: int64"", "">>> series.resample('30s').asfreq()[0:5]   # Select first 5 rows\n2000-01-01 00:00:00   0.0\n2000-01-01 00:00:30   NaN\n2000-01-01 00:01:00   1.0\n2000-01-01 00:01:30   NaN\n2000-01-01 00:02:00   2.0\nFreq: 30s, dtype: float64"", "">>> series.resample('30s').ffill()[0:5]\n2000-01-01 00:00:00    0\n2000-01-01 00:00:30    0\n2000-01-01 00:01:00    1\n2000-01-01 00:01:30    1\n2000-01-01 00:02:00    2\nFreq: 30s, dtype: int64"", "">>> series.resample('30s').bfill()[0:5]\n2000-01-01 00:00:00    0\n2000-01-01 00:00:30    1\n2000-01-01 00:01:00    1\n2000-01-01 00:01:30    2\n2000-01-01 00:02:00    2\nFreq: 30s, dtype: int64"", "">>> def custom_resampler(arraylike):\n...     return np.sum(arraylike) + 5\n...\n>>> series.resample('3min').apply(custom_resampler)\n2000-01-01 00:00:00     8\n2000-01-01 00:03:00    17\n2000-01-01 00:06:00    26\nFreq: 3min, dtype: int64"", "">>> d = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n...      'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n>>> df = pd.DataFrame(d)\n>>> df['week_starting'] = pd.date_range('01/01/2018',\n...                                     periods=8,\n...                                     freq='W')\n>>> df\n   price  volume week_starting\n0     10      50    2018-01-07\n1     11      60    2018-01-14\n2      9      40    2018-01-21\n3     13     100    2018-01-28\n4     14      50    2018-02-04\n5     18     100    2018-02-11\n6     17      40    2018-02-18\n7     19      50    2018-02-25\n>>> df.resample('ME', on='week_starting').mean()\n               price  volume\nweek_starting\n2018-01-31     10.75    62.5\n2018-02-28     17.00    60.0"", "">>> days = pd.date_range('1/1/2000', periods=4, freq='D')\n>>> d2 = {'price': [10, 11, 9, 13, 14, 18, 17, 19],\n...       'volume': [50, 60, 40, 100, 50, 100, 40, 50]}\n>>> df2 = pd.DataFrame(\n...     d2,\n...     index=pd.MultiIndex.from_product(\n...         [days, ['morning', 'afternoon']]\n...     )\n... )\n>>> df2\n                      price  volume\n2000-01-01 morning       10      50\n           afternoon     11      60\n2000-01-02 morning        9      40\n           afternoon     13     100\n2000-01-03 morning       14      50\n           afternoon     18     100\n2000-01-04 morning       17      40\n           afternoon     19      50\n>>> df2.resample('D', level=0).sum()\n            price  volume\n2000-01-01     21     110\n2000-01-02     22     140\n2000-01-03     32     150\n2000-01-04     36      90"", "">>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\n>>> rng = pd.date_range(start, end, freq='7min')\n>>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng)\n>>> ts\n2000-10-01 23:30:00     0\n2000-10-01 23:37:00     3\n2000-10-01 23:44:00     6\n2000-10-01 23:51:00     9\n2000-10-01 23:58:00    12\n2000-10-02 00:05:00    15\n2000-10-02 00:12:00    18\n2000-10-02 00:19:00    21\n2000-10-02 00:26:00    24\nFreq: 7min, dtype: int64"", "">>> ts.resample('17min').sum()\n2000-10-01 23:14:00     0\n2000-10-01 23:31:00     9\n2000-10-01 23:48:00    21\n2000-10-02 00:05:00    54\n2000-10-02 00:22:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='epoch').sum()\n2000-10-01 23:18:00     0\n2000-10-01 23:35:00    18\n2000-10-01 23:52:00    27\n2000-10-02 00:09:00    39\n2000-10-02 00:26:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='2000-01-01').sum()\n2000-10-01 23:24:00     3\n2000-10-01 23:41:00    15\n2000-10-01 23:58:00    45\n2000-10-02 00:15:00    45\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='start').sum()\n2000-10-01 23:30:00     9\n2000-10-01 23:47:00    21\n2000-10-02 00:04:00    54\n2000-10-02 00:21:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', offset='23h30min').sum()\n2000-10-01 23:30:00     9\n2000-10-01 23:47:00    21\n2000-10-02 00:04:00    54\n2000-10-02 00:21:00    24\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='end').sum()\n2000-10-01 23:35:00     0\n2000-10-01 23:52:00    18\n2000-10-02 00:09:00    27\n2000-10-02 00:26:00    63\nFreq: 17min, dtype: int64"", "">>> ts.resample('17min', origin='end_day').sum()\n2000-10-01 23:38:00     3\n2000-10-01 23:55:00    15\n2000-10-02 00:12:00    45\n2000-10-02 00:29:00    45\nFreq: 17min, dtype: int64""]"
1909,..\pandas\reference\api\pandas.Index.size.html,pandas.Index.size,property Index.size[source]# Return the number of elements in the underlying data.,No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.size\n3"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')\n>>> idx.size\n3""]"
1910,..\pandas\reference\api\pandas.Series.str.pad.html,pandas.Series.str.pad,"Series.str.pad(width, side='left', fillchar=' ')[source]# Pad strings in the Series/Index up to width.","Parameters: widthintMinimum width of resulting string; additional characters will be filled with character defined in fillchar. side{‘left’, ‘right’, ‘both’}, default ‘left’Side from which to fill resulting string. fillcharstr, default ‘ ‘Additional character for filling, default is whitespace. Returns: Series or Index of objectReturns Series or Index with minimum number of char in object.","['>>> s = pd.Series([""caribou"", ""tiger""])\n>>> s\n0    caribou\n1      tiger\ndtype: object', '>>> s.str.pad(width=10)\n0       caribou\n1         tiger\ndtype: object', "">>> s.str.pad(width=10, side='right', fillchar='-')\n0    caribou---\n1    tiger-----\ndtype: object"", "">>> s.str.pad(width=10, side='both', fillchar='-')\n0    -caribou--\n1    --tiger---\ndtype: object""]"
1911,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.start.html,pandas.tseries.offsets.BusinessHour.start,BusinessHour.start#,No parameters found,[]
1912,..\pandas\reference\api\pandas.tseries.offsets.FY5253Quarter.year_has_extra_week.html,pandas.tseries.offsets.FY5253Quarter.year_has_extra_week,FY5253Quarter.year_has_extra_week(dt)#,No parameters found,[]
1913,..\pandas\reference\api\pandas.Series.astype.html,pandas.Series.astype,"Series.astype(dtype, copy=None, errors='raise')[source]# Cast a pandas object to a specified dtype dtype. Notes Changed in version 2.0.0: Using astype to convert from timezone-naive dtype to timezone-aware dtype will raise an exception. Use Series.dt.tz_localize() instead.","Parameters: dtypestr, data type, Series or Mapping of column name -> data typeUse a str, numpy.dtype, pandas.ExtensionDtype or Python type to cast entire pandas object to the same type. Alternatively, use a mapping, e.g. {col: dtype, …}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame’s columns to column-specific types. copybool, default TrueReturn a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects). Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True errors{‘raise’, ‘ignore’}, default ‘raise’Control raising of exceptions on invalid data for provided dtype. raise : allow exceptions to be raised ignore : suppress exceptions. On error return original object. Returns: same type as caller","["">>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object"", "">>> df.astype('int32').dtypes\ncol1    int32\ncol2    int32\ndtype: object"", "">>> df.astype({'col1': 'int32'}).dtypes\ncol1    int32\ncol2    int64\ndtype: object"", "">>> ser = pd.Series([1, 2], dtype='int32')\n>>> ser\n0    1\n1    2\ndtype: int32\n>>> ser.astype('int64')\n0    1\n1    2\ndtype: int64"", "">>> ser.astype('category')\n0    1\n1    2\ndtype: category\nCategories (2, int32): [1, 2]"", '>>> from pandas.api.types import CategoricalDtype\n>>> cat_dtype = CategoricalDtype(\n...     categories=[2, 1], ordered=True)\n>>> ser.astype(cat_dtype)\n0    1\n1    2\ndtype: category\nCategories (2, int64): [2 < 1]', "">>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n>>> ser_date\n0   2020-01-01\n1   2020-01-02\n2   2020-01-03\ndtype: datetime64[ns]""]"
1914,..\pandas\reference\api\pandas.Series.str.partition.html,pandas.Series.str.partition,"Series.str.partition(sep=' ', expand=True)[source]# Split the string at the first occurrence of sep. This method splits the string at the first occurrence of sep, and returns 3 elements containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 elements containing the string itself, followed by two empty strings.","Parameters: sepstr, default whitespaceString to split on. expandbool, default TrueIf True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index. Returns: DataFrame/MultiIndex or Series/Index of objects","["">>> s = pd.Series(['Linda van der Berg', 'George Pitt-Rivers'])\n>>> s\n0    Linda van der Berg\n1    George Pitt-Rivers\ndtype: object"", '>>> s.str.partition()\n        0  1             2\n0   Linda     van der Berg\n1  George      Pitt-Rivers', '>>> s.str.rpartition()\n               0  1            2\n0  Linda van der            Berg\n1         George     Pitt-Rivers', "">>> s.str.partition('-')\n                    0  1       2\n0  Linda van der Berg\n1         George Pitt  -  Rivers"", "">>> s.str.partition('-', expand=False)\n0    (Linda van der Berg, , )\n1    (George Pitt, -, Rivers)\ndtype: object"", "">>> idx = pd.Index(['X 123', 'Y 999'])\n>>> idx\nIndex(['X 123', 'Y 999'], dtype='object')"", "">>> idx.str.partition()\nMultiIndex([('X', ' ', '123'),\n            ('Y', ' ', '999')],\n           )"", "">>> idx.str.partition(expand=False)\nIndex([('X', ' ', '123'), ('Y', ' ', '999')], dtype='object')""]"
1915,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.freqstr.html,pandas.tseries.offsets.WeekOfMonth.freqstr,WeekOfMonth.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1916,..\pandas\reference\api\pandas.Index.slice_indexer.html,pandas.Index.slice_indexer,"Index.slice_indexer(start=None, end=None, step=None)[source]# Compute the slice indexer for input labels and step. Index needs to be ordered and unique. Notes This function assumes that the data is sorted, so use at your own peril","Parameters: startlabel, default NoneIf None, defaults to the beginning. endlabel, default NoneIf None, defaults to the end. stepint, default None Returns: slice Raises: KeyErrorIf key does not exist, or key is not unique and index isnot ordered.","["">>> idx = pd.Index(list('abcd'))\n>>> idx.slice_indexer(start='b', end='c')\nslice(1, 3, None)"", "">>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n>>> idx.slice_indexer(start='b', end=('c', 'g'))\nslice(1, 3, None)""]"
1917,..\pandas\reference\api\pandas.tseries.offsets.BusinessHour.weekmask.html,pandas.tseries.offsets.BusinessHour.weekmask,BusinessHour.weekmask#,No parameters found,[]
1918,..\pandas\reference\api\pandas.tseries.offsets.Hour.copy.html,pandas.tseries.offsets.Hour.copy,Hour.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1919,..\pandas\reference\api\pandas.Series.at.html,pandas.Series.at,"property Series.at[source]# Access a single value for a row/column label pair. Similar to loc, in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series. Notes See Fast scalar value getting and setting for more details.",Raises: KeyErrorIf getting a value and ‘label’ does not exist in a DataFrame or Series. ValueErrorIf row/column label pair is not a tuple or if any label from the pair is not a scalar for DataFrame. If label is list-like (excluding NamedTuple) for Series.,"["">>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n4   0   2   3\n5   0   4   1\n6  10  20  30"", "">>> df.at[4, 'B']\n2"", "">>> df.at[4, 'B'] = 10\n>>> df.at[4, 'B']\n10"", "">>> df.loc[5].at['B']\n4""]"
1920,..\pandas\reference\api\pandas.Index.slice_locs.html,pandas.Index.slice_locs,"Index.slice_locs(start=None, end=None, step=None)[source]# Compute slice locations for input labels. Notes This method only works if the index is monotonic or unique.","Parameters: startlabel, default NoneIf None, defaults to the beginning. endlabel, default NoneIf None, defaults to the end. stepint, defaults NoneIf None, defaults to 1. Returns: tuple[int, int]","["">>> idx = pd.Index(list('abcd'))\n>>> idx.slice_locs(start='b', end='c')\n(1, 3)""]"
1921,..\pandas\reference\api\pandas.Series.str.removeprefix.html,pandas.Series.str.removeprefix,"Series.str.removeprefix(prefix)[source]# Remove a prefix from an object series. If the prefix is not present, the original string will be returned.",Parameters: prefixstrRemove the prefix of the string. Returns: Series/Index: objectThe Series or Index with given prefix removed.,"['>>> s = pd.Series([""str_foo"", ""str_bar"", ""no_prefix""])\n>>> s\n0    str_foo\n1    str_bar\n2    no_prefix\ndtype: object\n>>> s.str.removeprefix(""str_"")\n0    foo\n1    bar\n2    no_prefix\ndtype: object', '>>> s = pd.Series([""foo_str"", ""bar_str"", ""no_suffix""])\n>>> s\n0    foo_str\n1    bar_str\n2    no_suffix\ndtype: object\n>>> s.str.removesuffix(""_str"")\n0    foo\n1    bar\n2    no_suffix\ndtype: object']"
1922,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.html,pandas.tseries.offsets.WeekOfMonth,class pandas.tseries.offsets.WeekOfMonth# Describes monthly dates like “the Tuesday of the 2nd week of each month”. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code week weekday,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekint {0, 1, 2, 3, …}, default 0A specific integer for the week of the month. e.g. 0 is 1st week of month, 1 is the 2nd week, etc. weekdayint {0, 1, …, 6}, default 0A specific integer for the day of the week. 0 is Monday 1 is Tuesday 2 is Wednesday 3 is Thursday 4 is Friday 5 is Saturday 6 is Sunday.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.WeekOfMonth()\nTimestamp('2022-01-03 00:00:00')""]"
1923,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.copy.html,pandas.tseries.offsets.BusinessMonthBegin.copy,BusinessMonthBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
1924,..\pandas\reference\api\pandas.tseries.offsets.Hour.delta.html,pandas.tseries.offsets.Hour.delta,Hour.delta#,No parameters found,[]
1925,..\pandas\reference\api\pandas.Series.attrs.html,pandas.Series.attrs,property Series.attrs[source]# Dictionary of global attributes of this dataset. Warning attrs is experimental and may change without warning. Notes Many operations that create new datasets will copy attrs. Copies are always deep so that changing attrs will only affect the present dataset. pandas.concat copies attrs only if all input datasets have the same attrs.,No parameters found,"['>>> ser = pd.Series([1, 2, 3])\n>>> ser.attrs = {""A"": [10, 20, 30]}\n>>> ser.attrs\n{\'A\': [10, 20, 30]}', '>>> df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]})\n>>> df.attrs = {""A"": [10, 20, 30]}\n>>> df.attrs\n{\'A\': [10, 20, 30]}']"
1926,..\pandas\reference\api\pandas.Index.sort_values.html,pandas.Index.sort_values,"Index.sort_values(*, return_indexer=False, ascending=True, na_position='last', key=None)[source]# Return a sorted copy of the index. Return a sorted copy of the index, and optionally return the indices that sorted the index itself.","Parameters: return_indexerbool, default FalseShould the indices that would sort the index be returned. ascendingbool, default TrueShould the index values be sorted in an ascending order. na_position{‘first’ or ‘last’}, default ‘last’Argument ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end. keycallable, optionalIf not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect an Index and return an Index of the same shape. Returns: sorted_indexpandas.IndexSorted copy of the index. indexernumpy.ndarray, optionalThe indices that the index itself was sorted by.","["">>> idx = pd.Index([10, 100, 1, 1000])\n>>> idx\nIndex([10, 100, 1, 1000], dtype='int64')"", "">>> idx.sort_values()\nIndex([1, 10, 100, 1000], dtype='int64')"", "">>> idx.sort_values(ascending=False, return_indexer=True)\n(Index([1000, 100, 10, 1], dtype='int64'), array([3, 1, 0, 2]))""]"
1927,..\pandas\reference\api\pandas.Series.str.removesuffix.html,pandas.Series.str.removesuffix,"Series.str.removesuffix(suffix)[source]# Remove a suffix from an object series. If the suffix is not present, the original string will be returned.",Parameters: suffixstrRemove the suffix of the string. Returns: Series/Index: objectThe Series or Index with given suffix removed.,"['>>> s = pd.Series([""str_foo"", ""str_bar"", ""no_prefix""])\n>>> s\n0    str_foo\n1    str_bar\n2    no_prefix\ndtype: object\n>>> s.str.removeprefix(""str_"")\n0    foo\n1    bar\n2    no_prefix\ndtype: object', '>>> s = pd.Series([""foo_str"", ""bar_str"", ""no_suffix""])\n>>> s\n0    foo_str\n1    bar_str\n2    no_suffix\ndtype: object\n>>> s.str.removesuffix(""_str"")\n0    foo\n1    bar\n2    no_suffix\ndtype: object']"
1928,..\pandas\reference\api\pandas.tseries.offsets.Hour.freqstr.html,pandas.tseries.offsets.Hour.freqstr,Hour.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1929,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.freqstr.html,pandas.tseries.offsets.BusinessMonthBegin.freqstr,BusinessMonthBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
1930,..\pandas\reference\api\pandas.Series.at_time.html,pandas.Series.at_time,"Series.at_time(time, asof=False, axis=None)[source]# Select values at particular time of day (e.g., 9:30AM).","Parameters: timedatetime.time or strThe values to select. axis{0 or ‘index’, 1 or ‘columns’}, default 0For Series this parameter is unused and defaults to 0. Returns: Series or DataFrame Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='12h')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n                     A\n2018-04-09 00:00:00  1\n2018-04-09 12:00:00  2\n2018-04-10 00:00:00  3\n2018-04-10 12:00:00  4"", "">>> ts.at_time('12:00')\n                     A\n2018-04-09 12:00:00  2\n2018-04-10 12:00:00  4""]"
1931,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_anchored.html,pandas.tseries.offsets.WeekOfMonth.is_anchored,WeekOfMonth.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1932,..\pandas\reference\api\pandas.Series.str.repeat.html,pandas.Series.str.repeat,Series.str.repeat(repeats)[source]# Duplicate each string in the Series or Index.,Parameters: repeatsint or sequence of intSame value for all (int) or different value per (sequence). Returns: Series or pandas.IndexSeries or Index of repeated string objects specified by input parameter repeats.,"["">>> s = pd.Series(['a', 'b', 'c'])\n>>> s\n0    a\n1    b\n2    c\ndtype: object"", '>>> s.str.repeat(repeats=2)\n0    aa\n1    bb\n2    cc\ndtype: object', '>>> s.str.repeat(repeats=[1, 2, 3])\n0      a\n1     bb\n2    ccc\ndtype: object']"
1933,..\pandas\reference\api\pandas.Index.str.html,pandas.Index.str,"Index.str()[source]# Vectorized string functions for Series and Index. NAs stay NA unless handled otherwise by a particular method. Patterned after Python’s string methods, with some inspiration from R’s stringr package.",No parameters found,"['>>> s = pd.Series([""A_Str_Series""])\n>>> s\n0    A_Str_Series\ndtype: object', '>>> s.str.split(""_"")\n0    [A, Str, Series]\ndtype: object', '>>> s.str.replace(""_"", """")\n0    AStrSeries\ndtype: object']"
1934,..\pandas\reference\api\pandas.tseries.offsets.Hour.html,pandas.tseries.offsets.Hour,class pandas.tseries.offsets.Hour# Offset n hours. Examples You can use the parameter n to represent a shift of n hours. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of hours represented.","["">>> from pandas.tseries.offsets import Hour\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Hour()\nTimestamp('2022-12-09 16:00:00')\n>>> ts - Hour(4)\nTimestamp('2022-12-09 11:00:00')"", "">>> ts + Hour(-4)\nTimestamp('2022-12-09 11:00:00')""]"
1935,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.html,pandas.tseries.offsets.BusinessMonthBegin,class pandas.tseries.offsets.BusinessMonthBegin# DateOffset of one month at the first business day. BusinessMonthBegin goes to the next date which is the first business day of the month. Examples If you want to get the start of the current business month: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range.","["">>> ts = pd.Timestamp(2022, 11, 30)\n>>> ts + pd.offsets.BMonthBegin()\nTimestamp('2022-12-01 00:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 1)\n>>> ts + pd.offsets.BMonthBegin()\nTimestamp('2023-01-02 00:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 1)\n>>> pd.offsets.BMonthBegin().rollback(ts)\nTimestamp('2022-12-01 00:00:00')""]"
1936,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_anchored.html,pandas.tseries.offsets.Hour.is_anchored,Hour.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
1937,..\pandas\reference\api\pandas.Series.autocorr.html,pandas.Series.autocorr,Series.autocorr(lag=1)[source]# Compute the lag-N autocorrelation. This method computes the Pearson correlation between the Series and its shifted self. Notes If the Pearson correlation is not well defined return ‘NaN’.,"Parameters: lagint, default 1Number of lags to apply before performing autocorrelation. Returns: floatThe Pearson correlation between self and self.shift(lag).","['>>> s = pd.Series([0.25, 0.5, 0.2, -0.05])\n>>> s.autocorr()  \n0.10355...\n>>> s.autocorr(lag=2)  \n-0.99999...', '>>> s = pd.Series([1, 0, 0, 0])\n>>> s.autocorr()\nnan']"
1938,..\pandas\reference\api\pandas.Series.str.replace.html,pandas.Series.str.replace,"Series.str.replace(pat, repl, n=-1, case=None, flags=0, regex=False)[source]# Replace each occurrence of pattern/regex in the Series/Index. Equivalent to str.replace() or re.sub(), depending on the regex value. Notes When pat is a compiled regex, all flags should be included in the compiled regex. Use of case, flags, or regex=False with a compiled regex will raise an error.","Parameters: patstr or compiled regexString can be a character sequence or regular expression. replstr or callableReplacement string or a callable. The callable is passed the regex match object and must return a replacement string to be used. See re.sub(). nint, default -1 (all)Number of replacements to make from start. casebool, default NoneDetermines if replace is case sensitive: If True, case sensitive (the default if pat is a string) Set to False for case insensitive Cannot be set if pat is a compiled regex. flagsint, default 0 (no flags)Regex module flags, e.g. re.IGNORECASE. Cannot be set if pat is a compiled regex. regexbool, default FalseDetermines if the passed-in pattern is a regular expression: If True, assumes the passed-in pattern is a regular expression. If False, treats the pattern as a literal string Cannot be set to False if pat is a compiled regex or repl is a callable. Returns: Series or Index of objectA copy of the object with all matching occurrences of pat replaced by repl. Raises: ValueError if regex is False and repl is a callable or pat is a compiled regex if pat is a compiled regex and case or flags is set","["">>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f.', 'ba', regex=True)\n0    bao\n1    baz\n2    NaN\ndtype: object"", "">>> pd.Series(['f.o', 'fuz', np.nan]).str.replace('f.', 'ba', regex=False)\n0    bao\n1    fuz\n2    NaN\ndtype: object"", "">>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f', repr, regex=True)\n0    <re.Match object; span=(0, 1), match='f'>oo\n1    <re.Match object; span=(0, 1), match='f'>uz\n2                                            NaN\ndtype: object"", "">>> repl = lambda m: m.group(0)[::-1]\n>>> ser = pd.Series(['foo 123', 'bar baz', np.nan])\n>>> ser.str.replace(r'[a-z]+', repl, regex=True)\n0    oof 123\n1    rab zab\n2        NaN\ndtype: object"", '>>> pat = r""(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)""\n>>> repl = lambda m: m.group(\'two\').swapcase()\n>>> ser = pd.Series([\'One Two Three\', \'Foo Bar Baz\'])\n>>> ser.str.replace(pat, repl, regex=True)\n0    tWO\n1    bAR\ndtype: object', "">>> import re\n>>> regex_pat = re.compile(r'FUZ', flags=re.IGNORECASE)\n>>> pd.Series(['foo', 'fuz', np.nan]).str.replace(regex_pat, 'bar', regex=True)\n0    foo\n1    bar\n2    NaN\ndtype: object""]"
1939,..\pandas\reference\api\pandas.Index.symmetric_difference.html,pandas.Index.symmetric_difference,"Index.symmetric_difference(other, result_name=None, sort=None)[source]# Compute the symmetric difference of two Index objects. Notes symmetric_difference contains elements that appear in either idx1 or idx2 but not both. Equivalent to the Index created by idx1.difference(idx2) | idx2.difference(idx1) with duplicates dropped.","Parameters: otherIndex or array-like result_namestr sortbool or None, default NoneWhether to sort the resulting index. By default, the values are attempted to be sorted, but any TypeError from incomparable elements is caught by pandas. None : Attempt to sort the result, but catch any TypeErrors from comparing incomparable elements. False : Do not sort the result. True : Sort the result (which may raise TypeError). Returns: Index","["">>> idx1 = pd.Index([1, 2, 3, 4])\n>>> idx2 = pd.Index([2, 3, 4, 5])\n>>> idx1.symmetric_difference(idx2)\nIndex([1, 5], dtype='int64')""]"
1940,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_month_end.html,pandas.tseries.offsets.WeekOfMonth.is_month_end,WeekOfMonth.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1941,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_anchored.html,pandas.tseries.offsets.BusinessMonthBegin.is_anchored,BusinessMonthBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
1942,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_month_end.html,pandas.tseries.offsets.Hour.is_month_end,Hour.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1943,..\pandas\reference\api\pandas.Series.backfill.html,pandas.Series.backfill,"Series.backfill(*, axis=None, inplace=False, limit=None, downcast=<no_default>)[source]# Fill NA/NaN values by using the next valid observation to fill the gap. Deprecated since version 2.0: Series/DataFrame.backfill is deprecated. Use Series/DataFrame.bfill instead.",Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.,[]
1944,..\pandas\reference\api\pandas.Series.str.rfind.html,pandas.Series.str.rfind,"Series.str.rfind(sub, start=0, end=None)[source]# Return highest indexes in each strings in the Series/Index. Each of returned indexes corresponds to the position where the substring is fully contained between [start:end]. Return -1 on failure. Equivalent to standard str.rfind().",Parameters: substrSubstring being searched. startintLeft edge index. endintRight edge index. Returns: Series or Index of int.,"['>>> ser = pd.Series([""cow_"", ""duck_"", ""do_ve""])\n>>> ser.str.find(""_"")\n0   3\n1   4\n2   2\ndtype: int64', '>>> ser = pd.Series([""_cow_"", ""duck_"", ""do_v_e""])\n>>> ser.str.rfind(""_"")\n0   4\n1   4\n2   4\ndtype: int64']"
1945,..\pandas\reference\api\pandas.Index.T.html,pandas.Index.T,"property Index.T[source]# Return the transpose, which is by definition self.",No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.T\n0     Ant\n1    Bear\n2     Cow\ndtype: object"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx.T\nIndex([1, 2, 3], dtype='int64')""]"
1946,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_month_start.html,pandas.tseries.offsets.WeekOfMonth.is_month_start,WeekOfMonth.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1947,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_month_end.html,pandas.tseries.offsets.BusinessMonthBegin.is_month_end,BusinessMonthBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
1948,..\pandas\reference\api\pandas.Series.between.html,pandas.Series.between,"Series.between(left, right, inclusive='both')[source]# Return boolean Series equivalent to left <= series <= right. This function returns a boolean vector containing True wherever the corresponding Series element is between the boundary values left and right. NA values are treated as False. Notes This function is equivalent to (left <= ser) & (ser <= right)","Parameters: leftscalar or list-likeLeft boundary. rightscalar or list-likeRight boundary. inclusive{“both”, “neither”, “left”, “right”}Include boundaries. Whether to set each bound as closed or open. Changed in version 1.3.0. Returns: SeriesSeries representing whether each element is between left and right (inclusive).","['>>> s = pd.Series([2, 0, 4, 8, np.nan])', '>>> s.between(1, 4)\n0     True\n1    False\n2     True\n3    False\n4    False\ndtype: bool', '>>> s.between(1, 4, inclusive=""neither"")\n0     True\n1    False\n2    False\n3    False\n4    False\ndtype: bool', "">>> s = pd.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n>>> s.between('Anna', 'Daniel')\n0    False\n1     True\n2     True\n3    False\ndtype: bool""]"
1949,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_month_start.html,pandas.tseries.offsets.Hour.is_month_start,Hour.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1950,..\pandas\reference\api\pandas.Series.str.rindex.html,pandas.Series.str.rindex,"Series.str.rindex(sub, start=0, end=None)[source]# Return highest indexes in each string in Series/Index. Each of the returned indexes corresponds to the position where the substring is fully contained between [start:end]. This is the same as str.rfind except instead of returning -1, it raises a ValueError when the substring is not found. Equivalent to standard str.rindex.",Parameters: substrSubstring being searched. startintLeft edge index. endintRight edge index. Returns: Series or Index of object,"['>>> ser = pd.Series([""horse"", ""eagle"", ""donkey""])\n>>> ser.str.index(""e"")\n0   4\n1   0\n2   4\ndtype: int64', '>>> ser = pd.Series([""Deer"", ""eagle"", ""Sheep""])\n>>> ser.str.rindex(""e"")\n0   2\n1   4\n2   3\ndtype: int64']"
1951,..\pandas\reference\api\pandas.Index.take.html,pandas.Index.take,"Index.take(indices, axis=0, allow_fill=True, fill_value=None, **kwargs)[source]# Return a new Index of the values selected by the indices. For internal compatibility with numpy arrays.","Parameters: indicesarray-likeIndices to be taken. axisint, optionalThe axis over which to select values, always 0. allow_fillbool, default True fill_valuescalar, default NoneIf allow_fill=True and fill_value is not None, indices specified by -1 are regarded as NA. If Index doesn’t hold NA, raise ValueError. Returns: IndexAn index formed of elements at the given indices. Will be the same type as self, except for RangeIndex.","["">>> idx = pd.Index(['a', 'b', 'c'])\n>>> idx.take([2, 2, 1, 2])\nIndex(['c', 'c', 'b', 'c'], dtype='object')""]"
1952,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_on_offset.html,pandas.tseries.offsets.WeekOfMonth.is_on_offset,WeekOfMonth.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1953,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_month_start.html,pandas.tseries.offsets.BusinessMonthBegin.is_month_start,BusinessMonthBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
1954,..\pandas\reference\api\pandas.Series.between_time.html,pandas.Series.between_time,"Series.between_time(start_time, end_time, inclusive='both', axis=None)[source]# Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time, you can get the times that are not between the two times.","Parameters: start_timedatetime.time or strInitial time as a time filter limit. end_timedatetime.time or strEnd time as a time filter limit. inclusive{“both”, “neither”, “left”, “right”}, default “both”Include boundaries; whether to set each bound as closed or open. axis{0 or ‘index’, 1 or ‘columns’}, default 0Determine range time on index or columns value. For Series this parameter is unused and defaults to 0. Returns: Series or DataFrameData from the original object filtered to the specified dates range. Raises: TypeErrorIf the index is not  a DatetimeIndex","["">>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min')\n>>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i)\n>>> ts\n                     A\n2018-04-09 00:00:00  1\n2018-04-10 00:20:00  2\n2018-04-11 00:40:00  3\n2018-04-12 01:00:00  4"", "">>> ts.between_time('0:15', '0:45')\n                     A\n2018-04-10 00:20:00  2\n2018-04-11 00:40:00  3"", "">>> ts.between_time('0:45', '0:15')\n                     A\n2018-04-09 00:00:00  1\n2018-04-12 01:00:00  4""]"
1955,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_on_offset.html,pandas.tseries.offsets.Hour.is_on_offset,Hour.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1956,..\pandas\reference\api\pandas.Series.str.rjust.html,pandas.Series.str.rjust,"Series.str.rjust(width, fillchar=' ')[source]# Pad left side of strings in the Series/Index. Equivalent to str.rjust().","Parameters: widthintMinimum width of resulting string; additional characters will be filled with fillchar. fillcharstrAdditional character for filling, default is whitespace. Returns: Series/Index of objects.","["">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.center(8, fillchar='.')\n0   ..dog...\n1   ..bird..\n2   .mouse..\ndtype: object"", "">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.ljust(8, fillchar='.')\n0   dog.....\n1   bird....\n2   mouse...\ndtype: object"", "">>> ser = pd.Series(['dog', 'bird', 'mouse'])\n>>> ser.str.rjust(8, fillchar='.')\n0   .....dog\n1   ....bird\n2   ...mouse\ndtype: object""]"
1957,..\pandas\reference\api\pandas.Series.str.rpartition.html,pandas.Series.str.rpartition,"Series.str.rpartition(sep=' ', expand=True)[source]# Split the string at the last occurrence of sep. This method splits the string at the last occurrence of sep, and returns 3 elements containing the part before the separator, the separator itself, and the part after the separator. If the separator is not found, return 3 elements containing two empty strings, followed by the string itself.","Parameters: sepstr, default whitespaceString to split on. expandbool, default TrueIf True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index. Returns: DataFrame/MultiIndex or Series/Index of objects","["">>> s = pd.Series(['Linda van der Berg', 'George Pitt-Rivers'])\n>>> s\n0    Linda van der Berg\n1    George Pitt-Rivers\ndtype: object"", '>>> s.str.partition()\n        0  1             2\n0   Linda     van der Berg\n1  George      Pitt-Rivers', '>>> s.str.rpartition()\n               0  1            2\n0  Linda van der            Berg\n1         George     Pitt-Rivers', "">>> s.str.partition('-')\n                    0  1       2\n0  Linda van der Berg\n1         George Pitt  -  Rivers"", "">>> s.str.partition('-', expand=False)\n0    (Linda van der Berg, , )\n1    (George Pitt, -, Rivers)\ndtype: object"", "">>> idx = pd.Index(['X 123', 'Y 999'])\n>>> idx\nIndex(['X 123', 'Y 999'], dtype='object')"", "">>> idx.str.partition()\nMultiIndex([('X', ' ', '123'),\n            ('Y', ' ', '999')],\n           )"", "">>> idx.str.partition(expand=False)\nIndex([('X', ' ', '123'), ('Y', ' ', '999')], dtype='object')""]"
1958,..\pandas\reference\api\pandas.Index.to_frame.html,pandas.Index.to_frame,"Index.to_frame(index=True, name=<no_default>)[source]# Create a DataFrame with a column containing the Index.","Parameters: indexbool, default TrueSet the index of the returned DataFrame as the original Index. nameobject, defaults to index.nameThe passed name should substitute for the index name (if it has one). Returns: DataFrameDataFrame containing the original Index data.","["">>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')\n>>> idx.to_frame()\n       animal\nanimal\nAnt       Ant\nBear     Bear\nCow       Cow"", '>>> idx.to_frame(index=False)\n    animal\n0   Ant\n1  Bear\n2   Cow', "">>> idx.to_frame(index=False, name='zoo')\n    zoo\n0   Ant\n1  Bear\n2   Cow""]"
1959,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_quarter_end.html,pandas.tseries.offsets.WeekOfMonth.is_quarter_end,WeekOfMonth.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1960,..\pandas\reference\api\pandas.Series.bfill.html,pandas.Series.bfill,"Series.bfill(*, axis=None, inplace=False, limit=None, limit_area=None, downcast=<no_default>)[source]# Fill NA/NaN values by using the next valid observation to fill the gap.","Parameters: axis{0 or ‘index’} for Series, {0 or ‘index’, 1 or ‘columns’} for DataFrameAxis along which to fill missing values. For Series this parameter is unused and defaults to 0. inplacebool, default FalseIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limitint, default NoneIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. limit_area{None, ‘inside’, ‘outside’}, default NoneIf limit is specified, consecutive NaNs will be filled with this restriction. None: No fill restriction. ‘inside’: Only fill NaNs surrounded by valid values (interpolate). ‘outside’: Only fill NaNs outside valid values (extrapolate). Added in version 2.2.0. downcastdict, default is NoneA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Deprecated since version 2.2.0. Returns: Series/DataFrame or NoneObject with missing values filled or None if inplace=True.","['>>> s = pd.Series([1, None, None, 2])\n>>> s.bfill()\n0    1.0\n1    2.0\n2    2.0\n3    2.0\ndtype: float64\n>>> s.bfill(limit=1)\n0    1.0\n1    NaN\n2    2.0\n3    2.0\ndtype: float64', "">>> df = pd.DataFrame({'A': [1, None, None, 4], 'B': [None, 5, None, 7]})\n>>> df\n      A     B\n0   1.0   NaN\n1   NaN   5.0\n2   NaN   NaN\n3   4.0   7.0\n>>> df.bfill()\n      A     B\n0   1.0   5.0\n1   4.0   5.0\n2   4.0   7.0\n3   4.0   7.0\n>>> df.bfill(limit=1)\n      A     B\n0   1.0   5.0\n1   NaN   5.0\n2   4.0   7.0\n3   4.0   7.0""]"
1961,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_quarter_end.html,pandas.tseries.offsets.Hour.is_quarter_end,Hour.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1962,..\pandas\reference\api\pandas.Series.str.rsplit.html,pandas.Series.str.rsplit,"Series.str.rsplit(pat=None, *, n=-1, expand=False)[source]# Split strings around given separator/delimiter. Splits the string in the Series/Index from the end, at the specified delimiter string. Notes The handling of the n keyword depends on the number of found splits: If found splits > n,  make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively.","Parameters: patstr, optionalString to split on. If not specified, split on whitespace. nint, default -1 (all)Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits. expandbool, default FalseExpand the split strings into separate columns. If True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index, containing lists of strings. Returns: Series, Index, DataFrame or MultiIndexType matches caller unless expand=True (see Notes).","['>>> s = pd.Series(\n...     [\n...         ""this is a regular sentence"",\n...         ""https://docs.python.org/3/tutorial/index.html"",\n...         np.nan\n...     ]\n... )\n>>> s\n0                       this is a regular sentence\n1    https://docs.python.org/3/tutorial/index.html\n2                                              NaN\ndtype: object', '>>> s.str.split()\n0                   [this, is, a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.rsplit()\n0                   [this, is, a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.split(n=2)\n0                     [this, is, a regular sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.rsplit(n=2)\n0                     [this is a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.split(pat=""/"")\n0                         [this is a regular sentence]\n1    [https:, , docs.python.org, 3, tutorial, index...\n2                                                  NaN\ndtype: object', '>>> s.str.split(expand=True)\n                                               0     1     2        3         4\n0                                           this    is     a  regular  sentence\n1  https://docs.python.org/3/tutorial/index.html  None  None     None      None\n2                                            NaN   NaN   NaN      NaN       NaN', '>>> s.str.rsplit(""/"", n=1, expand=True)\n                                    0           1\n0          this is a regular sentence        None\n1  https://docs.python.org/3/tutorial  index.html\n2                                 NaN         NaN']"
1963,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_on_offset.html,pandas.tseries.offsets.BusinessMonthBegin.is_on_offset,BusinessMonthBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
1964,..\pandas\reference\api\pandas.Series.str.rstrip.html,pandas.Series.str.rstrip,Series.str.rstrip(to_strip=None)[source]# Remove trailing characters. Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from right side. Replaces any non-strings in Series with NaNs. Equivalent to str.rstrip().,"Parameters: to_stripstr or None, default NoneSpecifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed. Returns: Series or Index of object","["">>> s = pd.Series(['1. Ant.  ', '2. Bee!\\n', '3. Cat?\\t', np.nan, 10, True])\n>>> s\n0    1. Ant.\n1    2. Bee!\\n\n2    3. Cat?\\t\n3          NaN\n4           10\n5         True\ndtype: object"", '>>> s.str.strip()\n0    1. Ant.\n1    2. Bee!\n2    3. Cat?\n3        NaN\n4        NaN\n5        NaN\ndtype: object', "">>> s.str.lstrip('123.')\n0    Ant.\n1    Bee!\\n\n2    Cat?\\t\n3       NaN\n4       NaN\n5       NaN\ndtype: object"", "">>> s.str.rstrip('.!? \\n\\t')\n0    1. Ant\n1    2. Bee\n2    3. Cat\n3       NaN\n4       NaN\n5       NaN\ndtype: object"", "">>> s.str.strip('123.!? \\n\\t')\n0    Ant\n1    Bee\n2    Cat\n3    NaN\n4    NaN\n5    NaN\ndtype: object""]"
1965,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_quarter_start.html,pandas.tseries.offsets.Hour.is_quarter_start,Hour.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1966,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_quarter_start.html,pandas.tseries.offsets.WeekOfMonth.is_quarter_start,WeekOfMonth.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1967,..\pandas\reference\api\pandas.Series.bool.html,pandas.Series.bool,"Series.bool()[source]# Return the bool of a single element Series or DataFrame. Deprecated since version 2.1.0: bool is deprecated and will be removed in future version of pandas. For Series use pandas.Series.item. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).",Returns: boolThe value in the Series or DataFrame.,"['>>> pd.Series([True]).bool()  \nTrue\n>>> pd.Series([False]).bool()  \nFalse', "">>> pd.DataFrame({'col': [True]}).bool()  \nTrue\n>>> pd.DataFrame({'col': [False]}).bool()  \nFalse"", '>>> pd.Series([True]).item()  \nTrue\n>>> pd.Series([False]).item()  \nFalse']"
1968,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_quarter_end.html,pandas.tseries.offsets.BusinessMonthBegin.is_quarter_end,BusinessMonthBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
1969,..\pandas\reference\api\pandas.Index.to_list.html,pandas.Index.to_list,"Index.to_list()[source]# Return a list of the values. These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)",Returns: list,"['>>> s = pd.Series([1, 2, 3])\n>>> s.to_list()\n[1, 2, 3]', "">>> idx = pd.Index([1, 2, 3])\n>>> idx\nIndex([1, 2, 3], dtype='int64')"", '>>> idx.to_list()\n[1, 2, 3]']"
1970,..\pandas\reference\api\pandas.Series.str.slice.html,pandas.Series.str.slice,"Series.str.slice(start=None, stop=None, step=None)[source]# Slice substrings from each element in the Series or Index.","Parameters: startint, optionalStart position for slice operation. stopint, optionalStop position for slice operation. stepint, optionalStep size for slice operation. Returns: Series or Index of objectSeries or Index from sliced substring from original string object.","['>>> s = pd.Series([""koala"", ""dog"", ""chameleon""])\n>>> s\n0        koala\n1          dog\n2    chameleon\ndtype: object', '>>> s.str.slice(start=1)\n0        oala\n1          og\n2    hameleon\ndtype: object', '>>> s.str.slice(start=-1)\n0           a\n1           g\n2           n\ndtype: object', '>>> s.str.slice(stop=2)\n0    ko\n1    do\n2    ch\ndtype: object', '>>> s.str.slice(step=2)\n0      kaa\n1       dg\n2    caeen\ndtype: object', '>>> s.str.slice(start=0, stop=5, step=3)\n0    kl\n1     d\n2    cm\ndtype: object', '>>> s.str[0:5:3]\n0    kl\n1     d\n2    cm\ndtype: object']"
1971,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_year_end.html,pandas.tseries.offsets.Hour.is_year_end,Hour.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1972,..\pandas\reference\api\pandas.Series.case_when.html,pandas.Series.case_when,Series.case_when(caselist)[source]# Replace values where the conditions are True.,"Parameters: caselistA list of tuples of conditions and expected replacementsTakes the form:  (condition0, replacement0), (condition1, replacement1), … . condition should be a 1-D boolean array-like object or a callable. If condition is a callable, it is computed on the Series and should return a boolean Series or array. The callable must not change the input Series (though pandas doesn`t check it). replacement should be a 1-D array-like object, a scalar or a callable. If replacement is a callable, it is computed on the Series and should return a scalar or Series. The callable must not change the input Series (though pandas doesn`t check it). Added in version 2.2.0. Returns: Series","["">>> c = pd.Series([6, 7, 8, 9], name='c')\n>>> a = pd.Series([0, 0, 1, 2])\n>>> b = pd.Series([0, 3, 4, 5])"", '>>> c.case_when(caselist=[(a.gt(0), a),  # condition, replacement\n...                       (b.gt(0), b)])\n0    6\n1    3\n2    1\n3    2\nName: c, dtype: int64']"
1973,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_year_end.html,pandas.tseries.offsets.WeekOfMonth.is_year_end,WeekOfMonth.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1974,..\pandas\reference\api\pandas.Index.to_series.html,pandas.Index.to_series,"final Index.to_series(index=None, name=None)[source]# Create a Series with both index and values equal to the index keys. Useful with map for returning an indexer based on an index.","Parameters: indexIndex, optionalIndex of resulting Series. If None, defaults to original index. namestr, optionalName of resulting Series. If None, defaults to name of original index. Returns: SeriesThe dtype will be based on the type of the Index values.","["">>> idx = pd.Index(['Ant', 'Bear', 'Cow'], name='animal')"", '>>> idx.to_series()\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: animal, dtype: object', '>>> idx.to_series(index=[0, 1, 2])\n0     Ant\n1    Bear\n2     Cow\nName: animal, dtype: object', "">>> idx.to_series(name='zoo')\nanimal\nAnt      Ant\nBear    Bear\nCow      Cow\nName: zoo, dtype: object""]"
1975,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_quarter_start.html,pandas.tseries.offsets.BusinessMonthBegin.is_quarter_start,BusinessMonthBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
1976,..\pandas\reference\api\pandas.Series.str.slice_replace.html,pandas.Series.str.slice_replace,"Series.str.slice_replace(start=None, stop=None, repl=None)[source]# Replace a positional slice of a string with another value.","Parameters: startint, optionalLeft index position to use for the slice. If not specified (None), the slice is unbounded on the left, i.e. slice from the start of the string. stopint, optionalRight index position to use for the slice. If not specified (None), the slice is unbounded on the right, i.e. slice until the end of the string. replstr, optionalString for replacement. If not specified (None), the sliced region is replaced with an empty string. Returns: Series or IndexSame type as the original object.","["">>> s = pd.Series(['a', 'ab', 'abc', 'abdc', 'abcde'])\n>>> s\n0        a\n1       ab\n2      abc\n3     abdc\n4    abcde\ndtype: object"", "">>> s.str.slice_replace(1, repl='X')\n0    aX\n1    aX\n2    aX\n3    aX\n4    aX\ndtype: object"", "">>> s.str.slice_replace(stop=2, repl='X')\n0       X\n1       X\n2      Xc\n3     Xdc\n4    Xcde\ndtype: object"", "">>> s.str.slice_replace(start=1, stop=3, repl='X')\n0      aX\n1      aX\n2      aX\n3     aXc\n4    aXde\ndtype: object""]"
1977,..\pandas\reference\api\pandas.Series.cat.add_categories.html,pandas.Series.cat.add_categories,"Series.cat.add_categories(*args, **kwargs)[source]# Add new categories. new_categories will be included at the last/highest place in the categories and will be unused directly after this call.",Parameters: new_categoriescategory or list-like of categoryThe new categories to be included. Returns: CategoricalCategorical with new categories added. Raises: ValueErrorIf the new categories include old categories or do not validate as categories,"["">>> c = pd.Categorical(['c', 'b', 'c'])\n>>> c\n['c', 'b', 'c']\nCategories (2, object): ['b', 'c']"", "">>> c.add_categories(['d', 'a'])\n['c', 'b', 'c']\nCategories (4, object): ['b', 'c', 'd', 'a']""]"
1978,..\pandas\reference\api\pandas.tseries.offsets.Hour.is_year_start.html,pandas.tseries.offsets.Hour.is_year_start,Hour.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1979,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.is_year_start.html,pandas.tseries.offsets.WeekOfMonth.is_year_start,WeekOfMonth.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1980,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_year_end.html,pandas.tseries.offsets.BusinessMonthBegin.is_year_end,BusinessMonthBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
1981,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.kwds.html,pandas.tseries.offsets.WeekOfMonth.kwds,WeekOfMonth.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1982,..\pandas\reference\api\pandas.Series.cat.as_ordered.html,pandas.Series.cat.as_ordered,"Series.cat.as_ordered(*args, **kwargs)[source]# Set the Categorical to be ordered.",Returns: CategoricalOrdered Categorical.,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.ordered\nFalse\n>>> ser = ser.cat.as_ordered()\n>>> ser.cat.ordered\nTrue"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a'])\n>>> ci.ordered\nFalse\n>>> ci = ci.as_ordered()\n>>> ci.ordered\nTrue""]"
1983,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.is_year_start.html,pandas.tseries.offsets.BusinessMonthBegin.is_year_start,BusinessMonthBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
1984,..\pandas\reference\api\pandas.tseries.offsets.Hour.kwds.html,pandas.tseries.offsets.Hour.kwds,Hour.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1985,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.n.html,pandas.tseries.offsets.WeekOfMonth.n,WeekOfMonth.n#,No parameters found,[]
1986,..\pandas\reference\api\pandas.Series.str.split.html,pandas.Series.str.split,"Series.str.split(pat=None, *, n=-1, expand=False, regex=None)[source]# Split strings around given separator/delimiter. Splits the string in the Series/Index from the beginning, at the specified delimiter string. Notes The handling of the n keyword depends on the number of found splits: If found splits > n,  make first n splits only If found splits <= n, make all splits If for a certain row the number of found splits < n, append None for padding up to n if expand=True If using expand=True, Series and Index callers return DataFrame and MultiIndex objects, respectively. Use of regex =False with a pat as a compiled regex will raise an error.","Parameters: patstr or compiled regex, optionalString or regular expression to split on. If not specified, split on whitespace. nint, default -1 (all)Limit number of splits in output. None, 0 and -1 will be interpreted as return all splits. expandbool, default FalseExpand the split strings into separate columns. If True, return DataFrame/MultiIndex expanding dimensionality. If False, return Series/Index, containing lists of strings. regexbool, default NoneDetermines if the passed-in pattern is a regular expression: If True, assumes the passed-in pattern is a regular expression If False, treats the pattern as a literal string. If None and pat length is 1, treats pat as a literal string. If None and pat length is not 1, treats pat as a regular expression. Cannot be set to False if pat is a compiled regex Added in version 1.4.0. Returns: Series, Index, DataFrame or MultiIndexType matches caller unless expand=True (see Notes). Raises: ValueError if regex is False and pat is a compiled regex","['>>> s = pd.Series(\n...     [\n...         ""this is a regular sentence"",\n...         ""https://docs.python.org/3/tutorial/index.html"",\n...         np.nan\n...     ]\n... )\n>>> s\n0                       this is a regular sentence\n1    https://docs.python.org/3/tutorial/index.html\n2                                              NaN\ndtype: object', '>>> s.str.split()\n0                   [this, is, a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.rsplit()\n0                   [this, is, a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.split(n=2)\n0                     [this, is, a regular sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.rsplit(n=2)\n0                     [this is a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object', '>>> s.str.split(pat=""/"")\n0                         [this is a regular sentence]\n1    [https:, , docs.python.org, 3, tutorial, index...\n2                                                  NaN\ndtype: object', '>>> s.str.split(expand=True)\n                                               0     1     2        3         4\n0                                           this    is     a  regular  sentence\n1  https://docs.python.org/3/tutorial/index.html  None  None     None      None\n2                                            NaN   NaN   NaN      NaN       NaN', '>>> s.str.rsplit(""/"", n=1, expand=True)\n                                    0           1\n0          this is a regular sentence        None\n1  https://docs.python.org/3/tutorial  index.html\n2                                 NaN         NaN', '>>> s = pd.Series([""foo and bar plus baz""])\n>>> s.str.split(r""and|plus"", expand=True)\n    0   1   2\n0 foo bar baz', '>>> s = pd.Series([\'foojpgbar.jpg\'])\n>>> s.str.split(r""."", expand=True)\n           0    1\n0  foojpgbar  jpg', '>>> s.str.split(r""\\.jpg"", expand=True)\n           0 1\n0  foojpgbar', '>>> s.str.split(r""\\.jpg"", regex=True, expand=True)\n           0 1\n0  foojpgbar', '>>> import re\n>>> s.str.split(re.compile(r""\\.jpg""), expand=True)\n           0 1\n0  foojpgbar', '>>> s.str.split(r""\\.jpg"", regex=False, expand=True)\n               0\n0  foojpgbar.jpg']"
1987,..\pandas\reference\api\pandas.Series.cat.as_unordered.html,pandas.Series.cat.as_unordered,"Series.cat.as_unordered(*args, **kwargs)[source]# Set the Categorical to be unordered.",Returns: CategoricalUnordered Categorical.,"["">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.ordered\nTrue\n>>> ser = ser.cat.as_unordered()\n>>> ser.cat.ordered\nFalse"", "">>> ci = pd.CategoricalIndex(['a', 'b', 'c', 'a'], ordered=True)\n>>> ci.ordered\nTrue\n>>> ci = ci.as_unordered()\n>>> ci.ordered\nFalse""]"
1988,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.name.html,pandas.tseries.offsets.WeekOfMonth.name,WeekOfMonth.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1989,..\pandas\reference\api\pandas.Series.str.startswith.html,pandas.Series.str.startswith,"Series.str.startswith(pat, na=None)[source]# Test if the start of each string element matches a pattern. Equivalent to str.startswith().","Parameters: patstr or tuple[str, …]Character sequence or tuple of strings. Regular expressions are not accepted. naobject, default NaNObject shown if element tested is not a string. The default depends on dtype of the array. For object-dtype, numpy.nan is used. For StringDtype, pandas.NA is used. Returns: Series or Index of boolA Series of booleans indicating whether the given pattern matches the start of each string element.","["">>> s = pd.Series(['bat', 'Bear', 'cat', np.nan])\n>>> s\n0     bat\n1    Bear\n2     cat\n3     NaN\ndtype: object"", "">>> s.str.startswith('b')\n0     True\n1    False\n2    False\n3      NaN\ndtype: object"", "">>> s.str.startswith(('b', 'B'))\n0     True\n1     True\n2    False\n3      NaN\ndtype: object"", "">>> s.str.startswith('b', na=False)\n0     True\n1    False\n2    False\n3    False\ndtype: bool""]"
1990,..\pandas\reference\api\pandas.tseries.offsets.Hour.n.html,pandas.tseries.offsets.Hour.n,Hour.n#,No parameters found,[]
1991,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.kwds.html,pandas.tseries.offsets.BusinessMonthBegin.kwds,BusinessMonthBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
1992,..\pandas\reference\api\pandas.Series.cat.categories.html,pandas.Series.cat.categories,Series.cat.categories[source]# The categories of this categorical. Setting assigns new values to each category (effectively a rename of each individual category). The assigned value has to be a list-like object. All items must be unique and the number of items in the new categories must be the same as the number of items in the old categories.,Raises: ValueErrorIf the new categories do not validate as categories or if the number of new categories is unequal the number of old categories,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.categories\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], categories=['b', 'c', 'd'])\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.categories\nIndex(['b', 'c', 'd'], dtype='object')"", "">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.categories\nIndex(['a', 'b'], dtype='object')"", "">>> ci = pd.CategoricalIndex(['a', 'c', 'b', 'a', 'c', 'b'])\n>>> ci.categories\nIndex(['a', 'b', 'c'], dtype='object')"", "">>> ci = pd.CategoricalIndex(['a', 'c'], categories=['c', 'b', 'a'])\n>>> ci.categories\nIndex(['c', 'b', 'a'], dtype='object')""]"
1993,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.nanos.html,pandas.tseries.offsets.WeekOfMonth.nanos,WeekOfMonth.nanos#,No parameters found,[]
1994,..\pandas\reference\api\pandas.Series.str.strip.html,pandas.Series.str.strip,Series.str.strip(to_strip=None)[source]# Remove leading and trailing characters. Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from left and right sides. Replaces any non-strings in Series with NaNs. Equivalent to str.strip().,"Parameters: to_stripstr or None, default NoneSpecifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed. Returns: Series or Index of object","["">>> s = pd.Series(['1. Ant.  ', '2. Bee!\\n', '3. Cat?\\t', np.nan, 10, True])\n>>> s\n0    1. Ant.\n1    2. Bee!\\n\n2    3. Cat?\\t\n3          NaN\n4           10\n5         True\ndtype: object"", '>>> s.str.strip()\n0    1. Ant.\n1    2. Bee!\n2    3. Cat?\n3        NaN\n4        NaN\n5        NaN\ndtype: object', "">>> s.str.lstrip('123.')\n0    Ant.\n1    Bee!\\n\n2    Cat?\\t\n3       NaN\n4       NaN\n5       NaN\ndtype: object"", "">>> s.str.rstrip('.!? \\n\\t')\n0    1. Ant\n1    2. Bee\n2    3. Cat\n3       NaN\n4       NaN\n5       NaN\ndtype: object"", "">>> s.str.strip('123.!? \\n\\t')\n0    Ant\n1    Bee\n2    Cat\n3    NaN\n4    NaN\n5    NaN\ndtype: object""]"
1995,..\pandas\reference\api\pandas.tseries.offsets.Hour.name.html,pandas.tseries.offsets.Hour.name,Hour.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
1996,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.n.html,pandas.tseries.offsets.BusinessMonthBegin.n,BusinessMonthBegin.n#,No parameters found,[]
1997,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.normalize.html,pandas.tseries.offsets.WeekOfMonth.normalize,WeekOfMonth.normalize#,No parameters found,[]
1998,..\pandas\reference\api\pandas.Series.str.swapcase.html,pandas.Series.str.swapcase,Series.str.swapcase()[source]# Convert strings in the Series/Index to be swapcased. Equivalent to str.swapcase().,Returns: Series or Index of object,"["">>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object"", '>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object', '>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object', '>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object', '>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object', '>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object']"
1999,..\pandas\reference\api\pandas.Series.cat.codes.html,pandas.Series.cat.codes,Series.cat.codes[source]# Return Series of codes as well as the index.,No parameters found,"['>>> raw_cate = pd.Categorical([""a"", ""b"", ""c"", ""a""], categories=[""a"", ""b""])\n>>> ser = pd.Series(raw_cate)\n>>> ser.cat.codes\n0   0\n1   1\n2  -1\n3   0\ndtype: int8']"
2000,..\pandas\reference\api\pandas.tseries.offsets.Hour.nanos.html,pandas.tseries.offsets.Hour.nanos,Hour.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
2001,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.name.html,pandas.tseries.offsets.BusinessMonthBegin.name,BusinessMonthBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
2002,..\pandas\reference\api\pandas.Series.str.title.html,pandas.Series.str.title,Series.str.title()[source]# Convert strings in the Series/Index to titlecase. Equivalent to str.title().,Returns: Series or Index of object,"["">>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object"", '>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object', '>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object', '>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object', '>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object', '>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object']"
2003,..\pandas\reference\api\pandas.Series.cat.html,pandas.Series.cat,Series.cat()[source]# Accessor object for categorical properties of the Series values.,Parameters: dataSeries or CategoricalIndex,"['>>> s = pd.Series(list(""abbccc"")).astype(""category"")\n>>> s\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (3, object): [\'a\', \'b\', \'c\']', "">>> s.cat.categories\nIndex(['a', 'b', 'c'], dtype='object')"", '>>> s.cat.rename_categories(list(""cba""))\n0    c\n1    b\n2    b\n3    a\n4    a\n5    a\ndtype: category\nCategories (3, object): [\'c\', \'b\', \'a\']', '>>> s.cat.reorder_categories(list(""cba""))\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (3, object): [\'c\', \'b\', \'a\']', '>>> s.cat.add_categories([""d"", ""e""])\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (5, object): [\'a\', \'b\', \'c\', \'d\', \'e\']', '>>> s.cat.remove_categories([""a"", ""c""])\n0    NaN\n1      b\n2      b\n3    NaN\n4    NaN\n5    NaN\ndtype: category\nCategories (1, object): [\'b\']', '>>> s1 = s.cat.add_categories([""d"", ""e""])\n>>> s1.cat.remove_unused_categories()\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (3, object): [\'a\', \'b\', \'c\']', '>>> s.cat.set_categories(list(""abcde""))\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (5, object): [\'a\', \'b\', \'c\', \'d\', \'e\']', "">>> s.cat.as_ordered()\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (3, object): ['a' < 'b' < 'c']"", "">>> s.cat.as_unordered()\n0    a\n1    b\n2    b\n3    c\n4    c\n5    c\ndtype: category\nCategories (3, object): ['a', 'b', 'c']""]"
2004,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.rule_code.html,pandas.tseries.offsets.WeekOfMonth.rule_code,WeekOfMonth.rule_code#,No parameters found,[]
2005,..\pandas\reference\api\pandas.tseries.offsets.Hour.normalize.html,pandas.tseries.offsets.Hour.normalize,Hour.normalize#,No parameters found,[]
2006,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.nanos.html,pandas.tseries.offsets.BusinessMonthBegin.nanos,BusinessMonthBegin.nanos#,No parameters found,[]
2007,..\pandas\reference\api\pandas.Series.str.translate.html,pandas.Series.str.translate,Series.str.translate(table)[source]# Map all characters in the string through the given mapping table. Equivalent to standard str.translate().,"Parameters: tabledictTable is a mapping of Unicode ordinals to Unicode ordinals, strings, or None. Unmapped characters are left untouched. Characters mapped to None are deleted. str.maketrans() is a helper function for making translation tables. Returns: Series or Index","['>>> ser = pd.Series([""El niño"", ""Françoise""])\n>>> mytable = str.maketrans({\'ñ\': \'n\', \'ç\': \'c\'})\n>>> ser.str.translate(mytable)\n0   El nino\n1   Francoise\ndtype: object']"
2008,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.week.html,pandas.tseries.offsets.WeekOfMonth.week,WeekOfMonth.week#,No parameters found,[]
2009,..\pandas\reference\api\pandas.Series.cat.ordered.html,pandas.Series.cat.ordered,Series.cat.ordered[source]# Whether the categories have an ordered relationship.,No parameters found,"["">>> ser = pd.Series(['a', 'b', 'c', 'a'], dtype='category')\n>>> ser.cat.ordered\nFalse"", "">>> raw_cat = pd.Categorical(['a', 'b', 'c', 'a'], ordered=True)\n>>> ser = pd.Series(raw_cat)\n>>> ser.cat.ordered\nTrue"", "">>> cat = pd.Categorical(['a', 'b'], ordered=True)\n>>> cat.ordered\nTrue"", "">>> cat = pd.Categorical(['a', 'b'], ordered=False)\n>>> cat.ordered\nFalse"", "">>> ci = pd.CategoricalIndex(['a', 'b'], ordered=True)\n>>> ci.ordered\nTrue"", "">>> ci = pd.CategoricalIndex(['a', 'b'], ordered=False)\n>>> ci.ordered\nFalse""]"
2010,..\pandas\reference\api\pandas.tseries.offsets.Hour.rule_code.html,pandas.tseries.offsets.Hour.rule_code,Hour.rule_code#,No parameters found,[]
2011,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.normalize.html,pandas.tseries.offsets.BusinessMonthBegin.normalize,BusinessMonthBegin.normalize#,No parameters found,[]
2012,..\pandas\reference\api\pandas.Series.str.upper.html,pandas.Series.str.upper,Series.str.upper()[source]# Convert strings in the Series/Index to uppercase. Equivalent to str.upper().,Returns: Series or Index of object,"["">>> s = pd.Series(['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe'])\n>>> s\n0                 lower\n1              CAPITALS\n2    this is a sentence\n3              SwApCaSe\ndtype: object"", '>>> s.str.lower()\n0                 lower\n1              capitals\n2    this is a sentence\n3              swapcase\ndtype: object', '>>> s.str.upper()\n0                 LOWER\n1              CAPITALS\n2    THIS IS A SENTENCE\n3              SWAPCASE\ndtype: object', '>>> s.str.title()\n0                 Lower\n1              Capitals\n2    This Is A Sentence\n3              Swapcase\ndtype: object', '>>> s.str.capitalize()\n0                 Lower\n1              Capitals\n2    This is a sentence\n3              Swapcase\ndtype: object', '>>> s.str.swapcase()\n0                 LOWER\n1              capitals\n2    THIS IS A SENTENCE\n3              sWaPcAsE\ndtype: object']"
2013,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthBegin.rule_code.html,pandas.tseries.offsets.BusinessMonthBegin.rule_code,BusinessMonthBegin.rule_code#,No parameters found,[]
2014,..\pandas\reference\api\pandas.Series.cat.remove_categories.html,pandas.Series.cat.remove_categories,"Series.cat.remove_categories(*args, **kwargs)[source]# Remove the specified categories. removals must be included in the old categories. Values which were in the removed categories will be set to NaN",Parameters: removalscategory or list of categoriesThe categories which should be removed. Returns: CategoricalCategorical with removed categories. Raises: ValueErrorIf the removals are not contained in the categories,"["">>> c = pd.Categorical(['a', 'c', 'b', 'c', 'd'])\n>>> c\n['a', 'c', 'b', 'c', 'd']\nCategories (4, object): ['a', 'b', 'c', 'd']"", "">>> c.remove_categories(['d', 'a'])\n[NaN, 'c', 'b', 'c', NaN]\nCategories (2, object): ['b', 'c']""]"
2015,..\pandas\reference\api\pandas.tseries.offsets.WeekOfMonth.weekday.html,pandas.tseries.offsets.WeekOfMonth.weekday,WeekOfMonth.weekday#,No parameters found,[]
2016,..\pandas\reference\api\pandas.Series.str.wrap.html,pandas.Series.str.wrap,"Series.str.wrap(width, **kwargs)[source]# Wrap strings in Series/Index at specified line width. This method has the same keyword parameters and defaults as textwrap.TextWrapper. Notes Internally, this method uses a textwrap.TextWrapper instance with default settings. To achieve behavior matching R’s stringr library str_wrap function, use the arguments: expand_tabs = False replace_whitespace = True drop_whitespace = True break_long_words = False break_on_hyphens = False","Parameters: widthintMaximum line width. expand_tabsbool, optionalIf True, tab characters will be expanded to spaces (default: True). replace_whitespacebool, optionalIf True, each whitespace character (as defined by string.whitespace) remaining after tab expansion will be replaced by a single space (default: True). drop_whitespacebool, optionalIf True, whitespace that, after wrapping, happens to end up at the beginning or end of a line is dropped (default: True). break_long_wordsbool, optionalIf True, then words longer than width will be broken in order to ensure that no lines are longer than width. If it is false, long words will not be broken, and some lines may be longer than width (default: True). break_on_hyphensbool, optionalIf True, wrapping will occur preferably on whitespace and right after hyphens in compound words, as it is customary in English. If false, only whitespaces will be considered as potentially good places for line breaks, but you need to set break_long_words to false if you want truly insecable words (default: True). Returns: Series or Index","["">>> s = pd.Series(['line to be wrapped', 'another line to be wrapped'])\n>>> s.str.wrap(12)\n0             line to be\\nwrapped\n1    another line\\nto be\\nwrapped\ndtype: object""]"
2017,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.copy.html,pandas.tseries.offsets.LastWeekOfMonth.copy,LastWeekOfMonth.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2018,..\pandas\reference\api\pandas.Series.str.zfill.html,pandas.Series.str.zfill,Series.str.zfill(width)[source]# Pad strings in the Series/Index by prepending ‘0’ characters. Strings in the Series/Index are padded with ‘0’ characters on the left of the string to reach a total string length  width. Strings in the Series/Index with length greater or equal to width are unchanged. Notes Differs from str.zfill() which has special handling for ‘+’/’-’ in the string.,Parameters: widthintMinimum length of resulting string; strings with length less than width be prepended with ‘0’ characters. Returns: Series/Index of objects.,"["">>> s = pd.Series(['-1', '1', '1000', 10, np.nan])\n>>> s\n0      -1\n1       1\n2    1000\n3      10\n4     NaN\ndtype: object"", '>>> s.str.zfill(3)\n0     -01\n1     001\n2    1000\n3     NaN\n4     NaN\ndtype: object']"
2019,..\pandas\reference\api\pandas.Series.cat.remove_unused_categories.html,pandas.Series.cat.remove_unused_categories,"Series.cat.remove_unused_categories(*args, **kwargs)[source]# Remove categories which are not used.",Returns: CategoricalCategorical with unused categories dropped.,"["">>> c = pd.Categorical(['a', 'c', 'b', 'c', 'd'])\n>>> c\n['a', 'c', 'b', 'c', 'd']\nCategories (4, object): ['a', 'b', 'c', 'd']"", "">>> c[2] = 'a'\n>>> c[4] = 'c'\n>>> c\n['a', 'c', 'a', 'c', 'c']\nCategories (4, object): ['a', 'b', 'c', 'd']"", "">>> c.remove_unused_categories()\n['a', 'c', 'a', 'c', 'c']\nCategories (2, object): ['a', 'c']""]"
2020,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.copy.html,pandas.tseries.offsets.BusinessMonthEnd.copy,BusinessMonthEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2021,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.freqstr.html,pandas.tseries.offsets.LastWeekOfMonth.freqstr,LastWeekOfMonth.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
2022,..\pandas\reference\api\pandas.Series.struct.dtypes.html,pandas.Series.struct.dtypes,Series.struct.dtypes[source]# Return the dtype object of each child field of the struct.,Returns: pandas.SeriesThe data type of each child field.,"['>>> import pyarrow as pa\n>>> s = pd.Series(\n...     [\n...         {""version"": 1, ""project"": ""pandas""},\n...         {""version"": 2, ""project"": ""pandas""},\n...         {""version"": 1, ""project"": ""numpy""},\n...     ],\n...     dtype=pd.ArrowDtype(pa.struct(\n...         [(""version"", pa.int64()), (""project"", pa.string())]\n...     ))\n... )\n>>> s.struct.dtypes\nversion     int64[pyarrow]\nproject    string[pyarrow]\ndtype: object']"
2023,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.copy.html,pandas.tseries.offsets.YearBegin.copy,YearBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2024,..\pandas\reference\api\pandas.Series.cat.rename_categories.html,pandas.Series.cat.rename_categories,"Series.cat.rename_categories(*args, **kwargs)[source]# Rename categories.","Parameters: new_categorieslist-like, dict-like or callableNew categories which will replace old categories. list-like: all items must be unique and the number of items in the new categories must match the existing number of categories. dict-like: specifies a mapping from old categories to new. Categories not contained in the mapping are passed through and extra categories in the mapping are ignored. callable : a callable that is called on all items in the old categories and whose return values comprise the new categories. Returns: CategoricalCategorical with renamed categories. Raises: ValueErrorIf new categories are list-like and do not have the same number of items than the current categories or do not validate as categories","["">>> c = pd.Categorical(['a', 'a', 'b'])\n>>> c.rename_categories([0, 1])\n[0, 0, 1]\nCategories (2, int64): [0, 1]"", "">>> c.rename_categories({'a': 'A', 'c': 'C'})\n['A', 'A', 'b']\nCategories (2, object): ['A', 'b']"", "">>> c.rename_categories(lambda x: x.upper())\n['A', 'A', 'B']\nCategories (2, object): ['A', 'B']""]"
2025,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.freqstr.html,pandas.tseries.offsets.BusinessMonthEnd.freqstr,BusinessMonthEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
2026,..\pandas\reference\api\pandas.Series.struct.explode.html,pandas.Series.struct.explode,Series.struct.explode()[source]# Extract all child fields of a struct as a DataFrame.,Returns: pandas.DataFrameThe data corresponding to all child fields.,"['>>> import pyarrow as pa\n>>> s = pd.Series(\n...     [\n...         {""version"": 1, ""project"": ""pandas""},\n...         {""version"": 2, ""project"": ""pandas""},\n...         {""version"": 1, ""project"": ""numpy""},\n...     ],\n...     dtype=pd.ArrowDtype(pa.struct(\n...         [(""version"", pa.int64()), (""project"", pa.string())]\n...     ))\n... )', '>>> s.struct.explode()\n   version project\n0        1  pandas\n1        2  pandas\n2        1   numpy']"
2027,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.freqstr.html,pandas.tseries.offsets.YearBegin.freqstr,YearBegin.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
2028,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.html,pandas.tseries.offsets.LastWeekOfMonth,class pandas.tseries.offsets.LastWeekOfMonth# Describes monthly dates in last week of month. For example “the last Tuesday of each month”. Examples Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code week weekday,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. weekdayint {0, 1, …, 6}, default 0A specific integer for the day of the week. 0 is Monday 1 is Tuesday 2 is Wednesday 3 is Thursday 4 is Friday 5 is Saturday 6 is Sunday.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.LastWeekOfMonth()\nTimestamp('2022-01-31 00:00:00')""]"
2029,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.html,pandas.tseries.offsets.BusinessMonthEnd,class pandas.tseries.offsets.BusinessMonthEnd# DateOffset increments between the last business day of the month. BusinessMonthEnd goes to the next date which is the last business day of the month. Examples If you want to get the end of the current business month: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of months represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range.","["">>> ts = pd.Timestamp(2022, 11, 29)\n>>> ts + pd.offsets.BMonthEnd()\nTimestamp('2022-11-30 00:00:00')"", "">>> ts = pd.Timestamp(2022, 11, 30)\n>>> ts + pd.offsets.BMonthEnd()\nTimestamp('2022-12-30 00:00:00')"", "">>> ts = pd.Timestamp(2022, 11, 30)\n>>> pd.offsets.BMonthEnd().rollforward(ts)\nTimestamp('2022-11-30 00:00:00')""]"
2030,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.html,pandas.tseries.offsets.YearBegin,class pandas.tseries.offsets.YearBegin# DateOffset increments between calendar year begin dates. YearBegin goes to the next date which is the start of the year. Examples If you want to get the start of the current year: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. month n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of years represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. monthint, default 1A specific integer for the month of the year.","["">>> ts = pd.Timestamp(2022, 12, 1)\n>>> ts + pd.offsets.YearBegin()\nTimestamp('2023-01-01 00:00:00')"", "">>> ts = pd.Timestamp(2023, 1, 1)\n>>> ts + pd.offsets.YearBegin()\nTimestamp('2024-01-01 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.YearBegin(month=2)\nTimestamp('2022-02-01 00:00:00')"", "">>> ts = pd.Timestamp(2023, 1, 1)\n>>> pd.offsets.YearBegin().rollback(ts)\nTimestamp('2023-01-01 00:00:00')""]"
2031,..\pandas\reference\api\pandas.Series.struct.field.html,pandas.Series.struct.field,"Series.struct.field(name_or_index)[source]# Extract a child field of a struct as a Series. Notes The name of the resulting Series will be set using the following rules: For string, bytes, or integer name_or_index (or a list of these, for a nested selection), the Series name is set to the selected field’s name. For a pyarrow.compute.Expression, this is set to the string form of the expression. For list-like name_or_index, the name will be set to the name of the final field selected.","Parameters: name_or_indexstr | bytes | int | expression | listName or index of the child field to extract. For list-like inputs, this will index into a nested struct. Returns: pandas.SeriesThe data corresponding to the selected child field.","['>>> import pyarrow as pa\n>>> s = pd.Series(\n...     [\n...         {""version"": 1, ""project"": ""pandas""},\n...         {""version"": 2, ""project"": ""pandas""},\n...         {""version"": 1, ""project"": ""numpy""},\n...     ],\n...     dtype=pd.ArrowDtype(pa.struct(\n...         [(""version"", pa.int64()), (""project"", pa.string())]\n...     ))\n... )', '>>> s.struct.field(""project"")\n0    pandas\n1    pandas\n2     numpy\nName: project, dtype: string[pyarrow]', '>>> s.struct.field(0)\n0    1\n1    2\n2    1\nName: version, dtype: int64[pyarrow]', '>>> import pyarrow.compute as pc\n>>> s.struct.field(pc.field(""project""))\n0    pandas\n1    pandas\n2     numpy\nName: project, dtype: string[pyarrow]', '>>> version_type = pa.struct([\n...     (""major"", pa.int64()),\n...     (""minor"", pa.int64()),\n... ])\n>>> s = pd.Series(\n...     [\n...         {""version"": {""major"": 1, ""minor"": 5}, ""project"": ""pandas""},\n...         {""version"": {""major"": 2, ""minor"": 1}, ""project"": ""pandas""},\n...         {""version"": {""major"": 1, ""minor"": 26}, ""project"": ""numpy""},\n...     ],\n...     dtype=pd.ArrowDtype(pa.struct(\n...         [(""version"", version_type), (""project"", pa.string())]\n...     ))\n... )\n>>> s.struct.field([""version"", ""minor""])\n0     5\n1     1\n2    26\nName: minor, dtype: int64[pyarrow]\n>>> s.struct.field([0, 0])\n0    1\n1    2\n2    1\nName: major, dtype: int64[pyarrow]']"
2032,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_anchored.html,pandas.tseries.offsets.LastWeekOfMonth.is_anchored,LastWeekOfMonth.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
2033,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_anchored.html,pandas.tseries.offsets.BusinessMonthEnd.is_anchored,BusinessMonthEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
2034,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_month_end.html,pandas.tseries.offsets.LastWeekOfMonth.is_month_end,LastWeekOfMonth.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
2035,..\pandas\reference\api\pandas.Series.sub.html,pandas.Series.sub,"Series.sub(other, level=None, fill_value=None, axis=0)[source]# Return Subtraction of series and other, element-wise (binary operator sub). Equivalent to series - other, but with support to substitute a fill_value for missing data in either one of the inputs.","Parameters: otherSeries or scalar value levelint or nameBroadcast across a level, matching Index values on the passed MultiIndex level. fill_valueNone or float value, default None (NaN)Fill existing missing (NaN) values, and any new element needed for successful Series alignment, with this value before computation. If data in both corresponding Series locations is missing the result of filling (at that location) will be missing. axis{0 or ‘index’}Unused. Parameter needed for compatibility with DataFrame. Returns: SeriesThe result of the operation.","["">>> a = pd.Series([1, 1, 1, np.nan], index=['a', 'b', 'c', 'd'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ndtype: float64\n>>> b = pd.Series([1, np.nan, 1, np.nan], index=['a', 'b', 'd', 'e'])\n>>> b\na    1.0\nb    NaN\nd    1.0\ne    NaN\ndtype: float64\n>>> a.subtract(b, fill_value=0)\na    0.0\nb    1.0\nc    1.0\nd   -1.0\ne    NaN\ndtype: float64""]"
2036,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_anchored.html,pandas.tseries.offsets.YearBegin.is_anchored,YearBegin.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
2037,..\pandas\reference\api\pandas.Series.sum.html,pandas.Series.sum,"Series.sum(axis=None, skipna=True, numeric_only=False, min_count=0, **kwargs)[source]# Return the sum of the values over the requested axis. This is equivalent to the method numpy.sum.","Parameters: axis{index (0)}Axis for the function to be applied on. For Series this parameter is unused and defaults to 0. Warning The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar To retain the old behavior, pass axis=0 (or do not pass axis). Added in version 2.0.0. skipnabool, default TrueExclude NA/null values when computing the result. numeric_onlybool, default FalseInclude only float, int, boolean columns. Not implemented for Series. min_countint, default 0The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. **kwargsAdditional keyword arguments to be passed to the function. Returns: scalar or scalar","["">>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64"", '>>> s.sum()\n14', '>>> pd.Series([], dtype=""float64"").sum()  # min_count=0 is the default\n0.0', '>>> pd.Series([], dtype=""float64"").sum(min_count=1)\nnan', '>>> pd.Series([np.nan]).sum()\n0.0', '>>> pd.Series([np.nan]).sum(min_count=1)\nnan']"
2038,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_month_end.html,pandas.tseries.offsets.YearBegin.is_month_end,YearBegin.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
2039,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_month_start.html,pandas.tseries.offsets.LastWeekOfMonth.is_month_start,LastWeekOfMonth.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
2040,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_month_end.html,pandas.tseries.offsets.BusinessMonthEnd.is_month_end,BusinessMonthEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
2041,..\pandas\reference\api\pandas.Series.swaplevel.html,pandas.Series.swaplevel,"Series.swaplevel(i=-2, j=-1, copy=None)[source]# Swap levels i and j in a MultiIndex. Default is to swap the two innermost levels of the index.","Parameters: i, jint or strLevels of the indices to be swapped. Can pass level name as string. copybool, default TrueWhether to copy underlying data. Note The copy keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a copy keyword will use a lazy copy mechanism to defer the copy and ignore the copy keyword. The copy keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write pd.options.mode.copy_on_write = True Returns: SeriesSeries with levels swapped in MultiIndex.","['>>> s = pd.Series(\n...     [""A"", ""B"", ""A"", ""C""],\n...     index=[\n...         [""Final exam"", ""Final exam"", ""Coursework"", ""Coursework""],\n...         [""History"", ""Geography"", ""History"", ""Geography""],\n...         [""January"", ""February"", ""March"", ""April""],\n...     ],\n... )\n>>> s\nFinal exam  History     January      A\n            Geography   February     B\nCoursework  History     March        A\n            Geography   April        C\ndtype: object', '>>> s.swaplevel()\nFinal exam  January     History         A\n            February    Geography       B\nCoursework  March       History         A\n            April       Geography       C\ndtype: object', '>>> s.swaplevel(0)\nJanuary     History     Final exam      A\nFebruary    Geography   Final exam      B\nMarch       History     Coursework      A\nApril       Geography   Coursework      C\ndtype: object', '>>> s.swaplevel(0, 1)\nHistory     Final exam  January         A\nGeography   Final exam  February        B\nHistory     Coursework  March           A\nGeography   Coursework  April           C\ndtype: object']"
2042,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_month_start.html,pandas.tseries.offsets.YearBegin.is_month_start,YearBegin.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
2043,..\pandas\reference\api\pandas.Series.T.html,pandas.Series.T,"property Series.T[source]# Return the transpose, which is by definition self.",No parameters found,"["">>> s = pd.Series(['Ant', 'Bear', 'Cow'])\n>>> s\n0     Ant\n1    Bear\n2     Cow\ndtype: object\n>>> s.T\n0     Ant\n1    Bear\n2     Cow\ndtype: object"", "">>> idx = pd.Index([1, 2, 3])\n>>> idx.T\nIndex([1, 2, 3], dtype='int64')""]"
2044,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_on_offset.html,pandas.tseries.offsets.LastWeekOfMonth.is_on_offset,LastWeekOfMonth.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
2045,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_month_start.html,pandas.tseries.offsets.BusinessMonthEnd.is_month_start,BusinessMonthEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
2046,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_on_offset.html,pandas.tseries.offsets.YearBegin.is_on_offset,YearBegin.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
2047,..\pandas\reference\api\pandas.Series.tail.html,pandas.Series.tail,"Series.tail(n=5)[source]# Return the last n rows. This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of n, this function returns all rows except the first |n| rows, equivalent to df[|n|:]. If n is larger than the number of rows, this function returns all rows.","Parameters: nint, default 5Number of rows to select. Returns: type of callerThe last n rows of the caller object.","["">>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n>>> df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra"", '>>> df.tail()\n   animal\n4  monkey\n5  parrot\n6   shark\n7   whale\n8   zebra', '>>> df.tail(3)\n  animal\n6  shark\n7  whale\n8  zebra', '>>> df.tail(-3)\n   animal\n3    lion\n4  monkey\n5  parrot\n6   shark\n7   whale\n8   zebra']"
2048,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_quarter_end.html,pandas.tseries.offsets.LastWeekOfMonth.is_quarter_end,LastWeekOfMonth.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
2049,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_quarter_end.html,pandas.tseries.offsets.YearBegin.is_quarter_end,YearBegin.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
2050,..\pandas\reference\api\pandas.Series.take.html,pandas.Series.take,"Series.take(indices, axis=0, **kwargs)[source]# Return the elements in the given positional indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object.","Parameters: indicesarray-likeAn array of ints indicating which positions to take. axis{0 or ‘index’, 1 or ‘columns’, None}, default 0The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns. For Series this parameter is unused and defaults to 0. **kwargsFor compatibility with numpy.take(). Has no effect on the output. Returns: same type as callerAn array-like containing the elements taken from the object.","["">>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n...                    ('parrot', 'bird', 24.0),\n...                    ('lion', 'mammal', 80.5),\n...                    ('monkey', 'mammal', np.nan)],\n...                   columns=['name', 'class', 'max_speed'],\n...                   index=[0, 2, 3, 1])\n>>> df\n     name   class  max_speed\n0  falcon    bird      389.0\n2  parrot    bird       24.0\n3    lion  mammal       80.5\n1  monkey  mammal        NaN"", '>>> df.take([0, 3])\n     name   class  max_speed\n0  falcon    bird      389.0\n1  monkey  mammal        NaN', '>>> df.take([1, 2], axis=1)\n    class  max_speed\n0    bird      389.0\n2    bird       24.0\n3  mammal       80.5\n1  mammal        NaN', '>>> df.take([-1, -2])\n     name   class  max_speed\n1  monkey  mammal        NaN\n3    lion  mammal       80.5']"
2051,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_on_offset.html,pandas.tseries.offsets.BusinessMonthEnd.is_on_offset,BusinessMonthEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
2052,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_quarter_start.html,pandas.tseries.offsets.LastWeekOfMonth.is_quarter_start,LastWeekOfMonth.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
2053,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_quarter_start.html,pandas.tseries.offsets.YearBegin.is_quarter_start,YearBegin.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
2054,..\pandas\reference\api\pandas.Series.to_clipboard.html,pandas.Series.to_clipboard,"Series.to_clipboard(*, excel=True, sep=None, **kwargs)[source]# Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Notes Requirements for your platform. Linux : xclip, or xsel (with PyQt4 modules) Windows : none macOS : none This method uses the processes developed for the package pyperclip. A solution to render any output string format is given in the examples.","Parameters: excelbool, default TrueProduce output in a csv format for easy pasting into excel. True, use the provided separator for csv pasting. False, write a string representation of the object to the clipboard. sepstr, default '\t'Field delimiter. **kwargsThese parameters will be passed to DataFrame.to_csv.","["">>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])"", "">>> df.to_clipboard(sep=',')  \n... # Wrote the following to the system clipboard:\n... # ,A,B,C\n... # 0,1,2,3\n... # 1,4,5,6"", "">>> df.to_clipboard(sep=',', index=False)  \n... # Wrote the following to the system clipboard:\n... # A,B,C\n... # 1,2,3\n... # 4,5,6""]"
2055,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_quarter_end.html,pandas.tseries.offsets.BusinessMonthEnd.is_quarter_end,BusinessMonthEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
2056,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_year_end.html,pandas.tseries.offsets.LastWeekOfMonth.is_year_end,LastWeekOfMonth.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
2057,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_year_end.html,pandas.tseries.offsets.YearBegin.is_year_end,YearBegin.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
2058,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_quarter_start.html,pandas.tseries.offsets.BusinessMonthEnd.is_quarter_start,BusinessMonthEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
2059,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.is_year_start.html,pandas.tseries.offsets.LastWeekOfMonth.is_year_start,LastWeekOfMonth.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
2060,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.is_year_start.html,pandas.tseries.offsets.YearBegin.is_year_start,YearBegin.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
2061,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_year_end.html,pandas.tseries.offsets.BusinessMonthEnd.is_year_end,BusinessMonthEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
2062,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.kwds.html,pandas.tseries.offsets.LastWeekOfMonth.kwds,LastWeekOfMonth.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
2063,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.kwds.html,pandas.tseries.offsets.YearBegin.kwds,YearBegin.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
2064,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.is_year_start.html,pandas.tseries.offsets.BusinessMonthEnd.is_year_start,BusinessMonthEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
2065,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.month.html,pandas.tseries.offsets.YearBegin.month,YearBegin.month#,No parameters found,[]
2066,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.kwds.html,pandas.tseries.offsets.BusinessMonthEnd.kwds,BusinessMonthEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
2067,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.n.html,pandas.tseries.offsets.LastWeekOfMonth.n,LastWeekOfMonth.n#,No parameters found,[]
2068,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.n.html,pandas.tseries.offsets.YearBegin.n,YearBegin.n#,No parameters found,[]
2069,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.n.html,pandas.tseries.offsets.BusinessMonthEnd.n,BusinessMonthEnd.n#,No parameters found,[]
2070,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.name.html,pandas.tseries.offsets.LastWeekOfMonth.name,LastWeekOfMonth.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
2071,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.nanos.html,pandas.tseries.offsets.LastWeekOfMonth.nanos,LastWeekOfMonth.nanos#,No parameters found,[]
2072,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.name.html,pandas.tseries.offsets.BusinessMonthEnd.name,BusinessMonthEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
2073,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.name.html,pandas.tseries.offsets.YearBegin.name,YearBegin.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
2074,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.normalize.html,pandas.tseries.offsets.LastWeekOfMonth.normalize,LastWeekOfMonth.normalize#,No parameters found,[]
2075,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.nanos.html,pandas.tseries.offsets.YearBegin.nanos,YearBegin.nanos#,No parameters found,[]
2076,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.nanos.html,pandas.tseries.offsets.BusinessMonthEnd.nanos,BusinessMonthEnd.nanos#,No parameters found,[]
2077,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.rule_code.html,pandas.tseries.offsets.LastWeekOfMonth.rule_code,LastWeekOfMonth.rule_code#,No parameters found,[]
2078,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.normalize.html,pandas.tseries.offsets.BusinessMonthEnd.normalize,BusinessMonthEnd.normalize#,No parameters found,[]
2079,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.normalize.html,pandas.tseries.offsets.YearBegin.normalize,YearBegin.normalize#,No parameters found,[]
2080,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.week.html,pandas.tseries.offsets.LastWeekOfMonth.week,LastWeekOfMonth.week#,No parameters found,[]
2081,..\pandas\reference\api\pandas.tseries.offsets.BusinessMonthEnd.rule_code.html,pandas.tseries.offsets.BusinessMonthEnd.rule_code,BusinessMonthEnd.rule_code#,No parameters found,[]
2082,..\pandas\reference\api\pandas.tseries.offsets.LastWeekOfMonth.weekday.html,pandas.tseries.offsets.LastWeekOfMonth.weekday,LastWeekOfMonth.weekday#,No parameters found,[]
2083,..\pandas\reference\api\pandas.tseries.offsets.YearBegin.rule_code.html,pandas.tseries.offsets.YearBegin.rule_code,YearBegin.rule_code#,No parameters found,[]
2084,..\pandas\reference\api\pandas.tseries.offsets.BYearBegin.copy.html,pandas.tseries.offsets.BYearBegin.copy,BYearBegin.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2085,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.copy.html,pandas.tseries.offsets.YearEnd.copy,YearEnd.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2086,..\pandas\reference\api\pandas.tseries.offsets.Micro.copy.html,pandas.tseries.offsets.Micro.copy,Micro.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2087,..\pandas\reference\api\pandas.tseries.offsets.Micro.delta.html,pandas.tseries.offsets.Micro.delta,Micro.delta#,No parameters found,[]
2088,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.freqstr.html,pandas.tseries.offsets.YearEnd.freqstr,YearEnd.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
2089,..\pandas\reference\api\pandas.tseries.offsets.Micro.freqstr.html,pandas.tseries.offsets.Micro.freqstr,Micro.freqstr# Return a string representing the frequency.,No parameters found,"["">>> pd.DateOffset(5).freqstr\n'<5 * DateOffsets>'"", "">>> pd.offsets.BusinessHour(2).freqstr\n'2bh'"", "">>> pd.offsets.Nano().freqstr\n'ns'"", "">>> pd.offsets.Nano(-3).freqstr\n'-3ns'""]"
2090,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.html,pandas.tseries.offsets.YearEnd,class pandas.tseries.offsets.YearEnd# DateOffset increments between calendar year end dates. YearEnd goes to the next date which is the end of the year. Examples If you want to get the end of the current year: Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. month n name Return a string representing the base frequency. nanos normalize rule_code,"Parameters: nint, default 1The number of years represented. normalizebool, default FalseNormalize start/end dates to midnight before generating date range. monthint, default 12A specific integer for the month of the year.","["">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.YearEnd()\nTimestamp('2022-12-31 00:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 31)\n>>> ts + pd.offsets.YearEnd()\nTimestamp('2023-12-31 00:00:00')"", "">>> ts = pd.Timestamp(2022, 1, 1)\n>>> ts + pd.offsets.YearEnd(month=2)\nTimestamp('2022-02-28 00:00:00')"", "">>> ts = pd.Timestamp(2022, 12, 31)\n>>> pd.offsets.YearEnd().rollforward(ts)\nTimestamp('2022-12-31 00:00:00')""]"
2091,..\pandas\reference\api\pandas.tseries.offsets.Micro.html,pandas.tseries.offsets.Micro,class pandas.tseries.offsets.Micro# Offset n microseconds. Examples You can use the parameter n to represent a shift of n microseconds. Attributes base Returns a copy of the calling offset object with n=1 and all other attributes equal. delta freqstr Return a string representing the frequency. kwds Return a dict of extra parameters for the offset. n name Return a string representing the base frequency. nanos Return an integer of the total number of nanoseconds. normalize rule_code,"Parameters: nint, default 1The number of microseconds represented.","["">>> from pandas.tseries.offsets import Micro\n>>> ts = pd.Timestamp(2022, 12, 9, 15)\n>>> ts\nTimestamp('2022-12-09 15:00:00')"", "">>> ts + Micro(n=1000)\nTimestamp('2022-12-09 15:00:00.001000')"", "">>> ts - Micro(n=1000)\nTimestamp('2022-12-09 14:59:59.999000')"", "">>> ts + Micro(n=-1000)\nTimestamp('2022-12-09 14:59:59.999000')""]"
2092,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_anchored.html,pandas.tseries.offsets.YearEnd.is_anchored,YearEnd.is_anchored()# Return boolean whether the frequency is a unit frequency (n=1). Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use obj.n == 1 instead.,No parameters found,['>>> pd.DateOffset().is_anchored()\nTrue\n>>> pd.DateOffset(2).is_anchored()\nFalse']
2093,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_anchored.html,pandas.tseries.offsets.Micro.is_anchored,Micro.is_anchored()# Return False. Deprecated since version 2.2.0: is_anchored is deprecated and will be removed in a future version. Use False instead.,No parameters found,['>>> pd.offsets.Hour().is_anchored()\nFalse\n>>> pd.offsets.Hour(2).is_anchored()\nFalse']
2094,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_month_end.html,pandas.tseries.offsets.YearEnd.is_month_end,YearEnd.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
2095,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_month_end.html,pandas.tseries.offsets.Micro.is_month_end,Micro.is_month_end(ts)# Return boolean whether a timestamp occurs on the month end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_end(ts)\nFalse']"
2096,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_month_start.html,pandas.tseries.offsets.YearEnd.is_month_start,YearEnd.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
2097,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_month_start.html,pandas.tseries.offsets.Micro.is_month_start,Micro.is_month_start(ts)# Return boolean whether a timestamp occurs on the month start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_month_start(ts)\nTrue']"
2098,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_on_offset.html,pandas.tseries.offsets.YearEnd.is_on_offset,YearEnd.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
2099,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_on_offset.html,pandas.tseries.offsets.Micro.is_on_offset,Micro.is_on_offset(dt)# Return boolean whether a timestamp intersects with this frequency.,Parameters: dtdatetime.datetimeTimestamp to check intersections with frequency.,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Day(1)\n>>> freq.is_on_offset(ts)\nTrue', "">>> ts = pd.Timestamp(2022, 8, 6)\n>>> ts.day_name()\n'Saturday'\n>>> freq = pd.offsets.BusinessDay(1)\n>>> freq.is_on_offset(ts)\nFalse""]"
2100,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_quarter_end.html,pandas.tseries.offsets.YearEnd.is_quarter_end,YearEnd.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
2101,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_quarter_end.html,pandas.tseries.offsets.Micro.is_quarter_end,Micro.is_quarter_end(ts)# Return boolean whether a timestamp occurs on the quarter end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_end(ts)\nFalse']"
2102,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_quarter_start.html,pandas.tseries.offsets.YearEnd.is_quarter_start,YearEnd.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
2103,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_quarter_start.html,pandas.tseries.offsets.Micro.is_quarter_start,Micro.is_quarter_start(ts)# Return boolean whether a timestamp occurs on the quarter start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_quarter_start(ts)\nTrue']"
2104,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_year_end.html,pandas.tseries.offsets.YearEnd.is_year_end,YearEnd.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
2105,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.is_year_start.html,pandas.tseries.offsets.YearEnd.is_year_start,YearEnd.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
2106,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_year_end.html,pandas.tseries.offsets.Micro.is_year_end,Micro.is_year_end(ts)# Return boolean whether a timestamp occurs on the year end.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_end(ts)\nFalse']"
2107,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.kwds.html,pandas.tseries.offsets.YearEnd.kwds,YearEnd.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
2108,..\pandas\reference\api\pandas.tseries.offsets.Micro.is_year_start.html,pandas.tseries.offsets.Micro.is_year_start,Micro.is_year_start(ts)# Return boolean whether a timestamp occurs on the year start.,No parameters found,"['>>> ts = pd.Timestamp(2022, 1, 1)\n>>> freq = pd.offsets.Hour(5)\n>>> freq.is_year_start(ts)\nTrue']"
2109,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.month.html,pandas.tseries.offsets.YearEnd.month,YearEnd.month#,No parameters found,[]
2110,..\pandas\reference\api\pandas.tseries.offsets.Micro.kwds.html,pandas.tseries.offsets.Micro.kwds,Micro.kwds# Return a dict of extra parameters for the offset.,No parameters found,"['>>> pd.DateOffset(5).kwds\n{}', "">>> pd.offsets.FY5253Quarter().kwds\n{'weekday': 0,\n 'startingMonth': 1,\n 'qtr_with_extra_week': 1,\n 'variation': 'nearest'}""]"
2111,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.n.html,pandas.tseries.offsets.YearEnd.n,YearEnd.n#,No parameters found,[]
2112,..\pandas\reference\api\pandas.tseries.offsets.Micro.n.html,pandas.tseries.offsets.Micro.n,Micro.n#,No parameters found,[]
2113,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.name.html,pandas.tseries.offsets.YearEnd.name,YearEnd.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
2114,..\pandas\reference\api\pandas.tseries.offsets.Micro.name.html,pandas.tseries.offsets.Micro.name,Micro.name# Return a string representing the base frequency.,No parameters found,"["">>> pd.offsets.Hour().name\n'h'"", "">>> pd.offsets.Hour(5).name\n'h'""]"
2115,..\pandas\reference\api\pandas.tseries.offsets.Micro.nanos.html,pandas.tseries.offsets.Micro.nanos,Micro.nanos# Return an integer of the total number of nanoseconds.,Raises: ValueErrorIf the frequency is non-fixed.,['>>> pd.offsets.Hour(5).nanos\n18000000000000']
2116,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.nanos.html,pandas.tseries.offsets.YearEnd.nanos,YearEnd.nanos#,No parameters found,[]
2117,..\pandas\reference\api\pandas.tseries.offsets.Micro.normalize.html,pandas.tseries.offsets.Micro.normalize,Micro.normalize#,No parameters found,[]
2118,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.normalize.html,pandas.tseries.offsets.YearEnd.normalize,YearEnd.normalize#,No parameters found,[]
2119,..\pandas\reference\api\pandas.tseries.offsets.YearEnd.rule_code.html,pandas.tseries.offsets.YearEnd.rule_code,YearEnd.rule_code#,No parameters found,[]
2120,..\pandas\reference\api\pandas.tseries.offsets.Micro.rule_code.html,pandas.tseries.offsets.Micro.rule_code,Micro.rule_code#,No parameters found,[]
2121,..\pandas\reference\api\pandas.UInt16Dtype.html,pandas.UInt16Dtype,"class pandas.UInt16Dtype[source]# An ExtensionDtype for uint16 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
2122,..\pandas\reference\api\pandas.tseries.offsets.Milli.copy.html,pandas.tseries.offsets.Milli.copy,Milli.copy()# Return a copy of the frequency.,No parameters found,['>>> freq = pd.DateOffset(1)\n>>> freq_copy = freq.copy()\n>>> freq is freq_copy\nFalse']
2123,..\pandas\reference\api\pandas.UInt32Dtype.html,pandas.UInt32Dtype,"class pandas.UInt32Dtype[source]# An ExtensionDtype for uint32 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
2124,..\pandas\reference\api\pandas.UInt64Dtype.html,pandas.UInt64Dtype,"class pandas.UInt64Dtype[source]# An ExtensionDtype for uint64 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
2125,..\pandas\reference\api\pandas.UInt8Dtype.html,pandas.UInt8Dtype,"class pandas.UInt8Dtype[source]# An ExtensionDtype for uint8 integer data. Uses pandas.NA as its missing value, rather than numpy.nan. Attributes None Methods None",No parameters found,"['>>> ser = pd.Series([2, pd.NA], dtype=pd.Int8Dtype())\n>>> ser.dtype\nInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int16Dtype())\n>>> ser.dtype\nInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int32Dtype())\n>>> ser.dtype\nInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.Int64Dtype())\n>>> ser.dtype\nInt64Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt8Dtype())\n>>> ser.dtype\nUInt8Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt16Dtype())\n>>> ser.dtype\nUInt16Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt32Dtype())\n>>> ser.dtype\nUInt32Dtype()', '>>> ser = pd.Series([2, pd.NA], dtype=pd.UInt64Dtype())\n>>> ser.dtype\nUInt64Dtype()']"
2126,..\pandas\reference\api\pandas.unique.html,pandas.unique,pandas.unique(values)[source]# Return unique values based on a hash table. Uniques are returned in order of appearance. This does NOT sort. Significantly faster than numpy.unique for long enough sequences. Includes NA values.,Parameters: values1d array-like Returns: numpy.ndarray or ExtensionArrayThe return can be: Index : when the input is an Index Categorical : when the input is a Categorical dtype ndarray : when the input is a Series/ndarray Return numpy.ndarray or ExtensionArray.,"['>>> pd.unique(pd.Series([2, 1, 3, 3]))\narray([2, 1, 3])', '>>> pd.unique(pd.Series([2] + [1] * 5))\narray([2, 1])', '>>> pd.unique(pd.Series([pd.Timestamp(""20160101""), pd.Timestamp(""20160101"")]))\narray([\'2016-01-01T00:00:00.000000000\'], dtype=\'datetime64[ns]\')', '>>> pd.unique(\n...     pd.Series(\n...         [\n...             pd.Timestamp(""20160101"", tz=""US/Eastern""),\n...             pd.Timestamp(""20160101"", tz=""US/Eastern""),\n...         ]\n...     )\n... )\n<DatetimeArray>\n[\'2016-01-01 00:00:00-05:00\']\nLength: 1, dtype: datetime64[ns, US/Eastern]', '>>> pd.unique(\n...     pd.Index(\n...         [\n...             pd.Timestamp(""20160101"", tz=""US/Eastern""),\n...             pd.Timestamp(""20160101"", tz=""US/Eastern""),\n...         ]\n...     )\n... )\nDatetimeIndex([\'2016-01-01 00:00:00-05:00\'],\n        dtype=\'datetime64[ns, US/Eastern]\',\n        freq=None)', '>>> pd.unique(np.array(list(""baabc""), dtype=""O""))\narray([\'b\', \'a\', \'c\'], dtype=object)', '>>> pd.unique(pd.Series(pd.Categorical(list(""baabc""))))\n[\'b\', \'a\', \'c\']\nCategories (3, object): [\'a\', \'b\', \'c\']', '>>> pd.unique(pd.Series(pd.Categorical(list(""baabc""), categories=list(""abc""))))\n[\'b\', \'a\', \'c\']\nCategories (3, object): [\'a\', \'b\', \'c\']', '>>> pd.unique(\n...     pd.Series(\n...         pd.Categorical(list(""baabc""), categories=list(""abc""), ordered=True)\n...     )\n... )\n[\'b\', \'a\', \'c\']\nCategories (3, object): [\'a\' < \'b\' < \'c\']', '>>> pd.unique(pd.Series([(""a"", ""b""), (""b"", ""a""), (""a"", ""c""), (""b"", ""a"")]).values)\narray([(\'a\', \'b\'), (\'b\', \'a\'), (\'a\', \'c\')], dtype=object)']"
2127,..\pandas\reference\api\pandas.util.hash_array.html,pandas.util.hash_array,"pandas.util.hash_array(vals, encoding='utf8', hash_key='0123456789123456', categorize=True)[source]# Given a 1d array, return an array of deterministic integers.","Parameters: valsndarray or ExtensionArray encodingstr, default ‘utf8’Encoding for data & key when strings. hash_keystr, default _default_hash_keyHash_key for string key to encode. categorizebool, default TrueWhether to first categorize object arrays before hashing. This is more efficient when the array contains duplicate values. Returns: ndarray[np.uint64, ndim=1]Hashed values, same length as the vals.","['>>> pd.util.hash_array(np.array([1, 2, 3]))\narray([ 6238072747940578789, 15839785061582574730,  2185194620014831856],\n  dtype=uint64)']"
2128,..\pandas\reference\api\pandas.util.hash_pandas_object.html,pandas.util.hash_pandas_object,"pandas.util.hash_pandas_object(obj, index=True, encoding='utf8', hash_key='0123456789123456', categorize=True)[source]# Return a data hash of the Index/Series/DataFrame.","Parameters: objIndex, Series, or DataFrame indexbool, default TrueInclude the index in the hash (if Series/DataFrame). encodingstr, default ‘utf8’Encoding for data & key when strings. hash_keystr, default _default_hash_keyHash_key for string key to encode. categorizebool, default TrueWhether to first categorize object arrays before hashing. This is more efficient when the array contains duplicate values. Returns: Series of uint64, same length as the object","['>>> pd.util.hash_pandas_object(pd.Series([1, 2, 3]))\n0    14639053686158035780\n1     3869563279212530728\n2      393322362522515241\ndtype: uint64']"
2129,..\pandas\reference\api\pandas.wide_to_long.html,pandas.wide_to_long,"pandas.wide_to_long(df, stubnames, i, j, sep='', suffix='\\d+')[source]# Unpivot a DataFrame from wide to long format. Less flexible but more user-friendly than melt. With stubnames [‘A’, ‘B’], this function expects to find one or more group of columns with format A-suffix1, A-suffix2,…, B-suffix1, B-suffix2,… You specify what you want to call this suffix in the resulting long format with j (for example j=’year’) Each row of these wide variables are assumed to be uniquely identified by i (can be a single column name or a list of column names) All remaining variables in the data frame are left intact. Notes All extra variables are left untouched. This simply uses pandas.melt under the hood, but is hard-coded to “do the right thing” in a typical case.","Parameters: dfDataFrameThe wide-format DataFrame. stubnamesstr or list-likeThe stub name(s). The wide format variables are assumed to start with the stub names. istr or list-likeColumn(s) to use as id variable(s). jstrThe name of the sub-observation variable. What you wish to name your suffix in the long format. sepstr, default “”A character indicating the separation of the variable names in the wide format, to be stripped from the names in the long format. For example, if your column names are A-suffix1, A-suffix2, you can strip the hyphen by specifying sep=’-’. suffixstr, default ‘\d+’A regular expression capturing the wanted suffixes. ‘\d+’ captures numeric suffixes. Suffixes with no numbers could be specified with the negated character class ‘\D+’. You can also further disambiguate suffixes, for example, if your wide variables are of the form A-one, B-two,.., and you have an unrelated column A-rating, you can ignore the last one by specifying suffix=’(!?one|two)’. When all suffixes are numeric, they are cast to int64/float64. Returns: DataFrameA DataFrame that contains each stub name as a variable, with new index (i, j).","['>>> np.random.seed(123)\n>>> df = pd.DataFrame({""A1970"" : {0 : ""a"", 1 : ""b"", 2 : ""c""},\n...                    ""A1980"" : {0 : ""d"", 1 : ""e"", 2 : ""f""},\n...                    ""B1970"" : {0 : 2.5, 1 : 1.2, 2 : .7},\n...                    ""B1980"" : {0 : 3.2, 1 : 1.3, 2 : .1},\n...                    ""X""     : dict(zip(range(3), np.random.randn(3)))\n...                   })\n>>> df[""id""] = df.index\n>>> df\n  A1970 A1980  B1970  B1980         X  id\n0     a     d    2.5    3.2 -1.085631   0\n1     b     e    1.2    1.3  0.997345   1\n2     c     f    0.7    0.1  0.282978   2\n>>> pd.wide_to_long(df, [""A"", ""B""], i=""id"", j=""year"")\n... \n                X  A    B\nid year\n0  1970 -1.085631  a  2.5\n1  1970  0.997345  b  1.2\n2  1970  0.282978  c  0.7\n0  1980 -1.085631  d  3.2\n1  1980  0.997345  e  1.3\n2  1980  0.282978  f  0.1', "">>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9\n>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     1    2.8\n            2    3.4\n      2     1    2.9\n            2    3.8\n      3     1    2.2\n            2    2.9\n2     1     1    2.0\n            2    3.2\n      2     1    1.8\n            2    2.8\n      3     1    1.9\n            2    2.4\n3     1     1    2.2\n            2    3.3\n      2     1    2.3\n            2    3.4\n      3     1    2.1\n            2    2.9"", "">>> w = l.unstack()\n>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)\n>>> w.reset_index()\n   famid  birth  ht1  ht2\n0      1      1  2.8  3.4\n1      1      2  2.9  3.8\n2      1      3  2.2  2.9\n3      2      1  2.0  3.2\n4      2      2  1.8  2.8\n5      2      3  1.9  2.4\n6      3      1  2.2  3.3\n7      3      2  2.3  3.4\n8      3      3  2.1  2.9"", "">>> np.random.seed(0)\n>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),\n...                    'A(weekly)-2011': np.random.rand(3),\n...                    'B(weekly)-2010': np.random.rand(3),\n...                    'B(weekly)-2011': np.random.rand(3),\n...                    'X' : np.random.randint(3, size=3)})\n>>> df['id'] = df.index\n>>> df \n   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id\n0        0.548814        0.544883        0.437587        0.383442  0   0\n1        0.715189        0.423655        0.891773        0.791725  1   1\n2        0.602763        0.645894        0.963663        0.528895  1   2"", "">>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',\n...                 j='year', sep='-')\n... \n         X  A(weekly)  B(weekly)\nid year\n0  2010  0   0.548814   0.437587\n1  2010  1   0.715189   0.891773\n2  2010  1   0.602763   0.963663\n0  2011  0   0.544883   0.383442\n1  2011  1   0.423655   0.791725\n2  2011  1   0.645894   0.528895"", "">>> stubnames = sorted(\n...     set([match[0] for match in df.columns.str.findall(\n...         r'[A-B]\\(.*\\)').values if match != []])\n... )\n>>> list(stubnames)\n['A(weekly)', 'B(weekly)']"", "">>> df = pd.DataFrame({\n...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],\n...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]\n... })\n>>> df\n   famid  birth  ht_one  ht_two\n0      1      1     2.8     3.4\n1      1      2     2.9     3.8\n2      1      3     2.2     2.9\n3      2      1     2.0     3.2\n4      2      2     1.8     2.8\n5      2      3     1.9     2.4\n6      3      1     2.2     3.3\n7      3      2     2.3     3.4\n8      3      3     2.1     2.9"", "">>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',\n...                     sep='_', suffix=r'\\w+')\n>>> l\n... \n                  ht\nfamid birth age\n1     1     one  2.8\n            two  3.4\n      2     one  2.9\n            two  3.8\n      3     one  2.2\n            two  2.9\n2     1     one  2.0\n            two  3.2\n      2     one  1.8\n            two  2.8\n      3     one  1.9\n            two  2.4\n3     1     one  2.2\n            two  3.3\n      2     one  2.3\n            two  3.4\n      3     one  2.1\n            two  2.9""]"
